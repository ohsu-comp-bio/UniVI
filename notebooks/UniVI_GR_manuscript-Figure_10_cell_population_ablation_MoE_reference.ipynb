{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b891ca6d-e3e2-4460-863d-b0b4e1a9374b",
   "metadata": {},
   "source": [
    "## UniVI â€“ Figure 10 analysis (robust v1, no leakage)\n",
    "\n",
    "This notebook performs the Figure 10 coarse + fine celltype ablation analysis using PBMC Multiome data.\n",
    "\n",
    " - organized, single source of truth (no duplicates)\n",
    " - robust latent extraction\n",
    " - MoE fused latent computed ONCE in encoder (MoE-first)\n",
    " - STRICT: if MoE latent not available, FAIL (no avg/joint fallback)\n",
    " - grouped batching for ablated TRAIN to avoid mixed missingness in a batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb82b609-e813-4967-a241-d7e836ca4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 0) Imports / versions ----\n",
    "# =============================================================================\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, Sampler\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    silhouette_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# UniVI\n",
    "from univi.data import MultiModalDataset, align_paired_obs_names\n",
    "from univi import UniVIMultiModalVAE, ModalityConfig, UniVIConfig, TrainingConfig\n",
    "from univi.trainer import UniVITrainer\n",
    "\n",
    "print(\"scanpy:\", sc.__version__)\n",
    "print(\"torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee72139-871c-4433-a544-256dbaca976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 1) User config ----\n",
    "# =============================================================================\n",
    "RNA_PATH  = \"./data/10x_Genomics_Multiome_data/10x-Multiome-Pbmc10k-RNA.h5ad\"\n",
    "ATAC_PATH = \"./data/10x_Genomics_Multiome_data/10x-Multiome-Pbmc10k-ATAC.h5ad\"\n",
    "\n",
    "CELLTYPE_KEY = \"cell_type\"\n",
    "COUNTS_LAYER = \"counts\"\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"mps\"  # \"cuda\" / \"cpu\" also ok\n",
    "\n",
    "# preprocessing\n",
    "N_HVG = 2000\n",
    "RNA_TARGET_SUM = 1e4\n",
    "N_LSI = 101\n",
    "\n",
    "# training\n",
    "BATCH_SIZE = 128\n",
    "RECALL_KS = (1, 5, 10, 25, 50, 100)\n",
    "\n",
    "# STRICT behavior\n",
    "STRICT_MOE = True  # if True: require mu_moe, else crash\n",
    "# If you want to allow model-fused fallback (NOT avg), set STRICT_MOE=False and use fuse_mode=\"fused\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885deb0-7742-4048-aec3-b43cfe453217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 2) Small utilities (single source of truth) ----\n",
    "# =============================================================================\n",
    "def pick_device():\n",
    "    return torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available()\n",
    "        else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    )\n",
    "\n",
    "def ensure_counts_layer(adata: ad.AnnData, layer: str = \"counts\") -> None:\n",
    "    if layer not in adata.layers:\n",
    "        adata.layers[layer] = adata.X.copy()\n",
    "\n",
    "def _to_csr(X):\n",
    "    return X if sp.issparse(X) else sp.csr_matrix(X)\n",
    "\n",
    "def _as_numpy_2d(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    return x.astype(np.float32, copy=False)\n",
    "\n",
    "def _as_numpy_1d(y):\n",
    "    if y is None:\n",
    "        return None\n",
    "    if torch.is_tensor(y):\n",
    "        y = y.detach().cpu().numpy()\n",
    "    y = np.asarray(y)\n",
    "    return y.reshape(-1)\n",
    "\n",
    "def _to_numpy_ids(gids):\n",
    "    if gids is None:\n",
    "        return None\n",
    "    if torch.is_tensor(gids):\n",
    "        return gids.detach().cpu().numpy().astype(np.int64, copy=False)\n",
    "    return np.asarray(gids, dtype=np.int64)\n",
    "\n",
    "def _unwrap_model_output(out):\n",
    "    if torch.is_tensor(out) or isinstance(out, dict):\n",
    "        return out\n",
    "    if isinstance(out, (tuple, list)) and len(out) > 0:\n",
    "        for item in out:\n",
    "            if isinstance(item, dict) or torch.is_tensor(item):\n",
    "                return item\n",
    "        return out[0]\n",
    "    return out\n",
    "\n",
    "def _smart_call(fn, x_dict):\n",
    "    try:\n",
    "        sig = inspect.signature(fn)\n",
    "        params = sig.parameters\n",
    "        for name in (\"x_dict\", \"batch\", \"x\", \"inputs\"):\n",
    "            if name in params:\n",
    "                return fn(**{name: x_dict})\n",
    "        return fn(x_dict)\n",
    "    except TypeError:\n",
    "        return fn(x_dict)\n",
    "\n",
    "def _call_univi_encoder(model, x_dict):\n",
    "    if hasattr(model, \"encode\") and callable(getattr(model, \"encode\")):\n",
    "        out = _smart_call(model.encode, x_dict)\n",
    "        return _unwrap_model_output(out)\n",
    "    out = _smart_call(model, x_dict)\n",
    "    return _unwrap_model_output(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1b3bb-469c-4abe-b863-9db1c84fc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 3) Robust latent extraction + MoE fusion (THE FIX) ----\n",
    "# =============================================================================\n",
    "def _first_tensor(*xs):\n",
    "    for x in xs:\n",
    "        if torch.is_tensor(x):\n",
    "            return x\n",
    "    return None\n",
    "\n",
    "def _find_tensors_anywhere(obj):\n",
    "    out = []\n",
    "    if torch.is_tensor(obj):\n",
    "        out.append(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            out.extend(_find_tensors_anywhere(v))\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        for v in obj:\n",
    "            out.extend(_find_tensors_anywhere(v))\n",
    "    return out\n",
    "\n",
    "def _extract_latent(enc, which: str):\n",
    "    \"\"\"\n",
    "    which in {\"rna\",\"atac\",\"fused\"}.\n",
    "    Tries common UniVI layouts/keys; last resort: any 2D tensor.\n",
    "    \"\"\"\n",
    "    if enc is None:\n",
    "        return None\n",
    "\n",
    "    if torch.is_tensor(enc):\n",
    "        # if model directly returns a tensor, treat it as \"fused\"\n",
    "        return enc if which == \"fused\" else None\n",
    "\n",
    "    if not isinstance(enc, dict):\n",
    "        return None\n",
    "\n",
    "    # ---- direct keys like mu_rna / z_atac etc ----\n",
    "    v = _first_tensor(\n",
    "        enc.get(f\"mu_{which}\", None),\n",
    "        enc.get(f\"z_{which}\", None),\n",
    "        enc.get(f\"latent_{which}\", None),\n",
    "        enc.get(f\"embedding_{which}\", None),\n",
    "        enc.get(f\"emb_{which}\", None),\n",
    "    )\n",
    "    if v is not None:\n",
    "        return v\n",
    "\n",
    "    # ---- dict-of-modality patterns (mu_dict / z_dict) ----\n",
    "    for dkey in (\"mu_dict\", \"z_dict\", \"latent_dict\", \"emb_dict\"):\n",
    "        sub = enc.get(dkey, None)\n",
    "        if isinstance(sub, dict) and which in sub and torch.is_tensor(sub[which]):\n",
    "            return sub[which]\n",
    "\n",
    "    # ---- fused special keys (UniVI commonly returns mu_z / z as fused posterior) ----\n",
    "    if which == \"fused\":\n",
    "        v = _first_tensor(\n",
    "            enc.get(\"mu_moe\", None),\n",
    "            enc.get(\"mu_joint\", None),\n",
    "            enc.get(\"mu_fused\", None),\n",
    "            enc.get(\"mu_shared\", None),\n",
    "\n",
    "            # common UniVI fused outputs\n",
    "            enc.get(\"mu_z\", None),   # fused mean\n",
    "            enc.get(\"z\", None),      # fused sample\n",
    "\n",
    "            enc.get(\"mu\", None),\n",
    "            enc.get(\"mean\", None),\n",
    "            enc.get(\"latent\", None),\n",
    "            enc.get(\"embedding\", None),\n",
    "        )\n",
    "        if v is not None:\n",
    "            return v\n",
    "\n",
    "    # ---- nested dict possibilities ----\n",
    "    for container in (\n",
    "        \"latents\", \"latent\", \"posterior\", \"post\", \"qz\", \"q\",\n",
    "        \"enc\", \"encode\", \"outputs\", \"moe\", \"mix\", \"router\", \"extras\"\n",
    "    ):\n",
    "        sub = enc.get(container, None)\n",
    "        if isinstance(sub, dict):\n",
    "            v = _first_tensor(\n",
    "                sub.get(f\"mu_{which}\", None),\n",
    "                sub.get(f\"z_{which}\", None),\n",
    "                sub.get(f\"latent_{which}\", None),\n",
    "                sub.get(f\"embedding_{which}\", None),\n",
    "            )\n",
    "            if v is not None:\n",
    "                return v\n",
    "\n",
    "            if which == \"fused\":\n",
    "                v = _first_tensor(\n",
    "                    sub.get(\"mu_moe\", None),\n",
    "                    sub.get(\"mu_joint\", None),\n",
    "                    sub.get(\"mu_fused\", None),\n",
    "                    sub.get(\"mu_z\", None),\n",
    "                    sub.get(\"z\", None),\n",
    "                    sub.get(\"mu\", None),\n",
    "                )\n",
    "                if v is not None:\n",
    "                    return v\n",
    "\n",
    "    # last resort: any 2D tensor\n",
    "    ts = _find_tensors_anywhere(enc)\n",
    "    for t in ts:\n",
    "        if torch.is_tensor(t) and t.ndim == 2:\n",
    "            return t\n",
    "    return ts[0] if ts else None\n",
    "\n",
    "def _extract_gating_weights_from_enc_full(enc_full):\n",
    "    \"\"\"\n",
    "    Precision-derived implicit MoE weights from per-modality log-variances.\n",
    "\n",
    "    Returns:\n",
    "      (W, mods)\n",
    "        W: torch.Tensor (B, M) with per-cell modality weights (mean over latent dims),\n",
    "           where M = number of modalities with a tensor logvar.\n",
    "        mods: list[str] modality names in the same column order as W\n",
    "\n",
    "    If not available, returns (None, None).\n",
    "    \"\"\"\n",
    "    if not isinstance(enc_full, dict):\n",
    "        return None, None\n",
    "\n",
    "    lv = enc_full.get(\"logvar_dict\", None)\n",
    "    if not isinstance(lv, dict) or len(lv) == 0:\n",
    "        return None, None\n",
    "\n",
    "    # keep only modalities with tensor logvars\n",
    "    mods = [k for k, v in lv.items() if (v is not None and torch.is_tensor(v))]\n",
    "    if len(mods) < 2:\n",
    "        return None, None\n",
    "\n",
    "    # stack precisions: (M, B, D)\n",
    "    prec = torch.stack([torch.exp(-lv[m]) for m in mods], dim=0)\n",
    "\n",
    "    # normalize across modalities: (M, B, D)\n",
    "    denom = prec.sum(dim=0).clamp_min(1e-8)\n",
    "    w = prec / denom\n",
    "\n",
    "    # reduce over latent dims -> (M, B)\n",
    "    w_mean = w.mean(dim=-1)\n",
    "\n",
    "    # transpose -> (B, M)\n",
    "    W = w_mean.transpose(0, 1).contiguous()\n",
    "    return W, mods\n",
    "\n",
    "def _pick_explicit_moe_tensor(enc_full):\n",
    "    \"\"\"\n",
    "    If UniVI already returns a fused latent explicitly, prefer it.\n",
    "\n",
    "    IMPORTANT:\n",
    "    - In many UniVI versions, the fused posterior mean is returned as mu_z (NOT mu_moe).\n",
    "    - If gating is not exposed, mu_z is still the correct fused latent (not an average).\n",
    "    \"\"\"\n",
    "    if enc_full is None:\n",
    "        return None\n",
    "    if torch.is_tensor(enc_full):\n",
    "        return None\n",
    "    if not isinstance(enc_full, dict):\n",
    "        return None\n",
    "\n",
    "    v = _first_tensor(\n",
    "        enc_full.get(\"mu_moe\", None),\n",
    "        enc_full.get(\"mu_joint\", None),\n",
    "        enc_full.get(\"mu_z\", None),\n",
    "    )\n",
    "    if v is not None:\n",
    "        return v\n",
    "\n",
    "    for container in (\"moe\", \"mix\", \"mixture\", \"router\", \"extras\", \"outputs\", \"enc\", \"latent\", \"latents\", \"posterior\", \"post\"):\n",
    "        sub = enc_full.get(container, None)\n",
    "        if isinstance(sub, dict):\n",
    "            v = _first_tensor(\n",
    "                sub.get(\"mu_moe\", None),\n",
    "                sub.get(\"mu_joint\", None),\n",
    "                sub.get(\"mu_z\", None),\n",
    "            )\n",
    "            if v is not None:\n",
    "                return v\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f68f68-2029-482f-84e3-0e6f8187ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 4) Label coding + paired alignment metrics ----\n",
    "# =============================================================================\n",
    "def _build_label_encoder(y_all):\n",
    "    y_raw = np.asarray(y_all)\n",
    "    kind = y_raw.dtype.kind\n",
    "    if kind in (\"i\", \"u\", \"f\", \"b\"):\n",
    "        return y_raw, None\n",
    "    y_raw = y_raw.astype(str)\n",
    "    uniq = np.unique(y_raw)\n",
    "    id_to_int = {lab: i for i, lab in enumerate(uniq.tolist())}\n",
    "    return y_raw, id_to_int\n",
    "\n",
    "def _encode_labels_for_gids(y_raw, gids_np, *, id_to_int=None):\n",
    "    y_batch = y_raw[gids_np]\n",
    "    y_batch_arr = np.asarray(y_batch)\n",
    "    kind = y_batch_arr.dtype.kind\n",
    "\n",
    "    if kind in (\"i\", \"u\", \"b\"):\n",
    "        return torch.as_tensor(y_batch_arr.astype(np.int64, copy=False), dtype=torch.long), id_to_int\n",
    "    if kind == \"f\":\n",
    "        return torch.as_tensor(y_batch_arr.astype(np.float32, copy=False)), id_to_int\n",
    "\n",
    "    y_batch_str = y_batch_arr.astype(str)\n",
    "    if id_to_int is None:\n",
    "        uniq = np.unique(y_batch_str)\n",
    "        id_to_int = {lab: i for i, lab in enumerate(uniq.tolist())}\n",
    "\n",
    "    codes = np.empty(y_batch_str.shape[0], dtype=np.int64)\n",
    "    next_code = (max(id_to_int.values()) + 1) if len(id_to_int) else 0\n",
    "    for i, lab in enumerate(y_batch_str):\n",
    "        if lab not in id_to_int:\n",
    "            id_to_int[lab] = next_code\n",
    "            next_code += 1\n",
    "        codes[i] = id_to_int[lab]\n",
    "\n",
    "    return torch.as_tensor(codes, dtype=torch.long), id_to_int\n",
    "\n",
    "def _pairwise_sq_dists(X, Y):\n",
    "    X = _as_numpy_2d(X)\n",
    "    Y = _as_numpy_2d(Y)\n",
    "    x2 = np.sum(X * X, axis=1, keepdims=True)\n",
    "    y2 = np.sum(Y * Y, axis=1, keepdims=True).T\n",
    "    D = x2 + y2 - 2.0 * (X @ Y.T)\n",
    "    np.maximum(D, 0.0, out=D)\n",
    "    return D\n",
    "\n",
    "def foscttm(X, Y, *, symmetric=True):\n",
    "    D = _pairwise_sq_dists(X, Y)\n",
    "    n = D.shape[0]\n",
    "    if n <= 1:\n",
    "        return float(np.nan)\n",
    "    diag = np.diag(D)\n",
    "    denom = (n - 1)\n",
    "    frac_xy = (D < diag[:, None]).sum(axis=1) / denom\n",
    "    if not symmetric:\n",
    "        return float(frac_xy.mean())\n",
    "    frac_yx = (D < diag[None, :]).sum(axis=0) / denom\n",
    "    return float(0.5 * (frac_xy.mean() + frac_yx.mean()))\n",
    "\n",
    "def recall_at_k(X, Y, *, ks=(1, 10, 25, 50, 100), symmetric=True):\n",
    "    D = _pairwise_sq_dists(X, Y)\n",
    "    n = D.shape[0]\n",
    "    if n == 0:\n",
    "        return {int(k): float(np.nan) for k in ks}\n",
    "    ks = [int(k) for k in ks if int(k) >= 1]\n",
    "    order_xy = np.argsort(D, axis=1)\n",
    "\n",
    "    pos_xy = np.empty(n, dtype=np.int32)\n",
    "    for i in range(n):\n",
    "        pos_xy[i] = np.where(order_xy[i] == i)[0][0]\n",
    "\n",
    "    if symmetric:\n",
    "        order_yx = np.argsort(D.T, axis=1)\n",
    "        pos_yx = np.empty(n, dtype=np.int32)\n",
    "        for i in range(n):\n",
    "            pos_yx[i] = np.where(order_yx[i] == i)[0][0]\n",
    "\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        k_eff = min(k, n)\n",
    "        r_xy = float((pos_xy < k_eff).mean())\n",
    "        if not symmetric:\n",
    "            out[k] = r_xy\n",
    "        else:\n",
    "            r_yx = float((pos_yx < k_eff).mean())\n",
    "            out[k] = 0.5 * (r_xy + r_yx)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59fa0a-a4af-40be-91fc-a7a91d693ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 5) Preprocess RNA/ATAC (fit on TRAIN only, apply to val/test) ----\n",
    "# =============================================================================\n",
    "def _is_integer_like_sparse(X, max_check=200000) -> bool:\n",
    "    if sp.issparse(X):\n",
    "        data = X.data\n",
    "    else:\n",
    "        data = np.asarray(X).ravel()\n",
    "    if data.size == 0:\n",
    "        return True\n",
    "    if data.size > max_check:\n",
    "        data = data[:max_check]\n",
    "    return np.all(np.isfinite(data)) and np.all(np.abs(data - np.round(data)) < 1e-6)\n",
    "\n",
    "@dataclass\n",
    "class RNAFit:\n",
    "    hvg: List[str]\n",
    "    counts_layer: str = \"counts\"\n",
    "    target_sum: float = 1e4\n",
    "    out_layer: str = \"log1p\"\n",
    "\n",
    "@dataclass\n",
    "class ATACFit:\n",
    "    counts_layer: str = \"counts\"\n",
    "    tfidf: Any = None\n",
    "    svd: Any = None\n",
    "    scaler: Any = None\n",
    "    n_lsi: int = 101\n",
    "    drop_first: bool = True\n",
    "\n",
    "def fit_rna_on_train(\n",
    "    rna_train: ad.AnnData,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    n_hvg: int = 2000,\n",
    "    target_sum: float = 1e4,\n",
    "    out_layer: str = \"log1p\",\n",
    ") -> RNAFit:\n",
    "    rna = rna_train.copy()\n",
    "    if counts_layer not in rna.layers:\n",
    "        rna.layers[counts_layer] = rna.X.copy()\n",
    "\n",
    "    rna.layers[out_layer] = _to_csr(rna.layers[counts_layer]).copy()\n",
    "    sc.pp.normalize_total(rna, target_sum=float(target_sum), layer=out_layer)\n",
    "    sc.pp.log1p(rna, layer=out_layer)\n",
    "\n",
    "    counts_ok = _is_integer_like_sparse(rna.layers[counts_layer])\n",
    "    flavors = ([\"seurat_v3\"] if counts_ok else []) + [\"cell_ranger\", \"seurat\"]\n",
    "\n",
    "    last = None\n",
    "    for flavor in flavors:\n",
    "        try:\n",
    "            sc.pp.highly_variable_genes(\n",
    "                rna,\n",
    "                layer=out_layer,\n",
    "                n_top_genes=int(n_hvg),\n",
    "                flavor=flavor,\n",
    "                subset=False,\n",
    "            )\n",
    "            if \"highly_variable\" in rna.var.columns and int(rna.var[\"highly_variable\"].sum()) > 0:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "            continue\n",
    "    else:\n",
    "        raise RuntimeError(f\"HVG selection failed. Tried {flavors}. Last error: {last}\")\n",
    "\n",
    "    hvg = rna.var_names[rna.var[\"highly_variable\"].to_numpy()].tolist()\n",
    "    if len(hvg) == 0:\n",
    "        raise RuntimeError(\"No HVGs selected; check RNA values.\")\n",
    "    return RNAFit(hvg=hvg, counts_layer=counts_layer, target_sum=float(target_sum), out_layer=out_layer)\n",
    "\n",
    "def apply_rna_fit(rna: ad.AnnData, fit: RNAFit) -> ad.AnnData:\n",
    "    a = rna[:, fit.hvg].copy()\n",
    "    if fit.counts_layer not in a.layers:\n",
    "        a.layers[fit.counts_layer] = a.X.copy()\n",
    "\n",
    "    a.layers[fit.out_layer] = _to_csr(a.layers[fit.counts_layer]).copy()\n",
    "    sc.pp.normalize_total(a, target_sum=float(fit.target_sum), layer=fit.out_layer)\n",
    "    sc.pp.log1p(a, layer=fit.out_layer)\n",
    "    a.X = a.layers[fit.out_layer]\n",
    "    return a\n",
    "\n",
    "def fit_atac_on_train(\n",
    "    atac_train: ad.AnnData,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    n_lsi: int = 101,\n",
    "    seed: int = 0,\n",
    ") -> ATACFit:\n",
    "    a = atac_train.copy()\n",
    "    if counts_layer not in a.layers:\n",
    "        a.layers[counts_layer] = a.X.copy()\n",
    "    X = _to_csr(a.layers[counts_layer])\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=int(n_lsi), random_state=int(seed))\n",
    "    X_lsi = svd.fit_transform(X_tfidf)\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_lsi = scaler.fit_transform(X_lsi)\n",
    "\n",
    "    return ATACFit(counts_layer=counts_layer, tfidf=tfidf, svd=svd, scaler=scaler, n_lsi=int(n_lsi), drop_first=True)\n",
    "\n",
    "def apply_atac_fit(atac: ad.AnnData, fit: ATACFit) -> ad.AnnData:\n",
    "    a = atac.copy()\n",
    "    if fit.counts_layer not in a.layers:\n",
    "        a.layers[fit.counts_layer] = a.X.copy()\n",
    "\n",
    "    X = _to_csr(a.layers[fit.counts_layer])\n",
    "    X_tfidf = fit.tfidf.transform(X)\n",
    "    X_lsi = fit.svd.transform(X_tfidf)\n",
    "    X_lsi = fit.scaler.transform(X_lsi)\n",
    "\n",
    "    if fit.drop_first and X_lsi.shape[1] > 1:\n",
    "        X_lsi = X_lsi[:, 1:]\n",
    "\n",
    "    n_feats = X_lsi.shape[1]\n",
    "    var = pd.DataFrame(index=[f\"LSI_{i+1}\" for i in range(n_feats)])\n",
    "    atac_lsi = ad.AnnData(\n",
    "        X=X_lsi.astype(np.float32, copy=False),\n",
    "        obs=a.obs.copy(),\n",
    "        var=var,\n",
    "    )\n",
    "    atac_lsi.uns[\"lsi\"] = {\"n_components\": int(n_feats), \"drop_first\": bool(fit.drop_first)}\n",
    "    return atac_lsi\n",
    "\n",
    "def preprocess_splits(\n",
    "    rna: ad.AnnData,\n",
    "    atac: ad.AnnData,\n",
    "    train_idx: np.ndarray,\n",
    "    val_idx: np.ndarray,\n",
    "    test_idx: np.ndarray,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    n_hvg: int = 2000,\n",
    "    target_sum: float = 1e4,\n",
    "    n_lsi: int = 101,\n",
    "    seed: int = 0,\n",
    "):\n",
    "    rna_train, rna_val, rna_test = rna[train_idx].copy(), rna[val_idx].copy(), rna[test_idx].copy()\n",
    "    atac_train, atac_val, atac_test = atac[train_idx].copy(), atac[val_idx].copy(), atac[test_idx].copy()\n",
    "\n",
    "    rna_fit  = fit_rna_on_train(rna_train, counts_layer=counts_layer, n_hvg=n_hvg, target_sum=target_sum)\n",
    "    atac_fit = fit_atac_on_train(atac_train, counts_layer=counts_layer, n_lsi=n_lsi, seed=seed)\n",
    "\n",
    "    rna_tr = apply_rna_fit(rna_train, rna_fit)\n",
    "    rna_va = apply_rna_fit(rna_val,   rna_fit)\n",
    "    rna_te = apply_rna_fit(rna_test,  rna_fit)\n",
    "\n",
    "    atac_tr = apply_atac_fit(atac_train, atac_fit)\n",
    "    atac_va = apply_atac_fit(atac_val,   atac_fit)\n",
    "    atac_te = apply_atac_fit(atac_test,  atac_fit)\n",
    "\n",
    "    return rna_tr, atac_tr, rna_va, atac_va, rna_te, atac_te, rna_fit, atac_fit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d8e27-564c-4a61-8b81-d0c09a33cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 6) Load paired data + split + preprocess + build dataset ----\n",
    "# =============================================================================\n",
    "rna_raw  = sc.read_h5ad(RNA_PATH)\n",
    "atac_raw = sc.read_h5ad(ATAC_PATH)\n",
    "print(rna_raw)\n",
    "print(atac_raw)\n",
    "\n",
    "adata_dict_raw = {\"rna\": rna_raw, \"atac\": atac_raw}\n",
    "adata_dict_raw = align_paired_obs_names(adata_dict_raw)\n",
    "rna_raw  = adata_dict_raw[\"rna\"]\n",
    "atac_raw = adata_dict_raw[\"atac\"]\n",
    "\n",
    "assert (rna_raw.obs_names == atac_raw.obs_names).all(), \"RNA/ATAC obs_names not aligned\"\n",
    "ensure_counts_layer(rna_raw, COUNTS_LAYER)\n",
    "ensure_counts_layer(atac_raw, COUNTS_LAYER)\n",
    "\n",
    "assert CELLTYPE_KEY in rna_raw.obs.columns, f\"{CELLTYPE_KEY} missing in rna_raw.obs\"\n",
    "assert CELLTYPE_KEY in atac_raw.obs.columns, f\"{CELLTYPE_KEY} missing in atac_raw.obs\"\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "y = rna_raw.obs[CELLTYPE_KEY].to_numpy()\n",
    "idx_all = np.arange(rna_raw.n_obs)\n",
    "\n",
    "train_frac, val_frac = 0.75, 0.10\n",
    "train_idx, val_idx, test_idx = [], [], []\n",
    "for ct in np.unique(y):\n",
    "    idx_ct = idx_all[y == ct]\n",
    "    rng.shuffle(idx_ct)\n",
    "    n_ct = len(idx_ct)\n",
    "    n_tr = int(np.floor(train_frac * n_ct))\n",
    "    n_va = int(np.floor(val_frac * n_ct))\n",
    "    tr = idx_ct[:n_tr]\n",
    "    va = idx_ct[n_tr:n_tr+n_va]\n",
    "    te = idx_ct[n_tr+n_va:]\n",
    "    train_idx.append(tr); val_idx.append(va); test_idx.append(te)\n",
    "\n",
    "train_idx = np.concatenate(train_idx)\n",
    "val_idx   = np.concatenate(val_idx)\n",
    "test_idx  = np.concatenate(test_idx)\n",
    "\n",
    "rng.shuffle(train_idx); rng.shuffle(val_idx); rng.shuffle(test_idx)\n",
    "\n",
    "print(\"train/val/test:\", len(train_idx), len(val_idx), len(test_idx))\n",
    "print(\"train label counts:\\n\", pd.Series(y[train_idx]).value_counts())\n",
    "print(\"val label counts:\\n\", pd.Series(y[val_idx]).value_counts())\n",
    "print(\"test label counts:\\n\", pd.Series(y[test_idx]).value_counts())\n",
    "\n",
    "rna_tr, atac_tr, rna_va, atac_va, rna_te, atac_te, rna_fit, atac_fit = preprocess_splits(\n",
    "    rna_raw, atac_raw,\n",
    "    train_idx, val_idx, test_idx,\n",
    "    counts_layer=COUNTS_LAYER,\n",
    "    n_hvg=N_HVG,\n",
    "    target_sum=RNA_TARGET_SUM,\n",
    "    n_lsi=N_LSI,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "adata_dict = {\n",
    "    \"rna\":  rna_tr.concatenate(rna_va, rna_te, batch_key=None),\n",
    "    \"atac\": atac_tr.concatenate(atac_va, atac_te, batch_key=None),\n",
    "}\n",
    "adata_dict = align_paired_obs_names(adata_dict)\n",
    "dataset = MultiModalDataset(adata_dict=adata_dict, X_key=\"X\", device=None)\n",
    "\n",
    "n_tr = rna_tr.n_obs\n",
    "n_va = rna_va.n_obs\n",
    "n_te = rna_te.n_obs\n",
    "\n",
    "TRAIN_IDX = np.arange(0, n_tr)\n",
    "VAL_IDX   = np.arange(n_tr, n_tr + n_va)\n",
    "TEST_IDX  = np.arange(n_tr + n_va, n_tr + n_va + n_te)\n",
    "\n",
    "y_all = adata_dict[\"rna\"].obs[CELLTYPE_KEY].to_numpy()\n",
    "print(\"dataset n:\", len(dataset), \"train/val/test:\", n_tr, n_va, n_te)\n",
    "print(\"n unique cell types:\", len(np.unique(y_all)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c09e55-7370-4fdc-897d-19a8abd52a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 7) Ablation masking + grouped batching (TRAIN only) ----\n",
    "# =============================================================================\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, base_ds, *, global_indices):\n",
    "        self.base = base_ds\n",
    "        self.global_indices = np.asarray(global_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.base[i]\n",
    "        gid = int(self.global_indices[i])\n",
    "        return x, gid\n",
    "\n",
    "class DeterministicMaskDataset(Dataset):\n",
    "    # group=0: paired; group=1: RNA-only; group=2: ATAC-only\n",
    "    def __init__(self, base_ds: Dataset, groups: np.ndarray):\n",
    "        self.base = base_ds\n",
    "        self.groups = np.asarray(groups).astype(int)\n",
    "        assert len(self.base) == len(self.groups)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, gid = self.base[i]\n",
    "        g = int(self.groups[i])\n",
    "        if g == 0:\n",
    "            return x, gid\n",
    "        x2 = dict(x)\n",
    "        if g == 1:\n",
    "            x2.pop(\"atac\", None)\n",
    "        elif g == 2:\n",
    "            x2.pop(\"rna\", None)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown group {g}\")\n",
    "        return x2, gid\n",
    "\n",
    "class GroupedBatchSampler(Sampler[List[int]]):\n",
    "    \"\"\"\n",
    "    Samples batches from a single group at a time (no mixed missingness inside a batch).\n",
    "    \"\"\"\n",
    "    def __init__(self, groups: np.ndarray, batch_size: int, seed: int = 0, drop_last: bool = True):\n",
    "        self.groups = np.asarray(groups).astype(int)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.seed = int(seed)\n",
    "        self.drop_last = bool(drop_last)\n",
    "\n",
    "        self.group_to_indices = {}\n",
    "        for g in np.unique(self.groups):\n",
    "            self.group_to_indices[int(g)] = np.where(self.groups == g)[0].tolist()\n",
    "\n",
    "        self.nonempty_groups = [g for g, idxs in self.group_to_indices.items() if len(idxs) > 0]\n",
    "        if len(self.nonempty_groups) == 0:\n",
    "            raise ValueError(\"No samples in any group.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        pools = {}\n",
    "        for g in self.nonempty_groups:\n",
    "            idxs = self.group_to_indices[g].copy()\n",
    "            rng.shuffle(idxs)\n",
    "            pools[g] = idxs\n",
    "\n",
    "        while True:\n",
    "            available = [g for g in self.nonempty_groups if len(pools[g]) >= self.batch_size]\n",
    "            if len(available) == 0:\n",
    "                break\n",
    "            chosen = int(rng.choice(available))\n",
    "            batch = [pools[chosen].pop() for _ in range(self.batch_size)]\n",
    "            yield batch\n",
    "\n",
    "        if not self.drop_last:\n",
    "            for g in self.nonempty_groups:\n",
    "                while len(pools[g]) > 0:\n",
    "                    b = pools[g][: self.batch_size]\n",
    "                    pools[g] = pools[g][len(b):]\n",
    "                    yield b\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(self.group_to_indices[g]) // self.batch_size for g in self.nonempty_groups)\n",
    "\n",
    "def _stack_collate(batch):\n",
    "    x0 = batch[0][0]\n",
    "    keys = list(x0.keys())\n",
    "    x_dict = {k: torch.stack([b[0][k] for b in batch], 0) for k in keys}\n",
    "    gids = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    return x_dict, gids\n",
    "\n",
    "def make_loaders_for_celltype_ablation(\n",
    "    base_dataset,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    test_idx,\n",
    "    *,\n",
    "    y_all: np.ndarray,\n",
    "    ablate_cell_type: str,\n",
    "    drop_modality: str,   # \"rna\" or \"atac\"\n",
    "    seed: int = 0,\n",
    "    batch_size: int = 128,\n",
    "    num_workers: int = 0,\n",
    "    disable_ablation: bool = False,\n",
    "):\n",
    "    train_base = Subset(base_dataset, train_idx)\n",
    "    val_base   = Subset(base_dataset, val_idx)\n",
    "    test_base  = Subset(base_dataset, test_idx)\n",
    "\n",
    "    train_indexed = IndexedDataset(train_base, global_indices=train_idx)\n",
    "    val_indexed   = IndexedDataset(val_base,   global_indices=val_idx)\n",
    "    test_indexed  = IndexedDataset(test_base,  global_indices=test_idx)\n",
    "\n",
    "    y_train = y_all[train_idx]\n",
    "    groups = np.zeros(len(train_indexed), dtype=int)\n",
    "\n",
    "    if not disable_ablation:\n",
    "        is_ct = (y_train == ablate_cell_type)\n",
    "        if str(drop_modality) == \"atac\":\n",
    "            groups[is_ct] = 1  # RNA-only\n",
    "        elif str(drop_modality) == \"rna\":\n",
    "            groups[is_ct] = 2  # ATAC-only\n",
    "        else:\n",
    "            raise ValueError(\"drop_modality must be 'atac' or 'rna'\")\n",
    "\n",
    "    train_masked = DeterministicMaskDataset(train_indexed, groups=groups)\n",
    "    batch_sampler = GroupedBatchSampler(groups=groups, batch_size=int(batch_size), seed=int(seed), drop_last=True)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_masked,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=int(num_workers),\n",
    "        collate_fn=_stack_collate,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_indexed,\n",
    "        batch_size=int(batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=int(num_workers),\n",
    "        collate_fn=_stack_collate,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_indexed,\n",
    "        batch_size=int(batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=int(num_workers),\n",
    "        collate_fn=_stack_collate,\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51786997-fb5c-4fd0-add1-e63636ae2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 8) UniVI training helpers (Fig8-matched + warmup + compat) ----\n",
    "# =============================================================================\n",
    "def make_univi_cfg(rna_dim: int, atac_dim: int) -> UniVIConfig:\n",
    "    return UniVIConfig(\n",
    "        latent_dim=30,\n",
    "        beta=1.25,\n",
    "        gamma=4.35,\n",
    "        encoder_dropout=0.10,\n",
    "        decoder_dropout=0.05,\n",
    "        encoder_batchnorm=True,\n",
    "        decoder_batchnorm=False,\n",
    "        kl_anneal_start=50,\n",
    "        kl_anneal_end=100,\n",
    "        align_anneal_start=75,\n",
    "        align_anneal_end=125,\n",
    "        modalities=[\n",
    "            ModalityConfig(\n",
    "                name=\"rna\",\n",
    "                input_dim=int(rna_dim),\n",
    "                encoder_hidden=[512, 256, 128],\n",
    "                decoder_hidden=[128, 256, 512],\n",
    "                likelihood=\"gaussian\",\n",
    "            ),\n",
    "            ModalityConfig(\n",
    "                name=\"atac\",\n",
    "                input_dim=int(atac_dim),\n",
    "                encoder_hidden=[128, 64],\n",
    "                decoder_hidden=[64, 128],\n",
    "                likelihood=\"gaussian\",\n",
    "            ),\n",
    "        ],\n",
    "        class_heads=[],\n",
    "    )\n",
    "\n",
    "def make_train_cfg(device) -> TrainingConfig:\n",
    "    return TrainingConfig(\n",
    "        n_epochs=5000,\n",
    "        batch_size=int(BATCH_SIZE),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        device=device,\n",
    "        log_every=25,\n",
    "        grad_clip=5.0,\n",
    "        early_stopping=True,\n",
    "        patience=50,\n",
    "    )\n",
    "\n",
    "def _patch_trainer_best_epoch_warmup(trainer, warmup_epochs: int):\n",
    "    warmup_epochs = int(warmup_epochs or 0)\n",
    "    if warmup_epochs <= 0:\n",
    "        return trainer\n",
    "\n",
    "    candidate_method_names = [\n",
    "        \"_update_best\", \"_maybe_update_best\", \"_maybe_save_best\", \"_save_best\",\n",
    "        \"_early_stopping_step\", \"_check_early_stopping\", \"_maybe_early_stop\",\n",
    "        \"update_best\", \"maybe_save_best\", \"early_stopping_step\", \"check_early_stopping\",\n",
    "    ]\n",
    "\n",
    "    def _infer_epoch(args, kwargs):\n",
    "        if \"epoch\" in kwargs and kwargs[\"epoch\"] is not None:\n",
    "            return int(kwargs[\"epoch\"])\n",
    "        for attr in (\"epoch\", \"current_epoch\", \"global_epoch\"):\n",
    "            v = getattr(trainer, attr, None)\n",
    "            if isinstance(v, (int, np.integer)):\n",
    "                return int(v)\n",
    "        for a in args:\n",
    "            if isinstance(a, (int, np.integer)):\n",
    "                return int(a)\n",
    "        return None\n",
    "\n",
    "    patched_any = False\n",
    "    for name in candidate_method_names:\n",
    "        fn = getattr(trainer, name, None)\n",
    "        if fn is None or not callable(fn):\n",
    "            continue\n",
    "        orig = fn\n",
    "\n",
    "        def wrapped(*args, __orig=orig, **kwargs):\n",
    "            ep = _infer_epoch(args, kwargs)\n",
    "            if ep is not None and ep < warmup_epochs:\n",
    "                return None\n",
    "            return __orig(*args, **kwargs)\n",
    "\n",
    "        setattr(trainer, name, wrapped)\n",
    "        patched_any = True\n",
    "\n",
    "    trainer._best_epoch_warmup_epochs = warmup_epochs\n",
    "    trainer._best_epoch_warmup_patched = bool(patched_any)\n",
    "    return trainer\n",
    "\n",
    "def _build_univi_trainer_compat(*, model, train_cfg, train_loader, val_loader):\n",
    "    sig = inspect.signature(UniVITrainer.__init__)\n",
    "    params = sig.parameters\n",
    "    kwargs = {}\n",
    "    if \"model\" in params:\n",
    "        kwargs[\"model\"] = model\n",
    "    if \"train_loader\" in params:\n",
    "        kwargs[\"train_loader\"] = train_loader\n",
    "    if \"val_loader\" in params:\n",
    "        kwargs[\"val_loader\"] = val_loader\n",
    "\n",
    "    cfg_key = None\n",
    "    for cand in (\"train_cfg\", \"training_cfg\", \"cfg_train\", \"config\", \"cfg\", \"training_config\"):\n",
    "        if cand in params:\n",
    "            cfg_key = cand\n",
    "            break\n",
    "    if cfg_key is not None:\n",
    "        kwargs[cfg_key] = train_cfg\n",
    "        return UniVITrainer(**kwargs)\n",
    "\n",
    "    for k, v in vars(train_cfg).items():\n",
    "        if k in params:\n",
    "            kwargs[k] = v\n",
    "    return UniVITrainer(**kwargs)\n",
    "\n",
    "def _trainer_fit_compat(trainer, train_loader, val_loader):\n",
    "    try:\n",
    "        return trainer.fit()\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        return trainer.fit(train_loader=train_loader, val_loader=val_loader)\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return trainer.fit(train_loader, val_loader)\n",
    "\n",
    "def train_one_model(\n",
    "    *,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    rna_dim: int,\n",
    "    atac_dim: int,\n",
    "    seed: int = 0,\n",
    "    device=None,\n",
    "    loss_mode: str = \"v1\",\n",
    "    v1_recon: str = \"moe\",\n",
    "    best_epoch_warmup: int = 0,\n",
    "):\n",
    "    if device is None:\n",
    "        device = pick_device()\n",
    "    torch.manual_seed(int(seed))\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "    univi_cfg = make_univi_cfg(rna_dim=int(rna_dim), atac_dim=int(atac_dim))\n",
    "    train_cfg = make_train_cfg(device=device)\n",
    "\n",
    "    model = UniVIMultiModalVAE(\n",
    "        univi_cfg,\n",
    "        loss_mode=str(loss_mode),\n",
    "        v1_recon=str(v1_recon),\n",
    "    ).to(device)\n",
    "\n",
    "    trainer = _build_univi_trainer_compat(\n",
    "        model=model,\n",
    "        train_cfg=train_cfg,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "    )\n",
    "    if not hasattr(trainer, \"train_loader\"):\n",
    "        trainer.train_loader = train_loader\n",
    "    if not hasattr(trainer, \"val_loader\"):\n",
    "        trainer.val_loader = val_loader\n",
    "\n",
    "    _patch_trainer_best_epoch_warmup(trainer, warmup_epochs=int(best_epoch_warmup))\n",
    "    _trainer_fit_compat(trainer, train_loader=train_loader, val_loader=val_loader)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728af66-68bd-4eae-9afe-3d64d19fee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 9) Encoding (MoE-first, robust) + STRICT MoE accessor ----\n",
    "# =============================================================================\n",
    "@torch.no_grad()\n",
    "def encode_embeddings_with_labels(\n",
    "    model,\n",
    "    loader,\n",
    "    *,\n",
    "    device,\n",
    "    y_all,\n",
    "    id_to_int=None,\n",
    "    debug_first_batch=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns dict with:\n",
    "      mu_rna, mu_atac,\n",
    "      mu_moe: STRICT fused posterior mean if available (prefers mu_z / mu_moe / mu_joint),\n",
    "      mu_fused: model-provided fused latent (via _extract_latent(enc_full,\"fused\")),\n",
    "      moe_gating: (n_cells, n_modalities) precision-derived weights if logvar_dict is present,\n",
    "      moe_gating_mods: list[str] naming the columns of moe_gating,\n",
    "      y / gids / id_to_int,\n",
    "      moe_status\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    y_raw, global_map = _build_label_encoder(y_all)\n",
    "    kind = np.asarray(y_raw).dtype.kind\n",
    "    use_map = id_to_int if id_to_int is not None else (global_map if kind not in (\"i\", \"u\", \"f\", \"b\") else None)\n",
    "\n",
    "    mu_rna_all, mu_atac_all, mu_moe_all, mu_fused_all = [], [], [], []\n",
    "    moe_gating_all = []\n",
    "    y_list, gids_list = [], []\n",
    "\n",
    "    saw_gating = False\n",
    "    saw_explicit_moe = False\n",
    "    gating_shapes = set()\n",
    "\n",
    "    # store a single consistent column order for gating\n",
    "    gating_mods = None\n",
    "\n",
    "    for b_ix, (x_dict, gids) in enumerate(loader):\n",
    "        gids_np = _to_numpy_ids(gids)\n",
    "        if gids_np is None:\n",
    "            raise RuntimeError(\"Loader must provide gids for label lookup.\")\n",
    "\n",
    "        # move tensors to device\n",
    "        x_full = {}\n",
    "        for k, v in x_dict.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            x_full[k] = v.to(device) if torch.is_tensor(v) else v\n",
    "\n",
    "        # full encode (with whatever modalities are present)\n",
    "        enc_full = _unwrap_model_output(_call_univi_encoder(model, x_full))\n",
    "\n",
    "        # unimodal encodes (for alignment / plots)\n",
    "        enc_rna = None\n",
    "        if \"rna\" in x_full and torch.is_tensor(x_full[\"rna\"]):\n",
    "            enc_rna = _unwrap_model_output(_call_univi_encoder(model, {\"rna\": x_full[\"rna\"]}))\n",
    "        enc_atac = None\n",
    "        if \"atac\" in x_full and torch.is_tensor(x_full[\"atac\"]):\n",
    "            enc_atac = _unwrap_model_output(_call_univi_encoder(model, {\"atac\": x_full[\"atac\"]}))\n",
    "\n",
    "        # ---- gating weights (precision-derived from logvar_dict) ----\n",
    "        W, mods = _extract_gating_weights_from_enc_full(enc_full)\n",
    "        if torch.is_tensor(W):\n",
    "            saw_gating = True\n",
    "            gating_shapes.add(tuple(W.shape))\n",
    "            moe_gating_all.append(W.detach().cpu())\n",
    "\n",
    "            if gating_mods is None:\n",
    "                gating_mods = list(mods)\n",
    "            else:\n",
    "                # if it changes mid-run, something is wrong; fail loud\n",
    "                if list(mods) != list(gating_mods):\n",
    "                    raise RuntimeError(\n",
    "                        f\"moe_gating_mods changed across batches.\\n\"\n",
    "                        f\"first={gating_mods}\\nnow={list(mods)}\"\n",
    "                    )\n",
    "\n",
    "        if debug_first_batch and b_ix == 0:\n",
    "            def _keys(obj):\n",
    "                return sorted(obj.keys()) if isinstance(obj, dict) else [str(type(obj))]\n",
    "            print(\"[latent-debug] enc_full keys:\", _keys(enc_full))\n",
    "            print(\"[latent-debug] gating found:\", torch.is_tensor(W),\n",
    "                  (\"shape=\" + str(tuple(W.shape)) if torch.is_tensor(W) else \"\"),\n",
    "                  (\"mods=\" + str(mods) if mods is not None else \"\"))\n",
    "\n",
    "        # ---- extract unimodal means ----\n",
    "        mu_rna = _extract_latent(enc_rna, \"rna\")\n",
    "        if mu_rna is None:\n",
    "            mu_rna = _extract_latent(enc_rna, \"fused\")\n",
    "\n",
    "        mu_atac = _extract_latent(enc_atac, \"atac\")\n",
    "        if mu_atac is None:\n",
    "            mu_atac = _extract_latent(enc_atac, \"fused\")\n",
    "\n",
    "        # model-provided fused latent (not avg fallback)\n",
    "        mu_fused_from_model = _extract_latent(enc_full, \"fused\")\n",
    "\n",
    "        # ---- STRICT fused posterior mean (\"MoE latent\") ----\n",
    "        mu_moe = _pick_explicit_moe_tensor(enc_full)\n",
    "        if torch.is_tensor(mu_moe):\n",
    "            saw_explicit_moe = True\n",
    "        else:\n",
    "            mu_moe = None\n",
    "\n",
    "        # ---- accumulate latents ----\n",
    "        if torch.is_tensor(mu_rna):\n",
    "            mu_rna_all.append(mu_rna.detach().cpu())\n",
    "        if torch.is_tensor(mu_atac):\n",
    "            mu_atac_all.append(mu_atac.detach().cpu())\n",
    "        if torch.is_tensor(mu_moe):\n",
    "            mu_moe_all.append(mu_moe.detach().cpu())\n",
    "        if torch.is_tensor(mu_fused_from_model):\n",
    "            mu_fused_all.append(mu_fused_from_model.detach().cpu())\n",
    "\n",
    "        # ---- labels ----\n",
    "        y_tensor, use_map = _encode_labels_for_gids(y_raw, gids_np, id_to_int=use_map)\n",
    "        y_list.append(y_tensor.detach().cpu())\n",
    "        gids_list.append(torch.as_tensor(gids_np, dtype=torch.long))\n",
    "\n",
    "    def _cat(xs):\n",
    "        return torch.cat(xs, dim=0) if len(xs) else None\n",
    "\n",
    "    mu_moe_cat = _cat(mu_moe_all)\n",
    "    mu_fused_cat = _cat(mu_fused_all)\n",
    "    moe_gating_cat = _cat(moe_gating_all)\n",
    "\n",
    "    enc_out = {\n",
    "        \"mu_rna\": _cat(mu_rna_all),\n",
    "        \"mu_atac\": _cat(mu_atac_all),\n",
    "\n",
    "        \"mu_moe\": mu_moe_cat,\n",
    "        \"mu_fused\": mu_fused_cat,\n",
    "\n",
    "        # NEW: gating matrix + column order\n",
    "        \"moe_gating\": moe_gating_cat,\n",
    "        \"moe_gating_mods\": (list(gating_mods) if gating_mods is not None else None),\n",
    "\n",
    "        \"y\": _cat(y_list),\n",
    "        \"y_fused\": _cat(y_list),\n",
    "        \"gids\": _cat(gids_list),\n",
    "        \"id_to_int\": use_map,\n",
    "        \"moe_status\": {\n",
    "            \"saw_gating\": bool(saw_gating),\n",
    "            \"gating_shapes\": sorted(list(gating_shapes)),\n",
    "            \"moe_gating_mods\": (list(gating_mods) if gating_mods is not None else None),\n",
    "            \"saw_explicit_moe\": bool(saw_explicit_moe),\n",
    "            \"has_mu_moe\": (mu_moe_cat is not None),\n",
    "            \"has_mu_fused\": (mu_fused_cat is not None),\n",
    "            \"has_moe_gating\": (moe_gating_cat is not None),\n",
    "        },\n",
    "    }\n",
    "    return enc_out\n",
    "\n",
    "def report_moe_status(enc, *, prefix=\"[moe_status]\"):\n",
    "    st = (enc or {}).get(\"moe_status\", {})\n",
    "    print(\n",
    "        prefix,\n",
    "        \"saw_gating:\", st.get(\"saw_gating\", False),\n",
    "        \"| gating_shapes:\", st.get(\"gating_shapes\", []),\n",
    "        \"| moe_gating_mods:\", st.get(\"moe_gating_mods\", None),\n",
    "        \"| saw_explicit_moe:\", st.get(\"saw_explicit_moe\", False),\n",
    "        \"| has_mu_moe:\", st.get(\"has_mu_moe\", False),\n",
    "        \"| has_mu_fused:\", st.get(\"has_mu_fused\", False),\n",
    "        \"| has_moe_gating:\", st.get(\"has_moe_gating\", False),\n",
    "    )\n",
    "\n",
    "def compute_moe_latent_strict(enc: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    STRICT fused latent accessor.\n",
    "\n",
    "    Accepts either:\n",
    "      - the batch encoder output dict containing mu_z / mu_fused / z, OR\n",
    "      - the enc_out dict returned by encode_embeddings_with_labels (contains mu_moe / mu_fused).\n",
    "\n",
    "    Preference order:\n",
    "      1) mu_moe\n",
    "      2) mu_z / mu_fused\n",
    "      3) z\n",
    "    \"\"\"\n",
    "    if enc is None or not isinstance(enc, dict):\n",
    "        raise RuntimeError(\"enc must be a dict\")\n",
    "\n",
    "    for k in (\"mu_moe\", \"mu_z\", \"mu_fused\"):\n",
    "        if enc.get(k, None) is not None:\n",
    "            X = _as_numpy_2d(enc[k])\n",
    "            if X is not None:\n",
    "                return X\n",
    "\n",
    "    if enc.get(\"z\", None) is not None:\n",
    "        X = _as_numpy_2d(enc[\"z\"])\n",
    "        if X is not None:\n",
    "            return X\n",
    "\n",
    "    st = (enc or {}).get(\"moe_status\", {})\n",
    "    raise RuntimeError(\n",
    "        \"Requested STRICT fused latent but none was found.\\n\"\n",
    "        \"Looked for keys: mu_moe, mu_z, mu_fused, z.\\n\"\n",
    "        f\"moe_status={st}\\n\"\n",
    "    )\n",
    "\n",
    "def compute_fused_latent_model(enc: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns model-provided fused latent (mu_fused). No avg fallback.\n",
    "    \"\"\"\n",
    "    X = _as_numpy_2d(enc.get(\"mu_fused\", None)) if isinstance(enc, dict) else None\n",
    "    if X is None:\n",
    "        raise RuntimeError(\"Requested model fused latent but enc['mu_fused'] is missing.\")\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76519700-81ce-4df0-9bd2-017418207620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 10) kNN ACC + silhouette helper ----\n",
    "# =============================================================================\n",
    "def knn_acc_from_reference(X_ref, y_ref, X_query, y_query, *, k: int = 3):\n",
    "    X_ref = _as_numpy_2d(X_ref)\n",
    "    X_query = _as_numpy_2d(X_query)\n",
    "    y_ref = _as_numpy_1d(y_ref)\n",
    "    y_query = _as_numpy_1d(y_query)\n",
    "    if X_ref is None or X_query is None or y_ref is None or y_query is None:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "    if X_ref.shape[0] < 2 or X_query.shape[0] < 2:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "    k_eff = int(min(max(1, k), X_ref.shape[0]))\n",
    "    clf = KNeighborsClassifier(n_neighbors=k_eff, weights=\"distance\", metric=\"euclidean\")\n",
    "    clf.fit(X_ref, y_ref)\n",
    "    y_hat = clf.predict(X_query)\n",
    "    return {\"acc\": float(accuracy_score(y_query, y_hat)), \"macro_f1\": float(f1_score(y_query, y_hat, average=\"macro\"))}\n",
    "\n",
    "def safe_silhouette_by_true_labels(X, y_true, *, min_per_class=2, min_total=10):\n",
    "    X = _as_numpy_2d(X)\n",
    "    y = _as_numpy_1d(y_true)\n",
    "    if X is None or y is None:\n",
    "        return np.nan, 0, 0.0\n",
    "    uniq, counts = np.unique(y, return_counts=True)\n",
    "    keep = set(uniq[counts >= int(min_per_class)].tolist())\n",
    "    if len(keep) < 2:\n",
    "        return np.nan, 0, 0.0\n",
    "    mask = np.array([lab in keep for lab in y], dtype=bool)\n",
    "    n_used = int(mask.sum())\n",
    "    if n_used < int(min_total):\n",
    "        return np.nan, n_used, float(n_used / max(len(y), 1))\n",
    "    Xk = X[mask]\n",
    "    yk = y[mask]\n",
    "    uniq2, counts2 = np.unique(yk, return_counts=True)\n",
    "    if len(uniq2) < 2 or np.any(counts2 < 2):\n",
    "        return np.nan, n_used, float(n_used / max(len(y), 1))\n",
    "    try:\n",
    "        return float(silhouette_score(Xk, yk)), n_used, float(n_used / max(len(y), 1))\n",
    "    except Exception:\n",
    "        return np.nan, n_used, float(n_used / max(len(y), 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7550e5fd-d74f-4c7a-89b2-fa2ff1a01d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 11) UMAP plotting (SELF-CONTAINED, TWO COMPOSITE FIGURES ONLY) ----\n",
    "#   Outputs ONLY:\n",
    "#     1) <base>__spaces_celltype.png   (3 cols: RNA / ATAC / MoE, colored by cell type)\n",
    "#     2) <base>__summary_4col.png      (4 cols: overlap mod / overlap ct / MoE gradient / MoE ct)\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Minimal numeric helpers (self-contained)\n",
    "# -----------------------------------------------------------------------------\n",
    "def _as_numpy_1d(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.is_tensor(x):\n",
    "            x = x.detach().cpu().numpy()\n",
    "    except Exception:\n",
    "        pass\n",
    "    x = np.asarray(x)\n",
    "    return x.reshape(-1)\n",
    "\n",
    "def _as_numpy_2d(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.is_tensor(x):\n",
    "            x = x.detach().cpu().numpy()\n",
    "    except Exception:\n",
    "        pass\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    return x.astype(np.float32, copy=False)\n",
    "\n",
    "def _get_celltype_labels_from_enc(enc, y_all):\n",
    "    \"\"\"\n",
    "    Returns per-row cell-type string labels for the *paired* cells.\n",
    "    Priority:\n",
    "      1) enc['gids'] indexes into y_all\n",
    "      2) enc['y_fused'] or enc['y'] already stores labels\n",
    "    \"\"\"\n",
    "    if enc is None or not isinstance(enc, dict):\n",
    "        return None\n",
    "\n",
    "    gids = enc.get(\"gids\", None)\n",
    "    if gids is not None:\n",
    "        gids = _as_numpy_1d(gids)\n",
    "        if gids is None:\n",
    "            return None\n",
    "        gids = gids.astype(np.int64, copy=False)\n",
    "        labs = np.asarray(y_all)[gids]\n",
    "        return np.asarray(labs).astype(str)\n",
    "\n",
    "    y = enc.get(\"y_fused\", enc.get(\"y\", None))\n",
    "    if y is None:\n",
    "        return None\n",
    "    y = _as_numpy_1d(y)\n",
    "    return None if y is None else np.asarray(y).astype(str)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UMAP embedding (with PCA fallback)\n",
    "# -----------------------------------------------------------------------------\n",
    "def _umap_embed(X, *, seed=0, n_neighbors=15, min_dist=0.3, metric=\"euclidean\"):\n",
    "    X = _as_numpy_2d(X)\n",
    "    if X is None:\n",
    "        return None\n",
    "    try:\n",
    "        import umap\n",
    "        reducer = umap.UMAP(\n",
    "            n_neighbors=int(n_neighbors),\n",
    "            min_dist=float(min_dist),\n",
    "            metric=str(metric),\n",
    "            random_state=int(seed),\n",
    "        )\n",
    "        return reducer.fit_transform(X)\n",
    "    except Exception:\n",
    "        from sklearn.decomposition import PCA\n",
    "        return PCA(n_components=2, random_state=int(seed)).fit_transform(X)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Color/legend helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def _make_color_codes(labels, *, sort_legend_by=\"count\", cmap_name=\"tab20\"):\n",
    "    if labels is None:\n",
    "        return None, None, None, None, None\n",
    "    labels = np.asarray(labels).astype(str)\n",
    "\n",
    "    uniq, counts = np.unique(labels, return_counts=True)\n",
    "    if str(sort_legend_by).lower() == \"count\":\n",
    "        order = np.argsort(-counts)\n",
    "    else:\n",
    "        order = np.argsort(uniq.astype(str))\n",
    "    uniq = uniq[order]\n",
    "    counts = counts[order]\n",
    "\n",
    "    lab_to_int = {lab: i for i, lab in enumerate(uniq.tolist())}\n",
    "    c = np.array([lab_to_int[lab] for lab in labels], dtype=int)\n",
    "    cmap = plt.get_cmap(str(cmap_name), max(len(uniq), 1))\n",
    "    return c, uniq, counts, cmap, lab_to_int\n",
    "\n",
    "def _legend_handles_from_uniq(uniq, counts, cmap, *, legend_max=25):\n",
    "    if uniq is None or counts is None or cmap is None:\n",
    "        return None\n",
    "    n_cat = len(uniq)\n",
    "    if legend_max is None or int(legend_max) <= 0 or n_cat == 0:\n",
    "        return None\n",
    "    top_n = int(min(int(legend_max), n_cat))\n",
    "    handles = []\n",
    "    for i in range(top_n):\n",
    "        lab = uniq[i]\n",
    "        col = cmap(i)\n",
    "        handles.append(\n",
    "            Line2D([0], [0], marker=\"o\", color=\"none\",\n",
    "                   markerfacecolor=col, markeredgecolor=\"none\",\n",
    "                   markersize=6, label=f\"{lab} (n={counts[i]})\")\n",
    "        )\n",
    "    return handles\n",
    "\n",
    "def _plot_umap_scatter(\n",
    "    ax,\n",
    "    emb,\n",
    "    *,\n",
    "    color_codes=None,\n",
    "    cmap=None,\n",
    "    title=\"UMAP\",\n",
    "    point_size=8,\n",
    "    alpha=0.75,\n",
    "):\n",
    "    if emb is None:\n",
    "        ax.set_title(f\"{title} (embed failed)\")\n",
    "        ax.axis(\"off\")\n",
    "        return None\n",
    "    if color_codes is None or cmap is None:\n",
    "        sc = ax.scatter(emb[:, 0], emb[:, 1], s=point_size, alpha=alpha, linewidths=0)\n",
    "    else:\n",
    "        sc = ax.scatter(emb[:, 0], emb[:, 1], c=color_codes, cmap=cmap,\n",
    "                        s=point_size, alpha=alpha, linewidths=0)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"UMAP1\")\n",
    "    ax.set_ylabel(\"UMAP2\")\n",
    "    return sc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Overlap helper: stack RNA+ATAC\n",
    "# -----------------------------------------------------------------------------\n",
    "def _stack_for_overlap(enc):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_stack: (2n, d) stacked [rna; atac]\n",
    "      modality_labels: (2n,) [\"rna\"]*n + [\"atac\"]*n\n",
    "    \"\"\"\n",
    "    if enc is None or not isinstance(enc, dict):\n",
    "        return None, None\n",
    "    Xr = _as_numpy_2d(enc.get(\"mu_rna\", None))\n",
    "    Xa = _as_numpy_2d(enc.get(\"mu_atac\", None))\n",
    "    if Xr is None or Xa is None:\n",
    "        return None, None\n",
    "    if Xr.shape[1] != Xa.shape[1]:\n",
    "        raise ValueError(f\"Overlap requires same dim: mu_rna {Xr.shape} vs mu_atac {Xa.shape}\")\n",
    "    n = int(min(Xr.shape[0], Xa.shape[0]))\n",
    "    X = np.vstack([Xr[:n], Xa[:n]])\n",
    "    mod = np.array([\"rna\"] * n + [\"atac\"] * n, dtype=object)\n",
    "    return X, mod\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MoE gating gradient utilities (robust to missing mods)\n",
    "# -----------------------------------------------------------------------------\n",
    "def _get_moe_gating(enc):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      G: (N, M) float32\n",
    "      mods: list[str] length M (best-effort)\n",
    "    or (None, None) if missing.\n",
    "    \"\"\"\n",
    "    if not isinstance(enc, dict):\n",
    "        return None, None\n",
    "\n",
    "    G = _as_numpy_2d(enc.get(\"moe_gating\", None))\n",
    "    if G is None or G.ndim != 2 or G.shape[1] < 2:\n",
    "        return None, None\n",
    "\n",
    "    mods = enc.get(\"moe_gating_mods\", None)\n",
    "    if mods is None:\n",
    "        # Best-effort default for common 2-modality case\n",
    "        if G.shape[1] == 2:\n",
    "            mods = [\"rna\", \"atac\"]\n",
    "        else:\n",
    "            mods = [f\"mod{i}\" for i in range(G.shape[1])]\n",
    "    else:\n",
    "        mods = [str(x) for x in list(mods)]\n",
    "        if len(mods) < G.shape[1]:\n",
    "            mods = mods + [f\"mod{i}\" for i in range(len(mods), G.shape[1])]\n",
    "\n",
    "    return G.astype(np.float32, copy=False), mods\n",
    "\n",
    "def _rna_atac_delta_from_gating(enc):\n",
    "    \"\"\"\n",
    "    delta = w_rna - w_atac in [-1,1]\n",
    "    Returns (delta, w_rna, w_atac) or (None,None,None) if unavailable.\n",
    "    \"\"\"\n",
    "    G, mods = _get_moe_gating(enc)\n",
    "    if G is None:\n",
    "        return None, None, None\n",
    "\n",
    "    if \"rna\" in mods and \"atac\" in mods:\n",
    "        i_rna = mods.index(\"rna\")\n",
    "        i_atac = mods.index(\"atac\")\n",
    "    else:\n",
    "        # fallback (only if you ever have unnamed mods)\n",
    "        i_rna, i_atac = 0, 1\n",
    "\n",
    "    w_rna = G[:, i_rna].astype(np.float32, copy=False)\n",
    "    w_atac = G[:, i_atac].astype(np.float32, copy=False)\n",
    "\n",
    "    s = (w_rna + w_atac).clip(1e-8)\n",
    "    w_rna = w_rna / s\n",
    "    w_atac = w_atac / s\n",
    "\n",
    "    delta = (w_rna - w_atac).astype(np.float32, copy=False)\n",
    "    delta = np.clip(delta, -1.0, 1.0)\n",
    "    return delta, w_rna, w_atac\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main saver: ONLY TWO FIGURES\n",
    "# -----------------------------------------------------------------------------\n",
    "def save_umaps_for_enc(\n",
    "    enc,\n",
    "    *,\n",
    "    y_all,\n",
    "    out_png,  # base path; we'll append suffixes\n",
    "    seed=0,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.3,\n",
    "    point_size=8,\n",
    "    alpha=0.75,\n",
    "    dpi=450,\n",
    "    legend_max=25,\n",
    "    sort_legend_by=\"count\",\n",
    "    overlap_metric: str = \"euclidean\",\n",
    "\n",
    "    # --- NEW: make gradient visible ---\n",
    "    gradient_cmap: str = \"bwr\",\n",
    "    gradient_alpha: float = 0.95,     # higher opacity for gradient panel\n",
    "    gradient_point_size: int = None,  # default: 1.2x point_size\n",
    "    gradient_gain: float = 6.0,       # boost (w_rna - w_atac) before clipping\n",
    "    gradient_vlim: float = 0.5,       # clip range; smaller => more saturated\n",
    "):\n",
    "    \"\"\"\n",
    "    Outputs ONLY:\n",
    "      1) <base>__spaces_celltype.png   (3 cols: RNA / ATAC / MoE, colored by cell type)\n",
    "      2) <base>__summary_4col.png      (4 cols: overlap mod / overlap ct / MoE gradient / MoE ct)\n",
    "    \"\"\"\n",
    "    if enc is None or not isinstance(enc, dict):\n",
    "        return\n",
    "\n",
    "    out_dir = os.path.dirname(out_png)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    base = out_png[:-4] if out_png.lower().endswith(\".png\") else out_png\n",
    "\n",
    "    # ---- latents ----\n",
    "    X_rna  = _as_numpy_2d(enc.get(\"mu_rna\", None))\n",
    "    X_atac = _as_numpy_2d(enc.get(\"mu_atac\", None))\n",
    "\n",
    "    X_moe = enc.get(\"mu_moe\", None)\n",
    "    if X_moe is None:\n",
    "        X_moe = enc.get(\"mu_fused\", None)\n",
    "    X_moe = _as_numpy_2d(X_moe)\n",
    "\n",
    "    # ---- cell type labels ----\n",
    "    labs_ct = _get_celltype_labels_from_enc(enc, y_all)\n",
    "    if labs_ct is not None:\n",
    "        c_ct, uniq_ct, counts_ct, cmap_ct, _ = _make_color_codes(\n",
    "            labs_ct, sort_legend_by=sort_legend_by, cmap_name=\"tab20\"\n",
    "        )\n",
    "        legend_ct = _legend_handles_from_uniq(uniq_ct, counts_ct, cmap_ct, legend_max=legend_max)\n",
    "    else:\n",
    "        c_ct, cmap_ct, legend_ct = None, None, None\n",
    "\n",
    "    # =============================================================================\n",
    "    # FIGURE 1: spaces (RNA / ATAC / MoE), colored by cell type\n",
    "    # =============================================================================\n",
    "    fig = plt.figure(figsize=(18, 5), dpi=int(dpi))\n",
    "    axes = [fig.add_subplot(1, 3, i + 1) for i in range(3)]\n",
    "\n",
    "    for ax, X, title in [\n",
    "        (axes[0], X_rna,  \"RNA latent (mu_rna)\"),\n",
    "        (axes[1], X_atac, \"ATAC latent (mu_atac)\"),\n",
    "        (axes[2], X_moe,  \"MoE fused latent (mu_moe)\"),\n",
    "    ]:\n",
    "        if X is None:\n",
    "            ax.set_title(f\"{title} (missing)\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        emb = _umap_embed(X, seed=seed, n_neighbors=n_neighbors, min_dist=min_dist, metric=\"euclidean\")\n",
    "        _plot_umap_scatter(\n",
    "            ax, emb,\n",
    "            color_codes=c_ct, cmap=cmap_ct,\n",
    "            title=title,\n",
    "            point_size=point_size,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "    if legend_ct is not None:\n",
    "        fig.legend(\n",
    "            handles=legend_ct, loc=\"center left\", bbox_to_anchor=(1.01, 0.5),\n",
    "            frameon=False, title=f\"Cell types (top {min(int(legend_max), len(legend_ct))})\"\n",
    "        )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(base + \"__spaces_celltype.png\", dpi=int(dpi), bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # =============================================================================\n",
    "    # FIGURE 2: summary 4 columns\n",
    "    #   col1: overlap UMAP colored by modality\n",
    "    #   col2: overlap UMAP colored by cell type\n",
    "    #   col3: MoE UMAP colored by RNA<->ATAC gating gradient (delta=w_rna-w_atac)\n",
    "    #   col4: MoE UMAP colored by cell type\n",
    "    # =============================================================================\n",
    "    X_overlap, overlap_mod_labels = _stack_for_overlap(enc)\n",
    "    emb_overlap = None\n",
    "    if X_overlap is not None:\n",
    "        emb_overlap = _umap_embed(\n",
    "            X_overlap,\n",
    "            seed=seed,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_dist=min_dist,\n",
    "            metric=str(overlap_metric),\n",
    "        )\n",
    "\n",
    "    c_ov_mod, cmap_ov_mod, legend_ov_mod = None, None, None\n",
    "    if overlap_mod_labels is not None:\n",
    "        c_ov_mod, uniq_ov_mod, counts_ov_mod, cmap_ov_mod, _ = _make_color_codes(\n",
    "            overlap_mod_labels, sort_legend_by=\"alpha\", cmap_name=\"Set1\"\n",
    "        )\n",
    "        legend_ov_mod = _legend_handles_from_uniq(uniq_ov_mod, counts_ov_mod, cmap_ov_mod, legend_max=10)\n",
    "\n",
    "    c_ov_ct, cmap_ov_ct = None, None\n",
    "    if emb_overlap is not None and labs_ct is not None:\n",
    "        labs_overlap_ct = np.concatenate([labs_ct, labs_ct], axis=0)\n",
    "        c_ov_ct, _, _, cmap_ov_ct, _ = _make_color_codes(\n",
    "            labs_overlap_ct, sort_legend_by=sort_legend_by, cmap_name=\"tab20\"\n",
    "        )\n",
    "\n",
    "    emb_moe = None\n",
    "    if X_moe is not None:\n",
    "        emb_moe = _umap_embed(\n",
    "            X_moe,\n",
    "            seed=seed,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_dist=min_dist,\n",
    "            metric=\"euclidean\",\n",
    "        )\n",
    "\n",
    "    # gradient values for MoE panel\n",
    "    delta, w_rna, w_atac = _rna_atac_delta_from_gating(enc)\n",
    "    if emb_moe is not None and delta is not None:\n",
    "        m = int(min(len(delta), emb_moe.shape[0]))\n",
    "        delta = delta[:m]\n",
    "        emb_moe_use = emb_moe[:m]\n",
    "    else:\n",
    "        emb_moe_use = emb_moe\n",
    "\n",
    "    # --- visibility controls for gradient panel ---\n",
    "    g_alpha = float(gradient_alpha)\n",
    "    g_size = int(max(1, (int(point_size) if gradient_point_size is None else int(gradient_point_size))))\n",
    "    if gradient_point_size is None:\n",
    "        g_size = int(np.ceil(1.25 * g_size))\n",
    "\n",
    "    # boost + clip to make colors visible even if delta is near 0\n",
    "    vlim = float(gradient_vlim)\n",
    "    gain = float(gradient_gain)\n",
    "\n",
    "    fig = plt.figure(figsize=(28, 5), dpi=int(dpi))\n",
    "    ax1 = fig.add_subplot(1, 4, 1)\n",
    "    ax2 = fig.add_subplot(1, 4, 2)\n",
    "    ax3 = fig.add_subplot(1, 4, 3)\n",
    "    ax4 = fig.add_subplot(1, 4, 4)\n",
    "\n",
    "    # col1: overlap by modality\n",
    "    if emb_overlap is None:\n",
    "        ax1.set_title(\"Overlap (RNA+ATAC stacked) â€” colored by modality (missing)\")\n",
    "        ax1.axis(\"off\")\n",
    "    else:\n",
    "        _plot_umap_scatter(\n",
    "            ax1, emb_overlap,\n",
    "            color_codes=c_ov_mod, cmap=cmap_ov_mod,\n",
    "            title=\"Overlap (RNA+ATAC stacked) â€” colored by modality\",\n",
    "            point_size=point_size, alpha=alpha\n",
    "        )\n",
    "        if legend_ov_mod is not None:\n",
    "            ax1.legend(handles=legend_ov_mod, bbox_to_anchor=(1.02, 1), loc=\"upper left\", frameon=False)\n",
    "\n",
    "    # col2: overlap by cell type\n",
    "    if emb_overlap is None:\n",
    "        ax2.set_title(\"Overlap (RNA+ATAC stacked) â€” colored by cell type (missing)\")\n",
    "        ax2.axis(\"off\")\n",
    "    else:\n",
    "        title2 = \"Overlap (RNA+ATAC stacked) â€” colored by cell type\"\n",
    "        if labs_ct is None:\n",
    "            title2 += \" (labels missing)\"\n",
    "        _plot_umap_scatter(\n",
    "            ax2, emb_overlap,\n",
    "            color_codes=c_ov_ct if labs_ct is not None else None,\n",
    "            cmap=cmap_ov_ct if labs_ct is not None else None,\n",
    "            title=title2,\n",
    "            point_size=point_size, alpha=alpha\n",
    "        )\n",
    "\n",
    "    # col3: MoE gating gradient (VISIBLE)\n",
    "    if emb_moe_use is None:\n",
    "        ax3.set_title(\"MoE fused â€” RNAâ†”ATAC gating gradient (missing)\")\n",
    "        ax3.axis(\"off\")\n",
    "    else:\n",
    "        if delta is None:\n",
    "            ax3.set_title(\"MoE fused â€” RNAâ†”ATAC gating gradient (gating missing)\")\n",
    "            ax3.scatter(emb_moe_use[:, 0], emb_moe_use[:, 1], s=g_size, alpha=max(g_alpha, 0.85), linewidths=0)\n",
    "            ax3.set_xlabel(\"UMAP1\"); ax3.set_ylabel(\"UMAP2\")\n",
    "        else:\n",
    "            # boost + clip, then two-slope normalize around 0\n",
    "            score = (delta.astype(np.float32, copy=False) * gain)\n",
    "            score = np.clip(score, -vlim, vlim)\n",
    "            norm = TwoSlopeNorm(vmin=-vlim, vcenter=0.0, vmax=vlim)\n",
    "\n",
    "            sc3 = ax3.scatter(\n",
    "                emb_moe_use[:, 0], emb_moe_use[:, 1],\n",
    "                c=score,\n",
    "                cmap=str(gradient_cmap),\n",
    "                norm=norm,\n",
    "                s=g_size,\n",
    "                alpha=g_alpha,\n",
    "                linewidths=0,\n",
    "            )\n",
    "            ax3.set_title(\"MoE fused â€” gating gradient (w_RNA - w_ATAC)\")\n",
    "            ax3.set_xlabel(\"UMAP1\"); ax3.set_ylabel(\"UMAP2\")\n",
    "            cb = fig.colorbar(sc3, ax=ax3, fraction=0.046, pad=0.04)\n",
    "            cb.set_label(\"w_RNA - w_ATAC  (red=RNA, blue=ATAC)\")\n",
    "\n",
    "    # col4: MoE by cell type\n",
    "    if emb_moe_use is None:\n",
    "        ax4.set_title(\"MoE fused â€” colored by cell type (missing)\")\n",
    "        ax4.axis(\"off\")\n",
    "    else:\n",
    "        _plot_umap_scatter(\n",
    "            ax4, emb_moe_use,\n",
    "            color_codes=c_ct, cmap=cmap_ct,\n",
    "            title=\"MoE fused â€” colored by cell type\",\n",
    "            point_size=point_size, alpha=alpha\n",
    "        )\n",
    "\n",
    "    if legend_ct is not None:\n",
    "        fig.legend(\n",
    "            handles=legend_ct, loc=\"center left\", bbox_to_anchor=(1.01, 0.5),\n",
    "            frameon=False, title=f\"Cell types (top {min(int(legend_max), len(legend_ct))})\"\n",
    "        )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(base + \"__summary_4col.png\", dpi=int(dpi), bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f103f28-8f40-48dd-97a3-875594b2c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 12) One ablation run (STRICT MoE only; no avg fallback) ----\n",
    "# =============================================================================\n",
    "def run_one_ablation(\n",
    "    ablate_cell_type=None,\n",
    "    drop_modality: str = \"atac\",\n",
    "    *,\n",
    "    seed: int = 0,\n",
    "    fuse_mode: str = \"moe\",     # kept for API; \"moe\" is what we enforce\n",
    "    return_enc: bool = False,\n",
    "    debug_first_batch: bool = False,\n",
    "    acc_knn_k: int = 3,\n",
    "\n",
    "    save_umaps: bool = True,\n",
    "    umap_dir: str = \"results/fig9_ablation_analysis_results_MoE/figures\",\n",
    "    umap_seed: Optional[int] = None,\n",
    "    umap_n_neighbors: int = 15,\n",
    "    umap_min_dist: float = 0.3,\n",
    "    umap_point_size: int = 6,\n",
    "    umap_alpha: float = 0.75,\n",
    "    umap_dpi: int = 450,\n",
    "    umap_legend_max: int = 25,\n",
    "    sort_legend_by: str = \"count\",\n",
    "\n",
    "    print_moe_status: bool = True,\n",
    "):\n",
    "    baseline = (ablate_cell_type is None) or (str(ablate_cell_type).lower() in {\"none\", \"__none__\", \"__baseline__\", \"baseline\"})\n",
    "    dummy = \"__NO_SUCH_CELLTYPE__\"\n",
    "\n",
    "    train_loader, val_loader, test_loader = make_loaders_for_celltype_ablation(\n",
    "        dataset, TRAIN_IDX, VAL_IDX, TEST_IDX,\n",
    "        y_all=y_all,\n",
    "        ablate_cell_type=(dummy if baseline else str(ablate_cell_type)),\n",
    "        drop_modality=str(drop_modality),\n",
    "        seed=int(seed),\n",
    "        batch_size=int(BATCH_SIZE),\n",
    "        disable_ablation=bool(baseline),\n",
    "    )\n",
    "\n",
    "    rna_dim  = int(adata_dict[\"rna\"].X.shape[1])\n",
    "    atac_dim = int(adata_dict[\"atac\"].X.shape[1])\n",
    "    dev = torch.device(str(DEVICE)) if isinstance(DEVICE, str) else DEVICE\n",
    "\n",
    "    model = train_one_model(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        rna_dim=rna_dim,\n",
    "        atac_dim=atac_dim,\n",
    "        seed=int(seed),\n",
    "        device=dev,\n",
    "        loss_mode=\"v1\",\n",
    "        v1_recon=\"moe\",          # IMPORTANT: train with moe recon\n",
    "        best_epoch_warmup=50,\n",
    "    )\n",
    "\n",
    "    # ---- ENCODE (this is where you encode) ----\n",
    "    enc_val = encode_embeddings_with_labels(\n",
    "        model, val_loader, device=dev, y_all=y_all, id_to_int=None, debug_first_batch=bool(debug_first_batch),\n",
    "    )\n",
    "    enc_test = encode_embeddings_with_labels(\n",
    "        model, test_loader, device=dev, y_all=y_all, id_to_int=enc_val.get(\"id_to_int\", None), debug_first_batch=False,\n",
    "    )\n",
    "\n",
    "    if print_moe_status:\n",
    "        ab_name = \"NONE\" if baseline else str(ablate_cell_type)\n",
    "        print(f\"[ablation] drop={drop_modality} ablate={ab_name} seed={seed}\")\n",
    "        report_moe_status(enc_test)\n",
    "\n",
    "    # ---- STRICT: require MoE latent or crash ----\n",
    "    if bool(STRICT_MOE):\n",
    "        Z_val  = compute_moe_latent_strict(enc_val)\n",
    "        Z_test = compute_moe_latent_strict(enc_test)\n",
    "    else:\n",
    "        # explicit non-avg fallback (model fused)\n",
    "        Z_val  = compute_fused_latent_model(enc_val)\n",
    "        Z_test = compute_fused_latent_model(enc_test)\n",
    "\n",
    "    # ---- Save UMAPs (MoE only) ----\n",
    "    if bool(save_umaps):\n",
    "        u_seed = int(seed if umap_seed is None else umap_seed)\n",
    "        ab_name = \"NONE\" if baseline else str(ablate_cell_type)\n",
    "        ab_name_safe = ab_name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "        drop_safe = str(drop_modality).replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "        os.makedirs(umap_dir, exist_ok=True)\n",
    "\n",
    "        out_png = os.path.join(\n",
    "            umap_dir,\n",
    "            f\"drop-{drop_safe}__ablate-{ab_name_safe}__seed-{int(seed)}__MOE_latents.png\"\n",
    "        )\n",
    "        # replace mu_fused plot with mu_moe plot\n",
    "        enc_for_plot = dict(enc_test)\n",
    "        # ensure mu_moe exists if strict\n",
    "        enc_for_plot[\"mu_moe\"] = enc_test.get(\"mu_moe\", None)\n",
    "\n",
    "        save_umaps_for_enc(\n",
    "            enc_for_plot,\n",
    "            y_all=y_all,\n",
    "            out_png=out_png,\n",
    "            seed=u_seed,\n",
    "            n_neighbors=int(umap_n_neighbors),\n",
    "            min_dist=float(umap_min_dist),\n",
    "            point_size=int(umap_point_size),\n",
    "            alpha=float(umap_alpha),\n",
    "            dpi=int(umap_dpi),\n",
    "            legend_max=int(umap_legend_max),\n",
    "            sort_legend_by=str(sort_legend_by),\n",
    "        )\n",
    "\n",
    "    # ---- Metrics ----\n",
    "    mu_rna_test  = enc_test.get(\"mu_rna\", None)\n",
    "    mu_atac_test = enc_test.get(\"mu_atac\", None)\n",
    "\n",
    "    y_val  = enc_val.get(\"y_fused\", enc_val.get(\"y\", None))\n",
    "    y_test = enc_test.get(\"y_fused\", enc_test.get(\"y\", None))\n",
    "\n",
    "    row = {\n",
    "        \"drop_modality\": str(drop_modality),\n",
    "        \"ablated_cell_type\": (\"NONE\" if baseline else str(ablate_cell_type)),\n",
    "        \"fuse_mode\": \"moe_strict\" if STRICT_MOE else \"model_fused\",\n",
    "        \"ACC_fused_mode\": f\"knn_val_to_test_k{int(acc_knn_k)}\",\n",
    "    }\n",
    "\n",
    "    # Alignment metrics on paired TEST (still based on mu_rna/mu_atac)\n",
    "    if mu_rna_test is None or mu_atac_test is None:\n",
    "        row[\"FOSCTTM\"] = np.nan\n",
    "        for k in RECALL_KS:\n",
    "            row[f\"Recall@{k}\"] = np.nan\n",
    "    else:\n",
    "        row[\"FOSCTTM\"] = float(foscttm(mu_rna_test, mu_atac_test))\n",
    "        for k, v in recall_at_k(mu_rna_test, mu_atac_test, ks=RECALL_KS).items():\n",
    "            row[f\"Recall@{k}\"] = float(v)\n",
    "\n",
    "    # Fused (MoE) metrics\n",
    "    if Z_test is None or y_test is None:\n",
    "        row.update({\n",
    "            \"ARI_fused\": np.nan,\n",
    "            \"NMI_fused\": np.nan,\n",
    "            \"ACC_fused\": np.nan,\n",
    "            \"ACC_fused_macroF1\": np.nan,\n",
    "            \"SIL_fused_celltype\": np.nan,\n",
    "            \"SIL_fused_n\": 0,\n",
    "            \"SIL_fused_frac\": 0.0,\n",
    "        })\n",
    "    else:\n",
    "        X_test = _as_numpy_2d(Z_test)\n",
    "        y_test_np = _as_numpy_1d(y_test)\n",
    "\n",
    "        # KMeans ARI/NMI\n",
    "        try:\n",
    "            n_clusters = int(len(np.unique(y_test_np)))\n",
    "            if n_clusters >= 2 and X_test.shape[0] >= n_clusters:\n",
    "                km = KMeans(n_clusters=n_clusters, random_state=int(seed), n_init=20)\n",
    "                y_pred = km.fit_predict(X_test)\n",
    "                row[\"ARI_fused\"] = float(adjusted_rand_score(y_test_np, y_pred))\n",
    "                row[\"NMI_fused\"] = float(normalized_mutual_info_score(y_test_np, y_pred))\n",
    "            else:\n",
    "                row[\"ARI_fused\"] = np.nan\n",
    "                row[\"NMI_fused\"] = np.nan\n",
    "        except Exception:\n",
    "            row[\"ARI_fused\"] = np.nan\n",
    "            row[\"NMI_fused\"] = np.nan\n",
    "\n",
    "        # kNN ACC (val -> test)\n",
    "        knn_out = {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "        if Z_val is not None and y_val is not None:\n",
    "            try:\n",
    "                knn_out = knn_acc_from_reference(Z_val, y_val, Z_test, y_test, k=int(acc_knn_k))\n",
    "            except Exception:\n",
    "                knn_out = {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "\n",
    "        row[\"ACC_fused\"] = float(knn_out.get(\"acc\", np.nan))\n",
    "        row[\"ACC_fused_macroF1\"] = float(knn_out.get(\"macro_f1\", np.nan))\n",
    "\n",
    "        sil, sil_n, sil_frac = safe_silhouette_by_true_labels(X_test, y_test_np, min_per_class=2, min_total=10)\n",
    "        row[\"SIL_fused_celltype\"] = float(sil)\n",
    "        row[\"SIL_fused_n\"] = int(sil_n)\n",
    "        row[\"SIL_fused_frac\"] = float(sil_frac)\n",
    "\n",
    "    if return_enc:\n",
    "        return row, {\"enc_val\": enc_val, \"enc_test\": enc_test, \"model\": model}\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580936d1-6401-4a1b-aa6c-99e7eaca9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 13) Ablation grid runner ----\n",
    "# =============================================================================\n",
    "def run_ablation_grid(\n",
    "    *,\n",
    "    drop_modalities=(\"rna\", \"atac\"),\n",
    "    seed: int = 0,\n",
    "    save_umaps: bool = True,\n",
    "    umap_dir: str = \"results/fig9_ablation_analysis_results_MoE/figures\",\n",
    "):\n",
    "    cell_types = sorted(pd.unique(y_all))\n",
    "    rows = []\n",
    "    for dm in drop_modalities:\n",
    "        for ct in cell_types:\n",
    "            print(f\"[grid] drop_modality={dm:>4s}  ablate_cell_type={ct}\")\n",
    "            row, _out = run_one_ablation(\n",
    "                ct, dm,\n",
    "                seed=int(seed),\n",
    "                return_enc=bool(save_umaps),\n",
    "                save_umaps=bool(save_umaps),\n",
    "                umap_dir=str(umap_dir),\n",
    "            )\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76b15b-3dce-4ce4-bc05-c9e5908c95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 14) Example sanity checks ----\n",
    "# =============================================================================\n",
    "# Baseline (should PASS only if enc['mu_moe'] exists; otherwise it will crash loudly)\n",
    "row_base, out_base = run_one_ablation(None, \"atac\", seed=SEED, return_enc=True, debug_first_batch=True, save_umaps=True)\n",
    "print(row_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092efc16-f812-48c0-9cf8-9c03e0ce2e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Smoke test: ablate the first cell type\n",
    "cell_types = sorted(pd.unique(y_all))\n",
    "ct = cell_types[0]\n",
    "print(f\"Performing a smoke-test by ablating {ct} from atac.\")\n",
    "\n",
    "row_smoke, out_smoke = run_one_ablation(ct, \"atac\", seed=SEED, return_enc=True, save_umaps=False)\n",
    "print(row_smoke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407807f-c717-4a60-ad57-234bc18c5319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 15) Run grid + save ----\n",
    "# =============================================================================\n",
    "df_grid = run_ablation_grid(\n",
    "    drop_modalities=(\"rna\", \"atac\"),\n",
    "    seed=SEED,\n",
    "    save_umaps=True,\n",
    "    umap_dir=\"results/fig9_ablation_analysis_results_MoE/figures\",\n",
    ")\n",
    "print(df_grid.head())\n",
    "\n",
    "OUTDIR = \"./results/fig9_ablation_analysis_results_MoE\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "csv_path = os.path.join(OUTDIR, \"celltype_ablation_grid_metrics.csv\")\n",
    "df_grid.to_csv(csv_path, index=False)\n",
    "print(\"Wrote:\", csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17757b4-1b46-4f3b-a37c-c027b9b75d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ---- 16) REST OF ANALYSIS: heatmaps + coarse labels + saving ----\n",
    "# =============================================================================\n",
    "def plot_metric_heatmap(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    drop_modality: str,\n",
    "    *,\n",
    "    title_prefix: str = \"\",\n",
    "    sort_by: str = \"ablated_cell_type\",\n",
    "    dpi: int = 250,\n",
    "):\n",
    "    sub = df[df[\"drop_modality\"] == drop_modality].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"[skip] no rows for drop_modality={drop_modality}\")\n",
    "        return\n",
    "    if metric not in sub.columns:\n",
    "        print(f\"[skip] metric {metric} not in df\")\n",
    "        return\n",
    "\n",
    "    if sort_by in sub.columns:\n",
    "        sub[sort_by] = sub[sort_by].astype(str)\n",
    "        sub = sub.sort_values(sort_by)\n",
    "\n",
    "    x = sub[\"ablated_cell_type\"].astype(str).to_list()\n",
    "    vals = pd.to_numeric(sub[metric], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    M = vals[None, :]\n",
    "\n",
    "    plt.figure(figsize=(max(10, 0.45 * len(x)), 2.4), dpi=int(dpi))\n",
    "    im = plt.imshow(M, aspect=\"auto\")\n",
    "    plt.yticks([0], [metric])\n",
    "    plt.xticks(np.arange(len(x)), x, rotation=90)\n",
    "    plt.colorbar(im, fraction=0.03, pad=0.02)\n",
    "\n",
    "    prefix = (title_prefix + \" â€” \") if title_prefix else \"\"\n",
    "    plt.title(f\"{prefix}{metric} â€” ablate {drop_modality} in TRAIN (by cell type)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_metric_heatmap(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    drop_modality: str,\n",
    "    *,\n",
    "    title_prefix: str = \"\",\n",
    "    outdir: str = OUTDIR,\n",
    "    dpi: int = 250,\n",
    "):\n",
    "    sub = df[df[\"drop_modality\"] == drop_modality].copy()\n",
    "    if sub.empty or metric not in sub.columns:\n",
    "        return None\n",
    "\n",
    "    sub[\"ablated_cell_type\"] = sub[\"ablated_cell_type\"].astype(str)\n",
    "    sub = sub.sort_values(\"ablated_cell_type\")\n",
    "\n",
    "    x = sub[\"ablated_cell_type\"].to_list()\n",
    "    vals = pd.to_numeric(sub[metric], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    M = vals[None, :]\n",
    "\n",
    "    plt.figure(figsize=(max(10, 0.45 * len(x)), 2.4), dpi=int(dpi))\n",
    "    im = plt.imshow(M, aspect=\"auto\")\n",
    "    plt.yticks([0], [metric])\n",
    "    plt.xticks(np.arange(len(x)), x, rotation=90)\n",
    "    plt.colorbar(im, fraction=0.03, pad=0.02)\n",
    "\n",
    "    prefix = (title_prefix + \" â€” \") if title_prefix else \"\"\n",
    "    plt.title(f\"{prefix}{metric} â€” ablate {drop_modality} in TRAIN (by cell type)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fname = f\"heatmap_{title_prefix}_{drop_modality}_{metric}\".replace(\" \", \"_\").replace(\"@\", \"at\").replace(\"/\", \"_\")\n",
    "    path = os.path.join(outdir, fname + \".png\")\n",
    "    plt.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "def plot_big_heatmap_shared_scale(\n",
    "    df: pd.DataFrame,\n",
    "    metrics,\n",
    "    drop_modality: str,\n",
    "    *,\n",
    "    title: str = \"\",\n",
    "    sort_by: str = \"ablated_cell_type\",\n",
    "    vmin: float = -0.1,\n",
    "    vmax: float = 0.9,\n",
    "    cmap: str = \"viridis\",\n",
    "    show_nan_as: Optional[float] = None,\n",
    "    dpi: int = 200,\n",
    "):\n",
    "    sub = df[df[\"drop_modality\"] == drop_modality].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"[skip] no rows for drop_modality={drop_modality}\")\n",
    "        return\n",
    "\n",
    "    if sort_by in sub.columns:\n",
    "        sub[sort_by] = sub[sort_by].astype(str)\n",
    "        sub = sub.sort_values(sort_by)\n",
    "\n",
    "    xlabels = sub[\"ablated_cell_type\"].astype(str).to_list()\n",
    "    metrics_use = [m for m in metrics if m in sub.columns]\n",
    "    if not metrics_use:\n",
    "        print(\"[skip] none of the requested metrics exist in df columns\")\n",
    "        return\n",
    "\n",
    "    M = np.full((len(metrics_use), len(xlabels)), np.nan, dtype=float)\n",
    "    for i, m in enumerate(metrics_use):\n",
    "        M[i, :] = pd.to_numeric(sub[m], errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "    if show_nan_as is not None:\n",
    "        M = np.where(np.isfinite(M), M, float(show_nan_as))\n",
    "\n",
    "    fig_w = max(10, 0.30 * len(xlabels))\n",
    "    fig_h = max(4, 0.50 * len(metrics_use))\n",
    "    plt.figure(figsize=(fig_w, fig_h), dpi=int(dpi))\n",
    "\n",
    "    im = plt.imshow(M, aspect=\"auto\", vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    plt.colorbar(im, label=f\"value (shared scale [{vmin}, {vmax}])\")\n",
    "\n",
    "    plt.yticks(np.arange(len(metrics_use)), metrics_use)\n",
    "    plt.xticks(np.arange(len(xlabels)), xlabels, rotation=90)\n",
    "\n",
    "    plt.title(title or f\"BIG heatmap â€” drop {drop_modality} â€” shared scale [{vmin}, {vmax}]\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "METRICS_TO_PLOT = [\n",
    "    \"FOSCTTM\",\n",
    "    \"Recall@1\", \"Recall@10\", \"Recall@25\", \"Recall@50\", \"Recall@100\",\n",
    "    \"ARI_fused\", \"NMI_fused\", \"ACC_fused\", \"SIL_fused_celltype\",\n",
    "]\n",
    "'''\n",
    "for dm in (\"rna\", \"atac\"):\n",
    "    for m in METRICS_TO_PLOT:\n",
    "        if m in df_grid.columns:\n",
    "            plot_metric_heatmap(df_grid, m, dm, title_prefix=\"FINE\")\n",
    "\n",
    "for dm in (\"rna\", \"atac\"):\n",
    "    for m in METRICS_TO_PLOT:\n",
    "        p = save_metric_heatmap(df_grid, m, dm, title_prefix=\"FINE\")\n",
    "        if p is not None:\n",
    "            print(\"Saved:\", p)\n",
    "'''\n",
    "for dm in (\"rna\", \"atac\"):\n",
    "    plot_big_heatmap_shared_scale(\n",
    "        df_grid, METRICS_TO_PLOT, drop_modality=dm,\n",
    "        title=f\"FINE â€” drop {dm} â€” shared scale [-0.05, 1.0]\",\n",
    "        vmin=-0.05, vmax=1.0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f3f37-5759-414a-b12e-0c3f327b6bfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- coarse labels ---\n",
    "COARSE_MAP = {\n",
    "    \"CD14 Mono\": \"Monocyte\",\n",
    "    \"CD16 Mono\": \"Monocyte\",\n",
    "    \"CD4 Naive\": \"CD4 T\",\n",
    "    \"CD4 TCM\":   \"CD4 T\",\n",
    "    \"CD4 TEM\":   \"CD4 T\",\n",
    "    \"CD8 Naive\": \"CD8 T\",\n",
    "    \"CD8 TEM_1\": \"CD8 T\",\n",
    "    \"CD8 TEM_2\": \"CD8 T\",\n",
    "    \"Treg\":      \"Other T\",\n",
    "    \"MAIT\":      \"Other T\",\n",
    "    \"gdT\":       \"Other T\",\n",
    "    \"NK\": \"NK\",\n",
    "    \"Naive B\":         \"B\",\n",
    "    \"Memory B\":        \"B\",\n",
    "    \"Intermediate B\":  \"B\",\n",
    "    \"cDC\": \"DC\",\n",
    "    \"pDC\": \"DC\",\n",
    "    \"HSPC\":  \"HSPC\",\n",
    "    \"Plasma\":\"Plasma\",\n",
    "}\n",
    "\n",
    "def collapse_celltypes(y_labels, mapping=COARSE_MAP, unknown=\"Other\"):\n",
    "    y = np.asarray(y_labels).astype(str)\n",
    "    return np.array([mapping.get(lbl, unknown) for lbl in y], dtype=object)\n",
    "\n",
    "y_all_coarse = collapse_celltypes(y_all)\n",
    "print(\"fine unique:\", len(pd.unique(y_all)))\n",
    "print(\"coarse unique:\", len(pd.unique(y_all_coarse)))\n",
    "print(pd.Series(y_all_coarse).value_counts())\n",
    "\n",
    "def run_ablation_grid_with_labels(\n",
    "    y_labels,\n",
    "    *,\n",
    "    drop_modalities=(\"rna\", \"atac\"),\n",
    "    seed: int = 0,\n",
    "):\n",
    "    global y_all\n",
    "    y_all_orig = y_all\n",
    "    try:\n",
    "        y_all = np.asarray(y_labels)\n",
    "        cell_types = sorted(pd.unique(y_all))\n",
    "        rows = []\n",
    "        for dm in drop_modalities:\n",
    "            for ct in cell_types:\n",
    "                print(f\"[grid] labels=COARSE  drop_modality={dm:>4s}  ablate_cell_type={ct}\")\n",
    "                rows.append(run_one_ablation(ct, dm, seed=seed, acc_knn_k=3))\n",
    "        return pd.DataFrame(rows)\n",
    "    finally:\n",
    "        y_all = y_all_orig\n",
    "\n",
    "df_grid_coarse = run_ablation_grid_with_labels(y_all_coarse, drop_modalities=(\"rna\", \"atac\"), seed=SEED)\n",
    "coarse_csv = os.path.join(OUTDIR, \"celltype_ablation_grid_metrics_COARSE.csv\")\n",
    "df_grid_coarse.to_csv(coarse_csv, index=False)\n",
    "print(\"Wrote:\", coarse_csv)\n",
    "'''\n",
    "for dm in (\"rna\", \"atac\"):\n",
    "    for m in METRICS_TO_PLOT:\n",
    "        if m in df_grid_coarse.columns:\n",
    "            plot_metric_heatmap(df_grid_coarse, m, dm, title_prefix=\"COARSE\")\n",
    "\n",
    "for dm in (\"rna\", \"atac\"):\n",
    "    for m in METRICS_TO_PLOT:\n",
    "        p = save_metric_heatmap(df_grid_coarse, m, dm, title_prefix=\"COARSE\")\n",
    "        if p is not None:\n",
    "            print(\"Saved:\", p)\n",
    "'''\n",
    "for dm in (\"rna\", \"atac\"):\n",
    "    plot_big_heatmap_shared_scale(\n",
    "        df_grid_coarse, METRICS_TO_PLOT, drop_modality=dm,\n",
    "        title=f\"COARSE â€” drop {dm} â€” shared scale [-0.5, 1.0]\",\n",
    "        vmin=-0.05, vmax=1.0\n",
    "    )\n",
    "\n",
    "print(\"Done âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2273727-7f06-4ce9-9222-efffee0d70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee9bd5-8cc4-4309-9470-03a8df038b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fd8b5-f928-4e9a-b329-28e6e799079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ca76e-c9b9-4331-a21a-c4334bcb2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c846b4-33a3-4d02-a547-d7429502ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ca8ca-2278-48d4-8f78-0558918106d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b581d7c-f126-4da0-8720-5de416f73f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d886e5-1133-482f-985c-df618e661350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0fb41e-6f6b-4db1-a63b-972a997271c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac7758-2866-4884-9597-03e3f1edfbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa3497-37e7-4d71-bdd0-cf175f0f781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b977c9-5db2-44e8-b197-03cb91317ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3641bda-4fa1-4364-9ca9-7c479bfc1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea4298-ba06-4224-83dc-12b26ed05d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f350974-3896-474c-9b0f-662e276c50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806b04e-6172-4066-b526-83bddd9963f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335aa8b-52e5-4fb6-a407-8a7a6347da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4caad19-ee4e-4554-9a2c-291b85546718",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aef0f2-0915-412a-991f-4e8713a27866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92917e41-021e-452c-abe2-d671f4c6dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f581b-6d15-4d1b-aa1c-1741ddbd10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de412ff0-b174-44d3-9b7c-13169ebc6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2d565-695f-4d57-8db0-ea667722f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111597c-660d-4145-b055-cb52345f98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417247dc-cfa2-4c5b-9d5b-d1be5a4f95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fae402-4683-41e0-a52f-c85689ab3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e6cc0-469a-42ae-9073-9bc42cf1bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c741b6-91e9-4808-ab9a-28c6ede9331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fc34f-39a3-4c38-91e5-cf0e37a70ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bda4c-0efb-4f63-beae-56ecf1c46f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479a06e-91a7-4c54-8d5f-144a970bc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e543f6-2157-450f-9c50-e088bef8e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfa57c-3865-4a9b-b48f-d1395d41f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e949eb7a-4138-4dd1-bb29-f2c41b2ef486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e10458-e913-47a8-a1c2-823776aab40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f02b62-701a-4ac6-90d6-3eb2029d162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d1861-06af-4807-8320-285cab0c2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65460964-7a08-4900-8165-e3b3d976083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca26f6b-1a61-42f1-9bda-7f62aa82373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7858bed-d8c2-44d2-b8bb-42212cfa6781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae5692-d2c4-4304-91ef-6564c7ea5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a439bcd-6e93-4510-994f-be00533c5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec86a4-07c3-4807-b8bb-01616fc5c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073ca8b-4bd3-43e4-9677-56abed901970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89119a92-201f-4993-837a-6b45d3063005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0013b6-96a8-42d8-92da-bbeb2ee9c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0c164-c79e-42de-94f9-6b84e21e7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84202c9-6474-4a43-94b2-ce9f3d49b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (UniVI v0.3.9)",
   "language": "python",
   "name": "univi_v0.3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
