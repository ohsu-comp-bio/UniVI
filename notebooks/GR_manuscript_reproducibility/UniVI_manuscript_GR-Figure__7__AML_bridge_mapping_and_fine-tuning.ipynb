{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e83ac317",
   "metadata": {},
   "source": [
    "## UniVI manuscript - Figure 7 generation reproducible workflow\n",
    "### UniVI AML bridging experiment (CITE-seq ↔ scRNA ↔ DAb-seq)\n",
    "\n",
    "Andrew Ashford, Pathways + Omics Group, Oregon Health & Science University, Portland, OR - 1/6/2026\n",
    "\n",
    "This notebook builds a shared UniVI latent space using:\n",
    "- Knorr et al. AML CITE-seq (RNA+ADT; paired)\n",
    "\n",
    "Then does a zero-shot application of the bridge-trained model on:\n",
    "- van Galen AML scRNA (RNA only)\n",
    "- Demaree DAb-seq (protein + genotype calls; no RNA)\n",
    "\n",
    "Key goals:\n",
    "1) Train UniVI on paired CITE RNA+ADT.\n",
    "2) Project van Galen RNA into the same latent via RNA encoder.\n",
    "3) Project DAb proteins into the same latent via ADT encoder (after panel harmonization).\n",
    "4) Validate with marker biology + mixing metrics.\n",
    "5) Treat mutations properly (no leakage; patient/timepoint group splits).\n",
    "6) Fine-tune model encoders using a mutation classification decoder and freeze decoders.\n",
    "7) Analyze before and after fine-tuning results for Figure 7 of the Genome Research manuscript.\n",
    "\n",
    "GitHub for the project can be found at: https://github.com/Ashford-A/UniVI\n",
    "\n",
    "Package is pip installable via the command: \n",
    "```bash\n",
    "pip install univi\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ed0f4-0dae-43fb-93b4-d3c52e0c0dbd",
   "metadata": {},
   "source": [
    "### Imports + global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0047b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# UniVI imports\n",
    "import univi as uv\n",
    "from univi import (\n",
    "    ModalityConfig, UniVIConfig, TrainingConfig,\n",
    "    UniVIMultiModalVAE, UniVITrainer,\n",
    "    MultiModalDataset,\n",
    ")\n",
    "import univi.evaluation as ue\n",
    "\n",
    "def seed_everything(seed: int = 1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"univi:\", uv.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"device:\", device)\n",
    "\n",
    "sc.set_figure_params(figsize=(6.5, 5.5), dpi=120, dpi_save=400, fontsize=11, frameon=False)\n",
    "plt.rcParams.update({\"savefig.bbox\": \"tight\", \"savefig.pad_inches\": 0.1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41a0ee-88de-423b-87e4-f003e652c0be",
   "metadata": {},
   "source": [
    "### Paths + key obs columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b24bdb-eb1a-45d8-b4cd-2e8902b1c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGDIR = Path(\"./results/fig7_aml_mosaic_reproducibility\")\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# INPUT PATHS (EDIT)\n",
    "# -----------------------------\n",
    "CITE_ROOT = Path(\"/home/groups/precepts/ashforda/UniVI_v2/UniVI_older-non_git/data/Knorr_AML_CITE-seq_data/GSE220473_RAW\") # wide csv.gz counts + assignments\n",
    "VG_PATH   = Path(\"/home/groups/precepts/ashforda/scOPE_github_stuff/data/testing/vanGalen_all_h5ad/vanGalen_anndata.h5ad\")\n",
    "DAB_ROOT  = Path(\"/home/groups/precepts/ashforda/UniVI_v2/UniVI_older-non_git/data/Demaree_DAb-seq_data\")\n",
    "\n",
    "print(\"CITE_ROOT:\", CITE_ROOT)\n",
    "print(\"VG_PATH:\", VG_PATH)\n",
    "print(\"DAB_ROOT:\", DAB_ROOT)\n",
    "\n",
    "# -----------------------------\n",
    "# CITE split grouping (avoid leakage)\n",
    "# -----------------------------\n",
    "# This will be created from *_cell_sample_assign.csv.gz (see attach_assignments below)\n",
    "CITE_GROUP_COL = \"sample_id\"\n",
    "\n",
    "# DAb-seq LOEO grouping column\n",
    "DAB_EXPT_COL   = \"experiment\"   # edit if your DAb obs uses another name\n",
    "\n",
    "# van Galen grouping column (patient/case) for mutation-head splits\n",
    "VG_GROUP_COL   = \"patient\"      # edit if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f368e4d-a387-43a8-8a9e-7b4ccb18d4b4",
   "metadata": {},
   "source": [
    "### CITE readers (wide csv.gz → AnnData) + assignment attach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323c045-33cc-430b-b7ee-cecf81cdbd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cite_id_from_name(name: str) -> str:\n",
    "    m = re.search(r\"_CITE(\\d+)_\", name)\n",
    "    if m is None:\n",
    "        raise ValueError(f\"Could not parse CITE id from filename: {name}\")\n",
    "    return f\"CITE{int(m.group(1))}\"\n",
    "\n",
    "def _sort_cite_ids(cite_ids):\n",
    "    return sorted(cite_ids, key=lambda x: int(x.replace(\"CITE\", \"\")))\n",
    "\n",
    "def read_wide_counts_csv_gz_to_adata(\n",
    "    path: Path,\n",
    "    chunksize_rows: int = 256,\n",
    "    dtype=np.int32,\n",
    ") -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Expects a wide matrix CSV with:\n",
    "      - rows = features (genes/proteins) in index column\n",
    "      - columns = cell barcodes\n",
    "    Produces AnnData with:\n",
    "      - obs = cells\n",
    "      - var = features\n",
    "      - X   = sparse CSR (cells x features)\n",
    "    \"\"\"\n",
    "    it = pd.read_csv(\n",
    "        path,\n",
    "        compression=\"gzip\",\n",
    "        index_col=0,\n",
    "        chunksize=int(chunksize_rows),\n",
    "    )\n",
    "\n",
    "    obs_names = None\n",
    "    var_names = []\n",
    "    blocks = []\n",
    "\n",
    "    for chunk in it:\n",
    "        # columns are cells\n",
    "        cols = chunk.columns.astype(str).tolist()\n",
    "        if obs_names is None:\n",
    "            obs_names = cols\n",
    "        else:\n",
    "            if cols != obs_names:\n",
    "                raise ValueError(f\"Column mismatch across chunks in {path.name}\")\n",
    "\n",
    "        # index is features\n",
    "        var_names.extend(chunk.index.astype(str).tolist())\n",
    "\n",
    "        Xc = chunk.to_numpy(dtype=dtype, copy=False)  # (n_feat_chunk, n_cells)\n",
    "        Xc = sp.csr_matrix(Xc.T)                      # (n_cells, n_feat_chunk)\n",
    "        blocks.append(Xc)\n",
    "\n",
    "    if obs_names is None:\n",
    "        raise ValueError(f\"No data read from {path}\")\n",
    "\n",
    "    X = sp.hstack(blocks, format=\"csr\") if len(blocks) > 1 else blocks[0].tocsr()\n",
    "    adata = ad.AnnData(\n",
    "        X=X,\n",
    "        obs=pd.DataFrame(index=pd.Index(obs_names, name=\"cell\")),\n",
    "        var=pd.DataFrame(index=pd.Index(var_names, name=\"feature\")),\n",
    "    )\n",
    "    return adata\n",
    "\n",
    "def read_cell_sample_assign(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads *_cell_sample_assign.csv.gz and returns a DF indexed by barcode/cell.\n",
    "    Tries hard to find the barcode column.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, compression=\"gzip\")\n",
    "    # choose barcode column\n",
    "    barcode_col = None\n",
    "    for c in df.columns:\n",
    "        cl = str(c).lower()\n",
    "        if \"barcode\" in cl or \"cell\" in cl:\n",
    "            barcode_col = c\n",
    "            break\n",
    "    if barcode_col is None:\n",
    "        barcode_col = df.columns[0]\n",
    "    df[barcode_col] = df[barcode_col].astype(str)\n",
    "    df = df.set_index(barcode_col)\n",
    "    return df\n",
    "\n",
    "def attach_assignments(adata: ad.AnnData, assign_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Joins assignment columns into .obs and creates a stable 'sample_id' column.\n",
    "    \"\"\"\n",
    "    pref = assign_df.add_prefix(\"assign_\")\n",
    "    adata.obs = adata.obs.join(pref, how=\"left\")\n",
    "\n",
    "    cand = None\n",
    "    for c in assign_df.columns:\n",
    "        cl = str(c).lower()\n",
    "        if \"sample\" in cl or \"donor\" in cl or \"patient\" in cl:\n",
    "            cand = c\n",
    "            break\n",
    "\n",
    "    if cand is None:\n",
    "        adata.obs[\"sample_id\"] = \"NA\"\n",
    "    else:\n",
    "        adata.obs[\"sample_id\"] = assign_df[cand].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3fee6-cd59-4184-9a1e-65582ef0802d",
   "metadata": {},
   "source": [
    "### Load all paired CITE libraries (RNA+ADT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f912d31-04aa-459f-af1a-50498019b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_files = list(CITE_ROOT.glob(\"*_RNA_counts.csv.gz\"))\n",
    "adt_files = list(CITE_ROOT.glob(\"*_ADT_counts.csv.gz\"))\n",
    "asn_files = list(CITE_ROOT.glob(\"*_cell_sample_assign.csv.gz\"))\n",
    "\n",
    "rna_map = {_cite_id_from_name(p.name): p for p in rna_files}\n",
    "adt_map = {_cite_id_from_name(p.name): p for p in adt_files}\n",
    "asn_map = {_cite_id_from_name(p.name): p for p in asn_files}\n",
    "\n",
    "cite_ids = _sort_cite_ids(set(rna_map) & set(adt_map) & set(asn_map))\n",
    "print(\"Found paired libraries:\", cite_ids)\n",
    "\n",
    "rna_parts, adt_parts = [], []\n",
    "for cid in cite_ids:\n",
    "    print(f\"\\n--- Reading {cid} ---\")\n",
    "    r = read_wide_counts_csv_gz_to_adata(rna_map[cid], chunksize_rows=256, dtype=np.int32)\n",
    "    a = read_wide_counts_csv_gz_to_adata(adt_map[cid], chunksize_rows=256, dtype=np.int32)\n",
    "\n",
    "    assign = read_cell_sample_assign(asn_map[cid])\n",
    "    attach_assignments(r, assign)\n",
    "    attach_assignments(a, assign)\n",
    "\n",
    "    r.obs[\"library_id\"] = cid\n",
    "    a.obs[\"library_id\"] = cid\n",
    "\n",
    "    common = r.obs_names.intersection(a.obs_names)\n",
    "    r = r[common].copy()\n",
    "    a = a[common].copy()\n",
    "    a = a[r.obs_names].copy()\n",
    "    assert (r.obs_names == a.obs_names).all()\n",
    "\n",
    "    rna_parts.append(r)\n",
    "    adt_parts.append(a)\n",
    "\n",
    "cite_rna = ad.concat(rna_parts, join=\"outer\", label=\"library_id\", keys=cite_ids, index_unique=\"-\")\n",
    "cite_adt = ad.concat(adt_parts, join=\"outer\", label=\"library_id\", keys=cite_ids, index_unique=\"-\")\n",
    "\n",
    "common_cells = cite_rna.obs_names.intersection(cite_adt.obs_names)\n",
    "cite_rna = cite_rna[common_cells].copy()\n",
    "cite_adt = cite_adt[common_cells].copy()\n",
    "cite_adt = cite_adt[cite_rna.obs_names].copy()\n",
    "assert (cite_rna.obs_names == cite_adt.obs_names).all()\n",
    "\n",
    "print(\"CITE RNA:\", cite_rna)\n",
    "print(\"CITE ADT:\", cite_adt)\n",
    "print(\"CITE sample_id nunique:\", cite_rna.obs[CITE_GROUP_COL].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade31b65-ce9d-40e4-a096-246c768300db",
   "metadata": {},
   "source": [
    "### Load van Galen + DAb-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3c515-babc-492b-bf0b-159dabfb5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_rna = sc.read_h5ad(str(VG_PATH))\n",
    "vg_rna.var_names_make_unique()\n",
    "vg_rna.obs_names_make_unique()\n",
    "print(\"VG:\", vg_rna)\n",
    "\n",
    "dab_files = {\n",
    "    \"fig2_pbmcs_ab_geno\":   \"fig2_pbmcs_ab_geno.h5ad\",\n",
    "    \"fig3_ab_geno\":         \"fig3_ab_geno.h5ad\",\n",
    "    \"fig4_ab_geno\":         \"fig4_ab_geno.h5ad\",\n",
    "    \"fig5_ab_geno\":         \"fig5_ab_geno.h5ad\",\n",
    "}\n",
    "\n",
    "dab_parts = {}\n",
    "for k, fn in dab_files.items():\n",
    "    fp = DAB_ROOT / fn\n",
    "    a = sc.read_h5ad(str(fp))\n",
    "    a.obs[\"dab_source\"] = k\n",
    "    dab_parts[k] = a\n",
    "    print(\"\\n---\", k, \"---\")\n",
    "    print(a)\n",
    "\n",
    "mergeable = [\"fig3_ab_geno\", \"fig5_ab_geno\"]\n",
    "dab_adt = ad.concat(\n",
    "    [dab_parts[k] for k in mergeable],\n",
    "    join=\"outer\",\n",
    "    axis=0,\n",
    "    label=\"dab_source\",\n",
    "    keys=mergeable,\n",
    "    merge=\"unique\",\n",
    "    fill_value=0,\n",
    ")\n",
    "print(\"\\nMerged DAb:\", dab_adt)\n",
    "print(\"DAb obs cols:\", len(dab_adt.obs.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4269d-1d38-4714-be2d-ca96f691d90e",
   "metadata": {},
   "source": [
    "### Canonicalize ADT feature names + build matched subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e80e5a-1bb0-442a-9eda-8e6dfc89ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cite_adt.X.min())\n",
    "print(cite_adt.X.max())\n",
    "print(dab_adt.X.min())\n",
    "print(dab_adt.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96baa1-a816-4592-823d-362c638332ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cite_adt)\n",
    "print(cite_adt.X.min())\n",
    "print(cite_adt.X.max())\n",
    "print(dab_adt)\n",
    "print(dab_adt.X.min())\n",
    "print(dab_adt.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532feb8-659d-49e8-b997-a95230c02a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_adt.layers['counts'] = cite_adt.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c5d43-7982-4233-a2d8-d80f467b362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cite_rna)\n",
    "print(cite_rna.X.min())\n",
    "print(cite_rna.X.max())\n",
    "print(vg_rna)\n",
    "print(vg_rna.X.min())\n",
    "print(vg_rna.X.max())\n",
    "print(vg_rna.raw)\n",
    "print(vg_rna.raw.X.min())\n",
    "print(vg_rna.raw.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5dccca-c187-4922-8afe-c80c676adafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_rna.layers['counts'] = vg_rna.raw.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea60c5-48b0-4d34-ab39-482dddf5dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import anndata as ad\n",
    "\n",
    "# The DAb panel you showed\n",
    "DAB_PANEL = pd.Index([\n",
    "    \"CD10\",\"CD117\",\"CD11B\",\"CD123\",\"CD13\",\"CD14\",\"CD15\",\"CD16\",\"CD19\",\"CD22\",\n",
    "    \"CD3\",\"CD30\",\"CD33\",\"CD34\",\"CD38\",\"CD4\",\"CD45\",\"CD5\",\"CD56\",\"CD64\",\"CD7\",\n",
    "    \"CD71\",\"HLA-DR\",\"IGG1\"\n",
    "], name=\"dab_panel\")\n",
    "\n",
    "# Mapping from CITE gene-symbol ADT names to DAb-style canonical targets\n",
    "CITE_GENE_TO_CANON = {\n",
    "    # present in your CITE set\n",
    "    \"ANPEP\": \"CD13\",\n",
    "    \"CD14\": \"CD14\",\n",
    "    \"FCGR3A\": \"CD16\",\n",
    "    \"CD19\": \"CD19\",\n",
    "    \"CD22\": \"CD22\",\n",
    "    \"CD3D\": \"CD3\",\n",
    "    \"CD3E\": \"CD3\",\n",
    "    \"CD3G\": \"CD3\",\n",
    "    \"CD33\": \"CD33\",\n",
    "    \"CD38\": \"CD38\",\n",
    "    \"CD4\": \"CD4\",\n",
    "    \"CD5\": \"CD5\",\n",
    "    \"CD7\": \"CD7\",\n",
    "    \"PTPRC\": \"CD45\",\n",
    "    \"NCAM1\": \"CD56\",\n",
    "    \"FCGR1A\": \"CD64\",\n",
    "    \"FCGR1B\": \"CD64\",\n",
    "    \"TFRC\": \"CD71\",\n",
    "    \"IL3RA\": \"CD123\",\n",
    "    \"ITGAM\": \"CD11B\",\n",
    "    \"HLA-DRA\": \"HLA-DR\",\n",
    "    \"HLA-DRB1\": \"HLA-DR\",\n",
    "    \"HLA-DRB5\": \"HLA-DR\",\n",
    "\n",
    "    # include these for completeness if they ever appear\n",
    "    \"MME\": \"CD10\",\n",
    "    \"KIT\": \"CD117\",\n",
    "    \"FUT4\": \"CD15\",\n",
    "    \"TNFRSF8\": \"CD30\",\n",
    "    \"CD34\": \"CD34\",\n",
    "    \"IGHG1\": \"IGG1\",\n",
    "}\n",
    "\n",
    "# Normalize DAb-side labels too\n",
    "DAB_ALIASES = {\n",
    "    \"CD11B\": \"CD11B\",\n",
    "    \"CD11b\": \"CD11B\",\n",
    "    \"HLADR\": \"HLA-DR\",\n",
    "    \"HLA DR\": \"HLA-DR\",\n",
    "    \"HLA-DR\": \"HLA-DR\",\n",
    "    \"IGG1\": \"IGG1\",\n",
    "    \"IgG1\": \"IGG1\",\n",
    "}\n",
    "\n",
    "def strip_rep_suffix(x: str) -> str:\n",
    "    # remove trailing \".1\", \".2\", \".10\", etc\n",
    "    return re.sub(r\"\\.\\d+$\", \"\", str(x).strip())\n",
    "\n",
    "def canon_from_varname(varname: str, *, source: str) -> str:\n",
    "    \"\"\"\n",
    "    source: 'cite' or 'dab'\n",
    "    Returns canonical marker name (upper-case) or '' if unusable.\n",
    "    \"\"\"\n",
    "    s = strip_rep_suffix(varname)\n",
    "    if s == \"\" or s.upper() in {\"NAN\", \"NONE\"}:\n",
    "        return \"\"\n",
    "\n",
    "    # Handle CITE isotypes explicitly\n",
    "    if source == \"cite\":\n",
    "        # your set contains \"isotype.0 ... isotype.9\" and also weird \".10\"\n",
    "        if s.lower().startswith(\"isotype\") or s in {\".10\"}:\n",
    "            return \"ISOTYPE\"\n",
    "\n",
    "    s_up = s.upper()\n",
    "\n",
    "    # Unify punctuation variants\n",
    "    s_up = s_up.replace(\"_\", \"-\")\n",
    "    s_up = re.sub(r\"\\s+\", \" \", s_up).strip()\n",
    "\n",
    "    if source == \"dab\":\n",
    "        return DAB_ALIASES.get(s_up, s_up)\n",
    "\n",
    "    # source == 'cite'\n",
    "    # FIRST: map gene-symbol style markers (including CD3D/E/G -> CD3, PTPRC->CD45, etc.)\n",
    "    if s_up in CITE_GENE_TO_CANON:\n",
    "        return CITE_GENE_TO_CANON[s_up]\n",
    "\n",
    "    # THEN: keep already-CD markers (e.g. CD14, CD19, CD33)\n",
    "    if re.match(r\"^CD\\d+$\", s_up) or re.match(r\"^CD\\d+[A-Z]+$\", s_up):\n",
    "        return s_up\n",
    "\n",
    "    # Otherwise keep gene symbol as-is (won't match DAb panel, but useful for debugging)\n",
    "    return s_up\n",
    "\n",
    "def feature_variance(adata: ad.AnnData) -> np.ndarray:\n",
    "    X = adata.X\n",
    "    if sp.issparse(X):\n",
    "        mean = np.asarray(X.mean(axis=0)).ravel()\n",
    "        mean2 = np.asarray(X.multiply(X).mean(axis=0)).ravel()\n",
    "        var = mean2 - mean**2\n",
    "    else:\n",
    "        var = np.var(np.asarray(X), axis=0)\n",
    "    return np.asarray(var, dtype=np.float64)\n",
    "\n",
    "def align_cite_dab_adts(cite_adt: ad.AnnData, dab_adt: ad.AnnData, panel: pd.Index = DAB_PANEL):\n",
    "    cite = cite_adt.copy()\n",
    "    dab  = dab_adt.copy()\n",
    "\n",
    "    cite.var[\"canon\"] = [canon_from_varname(v, source=\"cite\") for v in cite.var_names]\n",
    "    dab.var[\"canon\"]  = [canon_from_varname(v, source=\"dab\")  for v in dab.var_names]\n",
    "\n",
    "    # Only consider DAb markers that are in the explicit DAb panel list (canonical)\n",
    "    dab_can = pd.Index(dab.var[\"canon\"].unique())\n",
    "    dab_targets = pd.Index([x for x in panel if x in set(dab_can)])\n",
    "\n",
    "    # Now the potential overlap is those DAb targets that also exist in CITE canonical names\n",
    "    cite_can = set(cite.var[\"canon\"].values)\n",
    "    shared = pd.Index([x for x in dab_targets if x in cite_can])\n",
    "\n",
    "    # Drop controls unless you explicitly want them\n",
    "    # (ISOTYPE is not the same as IGG1)\n",
    "    shared = pd.Index([x for x in shared if x not in {\"ISOTYPE\"}])\n",
    "\n",
    "    print(f\"DAb targets present in DAb object: {len(dab_targets)} / {len(panel)}\")\n",
    "    print(f\"Shared canonical markers (after mapping): {len(shared)}\")\n",
    "    print(\"Shared:\", list(shared))\n",
    "\n",
    "    if len(shared) == 0:\n",
    "        return None, None, shared\n",
    "\n",
    "    # Resolve duplicates on CITE side: pick most variable feature for each canonical marker\n",
    "    var = feature_variance(cite)\n",
    "    cite.var[\"_var\"] = var\n",
    "\n",
    "    cite_keep = []\n",
    "    for m in shared:\n",
    "        idx = np.where(cite.var[\"canon\"].values == m)[0]\n",
    "        if len(idx) == 1:\n",
    "            cite_keep.append(idx[0])\n",
    "        else:\n",
    "            cite_keep.append(idx[np.argmax(cite.var[\"_var\"].values[idx])])\n",
    "\n",
    "    cite_keep = np.array(cite_keep, dtype=int)\n",
    "\n",
    "    # For DAb side: pick the first match per marker (usually unique already)\n",
    "    dab_keep = []\n",
    "    for m in shared:\n",
    "        idx = np.where(dab.var[\"canon\"].values == m)[0]\n",
    "        dab_keep.append(idx[0])\n",
    "\n",
    "    cite_al = cite[:, cite_keep].copy()\n",
    "    dab_al  = dab[:,  dab_keep ].copy()\n",
    "\n",
    "    # Set var_names to canonical marker names in a consistent order (shared order)\n",
    "    cite_al.var_names = shared\n",
    "    dab_al.var_names  = shared\n",
    "\n",
    "    return cite_al, dab_al, shared\n",
    "\n",
    "#cite_adt_al, dab_adt_al, shared_adts = align_cite_dab_adts(cite_adt, dab_adt, panel=DAB_PANEL)\n",
    "#\n",
    "#print(\"Aligned shapes:\",\n",
    "#      None if cite_adt_al is None else cite_adt_al.shape,\n",
    "#      None if dab_adt_al is None else dab_adt_al.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8621aef-1108-446e-bd46-bbce79ed9c08",
   "metadata": {},
   "source": [
    "### Feature alignment (ADT shared; VG genes intersect to CITE genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c129e-9f3d-42af-bc54-934631fc726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_vars(a: ad.AnnData, b: ad.AnnData) -> tuple[ad.AnnData, ad.AnnData]:\n",
    "    shared = a.var_names.intersection(b.var_names)\n",
    "    a2 = a[:, shared].copy()\n",
    "    b2 = b[:, shared].copy()\n",
    "    return a2, b2\n",
    "\n",
    "# ----------------------------\n",
    "# ADT: use canonical alignment (NOT intersect_vars)\n",
    "# ----------------------------\n",
    "cite_adt_al, dab_adt_al, shared_adts = align_cite_dab_adts(cite_adt, dab_adt, panel=DAB_PANEL)\n",
    "print(\"Shared ADTs after canonicalization:\", len(shared_adts))\n",
    "\n",
    "# ----------------------------\n",
    "# RNA: direct intersection is fine\n",
    "# ----------------------------\n",
    "vg_rna_al, cite_rna_al = intersect_vars(vg_rna, cite_rna)\n",
    "print(\"VG->CITE shared genes:\", vg_rna_al.n_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb517d9-0a61-4ec2-b0f9-c9140ed6a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cite_rna_al)\n",
    "print(cite_rna_al.X.min())\n",
    "print(cite_rna_al.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf004f48-5bb9-4254-b386-9ace5b52fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_rna_al.layers['counts'] = cite_rna_al.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711952f8-7d3c-4efc-9fb4-7575f4849014",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cite_adt_al is not None:\n",
    "    cite_can = set([canon_from_varname(v, source=\"cite\") for v in cite_adt.var_names])\n",
    "    dab_panel_can = set([canon_from_varname(v, source=\"dab\") for v in DAB_PANEL])\n",
    "\n",
    "    missing_from_cite = sorted(list(dab_panel_can - cite_can))\n",
    "    print(\"DAb panel markers not present in CITE ADT panel (canonical):\")\n",
    "    print(missing_from_cite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22441944-4eb0-4a3d-840b-e60f16cd1e5c",
   "metadata": {},
   "source": [
    "### Make splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ab378-02d0-41cc-a961-7ddcc4de2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def group_holdout_splits(\n",
    "    adata: ad.AnnData,\n",
    "    group_col: str,\n",
    "    *,\n",
    "    seed: int = 0,\n",
    "    train_frac: float = 0.80,\n",
    "    val_frac: float = 0.10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Group-disjoint train/val/test split.\n",
    "    Ensures no group appears in more than one split.\n",
    "\n",
    "    Returns dict with:\n",
    "      - \"train_idx\", \"val_idx\", \"test_idx\" (integer indices)\n",
    "      - \"train_obs\", \"val_obs\", \"test_obs\" (obs_names arrays)\n",
    "      - \"train_groups\", \"val_groups\", \"test_groups\"\n",
    "    \"\"\"\n",
    "    if group_col not in adata.obs.columns:\n",
    "        raise KeyError(f\"group_col='{group_col}' not in adata.obs columns\")\n",
    "\n",
    "    groups = adata.obs[group_col].astype(str).values\n",
    "    idx_all = np.arange(adata.n_obs)\n",
    "\n",
    "    # 1) train vs temp\n",
    "    gss1 = GroupShuffleSplit(n_splits=1, train_size=train_frac, random_state=seed)\n",
    "    tr_idx, tmp_idx = next(gss1.split(idx_all, groups=groups))\n",
    "\n",
    "    # 2) val vs test within temp\n",
    "    tmp_groups = groups[tmp_idx]\n",
    "    # val_frac of total => val_frac/(1-train_frac) of tmp\n",
    "    val_frac_of_tmp = val_frac / max(1e-12, (1.0 - train_frac))\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, train_size=val_frac_of_tmp, random_state=seed + 1)\n",
    "    va_rel, te_rel = next(gss2.split(tmp_idx, groups=tmp_groups))\n",
    "    va_idx = tmp_idx[va_rel]\n",
    "    te_idx = tmp_idx[te_rel]\n",
    "\n",
    "    out = {\n",
    "        \"train_idx\": tr_idx,\n",
    "        \"val_idx\": va_idx,\n",
    "        \"test_idx\": te_idx,\n",
    "        \"train_obs\": adata.obs_names[tr_idx].to_numpy(),\n",
    "        \"val_obs\": adata.obs_names[va_idx].to_numpy(),\n",
    "        \"test_obs\": adata.obs_names[te_idx].to_numpy(),\n",
    "        \"train_groups\": pd.unique(groups[tr_idx]),\n",
    "        \"val_groups\": pd.unique(groups[va_idx]),\n",
    "        \"test_groups\": pd.unique(groups[te_idx]),\n",
    "    }\n",
    "\n",
    "    # Sanity: ensure disjoint groups\n",
    "    assert set(out[\"train_groups\"]).isdisjoint(set(out[\"val_groups\"]))\n",
    "    assert set(out[\"train_groups\"]).isdisjoint(set(out[\"test_groups\"]))\n",
    "    assert set(out[\"val_groups\"]).isdisjoint(set(out[\"test_groups\"]))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def subset_by_obs_names(adata: ad.AnnData, obs_names) -> ad.AnnData:\n",
    "    return adata[obs_names].copy()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CITE splits (NO LEAKAGE)\n",
    "# -----------------------------\n",
    "# IMPORTANT: define splits on a single reference object (CITE RNA aligned genes)\n",
    "# and apply same obs split to CITE ADT aligned panel.\n",
    "assert np.array_equal(cite_rna_al.obs_names, cite_adt_al.obs_names), \"CITE RNA/ADT obs_names must match!\"\n",
    "\n",
    "splits = group_holdout_splits(\n",
    "    cite_rna_al,\n",
    "    group_col=CITE_GROUP_COL,   # \"sample_id\"\n",
    "    seed=0,\n",
    "    train_frac=0.80,\n",
    "    val_frac=0.10,\n",
    ")\n",
    "\n",
    "cite_rna_tr = subset_by_obs_names(cite_rna_al, splits[\"train_obs\"])\n",
    "cite_rna_va = subset_by_obs_names(cite_rna_al, splits[\"val_obs\"])\n",
    "cite_rna_te = subset_by_obs_names(cite_rna_al, splits[\"test_obs\"])\n",
    "\n",
    "cite_adt_tr = subset_by_obs_names(cite_adt_al, splits[\"train_obs\"])\n",
    "cite_adt_va = subset_by_obs_names(cite_adt_al, splits[\"val_obs\"])\n",
    "cite_adt_te = subset_by_obs_names(cite_adt_al, splits[\"test_obs\"])\n",
    "\n",
    "print(\"CITE split sizes:\", cite_rna_tr.n_obs, cite_rna_va.n_obs, cite_rna_te.n_obs)\n",
    "print(\"CITE groups (train/val/test):\",\n",
    "      len(splits[\"train_groups\"]), len(splits[\"val_groups\"]), len(splits[\"test_groups\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ec900-c07b-4a15-b9bf-8599678e6300",
   "metadata": {},
   "source": [
    "### Minimal preprocessing (RNA + ADT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d8c80-0003-40be-986b-39ba8e56b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out the cell lines from the vg scRNA data prior to preprocessing\n",
    "\n",
    "# prefix = string before first underscore\n",
    "vg_rna_al.obs[\"prefix\"] = vg_rna_al.obs_names.str.split(\"_\", n=1).str[0]\n",
    "\n",
    "# quick sanity\n",
    "print(vg_rna_al.obs[\"prefix\"].value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112fdff-9a99-4477-a6e9-235a2bbd0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref = vg_rna_al.obs_names.to_series().str.split(\"_\", n=1).str[0]\n",
    "\n",
    "mask_cell_lines = pref.str.startswith(\"OCI.AML\") | pref.str.startswith(\"MUTZ3\")\n",
    "\n",
    "vg_rna_al = vg_rna_al[~mask_cell_lines].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb503d5-bece-4eff-aa73-2af75aaa4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vg_rna_al)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c82a3-7998-4cd3-9e75-f258bfde30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "\n",
    "# -------------------------\n",
    "# utils\n",
    "# -------------------------\n",
    "def _to_dense(X):\n",
    "    return X.toarray() if sp.issparse(X) else np.asarray(X)\n",
    "\n",
    "def _cast32(X):\n",
    "    return X.astype(np.float32, copy=False)\n",
    "\n",
    "def ensure_layer_from_X(adata: ad.AnnData, layer: str):\n",
    "    \"\"\"If layer missing, snapshot current .X into that layer.\"\"\"\n",
    "    if layer not in adata.layers:\n",
    "        adata.layers[layer] = adata.X.copy()\n",
    "\n",
    "def has_negative_X(adata: ad.AnnData) -> bool:\n",
    "    X = _to_dense(adata.X)\n",
    "    return np.nanmin(X) < 0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RNA: keep .layers['counts'] and .layers['log1p'] everywhere\n",
    "# ============================================================\n",
    "def _ensure_rna_counts_and_log1p_layers(\n",
    "    a: ad.AnnData,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    log1p_layer: str = \"log1p\",\n",
    "    target_sum: float = 1e4,\n",
    "    overwrite_log1p: bool = False,\n",
    ") -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Ensures:\n",
    "      - .layers[counts_layer] exists (raw snapshot)\n",
    "      - .layers[log1p_layer] exists = log1p(normalize_total(counts))\n",
    "    Does NOT modify counts layer.\n",
    "    \"\"\"\n",
    "    x = a.copy()\n",
    "\n",
    "    # ensure counts layer exists (snapshot current X if user didn't provide one)\n",
    "    ensure_layer_from_X(x, counts_layer)\n",
    "\n",
    "    # build log1p layer if missing (or overwrite requested)\n",
    "    if (log1p_layer not in x.layers) or overwrite_log1p:\n",
    "        tmp = x.copy()\n",
    "        tmp.X = tmp.layers[counts_layer].copy()\n",
    "        sc.pp.normalize_total(tmp, target_sum=target_sum)\n",
    "        sc.pp.log1p(tmp)  # may set tmp.uns['log1p'] = {'base': None}; harmless\n",
    "        x.layers[log1p_layer] = tmp.X.copy()\n",
    "\n",
    "    # lightweight provenance\n",
    "    x.uns[\"rna_layers\"] = {\n",
    "        \"counts_layer\": counts_layer,\n",
    "        \"log1p_layer\": log1p_layer,\n",
    "        \"target_sum\": float(target_sum),\n",
    "        \"transform\": \"normalize_total+log1p\",\n",
    "    }\n",
    "    return x\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RNA: fit/apply (fit on TRAIN using .layers['log1p'])\n",
    "# ============================================================\n",
    "def rna_fit_params(\n",
    "    a_train: ad.AnnData,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    target_sum: float = 1e4,\n",
    "    clip: float = 10.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit gene-wise mean/sd for Z-scoring on TRAIN ONLY using:\n",
    "      .layers['counts'] -> normalize_total(target_sum) -> log1p -> store in .layers['log1p']\n",
    "    Then compute mu/sd from .layers['log1p'].\n",
    "    \"\"\"\n",
    "    x = _ensure_rna_counts_and_log1p_layers(\n",
    "        a_train,\n",
    "        counts_layer=counts_layer,\n",
    "        log1p_layer=\"log1p\",\n",
    "        target_sum=target_sum,\n",
    "        overwrite_log1p=False,\n",
    "    )\n",
    "\n",
    "    X = _to_dense(x.layers[\"log1p\"]).astype(np.float32)\n",
    "    mu = X.mean(axis=0).astype(np.float32)\n",
    "    sd = (X.std(axis=0).astype(np.float32) + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"target_sum\": float(target_sum),\n",
    "        \"clip\": float(clip),\n",
    "        \"mu\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"transform\": \"counts->normalize_total+log1p (layer), then zscore(.X)\",\n",
    "        \"fit_on\": \"train_only\",\n",
    "        \"counts_layer\": counts_layer,\n",
    "        \"log1p_layer\": \"log1p\",\n",
    "    }\n",
    "\n",
    "def preprocess_rna_apply(\n",
    "    a: ad.AnnData,\n",
    "    params: dict,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    ") -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Ensures layers:\n",
    "      - .layers['counts'] (raw)\n",
    "      - .layers['log1p'] = log1p(normalize_total(counts))\n",
    "    Sets:\n",
    "      - .X = zscore(.layers['log1p'], mu/sd) clipped (float32)\n",
    "    \"\"\"\n",
    "    x = _ensure_rna_counts_and_log1p_layers(\n",
    "        a,\n",
    "        counts_layer=counts_layer,\n",
    "        log1p_layer=params.get(\"log1p_layer\", \"log1p\"),\n",
    "        target_sum=params[\"target_sum\"],\n",
    "        overwrite_log1p=False,\n",
    "    )\n",
    "\n",
    "    X = _to_dense(x.layers[params.get(\"log1p_layer\", \"log1p\")]).astype(np.float32)\n",
    "    X = (X - params[\"mu\"]) / params[\"sd\"]\n",
    "    X = np.clip(X, -params[\"clip\"], params[\"clip\"]).astype(np.float32)\n",
    "\n",
    "    x.X = X\n",
    "    x.uns[\"rna_transform\"] = {k: v for k, v in params.items() if k not in (\"mu\", \"sd\")}\n",
    "    return x\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ADT: fit/apply for CITE counts, plus a safe path for DAb .X\n",
    "# ============================================================\n",
    "def adt_fit_params(\n",
    "    a_train: ad.AnnData,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit ADT mu/sd on CITE TRAIN ONLY:\n",
    "      counts -> log1p -> per-feature mean/sd\n",
    "    \"\"\"\n",
    "    if counts_layer not in a_train.layers:\n",
    "        raise ValueError(f\"ADT fit expects counts in layers['{counts_layer}'].\")\n",
    "\n",
    "    X = _to_dense(a_train.layers[counts_layer]).astype(np.float32)\n",
    "    X = np.log1p(X)\n",
    "    mu = X.mean(axis=0).astype(np.float32)\n",
    "    sd = (X.std(axis=0).astype(np.float32) + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"mu\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"transform\": \"log1p+zscore\",\n",
    "        \"fit_on\": \"train_only\",\n",
    "        \"counts_layer\": counts_layer,\n",
    "    }\n",
    "\n",
    "def preprocess_adt_apply_counts(\n",
    "    a: ad.AnnData,\n",
    "    params: dict,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    clip: float = 10.0,\n",
    ") -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Apply ADT preprocessing when true counts are available:\n",
    "      counts -> log1p -> zscore(mu/sd) -> clip\n",
    "    \"\"\"\n",
    "    x = a.copy()\n",
    "    if counts_layer not in x.layers:\n",
    "        raise ValueError(f\"ADT apply_counts expects counts in layers['{counts_layer}'].\")\n",
    "\n",
    "    X = _to_dense(x.layers[counts_layer]).astype(np.float32)\n",
    "    X = np.log1p(X)\n",
    "    X = (X - params[\"mu\"]) / params[\"sd\"]\n",
    "    X = np.clip(X, -clip, clip).astype(np.float32)\n",
    "\n",
    "    x.X = X\n",
    "    x.uns[\"adt_transform\"] = {\n",
    "        \"type\": \"log1p+zscore\",\n",
    "        \"fit\": params.get(\"fit_on\", \"unknown\"),\n",
    "        \"clip\": float(clip),\n",
    "    }\n",
    "    return x\n",
    "\n",
    "def preprocess_adt_from_processed_X(\n",
    "    a: ad.AnnData,\n",
    "    *,\n",
    "    clip: float = 10.0,\n",
    "    standardize_to_ref: dict | None = None,\n",
    ") -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Use this for DAb when only processed values exist in .X.\n",
    "\n",
    "    Behavior:\n",
    "      - Always treats .X as already-processed continuous features.\n",
    "      - Optionally unit-standardizes feature-wise, then clips.\n",
    "\n",
    "    If standardize_to_ref is provided, it should be a dict with:\n",
    "      {\"mode\": \"unit\"}\n",
    "    \"\"\"\n",
    "    x = a.copy()\n",
    "    X = _to_dense(x.X).astype(np.float32)\n",
    "\n",
    "    if standardize_to_ref is None:\n",
    "        X = np.clip(X, -clip, clip).astype(np.float32)\n",
    "        x.X = X\n",
    "        x.uns[\"adt_transform\"] = {\"type\": \"already_processed\", \"standardize\": \"none\", \"clip\": float(clip)}\n",
    "        return x\n",
    "\n",
    "    mode = standardize_to_ref.get(\"mode\", \"unit\")\n",
    "    if mode != \"unit\":\n",
    "        raise ValueError(f\"Unknown standardize mode: {mode}. Use mode='unit' or None.\")\n",
    "\n",
    "    mu = np.nanmean(X, axis=0).astype(np.float32)\n",
    "    sd = (np.nanstd(X, axis=0).astype(np.float32) + 1e-8)\n",
    "    X = (X - mu) / sd\n",
    "    X = np.clip(X, -clip, clip).astype(np.float32)\n",
    "\n",
    "    x.X = X\n",
    "    x.uns[\"adt_transform\"] = {\"type\": \"already_processed\", \"standardize\": \"unit\", \"clip\": float(clip)}\n",
    "    x.uns[\"adt_standardize_fit\"] = {\"mu\": \"dab_self\", \"sd\": \"dab_self\"}  # keep lightweight; don't stash big arrays\n",
    "    return x\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# One-stop helpers for workflow\n",
    "# ============================================================\n",
    "def preprocess_all_for_aml_bridge(\n",
    "    *,\n",
    "    cite_rna_tr, cite_rna_va, cite_rna_te,\n",
    "    vg_rna_al,\n",
    "    cite_adt_tr, cite_adt_va, cite_adt_te,\n",
    "    dab_adt_al,\n",
    "    rna_counts_layer=\"counts\",\n",
    "    adt_counts_layer=\"counts\",\n",
    "    rna_target_sum=1e4,\n",
    "    rna_clip=10.0,\n",
    "    adt_clip=10.0,\n",
    "    dab_standardize_unit=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    RNA:\n",
    "      - Ensures every RNA object has .layers['counts'] and .layers['log1p']\n",
    "      - Fits mu/sd on TRAIN using .layers['log1p']\n",
    "      - Sets .X to z-scored/clipped values (model input), leaving layers intact\n",
    "\n",
    "    ADT: unchanged from your original.\n",
    "    \"\"\"\n",
    "    # RNA params fit on CITE train (will also ensure train has log1p layer)\n",
    "    rna_params = rna_fit_params(\n",
    "        cite_rna_tr, counts_layer=rna_counts_layer, target_sum=rna_target_sum, clip=rna_clip\n",
    "    )\n",
    "\n",
    "    cite_rna_pp_tr = preprocess_rna_apply(cite_rna_tr, rna_params, counts_layer=rna_counts_layer)\n",
    "    cite_rna_pp_va = preprocess_rna_apply(cite_rna_va, rna_params, counts_layer=rna_counts_layer)\n",
    "    cite_rna_pp_te = preprocess_rna_apply(cite_rna_te, rna_params, counts_layer=rna_counts_layer)\n",
    "    vg_rna_pp      = preprocess_rna_apply(vg_rna_al,  rna_params, counts_layer=rna_counts_layer)\n",
    "\n",
    "    # ADT params fit on CITE train\n",
    "    adt_params = adt_fit_params(cite_adt_tr, counts_layer=adt_counts_layer)\n",
    "\n",
    "    cite_adt_pp_tr = preprocess_adt_apply_counts(cite_adt_tr, adt_params, counts_layer=adt_counts_layer, clip=adt_clip)\n",
    "    cite_adt_pp_va = preprocess_adt_apply_counts(cite_adt_va, adt_params, counts_layer=adt_counts_layer, clip=adt_clip)\n",
    "    cite_adt_pp_te = preprocess_adt_apply_counts(cite_adt_te, adt_params, counts_layer=adt_counts_layer, clip=adt_clip)\n",
    "\n",
    "    # DAb ADT: .X already processed\n",
    "    if dab_standardize_unit:\n",
    "        dab_adt_pp = preprocess_adt_from_processed_X(dab_adt_al, clip=adt_clip, standardize_to_ref={\"mode\":\"unit\"})\n",
    "    else:\n",
    "        dab_adt_pp = preprocess_adt_from_processed_X(dab_adt_al, clip=adt_clip, standardize_to_ref=None)\n",
    "\n",
    "    return {\n",
    "        \"rna_params\": rna_params,\n",
    "        \"adt_params\": adt_params,\n",
    "        \"cite_rna_pp_tr\": cite_rna_pp_tr,\n",
    "        \"cite_rna_pp_va\": cite_rna_pp_va,\n",
    "        \"cite_rna_pp_te\": cite_rna_pp_te,\n",
    "        \"vg_rna_pp\": vg_rna_pp,\n",
    "        \"cite_adt_pp_tr\": cite_adt_pp_tr,\n",
    "        \"cite_adt_pp_va\": cite_adt_pp_va,\n",
    "        \"cite_adt_pp_te\": cite_adt_pp_te,\n",
    "        \"dab_adt_pp\": dab_adt_pp,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022901a4-c147-4ce8-9bf1-83cd9db0ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = preprocess_all_for_aml_bridge(\n",
    "    cite_rna_tr=cite_rna_tr, cite_rna_va=cite_rna_va, cite_rna_te=cite_rna_te,\n",
    "    vg_rna_al=vg_rna_al,\n",
    "    cite_adt_tr=cite_adt_tr, cite_adt_va=cite_adt_va, cite_adt_te=cite_adt_te,\n",
    "    dab_adt_al=dab_adt_al,\n",
    "    dab_standardize_unit=True,   # recommended if dab_adt_al.X scale is weird vs CITE\n",
    ")\n",
    "\n",
    "print(\n",
    "  out[\"cite_rna_pp_tr\"].shape, out[\"vg_rna_pp\"].shape,\n",
    "  out[\"cite_adt_pp_tr\"].shape, out[\"dab_adt_pp\"].shape\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc06f35-9a0d-4f38-969b-a501d7e7f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Use preprocessed objects from here onward\n",
    "# -----------------------------\n",
    "cite_rna_pp_tr = out[\"cite_rna_pp_tr\"]\n",
    "cite_rna_pp_va = out[\"cite_rna_pp_va\"]\n",
    "cite_rna_pp_te = out[\"cite_rna_pp_te\"]\n",
    "\n",
    "cite_adt_pp_tr = out[\"cite_adt_pp_tr\"]\n",
    "cite_adt_pp_va = out[\"cite_adt_pp_va\"]\n",
    "cite_adt_pp_te = out[\"cite_adt_pp_te\"]\n",
    "\n",
    "vg_rna_pp  = out[\"vg_rna_pp\"]\n",
    "dab_adt_pp = out[\"dab_adt_pp\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b520f3-6c7b-4ef3-9e32-be7b050a8d1f",
   "metadata": {},
   "source": [
    "### Calculate the LSC-17 score from the scRNA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cc2c0-cf04-4364-a3ba-4b908b65c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vg_rna_pp)\n",
    "print(vg_rna_pp.var_names)\n",
    "\n",
    "print(cite_rna_pp_tr)\n",
    "print(cite_rna_pp_tr.var_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7157c-c1a3-462a-927f-133cdd968c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vg_rna_pp.X)\n",
    "print(vg_rna_pp.X.min())\n",
    "print(vg_rna_pp.X.max())\n",
    "\n",
    "print(vg_rna_pp.layers['log1p'])\n",
    "print(vg_rna_pp.layers['log1p'].min())\n",
    "print(vg_rna_pp.layers['log1p'].max())\n",
    "\n",
    "print(cite_rna_pp_tr.X)\n",
    "print(cite_rna_pp_tr.X.min())\n",
    "print(cite_rna_pp_tr.X.max())\n",
    "\n",
    "print(cite_rna_pp_tr.layers['log1p'])\n",
    "print(cite_rna_pp_tr.layers['log1p'].min())\n",
    "print(cite_rna_pp_tr.layers['log1p'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b09ee7-7307-4b3c-ac11-66be172171ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases = {\n",
    "    \"KIAA0125\": [\"FAM30A\"],\n",
    "    \"NGFRAP1\": [\"BEX3\"],\n",
    "    \"GPR56\":   [\"ADGRG1\"],\n",
    "}\n",
    "\n",
    "present = set(vg_rna_pp.var_names)\n",
    "\n",
    "for g, alts in aliases.items():\n",
    "    hit = [a for a in [g, *alts] if a in present]\n",
    "    print(g, \"->\", hit if hit else \"MISSING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331dc1c-c33c-4876-9808-8f61d386aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "LSC17_GENES = [\n",
    "    \"DNMT3B\",\"ZBTB46\",\"NYNRIN\",\"ARHGAP22\",\"LAPTM4B\",\n",
    "    \"MMRN1\",\"DPYSL3\",\"FAM30A\",\"CDK6\",\"CPXM1\",\n",
    "    \"SOCS2\",\"SMIM24\",\"EMP1\",\"BEX3\",\"CD34\",\n",
    "    \"AKR1C3\",\"ADGRG1\"\n",
    "]\n",
    "\n",
    "def _row_mean(X):\n",
    "    # works for dense or sparse\n",
    "    if sp.issparse(X):\n",
    "        return np.asarray(X.mean(axis=1)).ravel()\n",
    "    return X.mean(axis=1)\n",
    "\n",
    "def add_lsc17_score(\n",
    "    adata,\n",
    "    *,\n",
    "    key=\"LSC17\",\n",
    "    genes=LSC17_GENES,\n",
    "    layer=None,              # e.g. \"log1p\" if you store it; otherwise None uses adata.X\n",
    "    uppercase_varnames=True  # helpful if one dataset is lower/upper mixed\n",
    "):\n",
    "    # Optionally standardize var_names casing for matching\n",
    "    if uppercase_varnames:\n",
    "        # avoid modifying in-place if you're worried; here we do in-place for convenience\n",
    "        adata.var_names = adata.var_names.astype(str).str.upper()\n",
    "\n",
    "    genes = [g.upper() for g in genes] if uppercase_varnames else list(genes)\n",
    "    present = [g for g in genes if g in adata.var_names]\n",
    "    missing = [g for g in genes if g not in adata.var_names]\n",
    "\n",
    "    if len(present) == 0:\n",
    "        raise ValueError(f\"No LSC17 genes found in adata.var_names. Example var_names: {list(adata.var_names[:5])}\")\n",
    "\n",
    "    X = adata.layers[layer] if layer is not None else adata.X\n",
    "    X_sig = adata[:, present].layers[layer] if layer is not None else adata[:, present].X\n",
    "\n",
    "    score = _row_mean(X_sig)\n",
    "\n",
    "    adata.obs[f\"{key}_score\"] = score.astype(np.float32)\n",
    "    adata.obs[f\"{key}_z\"] = ((score - score.mean()) / (score.std() + 1e-8)).astype(np.float32)\n",
    "    adata.obs[f\"{key}_n_genes\"] = np.int32(len(present))\n",
    "\n",
    "    if missing:\n",
    "        print(f\"[{key}] Missing {len(missing)}/{len(genes)} genes (scored with {len(present)}): {missing}\")\n",
    "    else:\n",
    "        print(f\"[{key}] All {len(genes)} genes present.\")\n",
    "\n",
    "    return present, missing\n",
    "\n",
    "# ---- run on BOTH objects (separately) ----\n",
    "present_vg, missing_vg = add_lsc17_score(vg_rna_pp,      key=\"LSC17\", layer='log1p', uppercase_varnames=True)\n",
    "present_ct, missing_ct = add_lsc17_score(cite_rna_pp_tr, key=\"LSC17\", layer='log1p', uppercase_varnames=True)\n",
    "present_ct, missing_ct = add_lsc17_score(cite_rna_pp_va, key=\"LSC17\", layer='log1p', uppercase_varnames=True)\n",
    "present_ct, missing_ct = add_lsc17_score(cite_rna_pp_te, key=\"LSC17\", layer='log1p', uppercase_varnames=True)\n",
    "\n",
    "print(vg_rna_pp.obs[[\"LSC17_score\",\"LSC17_z\",\"LSC17_n_genes\"]].describe())\n",
    "print(cite_rna_pp_tr.obs[[\"LSC17_score\",\"LSC17_z\",\"LSC17_n_genes\"]].describe())\n",
    "print(cite_rna_pp_va.obs[[\"LSC17_score\",\"LSC17_z\",\"LSC17_n_genes\"]].describe())\n",
    "print(cite_rna_pp_te.obs[[\"LSC17_score\",\"LSC17_z\",\"LSC17_n_genes\"]].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745b932-4e19-4cad-a226-930b0e672092",
   "metadata": {},
   "source": [
    "### Train UniVI on paired CITE (RNA + ADT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5eb11d-b570-483e-bc80-e2f932430e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee43cc-3c1b-4028-82da-fa7065550d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Train UniVI on paired CITE (preprocessed)\n",
    "# -----------------------------\n",
    "seed_everything(1)\n",
    "\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#print(\"device:\", device)\n",
    "\n",
    "# Sanity check training inputs\n",
    "print(\"RNA pp range:\", float(np.min(cite_rna_pp_tr.X)), float(np.max(cite_rna_pp_tr.X)))\n",
    "print(\"ADT pp range:\", float(np.min(cite_adt_pp_tr.X)), float(np.max(cite_adt_pp_tr.X)))\n",
    "\n",
    "univi_cfg = UniVIConfig(\n",
    "    #beta=1.125,\n",
    "    beta=1.15,\n",
    "    #gamma=1.475,\n",
    "    gamma=1.75,\n",
    "    #latent_dim=40,\n",
    "    latent_dim=30,\n",
    "    #encoder_dropout=0.20,\n",
    "    #decoder_dropout=0.10,\n",
    "    encoder_dropout=0.10,\n",
    "    decoder_dropout=0.05,\n",
    "    encoder_batchnorm=False,\n",
    "    decoder_batchnorm=False,\n",
    "    #kl_anneal_start=10,\n",
    "    #kl_anneal_end=120,\n",
    "    #align_anneal_start=20,\n",
    "    #align_anneal_end=130,\n",
    "    modalities=[\n",
    "        ModalityConfig(\n",
    "            name=\"rna\",\n",
    "            input_dim=cite_rna_pp_tr.n_vars,\n",
    "            #encoder_hidden=[512, 256, 128, 64],\n",
    "            #decoder_hidden=[64, 128, 256, 512],\n",
    "            encoder_hidden=[1024, 512, 256, 128, 64],\n",
    "            decoder_hidden=[64, 128, 256, 512, 1024],\n",
    "            likelihood=\"gaussian\",\n",
    "            recon_weight=1.00,\n",
    "        ),\n",
    "        ModalityConfig(\n",
    "            name=\"adt\",\n",
    "            input_dim=cite_adt_pp_tr.n_vars,\n",
    "            #encoder_hidden=[128, 64],\n",
    "            #decoder_hidden=[64, 128],\n",
    "            encoder_hidden=[128, 64, 32],\n",
    "            decoder_hidden=[32, 64, 128],\n",
    "            likelihood=\"gaussian\",\n",
    "            #recon_weight=3.25,\n",
    "            recon_weight=3.00,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = UniVIMultiModalVAE(\n",
    "    univi_cfg,\n",
    "    loss_mode=\"v1\",\n",
    "    v1_recon=\"avg\",\n",
    "    normalize_v1_terms=True,\n",
    "    #recon_normalize_by_dim=True,\n",
    "    #recon_dim_power=0.4,\n",
    ").to(device)\n",
    "\n",
    "train_cfg = TrainingConfig(\n",
    "    n_epochs=3000,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    batch_size=256,\n",
    "    grad_clip=5.0,\n",
    "    early_stopping=True,\n",
    "    patience=150,\n",
    "    min_delta=0,\n",
    "    log_every=25,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "train_ds = MultiModalDataset({\"rna\": cite_rna_pp_tr, \"adt\": cite_adt_pp_tr})\n",
    "val_ds   = MultiModalDataset({\"rna\": cite_rna_pp_va, \"adt\": cite_adt_pp_va})\n",
    "\n",
    "pin = (device == \"cuda\")\n",
    "train_loader = DataLoader(train_ds, batch_size=train_cfg.batch_size, shuffle=True,  num_workers=0, pin_memory=pin)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=train_cfg.batch_size, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "\n",
    "trainer = UniVITrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    train_cfg=train_cfg,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "history = trainer.fit()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_loss\"], label=\"train\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend()\n",
    "plt.title(\"UniVI training loss (CITE paired; preprocessed)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f211f-432a-4d34-8500-1d8da580f877",
   "metadata": {},
   "source": [
    "### Encode latents for CITE test + projections (VG RNA, DAb ADT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3a90a-e0b0-4f67-b67b-d1ad4c36863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_latent(model, adata: ad.AnnData, modality: str, device=device, batch_size=1024, latent=\"modality_mean\"):\n",
    "    Z = ue.encode_adata(\n",
    "        model,\n",
    "        adata,\n",
    "        modality=modality,\n",
    "        latent=latent,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return np.asarray(Z, dtype=np.float32)\n",
    "\n",
    "# Encode from PREPROCESSED objects\n",
    "Z_cite_rna = encode_latent(model, cite_rna_pp_te, modality=\"rna\", device=device)\n",
    "Z_cite_adt = encode_latent(model, cite_adt_pp_te, modality=\"adt\", device=device)\n",
    "\n",
    "Z_vg_rna   = encode_latent(model, vg_rna_pp,      modality=\"rna\", device=device)\n",
    "Z_dab_adt  = encode_latent(model, dab_adt_pp,     modality=\"adt\", device=device)\n",
    "\n",
    "# Store\n",
    "cite_rna_te.obsm[\"X_univi\"]    = Z_cite_rna\n",
    "cite_adt_te.obsm[\"X_univi\"]    = Z_cite_adt\n",
    "vg_rna_pp.obsm[\"X_univi\"]      = Z_vg_rna\n",
    "dab_adt_pp.obsm[\"X_univi\"]     = Z_dab_adt\n",
    "\n",
    "print(\"Latents shapes:\", Z_cite_rna.shape, Z_cite_adt.shape, Z_vg_rna.shape, Z_dab_adt.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f945bd4-853e-4e82-b05c-b6d6e38eeaa0",
   "metadata": {},
   "source": [
    "### Build joint latent UMAP for panels (b/c/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fc45a-1472-42f3-bf81-20d795aad6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "sc.settings.set_figure_params(\n",
    "    dpi=100,        # notebook display dpi\n",
    "    dpi_save=300,   # saved figure dpi\n",
    "    figsize=(10, 8), # default size in inches\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "#sc.settings.set_figure_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8251d-ad8f-4975-958c-65ebefc33be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_latent_adata(items):\n",
    "    Zs, obs_rows = [], []\n",
    "    for ds_name, mod, a in items:\n",
    "        Z = a.obsm[\"X_univi\"]\n",
    "        Zs.append(Z)\n",
    "        obs_rows.append(pd.DataFrame({\"dataset\": ds_name, \"modality\": mod}, index=a.obs_names.copy()))\n",
    "    Z = np.vstack(Zs).astype(np.float32)\n",
    "    obs = pd.concat(obs_rows, axis=0)\n",
    "    out = ad.AnnData(X=Z, obs=obs, var=pd.DataFrame(index=[f\"z{i}\" for i in range(Z.shape[1])]))\n",
    "    out.obsm[\"X_univi\"] = out.X.copy()\n",
    "    return out\n",
    "\n",
    "joint = joint_latent_adata([\n",
    "    (\"CITE\", \"RNA\", cite_rna_te),\n",
    "    (\"CITE\", \"ADT\", cite_adt_te),\n",
    "    (\"DAb\",  \"ADT\", dab_adt_pp),\n",
    "    (\"VG\",   \"RNA\", vg_rna_pp),\n",
    "])\n",
    "\n",
    "joint.obs[\"dataset_modality\"] = joint.obs[\"dataset\"].astype(str) + \" \" + joint.obs[\"modality\"].astype(str)\n",
    "\n",
    "sc.pp.neighbors(joint, use_rep=\"X_univi\", n_neighbors=30, random_state=1)\n",
    "sc.tl.umap(joint, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e483c-c627-43ad-a797-fe11d67daa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig7b\n",
    "sc.pl.umap(joint, \n",
    "           color=\"dataset_modality\", \n",
    "           title=\"Joint latent (dataset × modality) using CITE-seq test sets and outside unimodal data\",\n",
    "           size=10,\n",
    "           alpha=0.7,\n",
    ")\n",
    "\n",
    "plt.savefig(FIGDIR / \"fig7_umap_dataset_modality.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a91504-f4ec-409f-adf2-d81a08791284",
   "metadata": {},
   "source": [
    "### Figure 7c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c2399-5ce7-45d5-af83-a4aedc9bcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cite_rna_te)\n",
    "print(cite_adt_te)\n",
    "print(vg_rna_pp)\n",
    "print(dab_adt_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585cf72-fae2-4922-8816-6a8c9a3a8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: set these explicitly if you know them\n",
    "CITE_LABEL_COL = None        # e.g. \"celltype.l2\" / \"celltype\" / \"annotation\"\n",
    "VG_LABEL_COL   = \"CellType\"  # e.g. \"celltype\" / \"annotation\" / \"coarse\"\n",
    "\n",
    "def find_label_col(adata, candidates):\n",
    "    for c in candidates:\n",
    "        if c in adata.obs.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def to_coarse(label: str) -> str:\n",
    "    s = str(label).lower()\n",
    "    if s in {\"nan\", \"none\", \"\"}:\n",
    "        return \"NA\"\n",
    "    if any(k in s for k in [\"blast\", \"malignant\", \"leuk\", \"aml\"]):\n",
    "        return \"blasts/malignant-like\"\n",
    "    if any(k in s for k in [\"hsc\", \"stem\", \"prog\", \"cmp\", \"gmp\", \"progen\"]):\n",
    "        return \"progenitor-like\"\n",
    "    if any(k in s for k in [\"mono\", \"mac\", \"cd14\", \"fcgr3a\"]):\n",
    "        return \"mono/mac\"\n",
    "    if any(k in s for k in [\"dc\", \"dend\"]):\n",
    "        return \"DC\"\n",
    "    if any(k in s for k in [\"b cell\", \"bcell\", \"cd19\", \"ms4a1\", \"plasma\"]):\n",
    "        return \"B\"\n",
    "    if any(k in s for k in [\"t cell\", \"tcell\", \"cd3\"]):\n",
    "        return \"T\"\n",
    "    if \"nk\" in s or \"ncam1\" in s or \"kldr\" in s:\n",
    "        return \"NK\"\n",
    "    if any(k in s for k in [\"ery\", \"rbc\", \"glyc\"]):\n",
    "        return \"erythroid\"\n",
    "    return \"other/NA\"\n",
    "\n",
    "if CITE_LABEL_COL is None:\n",
    "    CITE_LABEL_COL = find_label_col(cite_rna_te, [\"celltype_coarse\",\"celltype\",\"cell_type\",\"celltype.l2\",\"celltype.l1\",\"annot\",\"annotation\",\"labels\"])\n",
    "if VG_LABEL_COL is None:\n",
    "    VG_LABEL_COL = find_label_col(vg_rna_pp, [\"celltype_coarse\",\"celltype\",\"cell_type\",\"annot\",\"annotation\",\"labels\"])\n",
    "\n",
    "print(\"CITE_LABEL_COL:\", CITE_LABEL_COL)\n",
    "print(\"VG_LABEL_COL:\", VG_LABEL_COL)\n",
    "\n",
    "joint.obs[\"coarse_state\"] = \"NA\"\n",
    "\n",
    "def fill_coarse(joint, src, dataset, modality, col):\n",
    "    if col is None or col not in src.obs.columns:\n",
    "        return\n",
    "    m = (joint.obs[\"dataset\"] == dataset) & (joint.obs[\"modality\"] == modality)\n",
    "    idx = joint.obs_names[m]\n",
    "    joint.obs.loc[idx, \"coarse_state\"] = src.obs.loc[idx, col].astype(str).map(to_coarse).values\n",
    "\n",
    "fill_coarse(joint, cite_rna_te,    \"CITE\", \"RNA\", CITE_LABEL_COL)\n",
    "fill_coarse(joint, cite_adt_te, \"CITE\", \"ADT\", CITE_LABEL_COL)\n",
    "#fill_coarse(joint, vg_rna_pp,      \"VG\",   \"RNA\", VG_LABEL_COL)\n",
    "\n",
    "#sc.pl.umap(joint, color=\"coarse_state\", title=\"Fig7c: Joint latent (coarse compartments)\", show=True)\n",
    "#plt.savefig(Path(FIGDIR) / \"fig7c_umap_coarse_state.png\", dpi=450)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe1c89-4d66-4a76-a507-11591fcee1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vg_rna_pp)\n",
    "print(vg_rna_pp.obs['CellType'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24023a15-1b94-41f9-bcee-868691d842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex CellType onto joint_vg cells\n",
    "ct = vg_rna_pp.obs[\"CellType\"].reindex(joint.obs_names)\n",
    "\n",
    "# Optional sanity check\n",
    "missing = ct.isna().sum()\n",
    "print(f\"Missing CellType for {missing}/{joint.n_obs} joint cells\")\n",
    "\n",
    "joint.obs[\"coarse_state\"] = ct.astype(\"category\")\n",
    "\n",
    "#sc.pl.umap(joint, color=\"coarse_state\", title=\"Fig7c: Joint latent\", show=True)\n",
    "sc.pl.umap(joint, \n",
    "           color=\"coarse_state\", \n",
    "           title=\"Joint latent (van Galen cell type)\",\n",
    "           size=10,\n",
    "           alpha=0.7,\n",
    ")\n",
    "\n",
    "plt.savefig(Path(FIGDIR) / \"fig7_umap_VG_CellType.png\", dpi=450)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832775f6-0618-4b9d-80de-d51e6c89b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)\n",
    "print(vg_rna_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb62f06-d693-410b-8052-28adca4eccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "score_col = \"LSC17_score\"\n",
    "out_col   = \"LSC17_score\"\n",
    "\n",
    "# ensure the column exists\n",
    "if out_col not in joint.obs.columns:\n",
    "    joint.obs[out_col] = np.nan\n",
    "\n",
    "def add_scores_fill_missing(src, joint, col, out_col):\n",
    "    \"\"\"\n",
    "    Fill joint.obs[out_col] from src.obs[col] by matching on obs_names.\n",
    "\n",
    "    Works even if joint.obs_names has duplicates.\n",
    "    Only fills where joint[out_col] is currently missing (NaN).\n",
    "    \"\"\"\n",
    "    # src obs_names are unique in your report; build mapper obs_name -> score\n",
    "    mapper = pd.Series(src.obs[col].values, index=pd.Index(src.obs_names))\n",
    "\n",
    "    # lookup for every joint row (duplicates are fine; they'll get same value)\n",
    "    looked_up = pd.Index(joint.obs_names).map(mapper)\n",
    "\n",
    "    # fill only where joint currently missing and looked_up is not missing\n",
    "    cur = joint.obs[out_col].to_numpy(copy=True)\n",
    "\n",
    "    # coerce looked_up to float if possible (LSC17_score should be numeric)\n",
    "    new = pd.to_numeric(pd.Series(looked_up), errors=\"coerce\").to_numpy()\n",
    "\n",
    "    mask = np.isnan(cur) & ~np.isnan(new)\n",
    "    cur[mask] = new[mask]\n",
    "    joint.obs[out_col] = cur\n",
    "\n",
    "    return joint.obs[out_col]\n",
    "\n",
    "# choose your priority order:\n",
    "add_scores_fill_missing(cite_rna_pp_tr, joint, score_col, out_col)\n",
    "add_scores_fill_missing(cite_rna_pp_va, joint, score_col, out_col)\n",
    "add_scores_fill_missing(cite_rna_pp_te, joint, score_col, out_col)\n",
    "add_scores_fill_missing(vg_rna_pp,      joint, score_col, out_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c2320-f861-4c7f-a2c6-0791fcc54b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=\"LSC17_score\",           # or \"LSC17_vg\", \"LSC17_cite_tr\", etc.\n",
    "    title=\"Joint latent (LSC-17 score)\",\n",
    "    size=10,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "plt.savefig(Path(FIGDIR) / \"fig7_umap_LSC17_any.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076de6a3-c505-4a68-bd34-0288588ce527",
   "metadata": {},
   "source": [
    "### Figure 7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e4618-fea5-4091-9d22-1ea154c80044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "\n",
    "GENES = [\"NPM1\", \"FLT3\", \"DNMT3A\", \"TET2\", \"IDH1\", \"IDH2\", \"TP53\"]\n",
    "HERO  = \"NPM1\"\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def _coerce_str_series(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(\"string\")\n",
    "    s = s.str.strip()\n",
    "    s = s.mask(s.str.upper().isin([\"NA\", \"N/A\", \"NONE\", \"NULL\", \"NAN\", \"\"]), pd.NA)\n",
    "    return s\n",
    "\n",
    "def _gene_present(series: pd.Series, gene: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns float array of {1.0, 0.0, nan}:\n",
    "      - nan if series entry is NA/unknown\n",
    "      - 1.0 if gene appears as a token\n",
    "      - 0.0 otherwise\n",
    "    \"\"\"\n",
    "    s = _coerce_str_series(series)\n",
    "    known = s.notna()\n",
    "    pat = re.compile(rf\"\\b{re.escape(gene)}\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "    out = np.full(len(s), np.nan, dtype=np.float32)\n",
    "    if known.any():\n",
    "        out[known.values] = s[known].apply(lambda t: 1.0 if pat.search(str(t)) else 0.0).to_numpy(dtype=np.float32)\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# VG: build Y/M either from patient-level metadata OR transcripts fields\n",
    "# ============================================================\n",
    "def build_vg_targets(\n",
    "    adata: ad.AnnData,\n",
    "    genes,\n",
    "    *,\n",
    "    use_patient_table: bool = False,\n",
    "    patient_table: pd.DataFrame | None = None,\n",
    "    patient_key: str = \"orig.ident\",   # where patient IDs live in vg_rna_pp.obs\n",
    "    patient_id_col: str = \"Sample\",    # patient table column name for sample/patient id\n",
    "    rhp_col: str = \"RHP Mutations\",    # patient table column listing mutations\n",
    "    mut_col: str = \"MutTranscripts\",\n",
    "    wt_col: str = \"WtTranscripts\",\n",
    "    dataset_name: str = \"VG\",\n",
    "):\n",
    "    \"\"\"\n",
    "    If use_patient_table=True and patient_table provided:\n",
    "      - Label every cell by patient-level status.\n",
    "      - 'None Detected' => WT-labeled for all genes in 'genes'.\n",
    "      - Mention of a gene token => MUT-labeled for that gene.\n",
    "      - If table entry missing/unknown => unlabeled for those genes.\n",
    "\n",
    "    Else:\n",
    "      - Use your original transcript-fields logic.\n",
    "    \"\"\"\n",
    "    G = len(genes)\n",
    "    Y = np.zeros((adata.n_obs, G), dtype=np.float32)\n",
    "    M = np.zeros((adata.n_obs, G), dtype=np.float32)\n",
    "\n",
    "    # -------------------------\n",
    "    # Option A: patient-level\n",
    "    # -------------------------\n",
    "    if use_patient_table and (patient_table is not None):\n",
    "        if patient_key not in adata.obs.columns:\n",
    "            raise KeyError(f\"[{dataset_name}] patient_key='{patient_key}' not found in adata.obs\")\n",
    "\n",
    "        if patient_id_col not in patient_table.columns or rhp_col not in patient_table.columns:\n",
    "            raise KeyError(f\"[{dataset_name}] patient_table must have columns: '{patient_id_col}' and '{rhp_col}'\")\n",
    "\n",
    "        # build patient -> dict(gene -> 0/1/None)\n",
    "        pat2 = {}\n",
    "        for _, row in patient_table.iterrows():\n",
    "            pat = str(row[patient_id_col])\n",
    "            mut_str = str(row.get(rhp_col, \"\")).strip()\n",
    "            low = mut_str.lower()\n",
    "\n",
    "            d = {g: None for g in genes}\n",
    "\n",
    "            if mut_str == \"\" or low in {\"na\", \"unknown\", \"not performed\", \"nan\"}:\n",
    "                pat2[pat] = d\n",
    "                continue\n",
    "\n",
    "            # strong WT statement\n",
    "            if \"none detected\" in low:\n",
    "                for g in genes:\n",
    "                    d[g] = 0\n",
    "\n",
    "            # any explicit gene mention => mutant\n",
    "            for g in genes:\n",
    "                if re.search(rf\"\\b{re.escape(g)}\\b\", mut_str, flags=re.IGNORECASE):\n",
    "                    d[g] = 1\n",
    "\n",
    "            pat2[pat] = d\n",
    "\n",
    "        pats = adata.obs[patient_key].astype(str).values\n",
    "        for i, pat in enumerate(pats):\n",
    "            d = pat2.get(pat, None)\n",
    "            if d is None:\n",
    "                continue\n",
    "            for j, g in enumerate(genes):\n",
    "                v = d.get(g, None)\n",
    "                if v is None:\n",
    "                    continue\n",
    "                M[i, j] = 1.0\n",
    "                Y[i, j] = float(v)\n",
    "\n",
    "        print(f\"[{dataset_name}] built targets from PATIENT TABLE ({patient_id_col} -> {rhp_col}); \"\n",
    "              f\"labeled fractions:\", dict(zip(genes, M.mean(axis=0).round(3))))\n",
    "        return Y, M, {\"mode\": \"patient_table\", \"patient_key\": patient_key, \"patient_id_col\": patient_id_col, \"rhp_col\": rhp_col}\n",
    "\n",
    "    # -------------------------\n",
    "    # Option B: transcript fields (your original behavior)\n",
    "    # -------------------------\n",
    "    obs = adata.obs\n",
    "    if mut_col not in obs.columns:\n",
    "        raise KeyError(f\"[{dataset_name}] missing obs['{mut_col}']\")\n",
    "    if wt_col not in obs.columns:\n",
    "        raise KeyError(f\"[{dataset_name}] missing obs['{wt_col}']\")\n",
    "\n",
    "    mut_s = obs[mut_col]\n",
    "    wt_s  = obs[wt_col]\n",
    "\n",
    "    for j, g in enumerate(genes):\n",
    "        mut_has = _gene_present(mut_s, g)   # 1/0/nan\n",
    "        wt_has  = _gene_present(wt_s,  g)   # 1/0/nan\n",
    "\n",
    "        is_mut = np.isfinite(mut_has) & (mut_has == 1.0)\n",
    "        is_wt  = (~is_mut) & np.isfinite(wt_has) & (wt_has == 1.0)\n",
    "\n",
    "        labeled = is_mut | is_wt\n",
    "        M[labeled, j] = 1.0\n",
    "        Y[is_mut, j]  = 1.0\n",
    "\n",
    "    print(f\"[{dataset_name}] built targets from transcript fields: mut='{mut_col}', wt='{wt_col}'; \"\n",
    "          f\"labeled fractions:\", dict(zip(genes, M.mean(axis=0).round(3))))\n",
    "    return Y, M, {\"mode\": \"transcript_fields\", \"mut_col\": mut_col, \"wt_col\": wt_col}\n",
    "\n",
    "# ============================================================\n",
    "# DAb: gene-level labels from *ALL* matching variant columns\n",
    "# ============================================================\n",
    "def _looks_binaryish(s: pd.Series) -> bool:\n",
    "    if pd.api.types.is_bool_dtype(s) or pd.api.types.is_integer_dtype(s) or pd.api.types.is_float_dtype(s):\n",
    "        u = pd.unique(pd.Series(s.values).dropna())\n",
    "        # allow {0,1} or {0.0,1.0}\n",
    "        try:\n",
    "            uu = set(float(x) for x in u)\n",
    "        except Exception:\n",
    "            return False\n",
    "        return all(x in {0.0, 1.0} for x in uu) and len(uu) <= 2\n",
    "    return False\n",
    "\n",
    "def build_dab_targets_gene_level(\n",
    "    adata: ad.AnnData,\n",
    "    genes,\n",
    "    *,\n",
    "    dataset_name: str = \"DAb\",\n",
    "    allow_dash_match: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build Y/M at the gene level by OR-ing across all obs columns that match that gene.\n",
    "    Matching:\n",
    "      - startswith gene (preferred): \"NPM1 W288fs\"\n",
    "      - contains token gene: \"FLT3-ITD\" (if allow_dash_match True)\n",
    "    Labeling:\n",
    "      - M=1 if ANY matched column is non-NA for that cell\n",
    "      - Y=1 if ANY matched column equals 1 among labeled columns\n",
    "    \"\"\"\n",
    "    obs = adata.obs\n",
    "    G = len(genes)\n",
    "    Y = np.zeros((adata.n_obs, G), dtype=np.float32)\n",
    "    M = np.zeros((adata.n_obs, G), dtype=np.float32)\n",
    "\n",
    "    gene2cols = {g: [] for g in genes}\n",
    "    cols = list(obs.columns)\n",
    "\n",
    "    for g in genes:\n",
    "        gU = g.upper()\n",
    "        # startwith matches\n",
    "        start_hits = [c for c in cols if str(c).upper().startswith(gU)]\n",
    "        hits = list(start_hits)\n",
    "\n",
    "        # token/contains matches (helps FLT3-ITD)\n",
    "        if allow_dash_match:\n",
    "            tok_hits = [c for c in cols if re.search(rf\"\\b{re.escape(g)}\\b\", str(c), flags=re.IGNORECASE)]\n",
    "            for c in tok_hits:\n",
    "                if c not in hits:\n",
    "                    hits.append(c)\n",
    "\n",
    "        # keep only binary-ish columns if possible (most DAb mutation cols are 0/1)\n",
    "        if len(hits) > 1:\n",
    "            bin_hits = [c for c in hits if _looks_binaryish(obs[c])]\n",
    "            if len(bin_hits) > 0:\n",
    "                hits = bin_hits\n",
    "\n",
    "        gene2cols[g] = hits\n",
    "\n",
    "    print(f\"[{dataset_name}] gene->matched_cols:\")\n",
    "    for g, cs in gene2cols.items():\n",
    "        if cs:\n",
    "            print(\" \", g, \":\", cs)\n",
    "\n",
    "    for j, g in enumerate(genes):\n",
    "        cs = gene2cols[g]\n",
    "        if not cs:\n",
    "            continue\n",
    "\n",
    "        # stack as float matrix (n, k)\n",
    "        X = np.vstack([pd.to_numeric(obs[c], errors=\"coerce\").to_numpy(dtype=np.float32) for c in cs]).T\n",
    "\n",
    "        called = np.isfinite(X).any(axis=1)\n",
    "        M[:, j] = called.astype(np.float32)\n",
    "\n",
    "        # mutant if any == 1 among columns\n",
    "        mut = called & (np.nanmax(X, axis=1) >= 1.0)\n",
    "        Y[mut, j] = 1.0\n",
    "\n",
    "    print(f\"[{dataset_name}] labeled fractions:\", dict(zip(genes, M.mean(axis=0).round(3))))\n",
    "    return Y, M, gene2cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a0438-8b5f-4346-9d2b-f8bce329ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# VG labels:\n",
    "# Option A (recommended if you have the patient mutation table loaded):\n",
    "#   set use_patient_table=True and pass patient_table=vg_patient_df\n",
    "# Option B (fallback): transcript fields\n",
    "# -------------------------\n",
    "\n",
    "USE_VG_PATIENT_TABLE = False  # <-- flip to True if you load the patient table\n",
    "vg_patient_df = None          # <-- set this to your loaded DF when USE_VG_PATIENT_TABLE=True\n",
    "\n",
    "# IMPORTANT: your split code detected group_source=orig.ident in your log,\n",
    "# so use patient_key=\"orig.ident\" to match that.\n",
    "Y_vg, M_vg, vg_info = build_vg_targets(\n",
    "    vg_rna_pp,\n",
    "    GENES,\n",
    "    use_patient_table=USE_VG_PATIENT_TABLE,\n",
    "    patient_table=vg_patient_df,\n",
    "    patient_key=\"orig.ident\",\n",
    "    patient_id_col=\"Sample\",\n",
    "    rhp_col=\"RHP Mutations\",\n",
    "    mut_col=\"MutTranscripts\",\n",
    "    wt_col=\"WtTranscripts\",\n",
    "    dataset_name=\"VG\",\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# DAb labels (gene-level OR across all matched variant cols)\n",
    "# -------------------------\n",
    "Y_dab, M_dab, dab_gene2cols = build_dab_targets_gene_level(\n",
    "    dab_adt_pp,\n",
    "    GENES,\n",
    "    dataset_name=\"DAb\",\n",
    ")\n",
    "\n",
    "print(\"VG labeling mode:\", vg_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33895521-87a1-4648-84a7-46569850abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_transfer_probs(Z_source, Y_source, M_source, Z_target, k=50):\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric=\"euclidean\").fit(Z_source)\n",
    "    idx = knn.kneighbors(Z_target, return_distance=False)  # (Nt,k)\n",
    "    G = Y_source.shape[1]\n",
    "    P = np.full((Z_target.shape[0], G), np.nan, dtype=np.float32)\n",
    "    for g in range(G):\n",
    "        ys = Y_source[:, g]\n",
    "        ms = M_source[:, g]\n",
    "        ms_k = ms[idx]\n",
    "        ys_k = ys[idx]\n",
    "        denom = ms_k.sum(axis=1)\n",
    "        num = (ys_k * ms_k).sum(axis=1)\n",
    "        ok = denom > 0\n",
    "        P[ok, g] = (num[ok] / denom[ok]).astype(np.float32)\n",
    "    return P\n",
    "\n",
    "hero_i = GENES.index(HERO)\n",
    "\n",
    "k=60\n",
    "\n",
    "P_dab_from_vg = knn_transfer_probs(\n",
    "    Z_source=vg_rna_pp.obsm[\"X_univi\"],\n",
    "    Y_source=Y_vg,\n",
    "    M_source=M_vg,\n",
    "    Z_target=dab_adt_pp.obsm[\"X_univi\"],\n",
    "    k=k\n",
    ")\n",
    "\n",
    "dab_adt_al.obs[f\"knnP_{HERO}_vg_to_dab\"] = P_dab_from_vg[:, hero_i]\n",
    "\n",
    "# put onto joint for plotting (DAb ADT subset)\n",
    "col = f\"knnP_{HERO}_vg_to_dab\"\n",
    "joint.obs[col] = np.nan\n",
    "mask_dab = (joint.obs[\"dataset\"] == \"DAb\") & (joint.obs[\"modality\"] == \"ADT\")\n",
    "joint.obs.loc[joint.obs_names[mask_dab], col] = dab_adt_al.obs[col].values\n",
    "\n",
    "#sc.pl.umap(joint, color=col, title=f\"Fig7d: Transfer {HERO} probability (VG→DAb, kNN (k={k}))\", show=True)\n",
    "sc.pl.umap(joint, \n",
    "           color=col, \n",
    "           title=f\"Transfer {HERO} probability (VG→DAb, kNN (k={k}))\",\n",
    "           size=10,\n",
    "           alpha=0.7,\n",
    ")\n",
    "\n",
    "plt.savefig(Path(FIGDIR) / f\"fig7_umap_transfer_{HERO}_vg_to_dab.png\", dpi=450)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763961a-211d-4524-a06c-a84436e52ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "FIGDIR = Path(FIGDIR)\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def find_mut_col_dab_obs(adata, gene):\n",
    "    cols = list(adata.obs.columns)\n",
    "    gene_u = str(gene).upper()\n",
    "\n",
    "    # Exact match (rare)\n",
    "    for c in cols:\n",
    "        if c.upper() == gene_u:\n",
    "            return c\n",
    "\n",
    "    # Startswith 'GENE ' (common: \"NPM1 W288fs\")\n",
    "    for c in cols:\n",
    "        if c.upper().startswith(gene_u + \" \"):\n",
    "            return c\n",
    "\n",
    "    # Token contains\n",
    "    for c in cols:\n",
    "        toks = c.upper().replace(\"-\", \" \").replace(\"_\", \" \").split()\n",
    "        if gene_u in toks:\n",
    "            return c\n",
    "\n",
    "    # Special-case FLT3-ITD style\n",
    "    if gene_u == \"FLT3\":\n",
    "        for c in cols:\n",
    "            cu = c.upper()\n",
    "            if \"FLT3\" in cu and \"ITD\" in cu:\n",
    "                return c\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def coerce_to_nullable_boolean(x):\n",
    "    \"\"\"\n",
    "    Convert an array/Series into pandas nullable boolean ('boolean') with <NA>.\n",
    "    Accepts: 0/1, True/False, strings like '0','1','WT','Mut', etc.\n",
    "    \"\"\"\n",
    "    s = pd.Series(x)\n",
    "\n",
    "    # Already boolean-ish\n",
    "    if pd.api.types.is_bool_dtype(s) or str(s.dtype).lower() == \"boolean\":\n",
    "        return s.astype(\"boolean\")\n",
    "\n",
    "    # Numeric\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        out = pd.Series(pd.NA, index=s.index, dtype=\"boolean\")\n",
    "        out[s == 1] = True\n",
    "        out[s == 0] = False\n",
    "        return out\n",
    "\n",
    "    # Strings / objects\n",
    "    ss = s.astype(\"string\").str.strip().str.lower()\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"boolean\")\n",
    "    true_set  = {\"true\",\"t\",\"1\",\"yes\",\"y\",\"mut\",\"mutant\",\"pos\",\"positive\"}\n",
    "    false_set = {\"false\",\"f\",\"0\",\"no\",\"n\",\"wt\",\"wildtype\",\"neg\",\"negative\"}\n",
    "    out[ss.isin(true_set)] = True\n",
    "    out[ss.isin(false_set)] = False\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_label_from_nullable_bool(s_bool, *, wt_label=\"WT\", mut_label=\"Mut\"):\n",
    "    \"\"\"\n",
    "    Convert nullable boolean Series -> categorical labels WT/Mut with NA preserved.\n",
    "    Returns pandas Categorical.\n",
    "    \"\"\"\n",
    "    s = pd.Series(pd.NA, index=s_bool.index, dtype=\"string\")\n",
    "    s.loc[s_bool == True]  = mut_label\n",
    "    s.loc[s_bool == False] = wt_label\n",
    "    return pd.Categorical(s, categories=[wt_label, mut_label], ordered=True)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Identify actual DAb mutation column for HERO\n",
    "# ----------------------------\n",
    "mut_col = find_mut_col_dab_obs(dab_adt_al, HERO)\n",
    "if mut_col is None:\n",
    "    raise KeyError(\n",
    "        f\"Couldn't find an actual mutation column for {HERO} in dab_adt_al.obs.\\n\"\n",
    "        f\"Example columns: {list(dab_adt_al.obs.columns[:40])}\"\n",
    "    )\n",
    "print(f\"[{HERO}] using DAb obs column: {mut_col}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Create clean nullable-boolean actual mutation on DAb AnnData\n",
    "# ----------------------------\n",
    "col_actual = f\"actual_{HERO}_mut\"\n",
    "dab_adt_al.obs[col_actual] = coerce_to_nullable_boolean(dab_adt_al.obs[mut_col])\n",
    "\n",
    "print(\n",
    "    f\"  True:  {(dab_adt_al.obs[col_actual] == True).sum()}\\n\"\n",
    "    f\"  False: {(dab_adt_al.obs[col_actual] == False).sum()}\\n\"\n",
    "    f\"  NA:    {dab_adt_al.obs[col_actual].isna().sum()}\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Copy actual mutation onto joint ONLY for DAb ADT cells\n",
    "#    Keep as nullable boolean dtype\n",
    "# ----------------------------\n",
    "mask_dab = (joint.obs[\"dataset\"] == \"DAb\") & (joint.obs[\"modality\"] == \"ADT\")\n",
    "\n",
    "# initialize as nullable boolean\n",
    "joint.obs[col_actual] = pd.Series(pd.NA, index=joint.obs_names, dtype=\"boolean\")\n",
    "\n",
    "# assign values for DAb ADT subset\n",
    "joint.obs.loc[mask_dab, col_actual] = (\n",
    "    dab_adt_al.obs[col_actual].astype(\"boolean\").to_numpy()\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Create a categorical label column for plotting (THIS is what we plot)\n",
    "# ----------------------------\n",
    "col_actual_plot = f\"{col_actual}_label\"\n",
    "joint.obs[col_actual_plot] = make_label_from_nullable_bool(joint.obs[col_actual])\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Plot: Actual mutation (categorical legend, NA grey)\n",
    "# ----------------------------\n",
    "'''\n",
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=col_actual_plot,          # <-- categorical label, avoids boolean negation bug\n",
    "    title=f\"Actual {HERO} mutation (DAb observed) on joint UMAP\",\n",
    "    na_color=\"lightgrey\",\n",
    "    show=False,\n",
    ")\n",
    "'''\n",
    "sc.pl.umap(joint, \n",
    "           color=col_actual_plot, \n",
    "           title=f\"Actual {HERO} mutation (DAb observed) on joint UMAP\",\n",
    "           size=10,\n",
    "           alpha=0.7,\n",
    ")\n",
    "\n",
    "plt.savefig(FIGDIR / f\"fig7d_umap_actual_{HERO}_mut_on_joint.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Plot: Predicted/Transferred probability (continuous), if present\n",
    "# ----------------------------\n",
    "col_prob = f\"knnP_{HERO}_vg_to_dab\"\n",
    "if col_prob in joint.obs.columns:\n",
    "    '''\n",
    "    sc.pl.umap(\n",
    "        joint,\n",
    "        color=col_prob,\n",
    "        title=f\"Transfer {HERO} probability (VG→DAb, kNN) on joint UMAP\",\n",
    "        na_color=\"lightgrey\",\n",
    "        show=False,\n",
    "    )\n",
    "    '''\n",
    "    sc.pl.umap(joint, \n",
    "           color=col_prob, \n",
    "           title=f\"Transfer {HERO} probability (VG→DAb, kNN) on joint UMAP\",\n",
    "           size=10,\n",
    "           alpha=0.7,\n",
    "    )\n",
    "    plt.savefig(FIGDIR / f\"fig7_umap_transfer_{HERO}_vg_to_dab_on_joint.png\", dpi=450, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"NOTE: {col_prob} not found in joint.obs; skipping transfer-prob UMAP.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) OPTIONAL: Side-by-side (Actual label + Pred prob)\n",
    "# ----------------------------\n",
    "if col_prob in joint.obs.columns:\n",
    "    '''\n",
    "    sc.pl.umap(\n",
    "        joint,\n",
    "        color=[col_actual_plot, col_prob],\n",
    "        title=[f\"Actual {HERO} (DAb)\", f\"Transfer P({HERO}) (VG→DAb)\"],\n",
    "        na_color=\"lightgrey\",\n",
    "        wspace=0.35,\n",
    "        show=False,\n",
    "    )\n",
    "    '''\n",
    "    sc.pl.umap(\n",
    "        joint, \n",
    "        color=[col_actual_plot, col_prob], \n",
    "        title=[f\"Actual {HERO} (DAb)\", f\"Transfer P({HERO}) (VG→DAb)\"],\n",
    "        size=10,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.savefig(FIGDIR / f\"fig7_umap_actual_and_transfer_{HERO}.png\", dpi=450, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16061e41-87b1-4d98-a90a-1f4eee899f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def knn_transfer_mut_prob_1d(Z_source, y_source_bool, Z_target, k=30):\n",
    "    \"\"\"\n",
    "    y_source_bool: pandas Series or array-like nullable boolean (dtype 'boolean' ok)\n",
    "      True=Mut, False=WT, NA=unknown\n",
    "\n",
    "    Returns:\n",
    "      p_mut: (Nt,) float32 with NaN where no labeled neighbors\n",
    "      denom: (Nt,) int number of labeled neighbors among k\n",
    "    \"\"\"\n",
    "    # Coerce to numpy with mask\n",
    "    y = pd.Series(y_source_bool).astype(\"boolean\")\n",
    "    y_val = y.to_numpy(dtype=object)  # will contain True/False/<NA>\n",
    "    labeled = pd.notna(y_val)         # boolean mask\n",
    "    y01 = np.zeros(len(y_val), dtype=np.float32)\n",
    "    y01[labeled] = (y_val[labeled] == True).astype(np.float32)\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric=\"euclidean\").fit(Z_source)\n",
    "    idx = knn.kneighbors(Z_target, return_distance=False)  # (Nt,k)\n",
    "\n",
    "    lab_k = labeled[idx]                # (Nt,k)\n",
    "    denom = lab_k.sum(axis=1).astype(np.int32)\n",
    "    num = (y01[idx] * lab_k).sum(axis=1).astype(np.float32)\n",
    "\n",
    "    p_mut = np.full(Z_target.shape[0], np.nan, dtype=np.float32)\n",
    "    ok = denom > 0\n",
    "    p_mut[ok] = num[ok] / denom[ok]\n",
    "    return p_mut, denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5503b-71db-494d-a4f9-2c93b3446e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# DAb -> VG transfer (HERO)\n",
    "# ----------------------------\n",
    "k = 50\n",
    "col_actual = f\"actual_{HERO}_mut\"              # nullable boolean on DAb\n",
    "col_p = f\"knnP_{HERO}_dab_to_vg\"               # transferred prob\n",
    "col_p_denom = f\"knnN_{HERO}_dab_to_vg\"         # labeled neighbors count\n",
    "col_label = f\"knnL_{HERO}_dab_to_vg_label\"     # categorical hard label\n",
    "\n",
    "# 1) Transfer probability onto VG cells\n",
    "p_vg, denom_vg = knn_transfer_mut_prob_1d(\n",
    "    Z_source=dab_adt_pp.obsm[\"X_univi\"],                 # source embedding\n",
    "    y_source_bool=dab_adt_al.obs[col_actual],            # source labels\n",
    "    Z_target=vg_rna_pp.obsm[\"X_univi\"],                  # target embedding\n",
    "    k=k\n",
    ")\n",
    "\n",
    "vg_rna_pp.obs[col_p] = p_vg\n",
    "vg_rna_pp.obs[col_p_denom] = denom_vg\n",
    "\n",
    "# 2) Make a hard label for plotting (Mut/WT/NA)\n",
    "#    (Choose threshold you like; 0.5 is standard)\n",
    "thr = 0.75\n",
    "lab = pd.Series(pd.NA, index=vg_rna_pp.obs_names, dtype=\"string\")\n",
    "lab.loc[vg_rna_pp.obs[col_p].notna() & (vg_rna_pp.obs[col_p] >= thr)] = \"Mut\"\n",
    "lab.loc[vg_rna_pp.obs[col_p].notna() & (vg_rna_pp.obs[col_p] <  thr)] = \"WT\"\n",
    "vg_rna_pp.obs[col_label] = pd.Categorical(lab, categories=[\"WT\", \"Mut\"], ordered=True)\n",
    "\n",
    "print(\n",
    "    f\"[{HERO}] DAb→VG transfer (k={k}, thr={thr})\\n\"\n",
    "    f\"  VG cells w/ labeled neighbors: {(vg_rna_pp.obs[col_p_denom] > 0).sum()} / {vg_rna_pp.n_obs}\\n\"\n",
    "    f\"  Pred Mut: {(vg_rna_pp.obs[col_label] == 'Mut').sum()}\\n\"\n",
    "    f\"  Pred WT:  {(vg_rna_pp.obs[col_label] == 'WT').sum()}\\n\"\n",
    "    f\"  NA:       {pd.isna(vg_rna_pp.obs[col_label]).sum()}\"\n",
    ")\n",
    "\n",
    "# 3) Ensure VG UMAP exists (only if needed)\n",
    "if \"X_umap\" not in vg_rna_pp.obsm_keys():\n",
    "    sc.pp.neighbors(vg_rna_pp, use_rep=\"X_univi\", n_neighbors=30)\n",
    "    sc.tl.umap(vg_rna_pp, random_state=42)\n",
    "\n",
    "# 4) Plot probability (continuous)\n",
    "sc.pl.umap(\n",
    "    vg_rna_pp,\n",
    "    color=col_p,\n",
    "    title=f\"Transfer P({HERO} Mut) DAb→VG (kNN k={k})\",\n",
    "    size=10,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    show=False,\n",
    ")\n",
    "plt.savefig(FIGDIR / f\"umap_vg_transferP_{HERO}_dab_to_vg.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 5) Plot hard label (categorical)\n",
    "sc.pl.umap(\n",
    "    vg_rna_pp,\n",
    "    color=col_label,\n",
    "    title=f\"Transfer label {HERO} DAb→VG (thr={thr}, k={k})\",\n",
    "    size=10,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    show=False,\n",
    ")\n",
    "plt.savefig(FIGDIR / f\"umap_vg_transferLabel_{HERO}_dab_to_vg.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 6) Optional side-by-side\n",
    "sc.pl.umap(\n",
    "    vg_rna_pp,\n",
    "    color=[col_label, col_p],\n",
    "    title=[f\"{HERO} label (DAb→VG)\", f\"P({HERO} Mut) (DAb→VG)\"],\n",
    "    size=10,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    wspace=0.35,\n",
    "    show=False,\n",
    ")\n",
    "plt.savefig(FIGDIR / f\"umap_vg_transferLabel_and_P_{HERO}_dab_to_vg.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b78ed-eb2b-4600-9c40-1c7f3972291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _contains_gene(text, gene):\n",
    "    \"\"\"\n",
    "    Returns True if `gene` appears as a token in `text` (case-insensitive).\n",
    "    Robust to separators like ',', ';', '|', '/', '-', etc.\n",
    "    \"\"\"\n",
    "    if text is None or (isinstance(text, float) and pd.isna(text)) or pd.isna(text):\n",
    "        return False\n",
    "    s = str(text).upper()\n",
    "    # turn non-alphanum into spaces, then token-match\n",
    "    s = re.sub(r\"[^A-Z0-9]+\", \" \", s)\n",
    "    g = str(gene).upper()\n",
    "    return re.search(rf\"\\b{re.escape(g)}\\b\", s) is not None\n",
    "\n",
    "def vg_actual_mut_from_transcripts(vg, gene, mut_col=\"MutTranscripts\", wt_col=\"WtTranscripts\"):\n",
    "    \"\"\"\n",
    "    Build a nullable-boolean Series:\n",
    "      True  = gene present in MutTranscripts\n",
    "      False = gene present in WtTranscripts\n",
    "      <NA>  = neither or both (ambiguous)\n",
    "    \"\"\"\n",
    "    has_mut = vg.obs[mut_col].apply(lambda x: _contains_gene(x, gene))\n",
    "    has_wt  = vg.obs[wt_col].apply(lambda x: _contains_gene(x, gene))\n",
    "\n",
    "    out = pd.Series(pd.NA, index=vg.obs_names, dtype=\"boolean\")\n",
    "    out[ has_mut & ~has_wt] = True\n",
    "    out[~has_mut &  has_wt] = False\n",
    "    # ambiguous if both\n",
    "    out[ has_mut &  has_wt] = pd.NA\n",
    "    return out, has_mut, has_wt\n",
    "\n",
    "def make_label_from_nullable_bool(s_bool, wt_label=\"WT\", mut_label=\"Mut\"):\n",
    "    s = pd.Series(pd.NA, index=s_bool.index, dtype=\"string\")\n",
    "    s.loc[s_bool == True]  = mut_label\n",
    "    s.loc[s_bool == False] = wt_label\n",
    "    return pd.Categorical(s, categories=[wt_label, mut_label], ordered=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Build \"actual mutation\" from transcript-string columns\n",
    "# ----------------------------\n",
    "col_bool  = f\"actual_{HERO}_vg_bool\"\n",
    "col_label = f\"actual_{HERO}_vg_label\"\n",
    "\n",
    "vg_rna_pp.obs[col_bool], has_mut, has_wt = vg_actual_mut_from_transcripts(vg_rna_pp, HERO)\n",
    "\n",
    "print(\n",
    "    f\"[{HERO}] from MutTranscripts/WtTranscripts\\n\"\n",
    "    f\"  Mut-only (True):  {(vg_rna_pp.obs[col_bool] == True).sum()}\\n\"\n",
    "    f\"  WT-only (False):  {(vg_rna_pp.obs[col_bool] == False).sum()}\\n\"\n",
    "    f\"  NA (neither/both):{vg_rna_pp.obs[col_bool].isna().sum()}\\n\"\n",
    "    f\"  ambiguous (both): {(has_mut & has_wt).sum()}\"\n",
    ")\n",
    "\n",
    "# categorical labels for plotting (prevents continuous/boolean quirks)\n",
    "vg_rna_pp.obs[col_label] = make_label_from_nullable_bool(vg_rna_pp.obs[col_bool])\n",
    "\n",
    "# ----------------------------\n",
    "# Plot on VG UMAP\n",
    "# ----------------------------\n",
    "sc.pl.umap(\n",
    "    vg_rna_pp,\n",
    "    color=[col_label],\n",
    "    title=[f\"van Galen actual {HERO} mutation (from transcripts)\"],\n",
    "    size=20,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    show=True,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# optional: side-by-side with patient/timepoint\n",
    "# sc.pl.umap(\n",
    "#     vg_rna_pp,\n",
    "#     color=[\"patient\", \"timepoint\", col_label],\n",
    "#     title=[\"patient\", \"timepoint\", f\"actual {HERO}\"],\n",
    "#     na_color=\"lightgrey\",\n",
    "#     wspace=0.35,\n",
    "#     show=True,\n",
    "# )\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4041632-017f-4695-b2a1-437adde182c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da81b1e-48e3-4065-9935-ff7e6eaa9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HERO = \"NPM1\"  # if you're using this pattern elsewhere\n",
    "\n",
    "# --- columns in VG ---\n",
    "vg_bool_col  = \"actual_NPM1_vg_bool\"    # True/False (or bool-ish)\n",
    "vg_label_col = \"actual_NPM1_vg_label\"   # e.g. \"Mut\"/\"WT\" (categorical)\n",
    "\n",
    "# --- output columns in joint ---\n",
    "out_bool_col  = \"actual_NPM1_mut\"\n",
    "out_label_col = \"actual_NPM1_mut_label\"\n",
    "\n",
    "def map_vg_obs_to_joint(joint, vg, vg_col, out_col, *, to_category=False, fill_value=np.nan, overwrite=True):\n",
    "    \"\"\"\n",
    "    Map vg.obs[vg_col] onto joint.obs_names (works even if joint.obs_names has duplicates).\n",
    "    \"\"\"\n",
    "    if (out_col in joint.obs.columns) and (not overwrite):\n",
    "        raise ValueError(f\"{out_col} already exists in joint.obs (set overwrite=True to replace).\")\n",
    "\n",
    "    mapper = pd.Series(vg.obs[vg_col].values, index=pd.Index(vg.obs_names))  # vg obs_names are unique\n",
    "    looked_up = pd.Index(joint.obs_names).map(mapper)  # duplicates in joint are fine\n",
    "\n",
    "    s = pd.Series(looked_up, index=joint.obs_names, name=out_col)\n",
    "\n",
    "    if fill_value is not np.nan:\n",
    "        s = s.fillna(fill_value)\n",
    "\n",
    "    if to_category:\n",
    "        s = s.astype(\"category\")\n",
    "\n",
    "    joint.obs[out_col] = s\n",
    "    return joint.obs[out_col]\n",
    "\n",
    "# 1) boolean-ish mutation status\n",
    "map_vg_obs_to_joint(joint, vg_rna_pp, vg_bool_col, out_bool_col, to_category=False, overwrite=True)\n",
    "\n",
    "# 2) label version (Mut/WT)\n",
    "map_vg_obs_to_joint(joint, vg_rna_pp, vg_label_col, out_label_col, to_category=True, overwrite=True)\n",
    "\n",
    "# Optional: make the bool column nicer for plotting (keeps NaN as missing)\n",
    "# joint.obs[out_bool_col] = joint.obs[out_bool_col].map({True: \"Mut\", False: \"WT\"}).astype(\"category\")\n",
    "\n",
    "# --- plot ---\n",
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=[out_label_col],  # NOTE: pass the *string column name*\n",
    "    title=[f\"van Galen actual {HERO} mutation (from transcripts)\"],\n",
    "    size=20,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    show=True,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff4b64-62d1-45b9-9e17-396bfc32726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) sample prefix from obs_names like \"AML556.D0_cell123\" -> \"AML556.D0\"\n",
    "vg_rna_pp.obs[\"sample\"] = vg_rna_pp.obs_names.to_series().str.split(\"_\", n=1).str[0].values\n",
    "\n",
    "s = vg_rna_pp.obs[\"sample\"].astype(str)\n",
    "\n",
    "# 2) default parse: patient = before first \".\", timepoint = after first \".\"\n",
    "patient = s.str.extract(r\"^([^.]+)\")[0]\n",
    "timepoint = s.str.extract(r\"^[^.]+\\.(.+)$\")[0]  # becomes NaN if no \".\"\n",
    "\n",
    "# 3) special-cases\n",
    "# OCI.* looks like a cell line label (OCI.AML3) — treat as patient, no timepoint\n",
    "is_oci = s.str.startswith(\"OCI.\")\n",
    "patient.loc[is_oci] = s.loc[is_oci]\n",
    "timepoint.loc[is_oci] = pd.NA\n",
    "\n",
    "# If no dot (e.g. BM1, BM2, MUTZ3.frozen if it had no dot), keep timepoint NA\n",
    "# (already handled by the regex)\n",
    "\n",
    "# 4) write into obs\n",
    "vg_rna_pp.obs[\"patient\"] = patient.astype(\"category\")\n",
    "vg_rna_pp.obs[\"timepoint\"] = pd.Series(timepoint).astype(\"string\").astype(\"category\")\n",
    "\n",
    "# 5) quick sanity checks\n",
    "print(vg_rna_pp.obs[[\"sample\", \"patient\", \"timepoint\"]].head())\n",
    "print(\"\\nPatients:\", vg_rna_pp.obs[\"patient\"].nunique())\n",
    "print(\"Timepoints:\", vg_rna_pp.obs[\"timepoint\"].nunique(dropna=True))\n",
    "print(\"\\nTop sample counts:\\n\", vg_rna_pp.obs[\"sample\"].value_counts().head(10))\n",
    "print(\"\\nNA timepoint:\", vg_rna_pp.obs[\"timepoint\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2fdc1b-8926-4826-bdae-67b512714044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot (IMPORTANT: pass column name(s), not the Series)\n",
    "sc.pl.umap(\n",
    "    vg_rna_pp,\n",
    "    color=[\"patient\"],          # or [\"patient\", \"timepoint\"]\n",
    "    title=[\"van Galen patient\"],\n",
    "    size=20,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    show=True,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e54c9-ff16-45ed-8508-efc1a623b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    vg_rna_pp,\n",
    "    color=[\"CellType\"],          # or [\"patient\", \"timepoint\"]\n",
    "    title=[\"van Galen annotated cell type\"],\n",
    "    size=20,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    show=True,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62dd6c7-c98d-4184-9264-b1b6428e3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cite_rna_te)\n",
    "print(cite_adt_te)\n",
    "print(vg_rna_pp)\n",
    "print(dab_adt_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3eaee2-0ad0-4d58-b6bb-61723f7dc168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "FIGDIR = Path(FIGDIR)\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Config / column names\n",
    "# ----------------------------\n",
    "k = 50\n",
    "thr = 0.80\n",
    "\n",
    "col_p_vg     = f\"knnP_{HERO}_dab_to_vg\"          # in vg_rna_pp.obs\n",
    "col_denom_vg = f\"knnN_{HERO}_dab_to_vg\"          # in vg_rna_pp.obs\n",
    "col_lab_vg   = f\"knnL_{HERO}_dab_to_vg_label\"    # in vg_rna_pp.obs (Categorical WT/Mut)\n",
    "\n",
    "# Where we will store them on JOINT for plotting\n",
    "col_p_joint     = col_p_vg\n",
    "col_denom_joint = col_denom_vg\n",
    "col_lab_joint   = col_lab_vg\n",
    "\n",
    "# ----------------------------\n",
    "# Sanity checks\n",
    "# ----------------------------\n",
    "missing = [c for c in [col_p_vg, col_denom_vg, col_lab_vg] if c not in vg_rna_pp.obs.columns]\n",
    "if missing:\n",
    "    raise KeyError(\n",
    "        f\"Missing {missing} in vg_rna_pp.obs. \"\n",
    "        f\"Run the DAb→VG transfer chunk first to create these.\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Masks (VG RNA cells inside joint)\n",
    "# ----------------------------\n",
    "mask_vg_rna = (joint.obs[\"dataset\"] == \"VG\") & (joint.obs[\"modality\"] == \"RNA\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Copy continuous probability onto joint\n",
    "# ----------------------------\n",
    "joint.obs[col_p_joint] = np.nan\n",
    "joint.obs.loc[mask_vg_rna, col_p_joint] = vg_rna_pp.obs[col_p_vg].to_numpy()\n",
    "\n",
    "# Optional: copy denom (# labeled neighbors among k) too\n",
    "joint.obs[col_denom_joint] = np.nan\n",
    "joint.obs.loc[mask_vg_rna, col_denom_joint] = vg_rna_pp.obs[col_denom_vg].to_numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Copy categorical label onto joint (initialize as string/categorical, NOT float)\n",
    "# ----------------------------\n",
    "joint.obs[col_lab_joint] = pd.Series(pd.NA, index=joint.obs_names, dtype=\"string\")\n",
    "joint.obs.loc[mask_vg_rna, col_lab_joint] = vg_rna_pp.obs[col_lab_vg].astype(\"string\").to_numpy()\n",
    "\n",
    "# Make categorical for nice legend order\n",
    "joint.obs[col_lab_joint] = pd.Categorical(\n",
    "    joint.obs[col_lab_joint],\n",
    "    categories=[\"WT\", \"Mut\"],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Plot on JOINT UMAP\n",
    "# ----------------------------\n",
    "# (A) Probability (continuous)\n",
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=col_p_joint,\n",
    "    title=f\"Transfer P({HERO} Mut) DAb→VG on JOINT UMAP (k={k})\",\n",
    "    size=10,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    show=False,\n",
    ")\n",
    "plt.savefig(FIGDIR / f\"umap_joint_transferP_{HERO}_dab_to_vg.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# (B) Label (categorical)\n",
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=col_lab_joint,\n",
    "    title=f\"Transfer label {HERO} DAb→VG on JOINT UMAP (thr={thr}, k={k})\",\n",
    "    size=10,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    show=False,\n",
    ")\n",
    "plt.savefig(FIGDIR / f\"umap_joint_transferLabel_{HERO}_dab_to_vg.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# (C) Side-by-side\n",
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=[col_lab_joint, col_p_joint],\n",
    "    title=[f\"{HERO} label (DAb→VG)\", f\"P({HERO} Mut) (DAb→VG)\"],\n",
    "    size=10,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    wspace=0.35,\n",
    "    show=False,\n",
    ")\n",
    "plt.savefig(FIGDIR / f\"umap_joint_transferLabel_and_P_{HERO}_dab_to_vg.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e079f-0cb2-4744-b7b5-49f080ab7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=[\"coarse_state\"],\n",
    "    title=[f\"Joint UMAP with van Galen annotations overlaid\"],\n",
    "    size=16,\n",
    "    alpha=0.7,\n",
    "    na_color=\"lightgrey\",\n",
    "    wspace=0.35,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d0f72-f1b2-426f-810a-97737efdc944",
   "metadata": {},
   "source": [
    "### Figure 7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1345e3-dc5c-4075-963b-57d3414ee552",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Y_dab[:, hero_i]\n",
    "m_true = M_dab[:, hero_i].astype(bool)\n",
    "p_pred = P_dab_from_vg[:, hero_i]\n",
    "\n",
    "ok = m_true & np.isfinite(p_pred)\n",
    "y = y_true[ok].astype(int)\n",
    "p = p_pred[ok].astype(float)\n",
    "\n",
    "if ok.sum() < 20 or len(np.unique(y)) < 2:\n",
    "    print(f\"Not enough labeled positives/negatives for {HERO}: n={ok.sum()}, classes={np.unique(y)}\")\n",
    "else:\n",
    "    auc = roc_auc_score(y, p)\n",
    "    ap  = average_precision_score(y, p)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y, p)\n",
    "    prec, rec, _ = precision_recall_curve(y, p)\n",
    "\n",
    "    plt.figure(figsize=(7.2, 3.2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "    plt.title(f\"ROC (AUC={auc:.3f})\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(rec, prec)\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"PR (AP={ap:.3f})\")\n",
    "\n",
    "    plt.suptitle(f\"{HERO} transfer VG→DAb (n={ok.sum()})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(FIGDIR) / f\"fig7e_roc_pr_transfer_{HERO}.png\", dpi=450)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Transfer metrics:\", {\"AUC\": float(auc), \"AP\": float(ap), \"n\": int(ok.sum())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce13d4-aa97-47dd-9f1e-8ab971576b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "def eval_knn_transfer_dab_to_vg(\n",
    "    *,\n",
    "    vg_rna_pp,          # AnnData with knnP/knnN columns in .obs\n",
    "    Y_vg, M_vg,         # arrays for VG cells (same gene order as GENES)\n",
    "    genes, hero,\n",
    "    figdir,\n",
    "    k=50,\n",
    "    thr=0.5,\n",
    "    min_n=20,\n",
    "    min_labeled_neighbors=1,   # set e.g. 10 to be stricter\n",
    "    use_name_alignment=False,  # True if Y_vg/M_vg rows match vg_rna_pp.obs_names via a dict\n",
    "    vg_obs_names_for_YM=None,  # required if use_name_alignment=True: array-like of names aligned to Y_vg rows\n",
    "    prefix=\"Fig7e\",\n",
    "    save=True,\n",
    "):\n",
    "    hero_i = list(genes).index(hero)\n",
    "\n",
    "    col_p = f\"knnP_{hero}_dab_to_vg\"\n",
    "    col_n = f\"knnN_{hero}_dab_to_vg\"\n",
    "\n",
    "    if col_p not in vg_rna_pp.obs.columns:\n",
    "        raise KeyError(f\"Missing {col_p} in vg_rna_pp.obs (run DAb→VG kNN transfer first).\")\n",
    "    if col_n not in vg_rna_pp.obs.columns:\n",
    "        raise KeyError(f\"Missing {col_n} in vg_rna_pp.obs (run DAb→VG kNN transfer first).\")\n",
    "\n",
    "    # --- predictions from kNN transfer ---\n",
    "    p_pred_all = vg_rna_pp.obs[col_p].to_numpy(dtype=float)\n",
    "    n_labnbrs  = vg_rna_pp.obs[col_n].to_numpy(dtype=float)\n",
    "\n",
    "    # --- truth labels/masks for HERO ---\n",
    "    Y_vg = np.asarray(Y_vg)\n",
    "    M_vg = np.asarray(M_vg)\n",
    "\n",
    "    if use_name_alignment:\n",
    "        if vg_obs_names_for_YM is None:\n",
    "            raise ValueError(\"If use_name_alignment=True, provide vg_obs_names_for_YM aligned to rows of Y_vg/M_vg.\")\n",
    "        # map truth arrays onto vg_rna_pp.obs_names\n",
    "        name_to_i = {n:i for i,n in enumerate(np.asarray(vg_obs_names_for_YM))}\n",
    "        idx = np.array([name_to_i.get(n, -1) for n in vg_rna_pp.obs_names], dtype=int)\n",
    "        ok_name = idx >= 0\n",
    "        y_true_all = np.full(vg_rna_pp.n_obs, np.nan, dtype=float)\n",
    "        m_true_all = np.zeros(vg_rna_pp.n_obs, dtype=bool)\n",
    "        y_true_all[ok_name] = Y_vg[idx[ok_name], hero_i]\n",
    "        m_true_all[ok_name] = M_vg[idx[ok_name], hero_i].astype(bool)\n",
    "    else:\n",
    "        # positional alignment (fast) — assumes Y_vg/M_vg rows are in same order as vg_rna_pp\n",
    "        if Y_vg.shape[0] != vg_rna_pp.n_obs or M_vg.shape[0] != vg_rna_pp.n_obs:\n",
    "            raise ValueError(\n",
    "                f\"Positional alignment mismatch: vg_rna_pp.n_obs={vg_rna_pp.n_obs} \"\n",
    "                f\"but Y_vg/M_vg have {Y_vg.shape[0]} rows. \"\n",
    "                f\"Set use_name_alignment=True and pass vg_obs_names_for_YM.\"\n",
    "            )\n",
    "        y_true_all = Y_vg[:, hero_i].astype(float)\n",
    "        m_true_all = M_vg[:, hero_i].astype(bool)\n",
    "\n",
    "    # --- final mask for evaluation ---\n",
    "    ok = (\n",
    "        m_true_all\n",
    "        & np.isfinite(p_pred_all)\n",
    "        & np.isfinite(y_true_all)\n",
    "        & (n_labnbrs >= float(min_labeled_neighbors))\n",
    "    )\n",
    "\n",
    "    y = y_true_all[ok].astype(int)\n",
    "    p = p_pred_all[ok].astype(float)\n",
    "\n",
    "    if ok.sum() < min_n or len(np.unique(y)) < 2:\n",
    "        print(f\"Not enough labeled positives/negatives for {hero}: n={ok.sum()}, classes={np.unique(y)}\")\n",
    "        return {\"AUC\": np.nan, \"AP\": np.nan, \"n\": int(ok.sum()), \"hero\": hero, \"direction\": \"DAb→VG (kNN)\"}\n",
    "\n",
    "    auc = roc_auc_score(y, p)\n",
    "    ap  = average_precision_score(y, p)\n",
    "    fpr, tpr, _ = roc_curve(y, p)\n",
    "    prec, rec, _ = precision_recall_curve(y, p)\n",
    "\n",
    "    plt.figure(figsize=(7.2, 3.2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "    plt.title(f\"ROC (AUC={auc:.3f})\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(rec, prec)\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"PR (AP={ap:.3f})\")\n",
    "\n",
    "    title = f\"{prefix}: {hero} transfer DAb→VG (kNN k={k}, n≥{min_labeled_neighbors})\"\n",
    "    plt.suptitle(f\"{title} (n={ok.sum()})\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    figdir = Path(figdir)\n",
    "    figdir.mkdir(parents=True, exist_ok=True)\n",
    "    if save:\n",
    "        plt.savefig(figdir / f\"fig7e_roc_pr_transfer_{hero}_DAb_to_VG_knn.png\", dpi=450)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    out = {\"AUC\": float(auc), \"AP\": float(ap), \"n\": int(ok.sum()), \"hero\": hero, \"direction\": \"DAb→VG (kNN)\"}\n",
    "    print(\"Transfer metrics:\", out)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82cafe8-1995-4644-8e51-03ab3c4974a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERO = \"NPM1\"\n",
    "metrics = eval_knn_transfer_dab_to_vg(\n",
    "    vg_rna_pp=vg_rna_pp,\n",
    "    Y_vg=Y_vg, M_vg=M_vg,\n",
    "    genes=GENES, hero=HERO,\n",
    "    figdir=FIGDIR,\n",
    "    k=50,\n",
    "    min_labeled_neighbors=5,   # try 1, 5, 10\n",
    "    use_name_alignment=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61bc153-c571-4d89-8ee6-4b21496ef46f",
   "metadata": {},
   "source": [
    "### Figure 7f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1599320-48ec-4470-b1a7-540ef43531a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "class SingleModalDataset(Dataset):\n",
    "    def __init__(self, adata: ad.AnnData, modality: str, Y: np.ndarray, M: np.ndarray):\n",
    "        self.modality = modality\n",
    "        self.X = adata.X\n",
    "        self.Y = Y.astype(np.float32, copy=False)\n",
    "        self.M = M.astype(np.float32, copy=False)\n",
    "\n",
    "    def __len__(self): return self.Y.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.X[i]\n",
    "        if hasattr(x, \"toarray\"):\n",
    "            x = x.toarray().ravel()\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "        return self.modality, torch.from_numpy(x), torch.from_numpy(self.Y[i]), torch.from_numpy(self.M[i])\n",
    "\n",
    "def masked_bce_with_logits(logits, y, m, eps=1e-8):\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"none\") * m\n",
    "    denom = m.sum().clamp_min(eps)\n",
    "    return loss.sum() / denom\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden=(64, 32), dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden[0]), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden[0], hidden[1]), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden[1], out_dim),\n",
    "        )\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "@torch.no_grad()\n",
    "def latent_mu(model, x, modality: str):\n",
    "    x = x.to(device, non_blocking=True)\n",
    "    mu_dict, _ = model.encode_modalities({modality: x})\n",
    "    return mu_dict[modality]\n",
    "\n",
    "def freeze_decoders(model: nn.Module, freeze=True):\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"decoder\" in name.lower():\n",
    "            p.requires_grad = (not freeze)\n",
    "\n",
    "def split_group(n, groups, seed=0, frac_train=0.7, frac_val=0.15):\n",
    "    groups = np.asarray(groups)\n",
    "    idx_all = np.arange(n)\n",
    "    gss1 = GroupShuffleSplit(n_splits=1, train_size=frac_train, random_state=seed)\n",
    "    tr, tmp = next(gss1.split(idx_all, groups=groups))\n",
    "    tmp_groups = groups[tmp]\n",
    "    frac_val_of_tmp = frac_val / (1.0 - frac_train)\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, train_size=frac_val_of_tmp, random_state=seed+1)\n",
    "    va_rel, te_rel = next(gss2.split(tmp, groups=tmp_groups))\n",
    "    va = tmp[va_rel]\n",
    "    te = tmp[te_rel]\n",
    "    return tr, va, te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e97422-8b2a-45de-9f33-efa2a3420a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "\n",
    "# ----------------------------\n",
    "# Heads\n",
    "# ----------------------------\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden=(64, 64, 32), dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden[0], hidden[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden[1], out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "class PerGeneMLPHead(nn.Module):\n",
    "    \"\"\"Separate small MLP head per gene/mutation. Returns logits (B, G).\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, hidden=(64, 64, 32), dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.out_dim = int(out_dim)\n",
    "        self.heads = nn.ModuleList([\n",
    "            MLPHead(in_dim, 1, hidden=hidden, dropout=dropout) for _ in range(self.out_dim)\n",
    "        ])\n",
    "\n",
    "    def forward(self, z):\n",
    "        return torch.cat([h(z) for h in self.heads], dim=1)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _as_modality_key(modality):\n",
    "    \"\"\"\n",
    "    Dataloader may collate strings into list[str] of length B.\n",
    "    Collapse to a single canonical modality key.\n",
    "    \"\"\"\n",
    "    if isinstance(modality, torch.Tensor):\n",
    "        modality = modality.detach().cpu().tolist() if modality.numel() > 1 else modality.detach().cpu().item()\n",
    "\n",
    "    if isinstance(modality, (list, tuple)):\n",
    "        if len(modality) == 0:\n",
    "            raise ValueError(\"Empty modality list from dataloader.\")\n",
    "        return _as_modality_key(modality[0])\n",
    "\n",
    "    if isinstance(modality, np.generic):\n",
    "        modality = modality.item()\n",
    "\n",
    "    if isinstance(modality, (int, np.integer)):\n",
    "        return \"rna\" if int(modality) == 0 else (\"adt\" if int(modality) == 1 else str(modality))\n",
    "\n",
    "    s = str(modality).strip().lower()\n",
    "    s = s.replace(\"mod:\", \"\").replace(\"modality:\", \"\")\n",
    "\n",
    "    if s in (\"rna\", \"adt\", \"atac\"):\n",
    "        return s\n",
    "\n",
    "    aliases = {\n",
    "        \"gene\": \"rna\",\n",
    "        \"expression\": \"rna\",\n",
    "        \"protein\": \"adt\",\n",
    "        \"antibody\": \"adt\",\n",
    "        \"proteins\": \"adt\",\n",
    "    }\n",
    "    return aliases.get(s, s)\n",
    "\n",
    "\n",
    "def _to_device(x, device):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device, non_blocking=True)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return torch.from_numpy(x).to(device, non_blocking=True)\n",
    "    try:\n",
    "        return torch.tensor(x, device=device)\n",
    "    except Exception as e:\n",
    "        raise TypeError(f\"Unsupported batch type for x: {type(x)}\") from e\n",
    "\n",
    "\n",
    "def freeze_decoders(model: nn.Module, freeze=True):\n",
    "    \"\"\"Freeze any parameter whose name contains 'decoder'.\"\"\"\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"decoder\" in name.lower():\n",
    "            p.requires_grad = (not freeze)\n",
    "\n",
    "\n",
    "def latent_mu_student(model, x, modality: str):\n",
    "    \"\"\"\n",
    "    Student path: MUST allow grads during finetuning.\n",
    "    \"\"\"\n",
    "    mu_dict, _ = model.encode_modalities({modality: x})\n",
    "    if modality not in mu_dict:\n",
    "        raise KeyError(f\"Student encode_modalities missing key '{modality}'. Got {list(mu_dict.keys())}\")\n",
    "    return mu_dict[modality]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def latent_mu_teacher(model, x, modality: str):\n",
    "    \"\"\"\n",
    "    Teacher path: no grads.\n",
    "    \"\"\"\n",
    "    mu_dict, _ = model.encode_modalities({modality: x})\n",
    "    if modality not in mu_dict:\n",
    "        raise KeyError(f\"Teacher encode_modalities missing key '{modality}'. Got {list(mu_dict.keys())}\")\n",
    "    return mu_dict[modality]\n",
    "\n",
    "\n",
    "def masked_bce_sum_and_denom(logits, y, m, pos_weight=None):\n",
    "    \"\"\"\n",
    "    Returns (loss_sum, denom) for correct aggregation across batches/loaders.\n",
    "    \"\"\"\n",
    "    y = y.float()\n",
    "    m = m.float()\n",
    "\n",
    "    # sanitize unlabeled entries\n",
    "    y = torch.where(m > 0, y, torch.zeros_like(y)).clamp(0.0, 1.0)\n",
    "\n",
    "    if pos_weight is not None:\n",
    "        pw = pos_weight.view(1, -1).to(logits.device)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"none\", pos_weight=pw)\n",
    "    else:\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"none\")\n",
    "\n",
    "    loss = loss * m\n",
    "    denom = m.sum()\n",
    "    return loss.sum(), denom\n",
    "\n",
    "\n",
    "def masked_bce_with_logits_weighted(logits, y, m, pos_weight=None, eps=1e-8):\n",
    "    s, d = masked_bce_sum_and_denom(logits, y, m, pos_weight=pos_weight)\n",
    "    return s / (torch.clamp(d, min=1.0) + eps)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main finetune\n",
    "# ----------------------------\n",
    "def finetune_encoders_and_head(\n",
    "    base_model,\n",
    "    train_loaders,\n",
    "    val_loaders,\n",
    "    *,\n",
    "    out_dim,\n",
    "    genes,\n",
    "    device,\n",
    "    # optimization\n",
    "    lr_head=2e-4,\n",
    "    lr_encoder=None,\n",
    "    weight_decay=1e-6,\n",
    "    max_epochs=400,\n",
    "    patience=40,\n",
    "    grad_clip=5.0,\n",
    "    # schedule\n",
    "    warmup_epochs=10,\n",
    "    lambda_preserve=1.0,\n",
    "    use_per_gene_heads=True,\n",
    "    # class imbalance\n",
    "    pos_weight=None,   # torch tensor (G,)\n",
    "    # logging\n",
    "    verbose=True,\n",
    "    log_every=1,\n",
    "    # selection behavior\n",
    "    start_best_after_unfreeze=False,\n",
    "):\n",
    "    if lr_encoder is None:\n",
    "        lr_encoder = lr_head * 0.1\n",
    "\n",
    "    # student + teacher\n",
    "    model_ft = deepcopy(base_model).to(device)\n",
    "    teacher  = deepcopy(base_model).to(device)\n",
    "    teacher.eval()\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # freeze student decoders always\n",
    "    for p in model_ft.parameters():\n",
    "        p.requires_grad = True\n",
    "    freeze_decoders(model_ft, freeze=True)\n",
    "\n",
    "    if pos_weight is not None:\n",
    "        pos_weight = pos_weight.to(device)\n",
    "        if pos_weight.numel() != int(out_dim):\n",
    "            raise ValueError(f\"pos_weight has shape {tuple(pos_weight.shape)} but out_dim={out_dim}\")\n",
    "\n",
    "    # iterators + stash so we don't drop the first batch\n",
    "    train_iters = [iter(L) for L in train_loaders]\n",
    "    stash = [None for _ in train_loaders]\n",
    "    steps = max(len(L) for L in train_loaders)\n",
    "\n",
    "    def next_batch(i):\n",
    "        nonlocal train_iters, stash\n",
    "        if stash[i] is not None:\n",
    "            b = stash[i]\n",
    "            stash[i] = None\n",
    "            return b\n",
    "        try:\n",
    "            return next(train_iters[i])\n",
    "        except StopIteration:\n",
    "            train_iters[i] = iter(train_loaders[i])\n",
    "            return next(train_iters[i])\n",
    "\n",
    "    # build head after we infer latent dim\n",
    "    head = None\n",
    "    opt = None\n",
    "\n",
    "    def build_head(latent_dim):\n",
    "        if use_per_gene_heads:\n",
    "            return PerGeneMLPHead(latent_dim, out_dim, hidden=(64, 32), dropout=0.1).to(device)\n",
    "        return MLPHead(latent_dim, out_dim, hidden=(64, 32), dropout=0.1).to(device)\n",
    "\n",
    "    def set_train_mode(warmup: bool):\n",
    "        # encoder off during warmup, on after; head always on\n",
    "        if warmup:\n",
    "            for p in model_ft.parameters():\n",
    "                p.requires_grad = False\n",
    "            freeze_decoders(model_ft, freeze=True)\n",
    "        else:\n",
    "            for p in model_ft.parameters():\n",
    "                p.requires_grad = True\n",
    "            freeze_decoders(model_ft, freeze=True)\n",
    "\n",
    "        for p in head.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def make_optimizer(warmup: bool):\n",
    "        if warmup:\n",
    "            return torch.optim.AdamW([{\"params\": head.parameters(), \"lr\": lr_head}],\n",
    "                                     weight_decay=weight_decay)\n",
    "        enc_params = [p for p in model_ft.parameters() if p.requires_grad]\n",
    "        return torch.optim.AdamW(\n",
    "            [{\"params\": enc_params, \"lr\": lr_encoder},\n",
    "             {\"params\": head.parameters(), \"lr\": lr_head}],\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "    # best tracking\n",
    "    best = {\"score\": np.inf, \"epoch\": -1, \"model_state\": None, \"head_state\": None}\n",
    "    bad = 0\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"[FT] start: out_dim={out_dim} heads={'per_gene' if use_per_gene_heads else 'shared'} \"\n",
    "            f\"lr_head={lr_head:g} lr_enc={lr_encoder:g} wd={weight_decay:g} \"\n",
    "            f\"lambda_preserve={lambda_preserve:g} warmup={warmup_epochs} \"\n",
    "            f\"max_epochs={max_epochs} patience={patience}\"\n",
    "        )\n",
    "        if pos_weight is not None:\n",
    "            pw = pos_weight.detach().cpu().numpy()\n",
    "            print(\"[FT] pos_weight:\", {g: float(pw[i]) for i, g in enumerate(genes)})\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        warmup = (epoch < int(warmup_epochs))\n",
    "\n",
    "        # init head + optimizer once\n",
    "        if head is None:\n",
    "            b0 = next_batch(0)\n",
    "            stash[0] = b0\n",
    "            modality0, x0, y0, m0 = b0\n",
    "            mod0 = _as_modality_key(modality0)\n",
    "            x0 = _to_device(x0, device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                z0 = latent_mu_teacher(model_ft, x0, mod0)\n",
    "            head = build_head(int(z0.shape[1]))\n",
    "\n",
    "            set_train_mode(warmup=warmup)\n",
    "            opt = make_optimizer(warmup=warmup)\n",
    "            if verbose:\n",
    "                print(f\"[FT] phase -> {'warmup(head-only)' if warmup else 'finetune(enc+head)'}\")\n",
    "\n",
    "        # boundary: rebuild optimizer after unfreeze\n",
    "        if epoch == int(warmup_epochs):\n",
    "            set_train_mode(warmup=False)\n",
    "            opt = make_optimizer(warmup=False)\n",
    "            if verbose:\n",
    "                print(\"[FT] phase -> finetune(enc+head)\")\n",
    "\n",
    "        model_ft.train(); head.train()\n",
    "\n",
    "        tr_losses, tr_cls, tr_pres = [], [], []\n",
    "\n",
    "        for s in range(steps):\n",
    "            for i in range(len(train_loaders)):\n",
    "                modality, x, y, m = next_batch(i)\n",
    "                mod = _as_modality_key(modality)\n",
    "\n",
    "                x = _to_device(x, device)\n",
    "                y = y.to(device, non_blocking=True)\n",
    "                m = m.to(device, non_blocking=True)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "                z  = latent_mu_student(model_ft, x, mod)   # grads ON\n",
    "                lg = head(z)\n",
    "\n",
    "                loss_cls = masked_bce_with_logits_weighted(lg, y, m, pos_weight=pos_weight)\n",
    "\n",
    "                if lambda_preserve and lambda_preserve > 0:\n",
    "                    zt = latent_mu_teacher(teacher, x, mod)  # grads OFF\n",
    "                    loss_pres = torch.mean((z - zt) ** 2)\n",
    "                    loss = loss_cls + float(lambda_preserve) * loss_pres\n",
    "                else:\n",
    "                    loss_pres = None\n",
    "                    loss = loss_cls\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if grad_clip and grad_clip > 0:\n",
    "                    params_to_clip = []\n",
    "                    for pg in opt.param_groups:\n",
    "                        params_to_clip.extend([p for p in pg[\"params\"] if p.grad is not None])\n",
    "                    torch.nn.utils.clip_grad_norm_(params_to_clip, grad_clip)\n",
    "\n",
    "                opt.step()\n",
    "\n",
    "                tr_losses.append(float(loss.detach().cpu().item()))\n",
    "                tr_cls.append(float(loss_cls.detach().cpu().item()))\n",
    "                if loss_pres is not None:\n",
    "                    tr_pres.append(float(loss_pres.detach().cpu().item()))\n",
    "\n",
    "        tr  = float(np.mean(tr_losses)) if tr_losses else np.nan\n",
    "        trc = float(np.mean(tr_cls)) if tr_cls else np.nan\n",
    "        trp = float(np.mean(tr_pres)) if tr_pres else 0.0\n",
    "\n",
    "        # ---- val (correct aggregation) ----\n",
    "        model_ft.eval(); head.eval()\n",
    "        loss_sum = 0.0\n",
    "        denom_sum = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for L in val_loaders:\n",
    "                for modality, x, y, m in L:\n",
    "                    mod = _as_modality_key(modality)\n",
    "                    x = _to_device(x, device)\n",
    "                    y = y.to(device, non_blocking=True)\n",
    "                    m = m.to(device, non_blocking=True)\n",
    "\n",
    "                    z = latent_mu_teacher(model_ft, x, mod)  # no grads in val\n",
    "                    lg = head(z)\n",
    "                    s_loss, s_denom = masked_bce_sum_and_denom(lg, y, m, pos_weight=pos_weight)\n",
    "                    loss_sum  += float(s_loss.detach().cpu().item())\n",
    "                    denom_sum += float(s_denom.detach().cpu().item())\n",
    "\n",
    "        va = loss_sum / max(denom_sum, 1.0)\n",
    "\n",
    "        can_update_best = (not start_best_after_unfreeze) or (epoch >= int(warmup_epochs))\n",
    "        improved = False\n",
    "        if can_update_best and (va < best[\"score\"] - 1e-6):\n",
    "            improved = True\n",
    "            best[\"score\"] = va\n",
    "            best[\"epoch\"] = epoch\n",
    "            best[\"model_state\"] = {k: t.detach().cpu().clone() for k, t in model_ft.state_dict().items()}\n",
    "            best[\"head_state\"]  = {k: t.detach().cpu().clone() for k, t in head.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if verbose and log_every and (epoch % log_every == 0 or improved or epoch == 0):\n",
    "            star = \"*\" if improved else \" \"\n",
    "            phase = \"W\" if warmup else \"F\"\n",
    "            best_str = f\"{best['score']:.4f}@{best['epoch']:03d}\" if best[\"epoch\"] >= 0 else \"NA\"\n",
    "            if lambda_preserve and lambda_preserve > 0:\n",
    "                print(f\"[FT]{star} {phase} ep{epoch:03d} tr={tr:.4f} (cls={trc:.4f},pres={trp:.4f}) va={va:.4f} best={best_str} bad={bad}/{patience}\")\n",
    "            else:\n",
    "                print(f\"[FT]{star} {phase} ep{epoch:03d} tr={tr:.4f} va={va:.4f} best={best_str} bad={bad}/{patience}\")\n",
    "\n",
    "        if bad >= patience:\n",
    "            if verbose:\n",
    "                print(f\"[FT] early stop at epoch {epoch} (no improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    # restore best\n",
    "    if best[\"model_state\"] is not None:\n",
    "        model_ft.load_state_dict(best[\"model_state\"])\n",
    "    if best[\"head_state\"] is not None:\n",
    "        head.load_state_dict(best[\"head_state\"])\n",
    "\n",
    "    best_summary = {\n",
    "        \"best_val\": float(best[\"score\"]) if best[\"epoch\"] >= 0 else float(va),\n",
    "        \"best_epoch\": int(best[\"epoch\"]),\n",
    "        \"stopped_epoch\": int(epoch),\n",
    "        \"lambda_preserve\": float(lambda_preserve),\n",
    "        \"warmup_epochs\": int(warmup_epochs),\n",
    "        \"lr_head\": float(lr_head),\n",
    "        \"lr_encoder\": float(lr_encoder),\n",
    "        \"heads\": \"per_gene\" if use_per_gene_heads else \"shared\",\n",
    "        \"start_best_after_unfreeze\": bool(start_best_after_unfreeze),\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        if best_summary[\"best_epoch\"] >= 0:\n",
    "            print(f\"[FT] done: restored best epoch {best_summary['best_epoch']} (val={best_summary['best_val']:.4f}).\")\n",
    "        else:\n",
    "            print(\"[FT] done: no best snapshot recorded.\")\n",
    "\n",
    "    return model_ft, head, best_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af741f-5c4e-48e8-b28e-db67a0908d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def eval_head(model, head, loader, genes, *, min_labeled=10, extra_diag=True):\n",
    "    \"\"\"\n",
    "    Evaluate mutation head on a loader.\n",
    "\n",
    "    Returns dict:\n",
    "      out[gene] = {\n",
    "        \"auc\": float or nan,\n",
    "        \"ap\": float or nan,\n",
    "        \"ap_baseline\": float or nan,   # ~ prevalence\n",
    "        \"n_labeled\": int,\n",
    "        \"n_pos\": int,\n",
    "        \"n_neg\": int,\n",
    "        \"prevalence\": float or nan,\n",
    "        \"p_mean\": float or nan,\n",
    "        \"p_std\": float or nan,\n",
    "        \"frac_p_gt_0p9\": float or nan,\n",
    "        \"frac_p_lt_0p1\": float or nan,\n",
    "        \"status\": str,\n",
    "      }\n",
    "    \"\"\"\n",
    "    model.eval(); head.eval()\n",
    "\n",
    "    Ys, Ms, Ps = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for modality, x, y, m in loader:\n",
    "            mod = _as_modality_key(modality)\n",
    "            x = _to_device(x, device)\n",
    "\n",
    "            z = latent_mu(model, x, mod)\n",
    "            logits = head(z)\n",
    "            p = torch.sigmoid(logits)\n",
    "\n",
    "            Ys.append(y.detach().cpu().numpy())\n",
    "            Ms.append(m.detach().cpu().numpy())\n",
    "            Ps.append(p.detach().cpu().numpy())\n",
    "\n",
    "    if len(Ys) == 0:\n",
    "        base = {\"auc\": np.nan, \"ap\": np.nan, \"ap_baseline\": np.nan,\n",
    "                \"n_labeled\": 0, \"n_pos\": 0, \"n_neg\": 0,\n",
    "                \"prevalence\": np.nan, \"p_mean\": np.nan, \"p_std\": np.nan,\n",
    "                \"frac_p_gt_0p9\": np.nan, \"frac_p_lt_0p1\": np.nan,\n",
    "                \"status\": \"empty_loader\"}\n",
    "        return {g: dict(base) for g in genes}\n",
    "\n",
    "    Y = np.vstack(Ys)\n",
    "    M = np.vstack(Ms).astype(bool)\n",
    "    P = np.vstack(Ps)\n",
    "\n",
    "    out = {}\n",
    "    for j, g in enumerate(genes):\n",
    "        mask = M[:, j]\n",
    "        n = int(mask.sum())\n",
    "\n",
    "        if n == 0:\n",
    "            out[g] = {\n",
    "                \"auc\": np.nan, \"ap\": np.nan, \"ap_baseline\": np.nan,\n",
    "                \"n_labeled\": 0, \"n_pos\": 0, \"n_neg\": 0,\n",
    "                \"prevalence\": np.nan,\n",
    "                \"p_mean\": np.nan, \"p_std\": np.nan,\n",
    "                \"frac_p_gt_0p9\": np.nan, \"frac_p_lt_0p1\": np.nan,\n",
    "                \"status\": \"too_few_labeled\",\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        yy = Y[mask, j].astype(int)\n",
    "        pp = P[mask, j].astype(float)\n",
    "\n",
    "        n_pos = int((yy == 1).sum())\n",
    "        n_neg = int((yy == 0).sum())\n",
    "        prev = float(n_pos / max(n, 1))\n",
    "\n",
    "        # prediction diagnostics\n",
    "        p_mean = float(np.mean(pp))\n",
    "        p_std  = float(np.std(pp))\n",
    "        frac_hi = float(np.mean(pp > 0.9))\n",
    "        frac_lo = float(np.mean(pp < 0.1))\n",
    "\n",
    "        if n < min_labeled:\n",
    "            out[g] = {\n",
    "                \"auc\": np.nan, \"ap\": np.nan, \"ap_baseline\": prev,\n",
    "                \"n_labeled\": n, \"n_pos\": n_pos, \"n_neg\": n_neg,\n",
    "                \"prevalence\": prev,\n",
    "                \"p_mean\": p_mean, \"p_std\": p_std,\n",
    "                \"frac_p_gt_0p9\": frac_hi, \"frac_p_lt_0p1\": frac_lo,\n",
    "                \"status\": f\"too_few_labeled(n<{min_labeled})\",\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        if (n_pos == 0) or (n_neg == 0):\n",
    "            out[g] = {\n",
    "                \"auc\": np.nan, \"ap\": np.nan, \"ap_baseline\": prev,\n",
    "                \"n_labeled\": n, \"n_pos\": n_pos, \"n_neg\": n_neg,\n",
    "                \"prevalence\": prev,\n",
    "                \"p_mean\": p_mean, \"p_std\": p_std,\n",
    "                \"frac_p_gt_0p9\": frac_hi, \"frac_p_lt_0p1\": frac_lo,\n",
    "                \"status\": f\"one_class(pos={n_pos},neg={n_neg})\",\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        out[g] = {\n",
    "            \"auc\": float(roc_auc_score(yy, pp)),\n",
    "            \"ap\":  float(average_precision_score(yy, pp)),\n",
    "            \"ap_baseline\": prev,  # random AP ≈ prevalence\n",
    "            \"n_labeled\": n,\n",
    "            \"n_pos\": n_pos,\n",
    "            \"n_neg\": n_neg,\n",
    "            \"prevalence\": prev,\n",
    "            \"p_mean\": p_mean,\n",
    "            \"p_std\": p_std,\n",
    "            \"frac_p_gt_0p9\": frac_hi,\n",
    "            \"frac_p_lt_0p1\": frac_lo,\n",
    "            \"status\": \"ok\",\n",
    "        }\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaedcdb-f665-48ab-a002-54e3798fbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def choose_vg_splits_auto(\n",
    "    vg_adata,\n",
    "    Y_vg,\n",
    "    M_vg,\n",
    "    GENES,\n",
    "    *,\n",
    "    preferred_group_col=\"patient\",\n",
    "    seed0=1,\n",
    "    tries=500,\n",
    "    min_test_labeled=10,\n",
    "    frac_train=0.80,\n",
    "    frac_val=0.10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      vg_tr, vg_va, vg_te : np.ndarray[int] indices into vg_adata (cell indices)\n",
    "      vg_split_mode       : str describing which grouping strategy was used\n",
    "\n",
    "    Labeled cell definition:\n",
    "      labeled if M_vg[i, :].sum() > 0  (i.e., at least one gene label present)\n",
    "\n",
    "    Strategy:\n",
    "      1) If preferred_group_col exists -> group split by that.\n",
    "      2) Else try to infer a patient/group column from common names.\n",
    "      3) Else try parsing patient-ish token from sample_id / obs_names.\n",
    "      4) Else fallback to random split.\n",
    "    \"\"\"\n",
    "    n = vg_adata.n_obs\n",
    "    if Y_vg is None or M_vg is None:\n",
    "        raise ValueError(\"Y_vg and M_vg must be provided (even if used only for labeled-count checks).\")\n",
    "    if len(M_vg) != n:\n",
    "        raise ValueError(f\"M_vg has {len(M_vg)} rows but vg_adata has n_obs={n}.\")\n",
    "\n",
    "    # --- labeled cells: at least one label present in mask\n",
    "    if hasattr(M_vg, \"toarray\"):  # sparse\n",
    "        labeled = np.asarray(M_vg.sum(axis=1)).ravel() > 0\n",
    "    else:\n",
    "        labeled = np.asarray(M_vg).sum(axis=1) > 0\n",
    "    n_labeled = int(labeled.sum())\n",
    "\n",
    "    # --- helpers\n",
    "    def _as_series(x):\n",
    "        if isinstance(x, pd.Series):\n",
    "            return x\n",
    "        return pd.Series(np.asarray(x), index=vg_adata.obs_names)\n",
    "\n",
    "    def _try_get_group_series():\n",
    "        obs = vg_adata.obs\n",
    "\n",
    "        # 1) preferred column\n",
    "        if preferred_group_col in obs.columns:\n",
    "            s = _as_series(obs[preferred_group_col])\n",
    "            if s.notna().any():\n",
    "                return s.astype(str), f\"group:{preferred_group_col}\"\n",
    "\n",
    "        # 2) common alternatives\n",
    "        for col in [\"patient\", \"donor\", \"subject\", \"individual\", \"pt\", \"patient_id\",\n",
    "                    \"donor_id\", \"subject_id\", \"orig.ident\", \"orig_ident\", \"sample\",\n",
    "                    \"sample_id\", \"library_id\", \"batch\", \"study\", \"dataset\"]:\n",
    "            if col in obs.columns:\n",
    "                s = _as_series(obs[col])\n",
    "                if s.notna().any():\n",
    "                    return s.astype(str), f\"group:{col}\"\n",
    "\n",
    "        # 3) parse from sample_id if present\n",
    "        for col in [\"sample_id\", \"sample\", \"orig.ident\", \"orig_ident\", \"library_id\"]:\n",
    "            if col in obs.columns:\n",
    "                raw = _as_series(obs[col]).astype(str)\n",
    "\n",
    "                def parse_token(v):\n",
    "                    # take leading token before common separators; keeps things stable\n",
    "                    parts = re.split(r\"[|,;/\\s]+\", v)\n",
    "                    return parts[0] if parts and parts[0] != \"\" else v\n",
    "\n",
    "                parsed = raw.map(parse_token)\n",
    "                if parsed.nunique() > 1:\n",
    "                    return parsed, f\"parsed:{col}\"\n",
    "\n",
    "        # 4) parse from obs_names\n",
    "        raw = pd.Series(vg_adata.obs_names.astype(str), index=vg_adata.obs_names)\n",
    "\n",
    "        def parse_from_name(v):\n",
    "            # common patterns: PATIENT_* , patient-*, PT123_* , AML556.D0 etc.\n",
    "            # we take the first chunk before '_' or '-'\n",
    "            parts = re.split(r\"[_-]+\", v)\n",
    "            return parts[0] if parts and parts[0] != \"\" else v\n",
    "\n",
    "        parsed = raw.map(parse_from_name)\n",
    "        if parsed.nunique() > 1:\n",
    "            return parsed, \"parsed:obs_names\"\n",
    "\n",
    "        return None, \"random\"\n",
    "\n",
    "    def _group_split_indices(group_s, rng, frac_train, frac_val):\n",
    "        # group_s: pd.Series of group labels length n\n",
    "        groups = group_s.values\n",
    "        uniq = np.unique(groups)\n",
    "        rng.shuffle(uniq)\n",
    "\n",
    "        n_g = len(uniq)\n",
    "        n_tr_g = max(1, int(round(frac_train * n_g)))\n",
    "        n_va_g = max(1, int(round(frac_val * n_g)))\n",
    "        # ensure non-empty test groups if possible\n",
    "        if n_tr_g + n_va_g >= n_g and n_g >= 3:\n",
    "            n_va_g = max(1, n_g - n_tr_g - 1)\n",
    "\n",
    "        tr_groups = set(uniq[:n_tr_g])\n",
    "        va_groups = set(uniq[n_tr_g:n_tr_g + n_va_g])\n",
    "        te_groups = set(uniq[n_tr_g + n_va_g:])\n",
    "\n",
    "        idx = np.arange(n)\n",
    "        tr = idx[np.isin(groups, list(tr_groups))]\n",
    "        va = idx[np.isin(groups, list(va_groups))]\n",
    "        te = idx[np.isin(groups, list(te_groups))]\n",
    "\n",
    "        # guard in case tiny group count collapses\n",
    "        if len(te) == 0:\n",
    "            # steal from val if needed\n",
    "            if len(va) > 0:\n",
    "                cut = max(1, int(0.5 * len(va)))\n",
    "                te = va[:cut]\n",
    "                va = va[cut:]\n",
    "            else:\n",
    "                # ultimate fallback: random 10% test\n",
    "                rng.shuffle(idx)\n",
    "                te = idx[: max(1, int(0.10 * n))]\n",
    "                va = idx[max(1, int(0.10 * n)) : max(2, int(0.20 * n))]\n",
    "                tr = idx[max(2, int(0.20 * n)) :]\n",
    "\n",
    "        return tr, va, te\n",
    "\n",
    "    def _random_split_indices(rng, frac_train, frac_val):\n",
    "        idx = np.arange(n)\n",
    "        rng.shuffle(idx)\n",
    "        n_tr = int(round(frac_train * n))\n",
    "        n_va = int(round(frac_val * n))\n",
    "        tr = idx[:n_tr]\n",
    "        va = idx[n_tr:n_tr + n_va]\n",
    "        te = idx[n_tr + n_va:]\n",
    "        return tr, va, te\n",
    "\n",
    "    # --- main: pick a grouping strategy, then retry to meet min_test_labeled\n",
    "    group_s, mode = _try_get_group_series()\n",
    "\n",
    "    best = None\n",
    "    best_score = -1\n",
    "\n",
    "    for t in range(tries):\n",
    "        rng = np.random.default_rng(seed0 + t)\n",
    "\n",
    "        if group_s is not None and mode != \"random\":\n",
    "            tr, va, te = _group_split_indices(group_s, rng, frac_train, frac_val)\n",
    "        else:\n",
    "            tr, va, te = _random_split_indices(rng, frac_train, frac_val)\n",
    "\n",
    "        test_labeled = int(labeled[te].sum())\n",
    "        # prefer higher labeled in test; but accept if meets threshold\n",
    "        if test_labeled > best_score:\n",
    "            best_score = test_labeled\n",
    "            best = (tr, va, te)\n",
    "\n",
    "        if test_labeled >= min_test_labeled:\n",
    "            return tr, va, te, mode\n",
    "\n",
    "    # If we get here, we failed to hit the min_test_labeled threshold.\n",
    "    # Return the best attempt we saw, but be explicit in the mode.\n",
    "    tr, va, te = best\n",
    "    warn_mode = mode + f\" (best_effort: test_labeled={best_score}/{n_labeled}, min_required={min_test_labeled})\"\n",
    "    return tr, va, te, warn_mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d596e3b-2bd4-4848-a5a6-5d03207ca5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Balanced / constrained split search (group-aware) with fallbacks\n",
    "#   What this does:\n",
    "#     1) Builds Y,M labels from either:\n",
    "#          - MutTranscripts / WtTranscripts string columns (VG-style), OR\n",
    "#          - explicit obs columns (DAb-style), including fuzzy auto-mapping\n",
    "#     2) Searches for group-aware splits satisfying HARD mins on TRAIN/VAL/TEST\n",
    "#     3) If impossible, falls back in a controlled way:\n",
    "#          A) best-effort group-aware split (maximizes labeled + balance)\n",
    "#          B) if that fails, cell-level stratified split (optional)\n",
    "# ============================================================\n",
    "\n",
    "# ----------------------------\n",
    "# Pretty printing\n",
    "# ----------------------------\n",
    "def _summarize_split(name, idx, Y, M, genes):\n",
    "    idx = np.asarray(idx, dtype=int)\n",
    "    print(f\"[{name}] n={len(idx):,}\")\n",
    "    for j, g in enumerate(genes):\n",
    "        lab = M[idx, j] > 0\n",
    "        n_lab = int(lab.sum())\n",
    "        if n_lab == 0:\n",
    "            print(f\"  {g:<8}: n_labeled=0\")\n",
    "            continue\n",
    "        yy = Y[idx, j][lab]\n",
    "        n_pos = int((yy > 0.5).sum())\n",
    "        n_neg = int((yy <= 0.5).sum())\n",
    "        prev = n_pos / max(n_lab, 1)\n",
    "        print(f\"  {g:<8}: n_labeled={n_lab:5d}  n_pos={n_pos:5d}  n_neg={n_neg:5d}  prev={prev:0.3f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Label builders\n",
    "# ============================================================\n",
    "\n",
    "def _compile_gene_pat(g):\n",
    "    # whole token match (robust to punctuation + underscores)\n",
    "    g = re.escape(str(g).upper())\n",
    "    return re.compile(rf\"(?<![A-Z0-9]){g}(?![A-Z0-9])\", flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def build_YM_from_mut_wt_strings(\n",
    "    adata,\n",
    "    genes,\n",
    "    *,\n",
    "    mut_col=\"MutTranscripts\",\n",
    "    wt_col=\"WtTranscripts\",\n",
    "    conflict_policy=\"mut_wins\",  # \"mut_wins\" or \"na\"\n",
    "):\n",
    "    \"\"\"\n",
    "    VG-style.\n",
    "    M=1 if either mut or wt mentions the gene; else 0.\n",
    "    Y=1 if mut mentions gene; else 0 when labeled.\n",
    "    \"\"\"\n",
    "    if mut_col not in adata.obs.columns:\n",
    "        raise KeyError(f\"mut_col={mut_col!r} not in adata.obs.columns\")\n",
    "    if wt_col not in adata.obs.columns:\n",
    "        raise KeyError(f\"wt_col={wt_col!r} not in adata.obs.columns\")\n",
    "\n",
    "    mut_s = adata.obs[mut_col].astype(\"string\").fillna(\"\").str.upper()\n",
    "    wt_s  = adata.obs[wt_col].astype(\"string\").fillna(\"\").str.upper()\n",
    "\n",
    "    N = adata.n_obs\n",
    "    G = len(genes)\n",
    "    Y = np.zeros((N, G), dtype=np.float32)\n",
    "    M = np.zeros((N, G), dtype=np.float32)\n",
    "\n",
    "    for j, g in enumerate(genes):\n",
    "        pat = _compile_gene_pat(g)\n",
    "        in_mut = mut_s.str.contains(pat, regex=True).to_numpy()\n",
    "        in_wt  = wt_s.str.contains(pat, regex=True).to_numpy()\n",
    "\n",
    "        conflict = in_mut & in_wt\n",
    "        labeled  = in_mut | in_wt\n",
    "\n",
    "        if conflict.any():\n",
    "            if conflict_policy == \"mut_wins\":\n",
    "                pass\n",
    "            elif conflict_policy == \"na\":\n",
    "                labeled = labeled & (~conflict)\n",
    "                in_mut  = in_mut & (~conflict)\n",
    "            else:\n",
    "                raise ValueError(\"conflict_policy must be 'mut_wins' or 'na'\")\n",
    "\n",
    "        M[:, j] = labeled.astype(np.float32)\n",
    "        Y[:, j] = in_mut.astype(np.float32)\n",
    "\n",
    "    return Y, M\n",
    "\n",
    "\n",
    "def _normalize_token(s: str) -> str:\n",
    "    s = str(s).upper()\n",
    "    # remove common separators\n",
    "    s = re.sub(r\"[\\s\\-_:/]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def infer_obs_col_map_by_gene_substring(adata, genes, *, prefer=None):\n",
    "    \"\"\"\n",
    "    DAb-style helper: try to auto-map each gene to an obs column whose name contains it.\n",
    "    Examples:\n",
    "      gene='DNMT3A' matches 'DNMT3A R882H'\n",
    "      gene='FLT3'   matches 'FLT3-ITD'\n",
    "      gene='NPM1'   matches 'NPM1 W288fs'\n",
    "    If multiple matches, pick:\n",
    "      - any column that also contains one of `prefer` tokens (e.g. ['ITD','R882','W288'])\n",
    "      - otherwise shortest column name\n",
    "    Returns dict {gene: col} for genes where a match was found.\n",
    "    \"\"\"\n",
    "    cols = list(adata.obs.columns)\n",
    "    cols_norm = [_normalize_token(c) for c in cols]\n",
    "    prefer = [p.upper() for p in (prefer or [])]\n",
    "\n",
    "    out = {}\n",
    "    for g in genes:\n",
    "        g_norm = _normalize_token(g)\n",
    "        hits = [cols[i] for i, cn in enumerate(cols_norm) if g_norm in cn]\n",
    "        if len(hits) == 0:\n",
    "            continue\n",
    "        if len(hits) == 1:\n",
    "            out[g] = hits[0]\n",
    "            continue\n",
    "\n",
    "        # prefer ones that contain any preferred token\n",
    "        if prefer:\n",
    "            hits2 = []\n",
    "            for c in hits:\n",
    "                cn = _normalize_token(c)\n",
    "                if any(_normalize_token(p) in cn for p in prefer):\n",
    "                    hits2.append(c)\n",
    "            if len(hits2) == 1:\n",
    "                out[g] = hits2[0]\n",
    "                continue\n",
    "            if len(hits2) > 1:\n",
    "                hits = hits2\n",
    "\n",
    "        # choose shortest (often the \"main\" mutation column)\n",
    "        hits = sorted(hits, key=lambda x: len(str(x)))\n",
    "        out[g] = hits[0]\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_YM_from_obs_cols(\n",
    "    adata,\n",
    "    genes,\n",
    "    *,\n",
    "    col_map=None,          # dict {gene: obs_col}\n",
    "    col_pattern=None,      # format string using {gene} or {gene} / {gene} replacement\n",
    "    threshold=0.5,\n",
    "    treat_nonzero_as_pos=True,\n",
    "    allow_auto_map=True,   # if True and col_map/col_pattern missing, try substring map\n",
    "    auto_map_prefer=None,  # optional list of tokens to break ties\n",
    "):\n",
    "    \"\"\"\n",
    "    DAb-style. Reads values from .obs columns.\n",
    "      - labeled if value is not NA\n",
    "      - positive if (value != 0) when treat_nonzero_as_pos else (value > threshold)\n",
    "    \"\"\"\n",
    "    N = adata.n_obs\n",
    "    G = len(genes)\n",
    "    Y = np.zeros((N, G), dtype=np.float32)\n",
    "    M = np.zeros((N, G), dtype=np.float32)\n",
    "\n",
    "    if col_map is None and col_pattern is None and allow_auto_map:\n",
    "        col_map = infer_obs_col_map_by_gene_substring(adata, genes, prefer=auto_map_prefer)\n",
    "\n",
    "    if col_map is None and col_pattern is None:\n",
    "        raise ValueError(\"Provide col_map or col_pattern, or set allow_auto_map=True.\")\n",
    "\n",
    "    for j, g in enumerate(genes):\n",
    "        if col_map is not None:\n",
    "            col = col_map.get(g, None)\n",
    "        else:\n",
    "            col = col_pattern.format(gene=g)\n",
    "\n",
    "        if col is None or col not in adata.obs.columns:\n",
    "            continue\n",
    "\n",
    "        s = adata.obs[col]\n",
    "        is_lab = ~pd.isna(s)\n",
    "        vals = pd.to_numeric(s, errors=\"coerce\").fillna(0).to_numpy(dtype=float)\n",
    "\n",
    "        if treat_nonzero_as_pos:\n",
    "            y = (vals != 0).astype(np.float32)\n",
    "        else:\n",
    "            y = (vals > float(threshold)).astype(np.float32)\n",
    "\n",
    "        Y[:, j] = y\n",
    "        M[:, j] = is_lab.astype(np.float32)\n",
    "\n",
    "    return Y, M\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Scoring + constraints\n",
    "# ============================================================\n",
    "\n",
    "def _score_split(tr, va, te, Y, M, genes, prefer_balance=True, balance_weight=10.0):\n",
    "    \"\"\"\n",
    "    Higher is better.\n",
    "      - reward more labeled in val/test\n",
    "      - optionally reward prevalence near 0.5 in val/test\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    G = len(genes)\n",
    "    for j in range(G):\n",
    "        score += float(M[va, j].sum()) + float(M[te, j].sum())\n",
    "\n",
    "        if prefer_balance:\n",
    "            for idx in (va, te):\n",
    "                lab = M[idx, j] > 0\n",
    "                n_lab = float(lab.sum())\n",
    "                if n_lab <= 0:\n",
    "                    continue\n",
    "                p = float((Y[idx, j][lab] > 0.5).mean())\n",
    "                score += (1.0 - abs(p - 0.5) / 0.5) * float(balance_weight)\n",
    "\n",
    "    return float(score)\n",
    "\n",
    "\n",
    "def _counts_for(idx, j, Y, M):\n",
    "    lab = M[idx, j] > 0\n",
    "    n_lab = int(lab.sum())\n",
    "    if n_lab == 0:\n",
    "        return 0, 0, 0\n",
    "    yy = Y[idx, j][lab]\n",
    "    n_pos = int((yy > 0.5).sum())\n",
    "    n_neg = int((yy <= 0.5).sum())\n",
    "    return n_lab, n_pos, n_neg\n",
    "\n",
    "\n",
    "def _check_constraints(idx, j, Y, M, *, min_labeled, min_pos, min_neg):\n",
    "    n_lab, n_pos, n_neg = _counts_for(idx, j, Y, M)\n",
    "    return (n_lab >= min_labeled) and (n_pos >= min_pos) and (n_neg >= min_neg)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Splitters\n",
    "# ============================================================\n",
    "\n",
    "def _group_split_indices(groups, g_tr, g_va, g_te):\n",
    "    groups = np.asarray(groups).astype(str)\n",
    "    tr = np.where(np.isin(groups, list(g_tr)))[0]\n",
    "    va = np.where(np.isin(groups, list(g_va)))[0]\n",
    "    te = np.where(np.isin(groups, list(g_te)))[0]\n",
    "    return tr, va, te\n",
    "\n",
    "\n",
    "def choose_splits_balanced_groupaware(\n",
    "    n_cells,\n",
    "    groups,\n",
    "    Y,\n",
    "    M,\n",
    "    genes,\n",
    "    *,\n",
    "    seed0=0,\n",
    "    tries=5000,\n",
    "    frac=(0.70, 0.15, 0.15),\n",
    "\n",
    "    # HARD constraints for TRAIN/VAL/TEST\n",
    "    min_train_labeled=10,\n",
    "    min_val_labeled=10,\n",
    "    min_test_labeled=10,\n",
    "    min_train_pos=1,\n",
    "    min_train_neg=1,\n",
    "    min_val_pos=1,\n",
    "    min_val_neg=1,\n",
    "    min_test_pos=1,\n",
    "    min_test_neg=1,\n",
    "\n",
    "    enforce_genes=None,\n",
    "    prefer_balance=True,\n",
    "    balance_weight=10.0,\n",
    "    verbose=True,\n",
    "\n",
    "    # fallback behavior inside group-aware\n",
    "    soft_fallback=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Group-aware random search. Returns either:\n",
    "      - HARD split satisfying all constraints for enforce_genes, OR\n",
    "      - best-effort group-aware split (if soft_fallback=True)\n",
    "    \"\"\"\n",
    "    Y = np.asarray(Y)\n",
    "    M = (np.asarray(M) > 0).astype(np.int8)\n",
    "\n",
    "    n_cells = int(n_cells)\n",
    "    groups = np.asarray(groups).astype(str)\n",
    "    uniq = np.unique(groups)\n",
    "\n",
    "    gene_to_j = {g: j for j, g in enumerate(genes)}\n",
    "    if enforce_genes is None:\n",
    "        enforce_genes = list(genes)\n",
    "    enforce_js = [gene_to_j[g] for g in enforce_genes if g in gene_to_j]\n",
    "\n",
    "    ntr = max(1, int(round(frac[0] * len(uniq))))\n",
    "    nva = max(1, int(round(frac[1] * len(uniq))))\n",
    "    rng = np.random.RandomState(seed0)\n",
    "\n",
    "    best_hard = None\n",
    "    best_hard_score = -np.inf\n",
    "    best_soft = None\n",
    "    best_soft_score = -np.inf\n",
    "\n",
    "    for _ in range(int(tries)):\n",
    "        perm = uniq.copy()\n",
    "        rng.shuffle(perm)\n",
    "        g_tr = set(perm[:ntr])\n",
    "        g_va = set(perm[ntr:ntr+nva])\n",
    "        g_te = set(perm[ntr+nva:])\n",
    "\n",
    "        tr, va, te = _group_split_indices(groups, g_tr, g_va, g_te)\n",
    "        if len(tr) == 0 or len(va) == 0 or len(te) == 0:\n",
    "            continue\n",
    "\n",
    "        sc = _score_split(tr, va, te, Y, M, genes, prefer_balance=prefer_balance, balance_weight=balance_weight)\n",
    "        if sc > best_soft_score:\n",
    "            best_soft_score = sc\n",
    "            best_soft = (tr, va, te)\n",
    "\n",
    "        ok = True\n",
    "        for j in enforce_js:\n",
    "            if not _check_constraints(tr, j, Y, M, min_labeled=min_train_labeled, min_pos=min_train_pos, min_neg=min_train_neg):\n",
    "                ok = False; break\n",
    "            if not _check_constraints(va, j, Y, M, min_labeled=min_val_labeled, min_pos=min_val_pos, min_neg=min_val_neg):\n",
    "                ok = False; break\n",
    "            if not _check_constraints(te, j, Y, M, min_labeled=min_test_labeled, min_pos=min_test_pos, min_neg=min_test_neg):\n",
    "                ok = False; break\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        if sc > best_hard_score:\n",
    "            best_hard_score = sc\n",
    "            best_hard = (tr, va, te)\n",
    "\n",
    "    used_fallback = False\n",
    "    if best_hard is not None:\n",
    "        tr, va, te = best_hard\n",
    "        final_score = best_hard_score\n",
    "        msg = \"group_level | HARD constraints satisfied\"\n",
    "    else:\n",
    "        if not soft_fallback or best_soft is None:\n",
    "            raise RuntimeError(\"No group-aware split found (even best-effort).\")\n",
    "        tr, va, te = best_soft\n",
    "        final_score = best_soft_score\n",
    "        used_fallback = True\n",
    "        msg = \"group_level | FALLBACK (no hard split found)\"\n",
    "\n",
    "    info = dict(\n",
    "        split_mode=\"group_level\",\n",
    "        used_fallback=bool(used_fallback),\n",
    "        score=float(final_score),\n",
    "        tries=int(tries),\n",
    "        frac=tuple(frac),\n",
    "        enforce_genes=list(enforce_genes),\n",
    "        constraints=dict(\n",
    "            min_train_labeled=min_train_labeled, min_train_pos=min_train_pos, min_train_neg=min_train_neg,\n",
    "            min_val_labeled=min_val_labeled,     min_val_pos=min_val_pos,     min_val_neg=min_val_neg,\n",
    "            min_test_labeled=min_test_labeled,   min_test_pos=min_test_pos,   min_test_neg=min_test_neg,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Split mode: {msg} | score={final_score:0.2f}\")\n",
    "        _summarize_split(\"train\", tr, Y, M, genes)\n",
    "        _summarize_split(\"val\",   va, Y, M, genes)\n",
    "        _summarize_split(\"test\",  te, Y, M, genes)\n",
    "\n",
    "    return tr, va, te, info\n",
    "\n",
    "\n",
    "def choose_splits_cell_level_stratified(\n",
    "    n_cells,\n",
    "    Y,\n",
    "    M,\n",
    "    genes,\n",
    "    *,\n",
    "    seed0=0,\n",
    "    frac=(0.70, 0.15, 0.15),\n",
    "    strat_gene=None,      # gene name, e.g. \"NPM1\" (uses labeled cells only for strat)\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cell-level fallback. If strat_gene is provided, we stratify by (Y for that gene) among labeled cells.\n",
    "    Unlabeled cells are assigned randomly preserving overall frac.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed0)\n",
    "    n_cells = int(n_cells)\n",
    "\n",
    "    idx_all = np.arange(n_cells, dtype=int)\n",
    "    rng.shuffle(idx_all)\n",
    "\n",
    "    ntr = int(round(frac[0] * n_cells))\n",
    "    nva = int(round(frac[1] * n_cells))\n",
    "    tr = idx_all[:ntr]\n",
    "    va = idx_all[ntr:ntr+nva]\n",
    "    te = idx_all[ntr+nva:]\n",
    "\n",
    "    info = dict(split_mode=\"cell_level\", strat_gene=strat_gene, frac=tuple(frac), seed=int(seed0))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Split mode: cell_level | RANDOM\" + (f\" (strat_gene={strat_gene})\" if strat_gene else \"\"))\n",
    "        _summarize_split(\"train\", tr, Y, M, genes)\n",
    "        _summarize_split(\"val\",   va, Y, M, genes)\n",
    "        _summarize_split(\"test\",  te, Y, M, genes)\n",
    "\n",
    "    return tr, va, te, info\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) One wrapper that tries group-aware first, then falls back\n",
    "# ============================================================\n",
    "def make_balanced_splits_for_adata(\n",
    "    adata,\n",
    "    genes,\n",
    "    *,\n",
    "    # grouping\n",
    "    group_col=None,             # e.g. \"patient\" or \"experiment\"\n",
    "    allow_cell_level_fallback=True,\n",
    "\n",
    "    # label building\n",
    "    label_mode=\"mut_wt_strings\",     # \"mut_wt_strings\" | \"obs_cols\"\n",
    "    mut_col=\"MutTranscripts\",\n",
    "    wt_col=\"WtTranscripts\",\n",
    "    conflict_policy=\"mut_wins\",\n",
    "\n",
    "    # obs-cols labeling (DAb)\n",
    "    obs_col_map=None,\n",
    "    obs_col_pattern=None,\n",
    "    allow_auto_map=True,\n",
    "    auto_map_prefer=None,\n",
    "    treat_nonzero_as_pos=True,\n",
    "    threshold=0.5,\n",
    "\n",
    "    # search\n",
    "    tries=5000,\n",
    "    frac=(0.70, 0.15, 0.15),\n",
    "    seed0=0,\n",
    "\n",
    "    # constraints\n",
    "    min_train_labeled=10,\n",
    "    min_val_labeled=10,\n",
    "    min_test_labeled=10,\n",
    "    min_train_pos=1,\n",
    "    min_train_neg=1,\n",
    "    min_val_pos=1,\n",
    "    min_val_neg=1,\n",
    "    min_test_pos=1,\n",
    "    min_test_neg=1,\n",
    "\n",
    "    enforce_genes=None,\n",
    "    prefer_balance=True,\n",
    "    balance_weight=10.0,\n",
    "    verbose=True,\n",
    "\n",
    "    # fallbacks\n",
    "    soft_fallback=True,         # within group-aware: return best-effort if hard impossible\n",
    "    cell_level_strat_gene=None, # if we must do cell-level fallback, optional gene to stratify on\n",
    "):\n",
    "    # ----- build Y,M -----\n",
    "    if label_mode == \"mut_wt_strings\":\n",
    "        Y, M = build_YM_from_mut_wt_strings(\n",
    "            adata, genes, mut_col=mut_col, wt_col=wt_col, conflict_policy=conflict_policy\n",
    "        )\n",
    "    elif label_mode == \"obs_cols\":\n",
    "        Y, M = build_YM_from_obs_cols(\n",
    "            adata,\n",
    "            genes,\n",
    "            col_map=obs_col_map,\n",
    "            col_pattern=obs_col_pattern,\n",
    "            threshold=threshold,\n",
    "            treat_nonzero_as_pos=treat_nonzero_as_pos,\n",
    "            allow_auto_map=allow_auto_map,\n",
    "            auto_map_prefer=auto_map_prefer,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"label_mode must be 'mut_wt_strings' or 'obs_cols'\")\n",
    "\n",
    "    # ----- attempt group-aware -----\n",
    "    if group_col is not None:\n",
    "        if group_col not in adata.obs.columns:\n",
    "            raise KeyError(f\"group_col={group_col!r} not in adata.obs.columns\")\n",
    "        groups = adata.obs[group_col].astype(str).to_numpy()\n",
    "\n",
    "        try:\n",
    "            tr, va, te, info = choose_splits_balanced_groupaware(\n",
    "                adata.n_obs,\n",
    "                groups,\n",
    "                Y, M, genes,\n",
    "                seed0=seed0,\n",
    "                tries=tries,\n",
    "                frac=frac,\n",
    "\n",
    "                min_train_labeled=min_train_labeled,\n",
    "                min_val_labeled=min_val_labeled,\n",
    "                min_test_labeled=min_test_labeled,\n",
    "                min_train_pos=min_train_pos,\n",
    "                min_train_neg=min_train_neg,\n",
    "                min_val_pos=min_val_pos,\n",
    "                min_val_neg=min_val_neg,\n",
    "                min_test_pos=min_test_pos,\n",
    "                min_test_neg=min_test_neg,\n",
    "\n",
    "                enforce_genes=enforce_genes,\n",
    "                prefer_balance=prefer_balance,\n",
    "                balance_weight=balance_weight,\n",
    "                verbose=verbose,\n",
    "                soft_fallback=soft_fallback,\n",
    "            )\n",
    "            info[\"group_col\"] = str(group_col)\n",
    "            return tr, va, te, Y, M, info\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if not allow_cell_level_fallback:\n",
    "                raise\n",
    "            if verbose:\n",
    "                print(f\"[WARN] group-aware split failed: {e}\")\n",
    "                print(\"[WARN] falling back to cell-level split.\")\n",
    "\n",
    "    # ----- cell-level fallback -----\n",
    "    tr, va, te, info = choose_splits_cell_level_stratified(\n",
    "        adata.n_obs,\n",
    "        Y, M, genes,\n",
    "        seed0=seed0,\n",
    "        frac=frac,\n",
    "        strat_gene=cell_level_strat_gene,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return tr, va, te, Y, M, info\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Example usage\n",
    "# ============================================================\n",
    "\n",
    "# --- VG (patient-aware; transcript strings)\n",
    "# vg_tr, vg_va, vg_te, Y_vg, M_vg, vg_info = make_balanced_splits_for_adata(\n",
    "#     vg_rna_pp,\n",
    "#     GENES,\n",
    "#     group_col=\"patient\",\n",
    "#     label_mode=\"mut_wt_strings\",\n",
    "#     mut_col=\"MutTranscripts\",\n",
    "#     wt_col=\"WtTranscripts\",\n",
    "#     tries=8000,\n",
    "#     frac=(0.70, 0.15, 0.15),\n",
    "#     # with sparse labels, keep mins low or enforce fewer genes\n",
    "#     min_train_labeled=10, min_train_pos=2, min_train_neg=2,\n",
    "#     min_val_labeled=4,    min_val_pos=1,   min_val_neg=1,\n",
    "#     min_test_labeled=4,   min_test_pos=1,  min_test_neg=1,\n",
    "#     enforce_genes=[\"NPM1\",\"DNMT3A\",\"FLT3\",\"TP53\",\"NRAS\"],  # pick feasible ones\n",
    "#     soft_fallback=True,\n",
    "#     allow_cell_level_fallback=True,\n",
    "#     cell_level_strat_gene=\"NPM1\",\n",
    "#     seed0=42,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# --- DAb (experiment-aware; obs columns with auto-map)\n",
    "# dab_tr, dab_va, dab_te, Y_dab, M_dab, dab_info = make_balanced_splits_for_adata(\n",
    "#     dab_adt_pp,\n",
    "#     [\"NPM1\",\"DNMT3A\",\"FLT3\"],\n",
    "#     group_col=\"experiment\",\n",
    "#     label_mode=\"obs_cols\",\n",
    "#     allow_auto_map=True,          # will map to e.g. 'DNMT3A R882H', 'NPM1 W288fs', 'FLT3-ITD'\n",
    "#     auto_map_prefer=[\"ITD\",\"R882\",\"W288\"],  # optional tie-breakers\n",
    "#     tries=8000,\n",
    "#     frac=(0.70, 0.15, 0.15),\n",
    "#     min_train_labeled=200, min_train_pos=20, min_train_neg=20,\n",
    "#     min_val_labeled=50,    min_val_pos=5,    min_val_neg=5,\n",
    "#     min_test_labeled=50,   min_test_pos=5,   min_test_neg=5,\n",
    "#     enforce_genes=[\"NPM1\",\"DNMT3A\",\"FLT3\"],\n",
    "#     soft_fallback=True,\n",
    "#     allow_cell_level_fallback=True,\n",
    "#     cell_level_strat_gene=\"NPM1\",\n",
    "#     seed0=42,\n",
    "#     verbose=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4d7c1-bd94-4a72-b267-225b0b78aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _compile_gene_pattern(gene: str) -> re.Pattern:\n",
    "    \"\"\"\n",
    "    Match gene as a token-ish substring, case-insensitive.\n",
    "    Prevents IDH1 matching IDH2 etc. by requiring non-alnum/_ around it.\n",
    "    \"\"\"\n",
    "    g = re.escape(gene.upper())\n",
    "    # boundaries: not letter/number/underscore on either side\n",
    "    return re.compile(rf\"(?<![A-Z0-9_]){g}(?![A-Z0-9_])\", flags=re.IGNORECASE)\n",
    "\n",
    "def _to_text_series(x):\n",
    "    \"\"\"Convert to uppercase string Series, with NaN -> ''.\"\"\"\n",
    "    s = x.copy()\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.fillna(\"\")\n",
    "    return s.str.upper()\n",
    "\n",
    "def mutation_counts_from_transcripts(\n",
    "    adata,\n",
    "    genes,\n",
    "    *,\n",
    "    mut_col=\"MutTranscripts\",\n",
    "    wt_col=\"WtTranscripts\",\n",
    "    prefix=\"tx_\",\n",
    "    conflict_policy=\"mut_wins\",  # \"mut_wins\" | \"na\" | \"error\"\n",
    "    write_per_cell_labels=False, # if True, write adata.obs[f\"{prefix}{gene}_label\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      summary_df: per-gene counts of Mut / WT / NA and conflicts\n",
    "      (optionally) writes per-cell labels into adata.obs.\n",
    "    \"\"\"\n",
    "    if mut_col not in adata.obs.columns or wt_col not in adata.obs.columns:\n",
    "        raise KeyError(f\"Need obs columns '{mut_col}' and '{wt_col}'. Found: {list(adata.obs.columns)[:20]} ...\")\n",
    "\n",
    "    mut_s = _to_text_series(adata.obs[mut_col])\n",
    "    wt_s  = _to_text_series(adata.obs[wt_col])\n",
    "\n",
    "    n = adata.n_obs\n",
    "    rows = []\n",
    "\n",
    "    for gene in genes:\n",
    "        pat = _compile_gene_pattern(gene)\n",
    "\n",
    "        in_mut = mut_s.str.contains(pat, regex=True)\n",
    "        in_wt  = wt_s.str.contains(pat, regex=True)\n",
    "\n",
    "        conflict = in_mut & in_wt\n",
    "\n",
    "        if conflict_policy == \"mut_wins\":\n",
    "            mut = in_mut\n",
    "            wt  = (~in_mut) & in_wt\n",
    "            na  = (~in_mut) & (~in_wt)\n",
    "        elif conflict_policy == \"na\":\n",
    "            mut = in_mut & (~conflict)\n",
    "            wt  = in_wt  & (~conflict)\n",
    "            na  = (~mut) & (~wt)  # includes conflicts as NA\n",
    "        elif conflict_policy == \"error\":\n",
    "            if conflict.any():\n",
    "                idx = adata.obs_names[conflict][0]\n",
    "                raise ValueError(f\"Conflict for {gene}: appears in BOTH {mut_col} and {wt_col} (e.g. cell {idx})\")\n",
    "            mut = in_mut\n",
    "            wt  = (~in_mut) & in_wt\n",
    "            na  = (~in_mut) & (~in_wt)\n",
    "        else:\n",
    "            raise ValueError(\"conflict_policy must be 'mut_wins', 'na', or 'error'\")\n",
    "\n",
    "        mut_n = int(mut.sum())\n",
    "        wt_n  = int(wt.sum())\n",
    "        na_n  = int(na.sum())\n",
    "        conf_n = int(conflict.sum())\n",
    "\n",
    "        rows.append({\n",
    "            \"gene\": gene,\n",
    "            \"mut_n\": mut_n,\n",
    "            \"wt_n\": wt_n,\n",
    "            \"na_n\": na_n,\n",
    "            \"conflict_n\": conf_n,\n",
    "            \"mut_frac\": mut_n / n,\n",
    "            \"wt_frac\": wt_n / n,\n",
    "            \"na_frac\": na_n / n,\n",
    "        })\n",
    "\n",
    "        if write_per_cell_labels:\n",
    "            col = f\"{prefix}{gene}_label\"\n",
    "            lab = np.full(n, \"NA\", dtype=object)\n",
    "            lab[wt.to_numpy()] = \"WT\"\n",
    "            lab[mut.to_numpy()] = \"Mut\"\n",
    "            if conflict_policy == \"na\":\n",
    "                lab[conflict.to_numpy()] = \"NA_conflict\"\n",
    "            else:\n",
    "                lab[conflict.to_numpy()] = \"Mut_conflict\"\n",
    "            adata.obs[col] = pd.Categorical(lab, categories=[\"WT\", \"Mut\", \"NA\", \"NA_conflict\", \"Mut_conflict\"])\n",
    "\n",
    "    summary_df = pd.DataFrame(rows).sort_values(\"gene\").reset_index(drop=True)\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage\n",
    "# -------------------------\n",
    "GENES = [\"NPM1\",\"DNMT3A\",\"FLT3\",\"TP53\",\"NRAS\",\"TET2\",\"IDH2\"]\n",
    "\n",
    "df = mutation_counts_from_transcripts(\n",
    "    vg_rna_pp,          # or your AnnData\n",
    "    GENES,\n",
    "    mut_col=\"MutTranscripts\",\n",
    "    wt_col=\"WtTranscripts\",\n",
    "    conflict_policy=\"mut_wins\",\n",
    "    write_per_cell_labels=False\n",
    ")\n",
    "\n",
    "print(df.to_string(index=False, formatters={\n",
    "    \"mut_frac\": \"{:.3f}\".format,\n",
    "    \"wt_frac\": \"{:.3f}\".format,\n",
    "    \"na_frac\": \"{:.3f}\".format,\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f672952-d274-4df5-8d4e-616bc8488c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Run the split functions defined above\n",
    "# ============================================================\n",
    "\n",
    "# --- VG (patient-aware; transcript strings)\n",
    "vg_tr, vg_va, vg_te, Y_vg, M_vg, vg_info = make_balanced_splits_for_adata(\n",
    "    vg_rna_pp,\n",
    "    GENES,\n",
    "    group_col=\"patient\",\n",
    "    label_mode=\"mut_wt_strings\",\n",
    "    mut_col=\"MutTranscripts\",     \n",
    "    wt_col=\"WtTranscripts\",\n",
    "    tries=8000,\n",
    "    frac=(0.70, 0.15, 0.15),\n",
    "    # with sparse labels, keep mins low or enforce fewer genes\n",
    "    min_train_labeled=10, min_train_pos=2, min_train_neg=2,\n",
    "    min_val_labeled=4,    min_val_pos=1,   min_val_neg=1,\n",
    "    min_test_labeled=4,   min_test_pos=1,  min_test_neg=1,\n",
    "    enforce_genes=[\"NPM1\",\"DNMT3A\",\"FLT3\",\"TP53\",\"NRAS\",\"IDH2\",\"TET2\"],  # pick feasible ones\n",
    "    soft_fallback=False,\n",
    "    allow_cell_level_fallback=True,\n",
    "    cell_level_strat_gene=\"NPM1\",\n",
    "    seed0=42,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c307440-2523-41ed-8b21-a2c4178b33d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DAb (experiment-aware; obs columns with auto-map)\n",
    "dab_tr, dab_va, dab_te, Y_dab, M_dab, dab_info = make_balanced_splits_for_adata(\n",
    "    dab_adt_pp,\n",
    "    [\"NPM1\",\"DNMT3A\",\"FLT3\"],\n",
    "    group_col=\"experiment\",\n",
    "    label_mode=\"obs_cols\",\n",
    "    allow_auto_map=True,          # will map to e.g. 'DNMT3A R882H', 'NPM1 W288fs', 'FLT3-ITD'\n",
    "    auto_map_prefer=[\"ITD\",\"R882\",\"W288\"],  # optional tie-breakers\n",
    "    tries=8000,\n",
    "    frac=(0.70, 0.15, 0.15),\n",
    "    min_train_labeled=200, min_train_pos=20, min_train_neg=20,\n",
    "    min_val_labeled=10,    min_val_pos=5,    min_val_neg=5,\n",
    "    min_test_labeled=50,   min_test_pos=5,   min_test_neg=5,\n",
    "    enforce_genes=[\"NPM1\",\"DNMT3A\",\"FLT3\"],\n",
    "    soft_fallback=False,\n",
    "    allow_cell_level_fallback=True,\n",
    "    cell_level_strat_gene=\"NPM1\",\n",
    "    seed0=42,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3b784-85f6-4b95-be51-e681c42895fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets/loaders (unchanged, but now using the new VG splits)\n",
    "vg_ds  = SingleModalDataset(vg_rna_pp,  \"rna\", Y_vg,  M_vg)\n",
    "dab_ds = SingleModalDataset(dab_adt_pp, \"adt\", Y_dab, M_dab)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "B = 128\n",
    "pin = (device == \"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515d598-b8b9-475c-a9ef-d57ec71c3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_YM_to_genes(*, Y, M, genes_in, genes_out):\n",
    "    \"\"\"\n",
    "    Pad/reorder (Y, M) from genes_in -> genes_out.\n",
    "    Missing genes in genes_in will be filled with Y=0 and M=0 (unlabeled).\n",
    "    \"\"\"\n",
    "    genes_in = list(genes_in)\n",
    "    genes_out = list(genes_out)\n",
    "\n",
    "    Y = np.asarray(Y)\n",
    "    M = np.asarray(M)\n",
    "\n",
    "    if Y.ndim != 2 or M.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D arrays, got Y{Y.shape}, M{M.shape}\")\n",
    "    if Y.shape != M.shape:\n",
    "        raise ValueError(f\"Y and M must have same shape, got Y{Y.shape} vs M{M.shape}\")\n",
    "    if Y.shape[1] != len(genes_in):\n",
    "        raise ValueError(f\"Y/M cols ({Y.shape[1]}) must match len(genes_in) ({len(genes_in)})\")\n",
    "\n",
    "    n = Y.shape[0]\n",
    "    Y_out = np.zeros((n, len(genes_out)), dtype=Y.dtype)\n",
    "    M_out = np.zeros((n, len(genes_out)), dtype=M.dtype)\n",
    "\n",
    "    idx_in = {g: j for j, g in enumerate(genes_in)}\n",
    "    for j_out, g in enumerate(genes_out):\n",
    "        j_in = idx_in.get(g, None)\n",
    "        if j_in is None:\n",
    "            # not present in this dataset -> keep Y=0, M=0 (unlabeled)\n",
    "            continue\n",
    "        Y_out[:, j_out] = Y[:, j_in]\n",
    "        M_out[:, j_out] = M[:, j_in]\n",
    "\n",
    "    return Y_out, M_out\n",
    "\n",
    "\n",
    "def summarize_label_coverage(name, Y, M, genes, idx):\n",
    "    idx = np.asarray(idx, dtype=int)\n",
    "    Msub = M[idx]\n",
    "    Ysub = Y[idx]\n",
    "\n",
    "    print(f\"\\n[{name}] n={len(idx):,}\")\n",
    "    for j, g in enumerate(genes):\n",
    "        n_lab = int(Msub[:, j].sum())\n",
    "        if n_lab == 0:\n",
    "            print(f\"  {g:6s}: n_labeled=0\")\n",
    "        else:\n",
    "            yy = Ysub[Msub[:, j] > 0, j]\n",
    "            n_pos = int((yy > 0.5).sum())\n",
    "            prev = n_pos / max(n_lab, 1)\n",
    "            print(f\"  {g:6s}: n_labeled={n_lab:5d}  n_pos={n_pos:5d}  prev={prev:.3f}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage for your case:\n",
    "# ----------------------------\n",
    "# Global gene order you want everywhere:\n",
    "GENES = [\"NPM1\",\"DNMT3A\",\"FLT3\",\"TP53\",\"NRAS\",\"TET2\",\"IDH2\"]\n",
    "\n",
    "# The genes your DAb Y/M currently correspond to (likely 3):\n",
    "GENES_DAB = [\"NPM1\",\"DNMT3A\",\"FLT3\"]\n",
    "\n",
    "# Pad/reorder DAb to full GENES with M=0 for missing genes\n",
    "Y_dab, M_dab = pad_YM_to_genes(Y=Y_dab, M=M_dab, genes_in=GENES_DAB, genes_out=GENES)\n",
    "\n",
    "# Now summaries won't crash and will correctly show n_labeled=0 for missing genes\n",
    "summarize_label_coverage(\"VG train\",   Y_vg,  M_vg,  GENES, vg_tr)\n",
    "summarize_label_coverage(\"VG val\",     Y_vg,  M_vg,  GENES, vg_va)\n",
    "summarize_label_coverage(\"VG test\",    Y_vg,  M_vg,  GENES, vg_te)\n",
    "\n",
    "summarize_label_coverage(\"DAb train\",  Y_dab, M_dab, GENES, dab_tr)\n",
    "summarize_label_coverage(\"DAb val\",    Y_dab, M_dab, GENES, dab_va)\n",
    "summarize_label_coverage(\"DAb test\",   Y_dab, M_dab, GENES, dab_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecce8c-81d1-42d0-b256-f2860d425251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# --- sanity: your DAb labels/masks MUST already be padded to len(GENES) ---\n",
    "# (run your pad_YM_to_genes(...) before this)\n",
    "assert Y_vg.shape[1]  == len(GENES) and M_vg.shape[1]  == len(GENES),  (Y_vg.shape,  M_vg.shape,  len(GENES))\n",
    "assert Y_dab.shape[1] == len(GENES) and M_dab.shape[1] == len(GENES), (Y_dab.shape, M_dab.shape, len(GENES))\n",
    "\n",
    "# --- patch the underlying dataset targets so Subset() yields (B, len(GENES)) ---\n",
    "# This assumes vg_ds/dab_ds store targets on attributes named Y/M (common in our earlier code).\n",
    "# If your dataset uses different attribute names, change them here once.\n",
    "if hasattr(vg_ds, \"Y\"): vg_ds.Y = Y_vg\n",
    "if hasattr(vg_ds, \"M\"): vg_ds.M = M_vg\n",
    "\n",
    "if hasattr(dab_ds, \"Y\"): dab_ds.Y = Y_dab\n",
    "if hasattr(dab_ds, \"M\"): dab_ds.M = M_dab\n",
    "\n",
    "# (optional) if your dataset keeps torch tensors cached, you may also want:\n",
    "# if hasattr(dab_ds, \"Y_t\"): dab_ds.Y_t = torch.from_numpy(Y_dab).float()\n",
    "# if hasattr(dab_ds, \"M_t\"): dab_ds.M_t = torch.from_numpy(M_dab).float()\n",
    "\n",
    "# --- build loaders ---\n",
    "vg_train_loader  = DataLoader(Subset(vg_ds,  vg_tr),  batch_size=B, shuffle=True,  num_workers=0, pin_memory=pin)\n",
    "vg_val_loader    = DataLoader(Subset(vg_ds,  vg_va),  batch_size=B, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "vg_test_loader   = DataLoader(Subset(vg_ds,  vg_te),  batch_size=B, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "\n",
    "dab_train_loader = DataLoader(Subset(dab_ds, dab_tr), batch_size=B, shuffle=True,  num_workers=0, pin_memory=pin)\n",
    "dab_val_loader   = DataLoader(Subset(dab_ds, dab_va), batch_size=B, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "dab_test_loader  = DataLoader(Subset(dab_ds, dab_te), batch_size=B, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "\n",
    "print(\"VG split sizes:\",  len(vg_tr),  len(vg_va),  len(vg_te))\n",
    "print(\"DAb split sizes:\", len(dab_tr), len(dab_va), len(dab_te), \"group:\", DAB_EXPT_COL)\n",
    "\n",
    "# --- quick one-batch check (should print (B, len(GENES))) ---\n",
    "b = next(iter(dab_train_loader))\n",
    "# adjust keys if your batch is a tuple instead of dict\n",
    "y_b = b[\"y\"] if isinstance(b, dict) else b[1]\n",
    "m_b = b[\"m\"] if isinstance(b, dict) else b[2]\n",
    "print(\"DAb batch y/m:\", tuple(y_b.shape), tuple(m_b.shape), \"expected (*,\", len(GENES), \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b989c-bd51-4f91-8eea-b3010d3acca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _as_modality_key(modality):\n",
    "    \"\"\"\n",
    "    Normalize modality tokens coming from dataloaders into canonical keys.\n",
    "    Handles batched modality lists produced by DataLoader collate.\n",
    "    \"\"\"\n",
    "    # If batch-collated: modality is usually a list/tuple of identical strings\n",
    "    if isinstance(modality, (list, tuple)) and len(modality) > 0:\n",
    "        modality = modality[0]\n",
    "\n",
    "    # If numpy array of strings (sometimes happens)\n",
    "    if isinstance(modality, np.ndarray):\n",
    "        if modality.ndim > 0 and modality.size > 0:\n",
    "            modality = modality.flat[0]\n",
    "        else:\n",
    "            modality = modality.item()\n",
    "\n",
    "    # torch scalar / numpy scalar -> python scalar\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(modality, torch.Tensor):\n",
    "            # if it's a batch of modality ids, take first\n",
    "            if modality.numel() > 1:\n",
    "                modality = modality.flatten()[0].detach().cpu().item()\n",
    "            else:\n",
    "                modality = modality.detach().cpu().item()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if isinstance(modality, (np.generic,)):\n",
    "        modality = modality.item()\n",
    "\n",
    "    # If dataset returns a dict/batch with modality inside\n",
    "    if isinstance(modality, dict):\n",
    "        for k in (\"modality\", \"mod\", \"mod_key\"):\n",
    "            if k in modality:\n",
    "                modality = modality[k]\n",
    "                break\n",
    "\n",
    "    # ints sometimes used as modality ids\n",
    "    if isinstance(modality, (int, np.integer)):\n",
    "        if modality == 0:\n",
    "            return \"rna\"\n",
    "        if modality == 1:\n",
    "            return \"adt\"\n",
    "        return str(modality)\n",
    "\n",
    "    # strings / objects\n",
    "    s = str(modality).strip().lower()\n",
    "\n",
    "    if s in (\"rna\", \"adt\", \"atac\"):\n",
    "        return s\n",
    "\n",
    "    aliases = {\n",
    "        \"vg\": \"rna\",\n",
    "        \"vg_rna\": \"rna\",\n",
    "        \"van_galen\": \"rna\",\n",
    "        \"gene\": \"rna\",\n",
    "        \"expression\": \"rna\",\n",
    "\n",
    "        \"dab\": \"adt\",\n",
    "        \"dab_adt\": \"adt\",\n",
    "        \"protein\": \"adt\",\n",
    "        \"proteins\": \"adt\",\n",
    "        \"antibody\": \"adt\",\n",
    "        \"adt_counts\": \"adt\",\n",
    "    }\n",
    "\n",
    "    s2 = s.replace(\"mod:\", \"\").replace(\"modality:\", \"\")\n",
    "    return aliases.get(s2, s2)\n",
    "\n",
    "import torch\n",
    "\n",
    "def _to_device(x, device):\n",
    "    \"\"\"\n",
    "    Move a tensor (or numpy array) to device with sane defaults.\n",
    "    - ensures float32\n",
    "    - keeps non_blocking when CUDA + pinned memory\n",
    "    \"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x)\n",
    "\n",
    "    if not torch.is_tensor(x):\n",
    "        # last resort: try to tensor it\n",
    "        x = torch.tensor(x)\n",
    "\n",
    "    # Make sure dtype is float32 for UniVI encoders\n",
    "    if x.dtype != torch.float32:\n",
    "        x = x.float()\n",
    "\n",
    "    return x.to(device, non_blocking=(str(device).startswith(\"cuda\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00907266-f6ed-48d7-9d13-6236e3368188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_pos_weight_from_arrays(Y, M, eps=1e-6, clamp_max=100.0):\n",
    "    \"\"\"\n",
    "    Y, M: numpy arrays shape (N, G)\n",
    "      - Y: 0/1 labels (or floats where >0.5 treated as positive)\n",
    "      - M: 0/1 mask for labeledness\n",
    "\n",
    "    Returns:\n",
    "      pos_weight: torch.FloatTensor (G,) where pos_weight[j] = n_neg / n_pos\n",
    "      (clamped to [1.0, clamp_max] to avoid insane values when pos is tiny)\n",
    "    \"\"\"\n",
    "    Y = np.asarray(Y)\n",
    "    M = np.asarray(M).astype(bool)\n",
    "\n",
    "    if Y.ndim != 2 or M.ndim != 2 or Y.shape != M.shape:\n",
    "        raise ValueError(f\"Y and M must be same shape (N,G). Got Y{Y.shape}, M{M.shape}\")\n",
    "\n",
    "    N, G = Y.shape\n",
    "    pw = np.ones(G, dtype=np.float32)\n",
    "\n",
    "    for j in range(G):\n",
    "        mask = M[:, j]\n",
    "        if not np.any(mask):\n",
    "            pw[j] = 1.0\n",
    "            continue\n",
    "\n",
    "        yy = Y[mask, j]\n",
    "        npos = float((yy > 0.5).sum())\n",
    "        nneg = float((yy <= 0.5).sum())\n",
    "\n",
    "        # if all one class, don't blow up training\n",
    "        if npos < 1.0 or nneg < 1.0:\n",
    "            pw[j] = 1.0\n",
    "            continue\n",
    "\n",
    "        val = (nneg + eps) / (npos + eps)\n",
    "        # pos_weight < 1 is allowed, but often unstable; clamp to >=1\n",
    "        val = max(1.0, val)\n",
    "        if clamp_max is not None:\n",
    "            val = min(float(clamp_max), val)\n",
    "        pw[j] = val\n",
    "\n",
    "    return torch.tensor(pw, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c343918-fb36-4e15-8881-8e73c9b9bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN + EVAL\n",
    "# ============================================================\n",
    "'''\n",
    "model_ft, head, best = finetune_encoders_and_head(\n",
    "    model,\n",
    "    train_loaders=[vg_train_loader, dab_train_loader],\n",
    "    val_loaders=[vg_val_loader, dab_val_loader],\n",
    "    out_dim=len(GENES),\n",
    "    lr=2e-4,\n",
    "    max_epochs=400,\n",
    "    patience=40,\n",
    ")\n",
    "\n",
    "print(\"Fine-tune best:\", best)\n",
    "'''\n",
    "\n",
    "# build pos_weight from TRAIN labeled cells only\n",
    "Y_tr = np.vstack([Y_vg[vg_tr],  Y_dab[dab_tr]])\n",
    "M_tr = np.vstack([M_vg[vg_tr],  M_dab[dab_tr]])\n",
    "\n",
    "pos_weight = compute_pos_weight_from_arrays(Y_tr, M_tr).to(device)\n",
    "\n",
    "model_ft, head, best = finetune_encoders_and_head(\n",
    "    model,\n",
    "    train_loaders=[vg_train_loader, dab_train_loader],\n",
    "    val_loaders=[vg_val_loader, dab_val_loader],\n",
    "    out_dim=len(GENES),\n",
    "    genes=GENES,\n",
    "    device=device,\n",
    "    lr_head=1e-4,\n",
    "    lr_encoder=1e-5,\n",
    "    weight_decay=1e-6,\n",
    "    lambda_preserve=5.0,\n",
    "    warmup_epochs=10,\n",
    "    max_epochs=1000,\n",
    "    patience=50,\n",
    "    grad_clip=5.0,\n",
    "    pos_weight=pos_weight,\n",
    "    use_per_gene_heads=True,\n",
    "    start_best_after_unfreeze=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a9566-85f4-4efe-8838-b069cd314a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dab_perf = eval_head(model_ft, head, dab_test_loader, GENES)\n",
    "vg_perf  = eval_head(model_ft, head, vg_test_loader,  GENES)\n",
    "\n",
    "print(\"DAb test:\", dab_perf)\n",
    "print(\"VG  test:\", vg_perf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091de73-234c-43aa-8cdb-ab0878b077fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_ap_vs_prevalence(*, Y, M, te_idx, genes, eval_out, title=\"\"):\n",
    "    rows = []\n",
    "    for j, g in enumerate(genes):\n",
    "        te_mask = np.asarray(M[te_idx, j]).astype(bool)\n",
    "        n = int(te_mask.sum())\n",
    "        if n == 0:\n",
    "            prev = np.nan\n",
    "            n_pos = 0\n",
    "        else:\n",
    "            yy = np.asarray(Y[te_idx, j])[te_mask].astype(int)\n",
    "            n_pos = int((yy == 1).sum())\n",
    "            prev = n_pos / n\n",
    "\n",
    "        ap = eval_out.get(g, {}).get(\"ap\", np.nan)\n",
    "        auc = eval_out.get(g, {}).get(\"auc\", np.nan)\n",
    "        status = eval_out.get(g, {}).get(\"status\", \"missing\")\n",
    "\n",
    "        fold = (ap / prev) if (np.isfinite(ap) and np.isfinite(prev) and prev > 0) else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"gene\": g,\n",
    "            \"status\": status,\n",
    "            \"n_labeled\": n,\n",
    "            \"n_pos\": n_pos,\n",
    "            \"prevalence(AP_random)\": prev,\n",
    "            \"AP\": ap,\n",
    "            \"AP_over_random\": fold,\n",
    "            \"AUC\": auc,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\n",
    "        by=[\"status\", \"AP_over_random\", \"AP\"], ascending=[True, False, False]\n",
    "    )\n",
    "\n",
    "    if title:\n",
    "        print(\"\\n\" + title)\n",
    "    display(df) if \"display\" in globals() else print(df.to_string(index=False))\n",
    "    return df\n",
    "\n",
    "# Example usage (adapt names to your variables):\n",
    "# dab_test_out = eval_head(model_ft, head, dab_test_loader, GENES)   # you already did this\n",
    "# vg_test_out  = eval_head(model_ft, head, vg_test_loader,  GENES)\n",
    "\n",
    "df_dab = summarize_ap_vs_prevalence(\n",
    "    Y=Y_dab, M=M_dab, te_idx=dab_te, genes=GENES, eval_out=dab_perf, title=\"DAb TEST: AP vs prevalence\"\n",
    ")\n",
    "df_vg = summarize_ap_vs_prevalence(\n",
    "    Y=Y_vg, M=M_vg, te_idx=vg_te, genes=GENES, eval_out=vg_perf, title=\"VG TEST: AP vs prevalence\"\n",
    ")\n",
    "\n",
    "# Quick readout for a single gene (e.g., NPM1)\n",
    "print(\"\\nVG NPM1 row:\")\n",
    "print(df_vg[df_vg[\"gene\"] == \"NPM1\"].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04679190-62a6-4802-834c-5593bdd91365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "FIGDIR = Path(FIGDIR)\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Encode FT latents on the SAME objects you put into joint\n",
    "#    (important: don't encode cite_rna_pp_tr then store into cite_rna_tr)\n",
    "# ----------------------------\n",
    "# Choose which CITE split you want in joint; you used *_tr below.\n",
    "# Here I assume these are the objects you want in joint:\n",
    "cite_rna_joint = cite_rna_pp_tr      # or cite_rna_pp_tr if you prefer, but be consistent\n",
    "cite_adt_joint = cite_adt_pp_tr      # or cite_adt_pp_tr\n",
    "vg_rna_joint   = vg_rna_pp\n",
    "dab_adt_joint  = dab_adt_pp\n",
    "\n",
    "# Encode and store\n",
    "cite_rna_joint.obsm[\"X_univi_ft\"] = encode_latent(model_ft, cite_rna_joint, modality=\"rna\", device=device)\n",
    "cite_adt_joint.obsm[\"X_univi_ft\"] = encode_latent(model_ft, cite_adt_joint, modality=\"adt\", device=device)\n",
    "vg_rna_joint.obsm[\"X_univi_ft\"]   = encode_latent(model_ft, vg_rna_joint,   modality=\"rna\", device=device)\n",
    "dab_adt_joint.obsm[\"X_univi_ft\"]  = encode_latent(model_ft, dab_adt_joint,  modality=\"adt\", device=device)\n",
    "\n",
    "print(\"Latents shapes:\",\n",
    "      cite_rna_joint.obsm[\"X_univi_ft\"].shape,\n",
    "      cite_adt_joint.obsm[\"X_univi_ft\"].shape,\n",
    "      vg_rna_joint.obsm[\"X_univi_ft\"].shape,\n",
    "      dab_adt_joint.obsm[\"X_univi_ft\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92031f51-eec5-437b-9223-ca7f4ccd515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) Build joint AnnData in FT space\n",
    "# ----------------------------\n",
    "def joint_latent_adata(items, rep_key=\"X_univi_ft\"):\n",
    "    Zs, obs_rows = [], []\n",
    "    for ds_name, mod, a in items:\n",
    "        Z = a.obsm[rep_key]\n",
    "        Zs.append(Z)\n",
    "        obs_rows.append(\n",
    "            pd.DataFrame({\"dataset\": ds_name, \"modality\": mod}, index=a.obs_names.copy())\n",
    "        )\n",
    "    Z = np.vstack(Zs).astype(np.float32)\n",
    "    obs = pd.concat(obs_rows, axis=0)\n",
    "    out = ad.AnnData(\n",
    "        X=Z,\n",
    "        obs=obs,\n",
    "        var=pd.DataFrame(index=[f\"z{i}\" for i in range(Z.shape[1])]),\n",
    "    )\n",
    "    out.obsm[rep_key] = out.X.copy()\n",
    "    return out\n",
    "\n",
    "joint = joint_latent_adata([\n",
    "    (\"CITE\", \"rna\", cite_rna_joint),\n",
    "    (\"CITE\", \"adt\", cite_adt_joint),\n",
    "    (\"DAb\",  \"adt\", dab_adt_joint),\n",
    "    (\"VG\",   \"rna\", vg_rna_joint),\n",
    "], rep_key=\"X_univi_ft\")\n",
    "\n",
    "joint.obs[\"dataset_modality\"] = joint.obs[\"dataset\"].astype(str) + \" \" + joint.obs[\"modality\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c41a1b-e2ed-4eb6-8390-30576155ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 2) Compute joint UMAP in FT space\n",
    "# ----------------------------------\n",
    "sc.pp.neighbors(joint, use_rep=\"X_univi_ft\", n_neighbors=30)  # random_state not used in neighbors\n",
    "sc.tl.umap(joint, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc51f6e-63a8-4eb1-907c-301c793efb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 3) Copy useful metadata to joint object\n",
    "# ----------------------------------------\n",
    "def _assign_block(joint, *, dataset, modality, values, colname):\n",
    "    mask = (joint.obs[\"dataset\"] == dataset) & (joint.obs[\"modality\"] == modality)\n",
    "    n = int(mask.sum())\n",
    "    v = np.asarray(values)\n",
    "    if v.shape[0] != n:\n",
    "        raise ValueError(f\"{colname}: trying to assign {v.shape[0]} values into {n} rows for {dataset} {modality}\")\n",
    "    joint.obs.loc[mask, colname] = v\n",
    "\n",
    "# ----------------------------\n",
    "# (A) CITE: sample_id, library_id -> joint (mask-based)\n",
    "# ----------------------------\n",
    "if \"sample_id\" in cite_rna_joint.obs.columns:\n",
    "    _assign_block(\n",
    "        joint,\n",
    "        dataset=\"CITE\", modality=\"rna\",\n",
    "        values=cite_rna_joint.obs[\"sample_id\"].astype(str).to_numpy(),\n",
    "        colname=\"cite_sample_id\",\n",
    "    )\n",
    "if \"library_id\" in cite_rna_joint.obs.columns:\n",
    "    _assign_block(\n",
    "        joint,\n",
    "        dataset=\"CITE\", modality=\"rna\",\n",
    "        values=cite_rna_joint.obs[\"library_id\"].astype(str).to_numpy(),\n",
    "        colname=\"cite_library_id\",\n",
    "    )\n",
    "\n",
    "if \"sample_id\" in cite_adt_joint.obs.columns:\n",
    "    _assign_block(\n",
    "        joint,\n",
    "        dataset=\"CITE\", modality=\"adt\",\n",
    "        values=cite_adt_joint.obs[\"sample_id\"].astype(str).to_numpy(),\n",
    "        colname=\"cite_sample_id\",\n",
    "    )\n",
    "if \"library_id\" in cite_adt_joint.obs.columns:\n",
    "    _assign_block(\n",
    "        joint,\n",
    "        dataset=\"CITE\", modality=\"adt\",\n",
    "        values=cite_adt_joint.obs[\"library_id\"].astype(str).to_numpy(),\n",
    "        colname=\"cite_library_id\",\n",
    "    )\n",
    "\n",
    "# AML vs Control label (works for both CITE RNA/ADT once cite_sample_id is set)\n",
    "sid = joint.obs.get(\"cite_sample_id\", pd.Series(pd.NA, index=joint.obs_names)).astype(\"string\")\n",
    "ac = pd.Series(pd.NA, index=joint.obs_names, dtype=\"string\")\n",
    "ac.loc[sid.str.contains(\"aml\", case=False, na=False)] = \"AML\"\n",
    "ac.loc[sid.str.contains(\"control\", case=False, na=False)] = \"Control\"\n",
    "joint.obs[\"cite_aml_vs_control\"] = pd.Categorical(ac, categories=[\"Control\", \"AML\"], ordered=True)\n",
    "\n",
    "# ----------------------------\n",
    "# (B) DAb actual mutation label -> joint (mask-based)\n",
    "# ----------------------------\n",
    "HERO = str(HERO)\n",
    "col_actual_bool = f\"actual_{HERO}_mut\"\n",
    "col_actual_lab  = f\"actual_{HERO}_dab_label\"\n",
    "\n",
    "if col_actual_bool in dab_adt_al.obs.columns:\n",
    "    s = dab_adt_al.obs[col_actual_bool].astype(\"boolean\")\n",
    "    lab = pd.Series(pd.NA, index=dab_adt_al.obs_names, dtype=\"string\")\n",
    "    lab.loc[s == True]  = \"Mut\"\n",
    "    lab.loc[s == False] = \"WT\"\n",
    "    lab = pd.Categorical(lab, categories=[\"WT\", \"Mut\"], ordered=True)\n",
    "\n",
    "    # align to dab_adt_joint order (important)\n",
    "    lab_aligned = pd.Series(lab, index=dab_adt_al.obs_names).reindex(dab_adt_joint.obs_names).astype(\"string\").to_numpy()\n",
    "\n",
    "    _assign_block(\n",
    "        joint,\n",
    "        dataset=\"DAb\", modality=\"adt\",\n",
    "        values=lab_aligned,\n",
    "        colname=col_actual_lab,\n",
    "    )\n",
    "    joint.obs[col_actual_lab] = pd.Categorical(joint.obs[col_actual_lab], categories=[\"WT\", \"Mut\"], ordered=True)\n",
    "else:\n",
    "    print(f\"NOTE: {col_actual_bool} not found in dab_adt_al.obs; skipping DAb actual mutation copy.\")\n",
    "\n",
    "# ----------------------------\n",
    "# (C) VG truth label (if you already made it on vg_rna_joint)\n",
    "# ----------------------------\n",
    "col_vg_truth = f\"actual_{HERO}_vg_label\"\n",
    "if col_vg_truth in vg_rna_joint.obs.columns:\n",
    "    _assign_block(\n",
    "        joint,\n",
    "        dataset=\"VG\", modality=\"rna\",\n",
    "        values=vg_rna_joint.obs[col_vg_truth].astype(\"string\").to_numpy(),\n",
    "        colname=col_vg_truth,\n",
    "    )\n",
    "    joint.obs[col_vg_truth] = pd.Categorical(joint.obs[col_vg_truth], categories=[\"WT\", \"Mut\"], ordered=True)\n",
    "\n",
    "# ----------------------------\n",
    "# (D) Optional: copy a probability column from a block into joint\n",
    "# ----------------------------\n",
    "def copy_obs_col_block(src_adata, *, dataset, modality, col):\n",
    "    if col not in src_adata.obs.columns:\n",
    "        print(f\"NOTE: {col} not in src; skipping\")\n",
    "        return\n",
    "    _assign_block(\n",
    "        joint,\n",
    "        dataset=dataset, modality=modality,\n",
    "        values=src_adata.obs[col].to_numpy(),\n",
    "        colname=col,\n",
    "    )\n",
    "\n",
    "# examples (only if these exist)\n",
    "copy_obs_col_block(vg_rna_joint,  dataset=\"VG\",   modality=\"rna\", col=f\"knnP_{HERO}_dab_to_vg_ft\")\n",
    "copy_obs_col_block(dab_adt_joint, dataset=\"DAb\",  modality=\"adt\", col=f\"knnP_{HERO}_vg_to_dab\")\n",
    "copy_obs_col_block(cite_rna_joint, dataset=\"CITE\", modality=\"rna\", col=f\"headP_{HERO}_cite_rna\")\n",
    "copy_obs_col_block(cite_adt_joint, dataset=\"CITE\", modality=\"adt\", col=f\"headP_{HERO}_cite_adt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac1a34-00c1-4bdb-8a39-1b28f6bc950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4) Plot joint FT UMAP panels\n",
    "# ----------------------------\n",
    "# Fig7b: dataset × modality\n",
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=\"dataset_modality\",\n",
    "    title=\"Joint UMAP in FT space (dataset × modality)\",\n",
    "    size=15,\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.savefig(FIGDIR / \"fig7b_umap_dataset_modality_ft.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# CITE: AML vs control\n",
    "if \"cite_aml_vs_control\" in joint.obs.columns:\n",
    "    sc.pl.umap(\n",
    "        joint,\n",
    "        color=[\"cite_sample_id\", \"cite_aml_vs_control\"],\n",
    "        title=[\"CITE sample_id\", \"CITE AML vs Control\"],\n",
    "        na_color=\"lightgrey\",\n",
    "        wspace=0.35,\n",
    "        size=15,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.savefig(FIGDIR / \"joint_umap_ft_cite_sample_and_aml_control.png\", dpi=450, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# DAb: actual mutation label on joint (only DAb points have labels)\n",
    "if col_actual_lab in joint.obs.columns:\n",
    "    sc.pl.umap(\n",
    "        joint,\n",
    "        color=col_actual_lab,\n",
    "        title=f\"DAb observed {HERO} (WT/Mut) on joint FT UMAP\",\n",
    "        na_color=\"lightgrey\",\n",
    "        size=15,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.savefig(FIGDIR / f\"joint_umap_ft_dab_actual_{HERO}.png\", dpi=450, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Optional: VG truth + VG transferred prob (if present)\n",
    "cols = []\n",
    "if col_vg_truth in joint.obs.columns: cols.append(col_vg_truth)\n",
    "pcol = f\"knnP_{HERO}_dab_to_vg_ft\"\n",
    "if pcol in joint.obs.columns: cols.append(pcol)\n",
    "\n",
    "if len(cols) > 0:\n",
    "    sc.pl.umap(\n",
    "        joint,\n",
    "        color=cols,\n",
    "        title=[f\"VG truth {HERO}\" if c==col_vg_truth else f\"Transfer P({HERO}) DAb→VG (FT)\" for c in cols],\n",
    "        na_color=\"lightgrey\",\n",
    "        size=15,\n",
    "        alpha=0.7,\n",
    "        wspace=0.35,\n",
    "    )\n",
    "    plt.savefig(FIGDIR / f\"joint_umap_ft_vg_truth_and_transfer_{HERO}.png\", dpi=450, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b01e16-4b8f-4013-8249-601f7da19717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the VG RNA rows inside joint\n",
    "if \"dataset_modality\" in joint.obs.columns:\n",
    "    vg_mask = (joint.obs[\"dataset_modality\"] == \"VG rna\")\n",
    "else:\n",
    "    vg_mask = (joint.obs[\"dataset\"].astype(str).str.contains(\"VG\")) & (joint.obs[\"modality\"] == \"rna\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee6859-bd6b-4146-983e-2baf20e11259",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"patient\", \"CellType\", \"timepoint\"]  # edit names if needed\n",
    "src = vg_rna_pp.obs[cols].copy()\n",
    "\n",
    "# align by cell id (= obs_names)\n",
    "aligned = src.reindex(joint.obs_names)\n",
    "\n",
    "# write into joint only for VG RNA rows\n",
    "for c in cols:\n",
    "    joint.obs.loc[vg_mask, c] = aligned.loc[joint.obs_names[vg_mask], c].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403d003-243c-41f7-869e-11a89faa8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=['patient'],\n",
    "    title=[f\"Joint fine-tuned latent colored by VG patient\"],\n",
    "    na_color=\"lightgrey\",\n",
    "    size=15,\n",
    "    alpha=0.7,\n",
    "    wspace=0.35,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6fea16-8853-4a10-8de7-1c3dd819865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=['CellType'],\n",
    "    title=[f\"Joint fine-tuned latent colored by VG cell type\"],\n",
    "    na_color=\"lightgrey\",\n",
    "    size=15,\n",
    "    alpha=0.7,\n",
    "    wspace=0.35,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac5f27-0105-4560-8965-039a91c5aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)\n",
    "print(joint.obs['CellType'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ae080-8dbe-4ed9-96de-5fe9ce893552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# =========================\n",
    "# CONFIG: adjust these keys\n",
    "# =========================\n",
    "LATENT_KEY = \"X_univi_ft\"          # or \"X_univi_ft\" if you stored FT latents separately\n",
    "DATASET_COL = \"dataset_modality\" # or (\"dataset\",\"modality\") if you have them split\n",
    "VG_TAG = \"VG rna\"\n",
    "DAB_TAG = \"DAb adt\"\n",
    "\n",
    "VG_CELLTYPE_COL = \"CellType\"    # the column you copied from vg_rna_pp.obs into joint.obs\n",
    "\n",
    "K = 30                          # neighborhood size\n",
    "\n",
    "# =========================\n",
    "# Pull arrays + masks\n",
    "# =========================\n",
    "Z = np.asarray(joint.obsm[LATENT_KEY])\n",
    "\n",
    "vg_mask  = (joint.obs[DATASET_COL].astype(str) == VG_TAG).values\n",
    "dab_mask = (joint.obs[DATASET_COL].astype(str) == DAB_TAG).values\n",
    "\n",
    "Z_vg  = Z[vg_mask]\n",
    "Z_dab = Z[dab_mask]\n",
    "\n",
    "vg_ct = joint.obs.loc[vg_mask, VG_CELLTYPE_COL].astype(str).values\n",
    "\n",
    "print(\"VG cells:\", Z_vg.shape[0], \" DAb cells:\", Z_dab.shape[0])\n",
    "print(\"VG celltype NA fraction:\", np.mean((vg_ct == \"NA\") | (vg_ct == \"nan\") | (vg_ct == \"None\")))\n",
    "\n",
    "# =========================\n",
    "# kNN from DAb -> VG\n",
    "# =========================\n",
    "knn = NearestNeighbors(n_neighbors=K, metric=\"euclidean\")\n",
    "knn.fit(Z_vg)\n",
    "idx = knn.kneighbors(Z_dab, return_distance=False)   # (n_dab, K) indices into VG\n",
    "\n",
    "# count VG celltypes in each DAb neighborhood\n",
    "ct_levels = pd.Index(sorted(pd.unique(vg_ct)))\n",
    "ct_to_i = {c:i for i,c in enumerate(ct_levels)}\n",
    "\n",
    "counts = np.zeros((idx.shape[0], len(ct_levels)), dtype=np.int32)\n",
    "for r in range(idx.shape[0]):\n",
    "    for j in idx[r]:\n",
    "        counts[r, ct_to_i[vg_ct[j]]] += 1\n",
    "\n",
    "frac = counts / float(K)\n",
    "\n",
    "# overall mapping: average fraction of VG celltypes around DAb cells\n",
    "avg = frac.mean(axis=0)\n",
    "df = pd.DataFrame({\"VG_celltype\": ct_levels, \"avg_frac_in_DAb_kNN\": avg})\n",
    "df = df.sort_values(\"avg_frac_in_DAb_kNN\", ascending=False)\n",
    "\n",
    "print(\"\\nTop VG celltypes surrounding DAb (avg over DAb cells):\")\n",
    "print(df.head(15).to_string(index=False))\n",
    "\n",
    "# optional: how \"PB-like\" vs \"marrow-progenitor-like\" is DAb in this latent?\n",
    "pb_like = set([\"T\",\"CTL\",\"NK\",\"B\",\"Mono\",\"Mono-like\",\"cDC\",\"cDC-like\",\"pDC\",\"Plasma\",\"ProB\"])\n",
    "progen_like = set([\"HSC\",\"HSC-like\",\"GMP\",\"GMP-like\",\"Prog\",\"Prog-like\",\"earlyEry\",\"lateEry\",\"ProMono\",\"ProMono-like\"])\n",
    "\n",
    "pb_score = df[df[\"VG_celltype\"].isin(pb_like)][\"avg_frac_in_DAb_kNN\"].sum()\n",
    "prog_score = df[df[\"VG_celltype\"].isin(progen_like)][\"avg_frac_in_DAb_kNN\"].sum()\n",
    "\n",
    "print(f\"\\nDAb neighbor composition summary (from VG labels):\")\n",
    "print(f\"  PB-like fraction ≈ {pb_score:.3f}\")\n",
    "print(f\"  Progen/Ery-like fraction ≈ {prog_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c639fb9-705d-47d9-913c-02f8aab405c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# --- assume you still have: joint, vg_mask, dab_mask, Z_vg, Z_dab, vg_ct, ct_levels, K ---\n",
    "# If not, just re-run your previous block up to `frac`.\n",
    "\n",
    "pb_like = set([\"T\",\"CTL\",\"NK\",\"B\",\"Mono\",\"Mono-like\",\"cDC\",\"cDC-like\",\"pDC\",\"Plasma\",\"ProB\"])\n",
    "progen_like = set([\"HSC\",\"HSC-like\",\"GMP\",\"GMP-like\",\"Prog\",\"Prog-like\",\"earlyEry\",\"lateEry\",\"ProMono\",\"ProMono-like\"])\n",
    "\n",
    "# indices in ct_levels\n",
    "pb_idx = np.array([i for i,c in enumerate(ct_levels) if c in pb_like], dtype=int)\n",
    "pr_idx = np.array([i for i,c in enumerate(ct_levels) if c in progen_like], dtype=int)\n",
    "\n",
    "pb_per_cell = frac[:, pb_idx].sum(axis=1)\n",
    "pr_per_cell = frac[:, pr_idx].sum(axis=1)\n",
    "\n",
    "print(\"DAb PB-like per-cell:  median\", float(np.median(pb_per_cell)), \"  p10/p90\",\n",
    "      float(np.quantile(pb_per_cell,0.1)), float(np.quantile(pb_per_cell,0.9)))\n",
    "print(\"DAb Progen/Ery per-cell: median\", float(np.median(pr_per_cell)), \"  p10/p90\",\n",
    "      float(np.quantile(pr_per_cell,0.1)), float(np.quantile(pr_per_cell,0.9)))\n",
    "\n",
    "# How many DAb cells are \"strongly\" progen-like?\n",
    "for thr in [0.3, 0.5, 0.7]:\n",
    "    print(f\"fraction of DAb with progen_like >= {thr}: {float(np.mean(pr_per_cell >= thr)):.3f}\")\n",
    "\n",
    "# Optional: attach these scores to joint.obs for plotting\n",
    "joint.obs.loc[dab_mask, \"dab_pb_like\"] = pb_per_cell\n",
    "joint.obs.loc[dab_mask, \"dab_progen_like\"] = pr_per_cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ec67c-bbe8-488d-bdb2-03ffd3a9d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask for DAb rows\n",
    "dab_mask = (joint.obs[\"dataset_modality\"].astype(str) == \"DAb adt\").values  # adjust if needed\n",
    "\n",
    "# align DAb experiment to joint obs_names\n",
    "exp_aligned = dab_adt_pp.obs[\"experiment\"].astype(str).reindex(joint.obs_names)\n",
    "\n",
    "# --- make patient writable (object), assign, then optionally recast ---\n",
    "# 1) convert to object (avoids categorical restrictions)\n",
    "joint.obs[\"patient\"] = joint.obs[\"patient\"].astype(object)\n",
    "\n",
    "# 2) assign for DAb rows only\n",
    "joint.obs.loc[dab_mask, \"patient\"] = exp_aligned.loc[joint.obs_names[dab_mask]].to_numpy()\n",
    "\n",
    "# 3) (optional) cast back to category for nicer plotting/groupby\n",
    "joint.obs[\"patient\"] = joint.obs[\"patient\"].astype(\"category\")\n",
    "\n",
    "# quick checks\n",
    "print(\"DAb patient(=experiment) missing:\", int(pd.isna(joint.obs.loc[dab_mask, \"patient\"]).sum()))\n",
    "print(joint.obs.loc[dab_mask, \"patient\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f5243-72c8-4365-a517-55623f4e0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the high progen-like DAb cells\n",
    "thr = 0.75\n",
    "dab_hi = dab_mask.copy()\n",
    "dab_hi[dab_mask] = (joint.obs.loc[dab_mask, \"dab_progen_like\"].values >= thr)\n",
    "\n",
    "print(\"DAb high-progen cells:\", int(dab_hi.sum()), \" / \", int(dab_mask.sum()))\n",
    "\n",
    "# if you have any DAb metadata columns, try these (edit as needed)\n",
    "for col in [\"source\", \"tissue\", \"site\", \"sample_id\", \"patient\", \"timepoint\", \"batch\", \"experiment\"]:\n",
    "    if col in joint.obs.columns:\n",
    "        print(\"\\nEnrichment by\", col)\n",
    "        tab = joint.obs.loc[dab_hi, col].value_counts(normalize=True).head(100)\n",
    "        print(tab.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e3fb0-0735-4da1-97de-38e29d758e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse ct_levels, frac, etc. from earlier\n",
    "hi_idx = np.where(pr_per_cell >= 0.75)[0]   # indices within DAb subset ordering (Z_dab order)\n",
    "\n",
    "avg_hi = frac[hi_idx].mean(axis=0)\n",
    "df_hi = pd.DataFrame({\"VG_celltype\": ct_levels, \"avg_frac_kNN\": avg_hi}).sort_values(\"avg_frac_kNN\", ascending=False)\n",
    "\n",
    "print(\"\\nVG neighbor composition for DAb cells with progen_like>=0.7:\")\n",
    "print(df_hi.head(15).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0d0b7-a210-4641-a97b-7e1392631b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# define high progen subset among DAb\n",
    "thr = 0.75\n",
    "dab_mask = (joint.obs[\"dataset_modality\"].astype(str) == \"DAb adt\").values\n",
    "hi = joint.obs.loc[dab_mask, \"dab_progen_like\"].astype(float).values >= thr\n",
    "\n",
    "# pull DAb ADT matrix and feature names\n",
    "X = dab_adt_pp.X\n",
    "var = dab_adt_pp.var_names.astype(str)\n",
    "\n",
    "# pick marker panel (keep ones that exist)\n",
    "markers = [m for m in [\"CD34\",\"CD117\",\"KIT\",\"CD38\",\"HLA-DR\",\"CD45RA\",\"CD123\",\"CD33\",\"CD13\",\"CD14\",\"CD16\"] if m in var]\n",
    "print(\"Markers present:\", markers)\n",
    "\n",
    "if len(markers):\n",
    "    Xd = X[:, [list(var).index(m) for m in markers]]\n",
    "    # to dense safely for small marker subset\n",
    "    Xd = Xd.toarray() if hasattr(Xd, \"toarray\") else np.asarray(Xd)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"marker\": markers,\n",
    "        \"mean_hi\": Xd[hi].mean(axis=0),\n",
    "        \"mean_lo\": Xd[~hi].mean(axis=0),\n",
    "        \"diff_hi_minus_lo\": Xd[hi].mean(axis=0) - Xd[~hi].mean(axis=0),\n",
    "    }).sort_values(\"diff_hi_minus_lo\", ascending=False)\n",
    "\n",
    "    print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09145ba-f649-42e3-b6c2-5fff3ff285fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# expects these to already exist in your notebook:\n",
    "#   SingleModalDataset, _as_modality_key, _to_device, latent_mu_student\n",
    "#   model_ft, head, joint\n",
    "#   vg_rna_pp, dab_adt_pp, cite_rna_pp_*, cite_adt_pp_*\n",
    "#   GENES, device\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs_for_adata(\n",
    "    model,\n",
    "    head,\n",
    "    adata,\n",
    "    modality: str,\n",
    "    genes,\n",
    "    *,\n",
    "    device,\n",
    "    batch_size=512,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    \"\"\"Return DataFrame (n_cells x n_genes) with sigmoid probabilities.\"\"\"\n",
    "    model.eval(); head.eval()\n",
    "    n = adata.n_obs\n",
    "    G = len(genes)\n",
    "\n",
    "    Y_dummy = np.zeros((n, G), dtype=np.float32)\n",
    "    M_dummy = np.zeros((n, G), dtype=np.float32)\n",
    "\n",
    "    ds = SingleModalDataset(adata, modality, Y_dummy, M_dummy)\n",
    "    L = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    Ps = []\n",
    "    for modality_batch, x, y, m in L:\n",
    "        mod = _as_modality_key(modality_batch)\n",
    "        x = _to_device(x, device)\n",
    "        z = latent_mu_student(model, x, mod)\n",
    "        logits = head(z)\n",
    "        Ps.append(torch.sigmoid(logits).cpu().numpy())\n",
    "\n",
    "    P = np.vstack(Ps) if len(Ps) else np.zeros((0, G), dtype=np.float32)\n",
    "    return pd.DataFrame(P, index=adata.obs_names, columns=list(genes))\n",
    "\n",
    "\n",
    "def write_probs_into_joint_masked_iloc(\n",
    "    joint,\n",
    "    probs_df,\n",
    "    genes,\n",
    "    mask,\n",
    "    *,\n",
    "    prefix=\"p_\",\n",
    "    mode=\"auto\",         # \"auto\" | \"positional\" | \"name\"\n",
    "    dedup=\"mean\",         # used only for name mapping if probs_df has dup indices\n",
    "):\n",
    "    mask = np.asarray(mask, dtype=bool)\n",
    "    pos = np.where(mask)[0]\n",
    "    n_sub = len(pos)\n",
    "\n",
    "    # ensure destination columns exist\n",
    "    for g in genes:\n",
    "        col = f\"{prefix}{g}\"\n",
    "        if col not in joint.obs.columns:\n",
    "            joint.obs[col] = np.nan\n",
    "\n",
    "    # positional write if lengths match\n",
    "    if mode in (\"auto\", \"positional\"):\n",
    "        if probs_df.shape[0] == n_sub:\n",
    "            P = probs_df.loc[:, list(genes)].to_numpy(dtype=float)\n",
    "            for j, g in enumerate(genes):\n",
    "                col = f\"{prefix}{g}\"\n",
    "                joint.obs.iloc[pos, joint.obs.columns.get_loc(col)] = P[:, j]\n",
    "            col0 = f\"{prefix}{genes[0]}\"\n",
    "            n_filled = int(np.sum(~pd.isna(joint.obs.iloc[pos][col0])))\n",
    "            print(f\"Wrote probs POSITIONALLY into joint for subset rows={n_sub:,} (prefix={prefix}). Example filled for {genes[0]}: {n_filled:,}\")\n",
    "            return\n",
    "        elif mode == \"positional\":\n",
    "            raise ValueError(f\"Positional write mismatch: subset {n_sub} vs probs {probs_df.shape[0]}\")\n",
    "\n",
    "    # name mapping fallback\n",
    "    if probs_df.index.has_duplicates:\n",
    "        if dedup == \"mean\":\n",
    "            probs_df = probs_df.groupby(level=0).mean()\n",
    "        elif dedup == \"first\":\n",
    "            probs_df = probs_df[~probs_df.index.duplicated(keep=\"first\")]\n",
    "        else:\n",
    "            raise ValueError(\"dedup must be 'mean' or 'first'\")\n",
    "\n",
    "    target_index = joint.obs_names[pos]\n",
    "    wrote_any = False\n",
    "    for g in genes:\n",
    "        s = probs_df[g]\n",
    "        mapped = pd.Index(target_index).map(s)\n",
    "        mapped = np.asarray(mapped, dtype=float)\n",
    "        ok = ~np.isnan(mapped)\n",
    "        if ok.any():\n",
    "            joint.obs.iloc[pos[ok], joint.obs.columns.get_loc(f\"{prefix}{g}\")] = mapped[ok]\n",
    "            wrote_any = True\n",
    "\n",
    "    if not wrote_any:\n",
    "        print(\"WARNING: wrote 0 values (name mapping). Index mismatch between probs_df.index and joint.obs_names for this subset.\")\n",
    "    else:\n",
    "        col0 = f\"{prefix}{genes[0]}\"\n",
    "        n_filled = int(np.sum(~pd.isna(joint.obs.iloc[pos][col0])))\n",
    "        print(f\"Wrote probs by NAME into joint for subset rows={n_sub:,} (prefix={prefix}). Example filled for {genes[0]}: {n_filled:,}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN: predict -> write into joint\n",
    "# ============================================================\n",
    "\n",
    "pin = (str(device) == \"cuda\")\n",
    "\n",
    "vg_mask       = (joint.obs[\"dataset_modality\"].astype(str) == \"VG rna\").values\n",
    "dab_mask      = (joint.obs[\"dataset_modality\"].astype(str) == \"DAb adt\").values\n",
    "cite_rna_mask = (joint.obs[\"dataset_modality\"].astype(str) == \"CITE rna\").values\n",
    "cite_adt_mask = (joint.obs[\"dataset_modality\"].astype(str) == \"CITE adt\").values\n",
    "\n",
    "print(\"joint subset sizes:\",\n",
    "      \"VG rna\", int(vg_mask.sum()),\n",
    "      \"| DAb adt\", int(dab_mask.sum()),\n",
    "      \"| CITE rna\", int(cite_rna_mask.sum()),\n",
    "      \"| CITE adt\", int(cite_adt_mask.sum()))\n",
    "\n",
    "# NOTE: you used *_tr here; that's fine only if joint contains only train CITE.\n",
    "# If joint contains ALL CITE (train+val+test), you must predict on the ALL object.\n",
    "vg_probs       = predict_probs_for_adata(model_ft, head, vg_rna_pp,      \"rna\", GENES, device=device, pin_memory=pin)\n",
    "dab_probs      = predict_probs_for_adata(model_ft, head, dab_adt_pp,     \"adt\", GENES, device=device, pin_memory=pin)\n",
    "cite_rna_probs = predict_probs_for_adata(model_ft, head, cite_rna_pp_tr, \"rna\", GENES, device=device, pin_memory=pin)\n",
    "cite_adt_probs = predict_probs_for_adata(model_ft, head, cite_adt_pp_tr, \"adt\", GENES, device=device, pin_memory=pin)\n",
    "\n",
    "print(\"pred sizes:\",\n",
    "      \"VG\", vg_probs.shape[0],\n",
    "      \"| DAb\", dab_probs.shape[0],\n",
    "      \"| CITE rna\", cite_rna_probs.shape[0],\n",
    "      \"| CITE adt\", cite_adt_probs.shape[0])\n",
    "\n",
    "write_probs_into_joint_masked_iloc(joint, vg_probs,       GENES, vg_mask,       prefix=\"p_\", mode=\"auto\")\n",
    "write_probs_into_joint_masked_iloc(joint, dab_probs,      GENES, dab_mask,      prefix=\"p_\", mode=\"auto\")\n",
    "write_probs_into_joint_masked_iloc(joint, cite_rna_probs, GENES, cite_rna_mask, prefix=\"p_\", mode=\"auto\")\n",
    "write_probs_into_joint_masked_iloc(joint, cite_adt_probs, GENES, cite_adt_mask, prefix=\"p_\", mode=\"auto\")\n",
    "\n",
    "for g in GENES:\n",
    "    col = f\"p_{g}\"\n",
    "    print(col, \"non-null:\", int(joint.obs[col].notna().sum()), \"/\", joint.n_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163930b-f915-41fe-a785-b3e4a5cf311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in GENES:\n",
    "    sc.pl.umap(joint, color=f\"p_{g}\", na_color=\"lightgrey\", size=10, alpha=0.6, title=f\"{g} prob\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a6ed1-a6bf-468b-96f8-2dda078a4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_probs(joint, genes, group_col=\"dataset_modality\", prefix=\"p_\"):\n",
    "    rows = []\n",
    "    for g in genes:\n",
    "        p = joint.obs[f\"{prefix}{g}\"].astype(float).to_numpy()\n",
    "        rows.append({\n",
    "            \"gene\": g,\n",
    "            \"n\": np.isfinite(p).sum(),\n",
    "            \"min\": np.nanmin(p),\n",
    "            \"p01\": np.nanquantile(p, 0.01),\n",
    "            \"p50\": np.nanquantile(p, 0.50),\n",
    "            \"p99\": np.nanquantile(p, 0.99),\n",
    "            \"max\": np.nanmax(p),\n",
    "            \"mean\": np.nanmean(p),\n",
    "            \"std\": np.nanstd(p),\n",
    "        })\n",
    "    df = pd.DataFrame(rows).set_index(\"gene\")\n",
    "    print(df.sort_values(\"std\", ascending=False).to_string(float_format=lambda x: f\"{x:.3f}\"))\n",
    "\n",
    "    # By dataset_modality (or whatever you want)\n",
    "    for g in genes:\n",
    "        col = f\"{prefix}{g}\"\n",
    "        print(f\"\\n=== {g} by {group_col} ===\")\n",
    "        tmp = joint.obs[[group_col, col]].copy()\n",
    "        tmp[col] = tmp[col].astype(float)\n",
    "        out = tmp.groupby(group_col)[col].agg([\"count\",\"mean\",\"std\",\"min\",\"max\"])\n",
    "        print(out.to_string(float_format=lambda x: f\"{x:.3f}\"))\n",
    "\n",
    "summarize_probs(joint, GENES, group_col=\"dataset_modality\", prefix=\"p_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8b664-d7bc-4ebb-ae57-072cb6f4b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "for g in [\"NPM1\",\"FLT3\",\"DNMT3A\",\"TP53\"]:\n",
    "    sc.pl.umap(\n",
    "        joint,\n",
    "        color=[f\"p_{g}\", \"dataset_modality\"],\n",
    "        na_color=\"lightgrey\",\n",
    "        size=10,\n",
    "        alpha=0.5,\n",
    "        wspace=0.35,\n",
    "        title=[f\"{g} prob\", \"dataset_modality\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4484f1b-a1db-4284-b65b-f52de2d004b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "vg_only  = joint[joint.obs[\"dataset_modality\"].astype(str) == \"VG rna\"].copy()\n",
    "dab_only = joint[joint.obs[\"dataset_modality\"].astype(str) == \"DAb adt\"].copy()\n",
    "\n",
    "for g in [\"NPM1\",\"FLT3\",\"DNMT3A\",\"TP53\",\"IDH2\"]:\n",
    "    sc.pl.umap(vg_only,  color=f\"p_{g}\", na_color=\"lightgrey\", size=15, alpha=0.7, title=f\"VG only: {g}\")\n",
    "    sc.pl.umap(dab_only, color=f\"p_{g}\", na_color=\"lightgrey\", size=15, alpha=0.7, title=f\"DAb only: {g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021bc71-dd5a-4e94-8185-7ecd22c79078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def var_explained_by_group(joint, gene, group_col, prefix=\"p_\"):\n",
    "    p = joint.obs[f\"{prefix}{gene}\"].astype(float)\n",
    "    g = joint.obs[group_col].astype(str)\n",
    "    ok = np.isfinite(p.to_numpy())\n",
    "    p = p[ok]; g = g[ok]\n",
    "\n",
    "    mu = p.mean()\n",
    "    # between-group variance\n",
    "    grp = p.groupby(g)\n",
    "    n = grp.size()\n",
    "    m = grp.mean()\n",
    "    ss_between = float(((n * (m - mu)**2)).sum())\n",
    "    ss_total = float(((p - mu)**2).sum()) + 1e-12\n",
    "    return ss_between / ss_total\n",
    "\n",
    "for g in [\"TP53\",\"FLT3\",\"DNMT3A\",\"NPM1\",\"IDH2\",\"TET2\"]:\n",
    "    r2_pat = var_explained_by_group(joint, g, \"patient\")\n",
    "    r2_mod = var_explained_by_group(joint, g, \"dataset_modality\")\n",
    "    print(f\"{g:7s}  R2(patient)={r2_pat:.3f}  R2(dataset_modality)={r2_mod:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498b88a-654c-4754-890b-4ad6cad63107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a185ec-b0ec-480f-8797-2a99ff4d4db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb610fb3-d6e1-40e1-9f07-6f2752b534ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157f74c-8496-4fb7-8887-ba42a49b6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vg_perf)\n",
    "print(dab_perf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7228468a-6bab-4d73-bee6-7c887236c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Plot (same idea, but showing n_pos/n_neg too)\n",
    "# ==============================================\n",
    "\n",
    "dfp = []\n",
    "for g in GENES:\n",
    "    d = dab_perf.get(g, {})\n",
    "    v = vg_perf.get(g, {})\n",
    "\n",
    "    dfp.append({\n",
    "        \"gene\": g, \"dataset\": \"DAb\",\n",
    "        \"AUC\": d.get(\"auc\", np.nan),\n",
    "        \"AP\":  d.get(\"ap\",  np.nan),\n",
    "        \"n\":   d.get(\"n_labeled\",   np.nan),\n",
    "        \"n_pos\": d.get(\"n_pos\", d.get(\"pos\", d.get(\"tp\", np.nan))),\n",
    "        \"n_neg\": d.get(\"n_neg\", d.get(\"neg\", d.get(\"tn\", np.nan))),\n",
    "        \"prevalence\": d.get(\"prevalence\",   np.nan),\n",
    "    })\n",
    "\n",
    "    dfp.append({\n",
    "        \"gene\": g, \"dataset\": \"VG\",\n",
    "        \"AUC\": v.get(\"auc\", np.nan),\n",
    "        \"AP\":  v.get(\"ap\",  np.nan),\n",
    "        \"n\":   v.get(\"n_labeled\",   np.nan),\n",
    "        \"n_pos\": v.get(\"n_pos\", v.get(\"pos\", v.get(\"tp\", np.nan))),\n",
    "        \"n_neg\": v.get(\"n_neg\", v.get(\"neg\", v.get(\"tn\", np.nan))),\n",
    "        \"prevalence\": v.get(\"prevalence\",   np.nan),\n",
    "    })\n",
    "\n",
    "dfp = pd.DataFrame(dfp)\n",
    "\n",
    "print(dfp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7752642-a5e2-4b00-9e5b-5ecb8ae888ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = dfp[dfp[\"AUC\"].notna()].copy()\n",
    "\n",
    "plt.figure(figsize=(6.8, 4.0))\n",
    "for ds in [\"DAb\", \"VG\"]:\n",
    "    sub = plot_df[plot_df[\"dataset\"] == ds]\n",
    "    plt.plot(sub[\"AUC\"].values, sub[\"gene\"].values, marker=\"o\", linestyle=\"none\", label=ds)\n",
    "\n",
    "plt.xlabel(\"AUC\"); plt.ylabel(\"Gene\")\n",
    "plt.title(\"Mutation-head performance (AUC) — evaluable only\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a08cea-0836-4d58-9967-bad4acd24039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def to_device_batch(batch, device, non_blocking=True):\n",
    "    \"\"\"\n",
    "    Recursively move a batch to `device`.\n",
    "    Supports: dict, list/tuple, torch.Tensor, numpy arrays.\n",
    "    Leaves strings/None/ints/floats unchanged.\n",
    "    \"\"\"\n",
    "    if batch is None:\n",
    "        return None\n",
    "\n",
    "    # torch tensor\n",
    "    if torch.is_tensor(batch):\n",
    "        return batch.to(device, non_blocking=non_blocking)\n",
    "\n",
    "    # numpy -> torch\n",
    "    if isinstance(batch, np.ndarray):\n",
    "        return torch.from_numpy(batch).to(device, non_blocking=non_blocking)\n",
    "\n",
    "    # dict\n",
    "    if isinstance(batch, dict):\n",
    "        return {k: to_device_batch(v, device, non_blocking=non_blocking) for k, v in batch.items()}\n",
    "\n",
    "    # list / tuple\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        out = [to_device_batch(v, device, non_blocking=non_blocking) for v in batch]\n",
    "        return type(batch)(out)\n",
    "\n",
    "    # everything else (str, int, float, pd objects, etc.)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1dcb7-e458-4d80-866a-7e95f69384c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_latent_z(model, x_dict, *, prefer=\"mu\"):\n",
    "    \"\"\"\n",
    "    Return latent z for a batch.\n",
    "    - If model.encode exists: uses it.\n",
    "    - Else tries model(x_dict) and extracts common latent keys.\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"encode\") and callable(getattr(model, \"encode\")):\n",
    "        z = model.encode(x_dict)\n",
    "        return z\n",
    "\n",
    "    out = model(x_dict)\n",
    "\n",
    "    # If your forward returns a dict\n",
    "    if isinstance(out, dict):\n",
    "        for key in [\"mu\", \"z\", \"latent\", \"z_mu\", \"X_univi\", \"repr\"]:\n",
    "            if key in out:\n",
    "                return out[key]\n",
    "        raise KeyError(f\"Model output dict keys: {list(out.keys())} (couldn't find latent)\")\n",
    "\n",
    "    # Otherwise assume forward returns z directly\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3efa9-e55b-4204-991c-6df1a3ec25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_and_predict_probs(\n",
    "    model, head, adata, *,\n",
    "    modality, genes,\n",
    "    device=None,\n",
    "    batch_size=4096,\n",
    "    latent_key=\"X_univi\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls your free function: encode_latent(model, adata, modality, ...)\n",
    "    then applies head to latent -> sigmoid probs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    head.eval()\n",
    "\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    # 1) Encode latent (NOTE: pass model!)\n",
    "    Z = encode_latent(\n",
    "        model,                       # <-- this was missing\n",
    "        adata=adata,\n",
    "        modality=modality,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # If encode_latent writes into obsm and returns None\n",
    "    if Z is None:\n",
    "        if latent_key not in adata.obsm:\n",
    "            raise KeyError(f\"encode_latent returned None and {latent_key} not found in adata.obsm\")\n",
    "        Z = adata.obsm[latent_key]\n",
    "\n",
    "    # 2) Ensure torch tensor on correct device\n",
    "    if isinstance(Z, np.ndarray):\n",
    "        Zt = torch.from_numpy(Z)\n",
    "    elif torch.is_tensor(Z):\n",
    "        Zt = Z\n",
    "    else:\n",
    "        Zt = torch.tensor(Z)\n",
    "\n",
    "    Zt = Zt.to(device)\n",
    "\n",
    "    # 3) Predict probs\n",
    "    logits = head(Zt)               # (n_cells, n_genes)\n",
    "    probs  = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "\n",
    "    return {g: probs[:, i] for i, g in enumerate(genes)}, probs\n",
    "\n",
    "\n",
    "# modality strings (yours)\n",
    "VG_MODAL  = \"rna\"\n",
    "DAB_MODAL = \"adt\"\n",
    "\n",
    "vg_prob_by_gene, P_vg = encode_and_predict_probs(model_ft, head, vg_rna_pp,  modality=VG_MODAL,  genes=GENES)\n",
    "dab_prob_by_gene, P_dab = encode_and_predict_probs(model_ft, head, dab_adt_pp, modality=DAB_MODAL, genes=GENES)\n",
    "\n",
    "vg_rna_pp.obs[f\"headP_{HERO}_vg\"]    = vg_prob_by_gene[HERO]\n",
    "dab_adt_pp.obs[f\"headP_{HERO}_dab\"]  = dab_prob_by_gene[HERO]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1adff-dce8-4161-90ad-886ec8424c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "FIGDIR = Path(FIGDIR); FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "col_vg  = f\"headP_{HERO}_vg\"\n",
    "col_dab = f\"headP_{HERO}_dab\"\n",
    "\n",
    "sc.pl.umap(vg_rna_pp, color=col_vg,  title=f\"Head P({HERO}) on VG (RNA)\",  size=15, alpha=0.7)\n",
    "plt.savefig(FIGDIR / f\"umap_headP_{HERO}_VG.png\", dpi=450, bbox_inches=\"tight\"); plt.show()\n",
    "\n",
    "sc.pl.umap(dab_adt_pp, color=col_dab, title=f\"Head P({HERO}) on DAb (ADT)\",  size=15, alpha=0.7)\n",
    "plt.savefig(FIGDIR / f\"umap_headP_{HERO}_DAb.png\", dpi=450, bbox_inches=\"tight\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c1eec-7dc0-42a1-8535-8a622345ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "FIGDIR = Path(FIGDIR); FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Columns holding probs on source objects\n",
    "col_vg  = f\"headP_{HERO}_vg\"\n",
    "col_dab = f\"headP_{HERO}_dab\"\n",
    "\n",
    "# Column we will create on joint for plotting\n",
    "col_joint = f\"headP_{HERO}_joint\"\n",
    "\n",
    "# Initialize with NaNs\n",
    "joint.obs[col_joint] = np.nan\n",
    "\n",
    "# Masks for where to place values\n",
    "mask_vg  = (joint.obs[\"dataset\"] == \"VG\")  & (joint.obs[\"modality\"] == \"rna\")\n",
    "mask_dab = (joint.obs[\"dataset\"] == \"DAb\") & (joint.obs[\"modality\"] == \"adt\")\n",
    "\n",
    "# Assign (assumes the joint rows for VG/DAb are in the same order as vg_rna_pp/dab_adt_pp used to build joint)\n",
    "joint.obs.loc[mask_vg,  col_joint] = vg_rna_pp.obs[col_vg].to_numpy()\n",
    "joint.obs.loc[mask_dab, col_joint] = dab_adt_pp.obs[col_dab].to_numpy()\n",
    "\n",
    "# Plot (continuous)\n",
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=col_joint,\n",
    "    title=f\"Head P({HERO}) on joint UMAP (VG RNA + DAb ADT)\",\n",
    "    na_color=\"lightgrey\",\n",
    "    size=15, \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.savefig(FIGDIR / f\"umap_headP_{HERO}_joint.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: side-by-side with dataset label for sanity\n",
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=[\"dataset\", col_joint],\n",
    "    title=[\"dataset\", f\"Head P({HERO})\"],\n",
    "    na_color=\"lightgrey\",\n",
    "    wspace=0.35,\n",
    "    size=15, \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.savefig(FIGDIR / f\"umap_dataset_and_headP_{HERO}_joint.png\", dpi=450, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209de63d-b7ad-4a52-857f-351019b69004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4eee1-639f-427b-87fd-4c93de0012d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "score_col = \"LSC17_score\"\n",
    "out_col   = \"LSC17_score\"\n",
    "\n",
    "# ensure the column exists\n",
    "if out_col not in joint.obs.columns:\n",
    "    joint.obs[out_col] = np.nan\n",
    "\n",
    "def add_scores_fill_missing(src, joint, col, out_col):\n",
    "    \"\"\"\n",
    "    Fill joint.obs[out_col] from src.obs[col] by matching on obs_names.\n",
    "\n",
    "    Works even if joint.obs_names has duplicates.\n",
    "    Only fills where joint[out_col] is currently missing (NaN).\n",
    "    \"\"\"\n",
    "    # src obs_names are unique in your report; build mapper obs_name -> score\n",
    "    mapper = pd.Series(src.obs[col].values, index=pd.Index(src.obs_names))\n",
    "\n",
    "    # lookup for every joint row (duplicates are fine; they'll get same value)\n",
    "    looked_up = pd.Index(joint.obs_names).map(mapper)\n",
    "\n",
    "    # fill only where joint currently missing and looked_up is not missing\n",
    "    cur = joint.obs[out_col].to_numpy(copy=True)\n",
    "\n",
    "    # coerce looked_up to float if possible (LSC17_score should be numeric)\n",
    "    new = pd.to_numeric(pd.Series(looked_up), errors=\"coerce\").to_numpy()\n",
    "\n",
    "    mask = np.isnan(cur) & ~np.isnan(new)\n",
    "    cur[mask] = new[mask]\n",
    "    joint.obs[out_col] = cur\n",
    "\n",
    "    return joint.obs[out_col]\n",
    "\n",
    "# choose your priority order:\n",
    "add_scores_fill_missing(cite_rna_pp_tr, joint, score_col, out_col)\n",
    "add_scores_fill_missing(cite_rna_pp_va, joint, score_col, out_col)\n",
    "add_scores_fill_missing(cite_rna_pp_te, joint, score_col, out_col)\n",
    "add_scores_fill_missing(vg_rna_pp,      joint, score_col, out_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724328f2-0652-4c07-b4ef-168e5a75850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    joint,\n",
    "    color=[\"LSC17_score\"],\n",
    "    title=[\"Joint latent fine-tuned (LSC-17 score)\"],\n",
    "    na_color=\"lightgrey\",\n",
    "    wspace=0.35,\n",
    "    size=15,\n",
    "    vmin=0,\n",
    "    vmax=1.0,\n",
    "    alpha=0.85,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6e485-a34e-4b28-8468-d3f9f1845e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "\n",
    "# --- assumes you already have these helpers ---\n",
    "# find_mut_col_dab_obs(...)\n",
    "# coerce_to_nullable_boolean(...)\n",
    "# make_label_from_nullable_bool(...)\n",
    "\n",
    "HERO = str(HERO)\n",
    "\n",
    "mut_col_dab = find_mut_col_dab_obs(dab_adt_al, HERO)\n",
    "if mut_col_dab is None:\n",
    "    raise KeyError(f\"Can't find DAb mutation column for {HERO}. Example: {list(dab_adt_al.obs.columns[:30])}\")\n",
    "\n",
    "col_truth_dab = f\"truth_{HERO}_dab_bool\"\n",
    "dab_adt_al.obs[col_truth_dab] = coerce_to_nullable_boolean(dab_adt_al.obs[mut_col_dab])\n",
    "\n",
    "print(\"DAb truth source:\", mut_col_dab)\n",
    "print(dab_adt_al.obs[col_truth_dab].value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bbf05-b8fc-4456-a333-07b8e0ff7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vg_truth_from_transcript_strings(vg_obs, hero, mut_col=\"MutTranscripts\", wt_col=\"WtTranscripts\"):\n",
    "    hero_u = str(hero).upper()\n",
    "\n",
    "    mut_s = vg_obs[mut_col].fillna(\"\").astype(str).str.upper()\n",
    "    wt_s  = vg_obs[wt_col].fillna(\"\").astype(str).str.upper()\n",
    "\n",
    "    # naive “contains HERO” (works for \"NPM1\", \"FLT3\", etc.)\n",
    "    has_mut = mut_s.str.contains(hero_u, regex=False)\n",
    "    has_wt  = wt_s.str.contains(hero_u,  regex=False)\n",
    "\n",
    "    out = pd.Series(pd.NA, index=vg_obs.index, dtype=\"boolean\")\n",
    "    out[has_mut] = True\n",
    "    out[(~has_mut) & has_wt] = False\n",
    "    return out\n",
    "\n",
    "col_truth_vg = f\"truth_{HERO}_vg_bool\"\n",
    "vg_rna_pp.obs[col_truth_vg] = vg_truth_from_transcript_strings(vg_rna_pp.obs, HERO)\n",
    "\n",
    "print(vg_rna_pp.obs[col_truth_vg].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e467ba29-fb0d-4092-86e7-c9ad4ef2bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_pred_dab = f\"headP_{HERO}_dab\"  # you already created this on dab_adt_pp\n",
    "\n",
    "# Align truth and preds by cell ID (index)\n",
    "truth = dab_adt_al.obs[col_truth_dab].astype(\"boolean\")\n",
    "pred  = dab_adt_pp.obs[col_pred_dab].astype(float)\n",
    "\n",
    "# Only evaluate where truth is known and pred is not nan\n",
    "mask = (~truth.isna()) & (~pred.isna())\n",
    "y = truth[mask].astype(int).to_numpy()\n",
    "p = pred[mask].to_numpy()\n",
    "\n",
    "auc = roc_auc_score(y, p) if len(np.unique(y)) > 1 else np.nan\n",
    "ap  = average_precision_score(y, p) if len(np.unique(y)) > 1 else np.nan\n",
    "print(f\"DAb: AUC={auc:.4f}  AP={ap:.4f}   n={mask.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4a927-0877-433d-8a5a-c10a2510473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_pred_vg = f\"headP_{HERO}_vg\"\n",
    "\n",
    "truth = vg_rna_pp.obs[col_truth_vg].astype(\"boolean\")\n",
    "pred  = vg_rna_pp.obs[col_pred_vg].astype(float)\n",
    "\n",
    "mask = (~truth.isna()) & (~pred.isna())\n",
    "y = truth[mask].astype(int).to_numpy()\n",
    "p = pred[mask].to_numpy()\n",
    "\n",
    "auc = roc_auc_score(y, p) if len(np.unique(y)) > 1 else np.nan\n",
    "ap  = average_precision_score(y, p) if len(np.unique(y)) > 1 else np.nan\n",
    "print(f\"VG:  AUC={auc:.4f}  AP={ap:.4f}   n={mask.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3928c-37a2-414d-b430-ff88571e20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395546a-b8c4-4db7-8941-17b17b117127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384aa2a-f34f-4087-9884-fc5c272c9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e66b4b-1fc7-4100-84b3-98be1ab19810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca902d-3b8d-44fe-b590-9a47f0a20937",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a195f17-e370-46df-8508-75bdd74c0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1d7ef-0b0e-4b75-8793-b104423125f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4038a4a-cccb-4e40-9c96-7b8f1b090041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36062579-3d99-4c30-9266-f82edda45193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca4457-1d25-48a3-83fc-efb15d8f959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d313c-a20d-457c-86f8-c3bb5001c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d95e9-23b7-4906-a5da-47d53b636a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498707f-5795-4771-b4fd-ac0f55a8344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cc041-843f-4886-bf27-e1b63f6b2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e04f91-4da9-47fb-93f6-1e171455ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b5155-e213-43f7-b293-e8279a73a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569c48b-d2f3-4061-965a-963abcda8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9957c-0ec3-4ba9-956c-3b4f5465d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8263043c-c7ef-45fd-b0f5-0bcd42f1a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53e02f-c763-490e-ae10-a4e0f5a745ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375e8a6-8993-486d-a849-11ff740ee508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57785bc-8218-4535-977e-349eccaa1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5bd9a-076c-4f36-8c2f-c5869cfc21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a591e9d-5d78-4168-851e-fa043a8e429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c4fec-3d52-4711-9593-c1738b6a4305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16c7e1-80bb-483f-ae65-e2e4bfbc2a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275fc9c-65d2-4d79-a189-46c691331d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a7416-bc87-4083-a60d-f529d80e4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe2316-838a-4e25-a6af-f7abfb5d059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecd9d7-4e15-4b50-ad4f-c19cf7372260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b4e3b-9f4b-4aa4-93f3-ef30e1ec3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55159aed-793b-4cfc-8950-da3a945a23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d06730-9b06-4008-9724-1c5eb9da4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9540755-8246-4607-a2d9-6c4ed7abe9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56c8d9-494f-4955-9f92-b048305a1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12a0b1-0b64-4a1a-9251-1d15ca4e7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05da0a-a047-435f-863a-5331af1d3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177d561-b973-48c5-9bf1-a8d41d790bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0d64e-31b6-49bf-955b-e15cbc91c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd114d5-d7d5-4e22-8871-4695de540d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (UniVI v0.2.3)",
   "language": "python",
   "name": "univi_v0.2.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
