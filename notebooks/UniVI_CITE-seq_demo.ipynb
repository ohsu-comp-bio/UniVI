{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8372420d",
   "metadata": {},
   "source": [
    "# UniVI CITE-seq data integration demonstration/tutorial\n",
    "\n",
    "Andrew Ashford, Pathways + Omics Group, Oregon Health & Science University - 11/17/2025\n",
    "\n",
    "This Jupyter Notebook will be used to outline the training steps for a UniVI model using human PBMC CITE-seq data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2eb788",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3fa61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56f7197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/groups/precepts/ashforda/UniVI_v2/UniVI\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 0. Wire up package import\n",
    "# -------------------------\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from univi import (\n",
    "    UniVIMultiModalVAE,\n",
    "    ModalityConfig,\n",
    "    UniVIConfig,\n",
    "    TrainingConfig,\n",
    "    matching,\n",
    ")\n",
    "from univi.data import MultiModalDataset\n",
    "from univi.trainer import UniVITrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850cdbf",
   "metadata": {},
   "source": [
    "#### Initialize data via data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbae2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1. Load config JSON\n",
    "# -------------------------\n",
    "with open(\"../parameter_files/defaults_cite_seq_raw_counts.json\") as f:\n",
    "    cfg_json = json.load(f)\n",
    "\n",
    "data_cfg = cfg_json[\"data\"]\n",
    "model_cfg = cfg_json[\"model\"]\n",
    "train_cfg_json = cfg_json[\"training\"]\n",
    "\n",
    "device = train_cfg_json.get(\"device\", \"cuda\")\n",
    "\n",
    "if device == \"cuda\" and not torch.cuda.is_available():\n",
    "    print(\"CUDA not available; falling back to CPU.\")\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2596d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modalities': [{'name': 'rna', 'h5ad_path': '../data/Hao_RNA_data.h5ad', 'layer': None, 'X_key': 'X', 'hidden_dims': [512, 256], 'likelihood': 'nb'}, {'name': 'adt', 'h5ad_path': '../data/Hao_ADT_data.h5ad', 'layer': None, 'X_key': 'X', 'hidden_dims': [128, 64], 'likelihood': 'nb'}]}\n",
      "{'latent_dim': 40, 'hidden_dims_default': [256, 128], 'dropout': 0.1, 'batchnorm': True, 'beta': 20.0, 'gamma': 80.0, 'kl_anneal_start': 10, 'kl_anneal_end': 50, 'align_anneal_start': 10, 'align_anneal_end': 50}\n",
      "{'seed': 42, 'train_fraction': 0.8, 'batch_size': 256, 'num_workers': 0, 'n_epochs': 200, 'lr': 0.001, 'weight_decay': 0.0001, 'grad_clip': 5.0, 'device': 'cpu', 'log_every': 1, 'early_stopping': True, 'patience': 20, 'min_delta': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(data_cfg)\n",
    "print(model_cfg)\n",
    "print(train_cfg_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e308a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/anndata/compat/__init__.py:232: FutureWarning: Moving element from .uns['neighbors']['distances'] to .obsp['distances'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2. Load AnnData objects\n",
    "# -------------------------\n",
    "# Load RNA AnnData object\n",
    "rna_adata = sc.read_h5ad(\"../data/Hao_RNA_data.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a063106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 161764 × 20729\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features'\n",
      "    uns: 'neighbors'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'PCs', 'SPCA'\n",
      "    obsp: 'distances'\n",
      "  (0, 1)\t0.6931471805599453\n",
      "  (0, 19)\t0.6931471805599453\n",
      "  (0, 23)\t0.6931471805599453\n",
      "  (0, 28)\t0.6931471805599453\n",
      "  (0, 30)\t1.3862943611198906\n",
      "  (0, 48)\t0.6931471805599453\n",
      "  (0, 49)\t1.3862943611198906\n",
      "  (0, 54)\t0.6931471805599453\n",
      "  (0, 57)\t1.0986122886681096\n",
      "  (0, 63)\t0.6931471805599453\n",
      "  (0, 82)\t2.1972245773362196\n",
      "  (0, 98)\t0.6931471805599453\n",
      "  (0, 99)\t0.6931471805599453\n",
      "  (0, 111)\t1.791759469228055\n",
      "  (0, 127)\t0.6931471805599453\n",
      "  (0, 138)\t0.6931471805599453\n",
      "  (0, 149)\t0.6931471805599453\n",
      "  (0, 151)\t1.6094379124341003\n",
      "  (0, 157)\t0.6931471805599453\n",
      "  (0, 159)\t0.6931471805599453\n",
      "  (0, 160)\t0.6931471805599453\n",
      "  (0, 161)\t0.6931471805599453\n",
      "  (0, 162)\t0.6931471805599453\n",
      "  (0, 170)\t1.0986122886681096\n",
      "  (0, 182)\t0.6931471805599453\n",
      "  :\t:\n",
      "  (161763, 16110)\t1.6094379124341003\n",
      "  (161763, 16113)\t1.6094379124341003\n",
      "  (161763, 16130)\t0.6931471805599453\n",
      "  (161763, 16158)\t1.3862943611198906\n",
      "  (161763, 16159)\t1.6094379124341003\n",
      "  (161763, 16160)\t2.1972245773362196\n",
      "  (161763, 16163)\t0.6931471805599453\n",
      "  (161763, 16184)\t3.5263605246161616\n",
      "  (161763, 16185)\t3.5553480614894135\n",
      "  (161763, 16186)\t4.605170185988092\n",
      "  (161763, 16187)\t4.290459441148391\n",
      "  (161763, 16188)\t0.6931471805599453\n",
      "  (161763, 16189)\t4.174387269895637\n",
      "  (161763, 16190)\t4.330733340286331\n",
      "  (161763, 16191)\t3.9889840465642745\n",
      "  (161763, 16193)\t3.4657359027997265\n",
      "  (161763, 16194)\t2.302585092994046\n",
      "  (161763, 16196)\t3.9512437185814275\n",
      "  (161763, 16350)\t0.6931471805599453\n",
      "  (161763, 16585)\t0.6931471805599453\n",
      "  (161763, 16688)\t0.6931471805599453\n",
      "  (161763, 17511)\t0.6931471805599453\n",
      "  (161763, 17866)\t1.3862943611198906\n",
      "  (161763, 18249)\t0.6931471805599453\n",
      "  (161763, 18702)\t0.6931471805599453\n",
      "0.0\n",
      "9.049467146388098\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(rna_adata)\n",
    "print(rna_adata.X)\n",
    "print(rna_adata.X.min())\n",
    "print(rna_adata.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ded2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the RNA counts to raw counts so they're not log-normalized and use ZINB or NB as the decoder distribution\n",
    "# for model training\n",
    "rna_adata.layers['log1p'] = rna_adata.X\n",
    "rna_adata.X = rna_adata.raw.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694fc073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (0, 19)\t1.0\n",
      "  (0, 23)\t1.0\n",
      "  (0, 28)\t1.0\n",
      "  (0, 30)\t3.0\n",
      "  (0, 48)\t1.0\n",
      "  (0, 49)\t3.0\n",
      "  (0, 54)\t1.0\n",
      "  (0, 57)\t2.0\n",
      "  (0, 63)\t1.0\n",
      "  (0, 82)\t8.0\n",
      "  (0, 98)\t1.0\n",
      "  (0, 99)\t1.0\n",
      "  (0, 111)\t5.0\n",
      "  (0, 127)\t1.0\n",
      "  (0, 138)\t1.0\n",
      "  (0, 149)\t1.0\n",
      "  (0, 151)\t4.0\n",
      "  (0, 157)\t1.0\n",
      "  (0, 159)\t1.0\n",
      "  (0, 160)\t1.0\n",
      "  (0, 161)\t1.0\n",
      "  (0, 162)\t1.0\n",
      "  (0, 170)\t2.0\n",
      "  (0, 182)\t1.0\n",
      "  :\t:\n",
      "  (161763, 16110)\t4.0\n",
      "  (161763, 16113)\t4.0\n",
      "  (161763, 16130)\t1.0\n",
      "  (161763, 16158)\t3.0\n",
      "  (161763, 16159)\t4.0\n",
      "  (161763, 16160)\t8.0\n",
      "  (161763, 16163)\t1.0\n",
      "  (161763, 16184)\t33.0\n",
      "  (161763, 16185)\t34.0\n",
      "  (161763, 16186)\t99.0\n",
      "  (161763, 16187)\t72.0\n",
      "  (161763, 16188)\t1.0\n",
      "  (161763, 16189)\t64.0\n",
      "  (161763, 16190)\t75.0\n",
      "  (161763, 16191)\t53.0\n",
      "  (161763, 16193)\t31.0\n",
      "  (161763, 16194)\t9.0\n",
      "  (161763, 16196)\t51.0\n",
      "  (161763, 16350)\t1.0\n",
      "  (161763, 16585)\t1.0\n",
      "  (161763, 16688)\t1.0\n",
      "  (161763, 17511)\t1.0\n",
      "  (161763, 17866)\t3.0\n",
      "  (161763, 18249)\t1.0\n",
      "  (161763, 18702)\t1.0\n",
      "0.0\n",
      "8513.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(rna_adata.X)\n",
    "print(rna_adata.X.min())\n",
    "print(rna_adata.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10182ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py:216: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  disp_grouped = df.groupby('mean_bin')['dispersions']\n"
     ]
    }
   ],
   "source": [
    "# If your counts are in rna_adata.X (raw or log-normalized), this is fine:\n",
    "sc.pp.highly_variable_genes(\n",
    "    rna_adata,\n",
    "    layer='log1p',\n",
    "    n_top_genes=2000,\n",
    "    flavor=\"seurat\",   # or \"cell_ranger\" / \"seurat_v3\"\n",
    "    inplace=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e2b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2000 highly variable genes.\n",
      "['HES4', 'ISG15', 'TNFRSF18', 'TNFRSF4', 'RBP7', 'EPHA2', 'PADI4', 'CDA', 'EIF4G3', 'AL031005.1', 'C1QA', 'C1QC', 'C1QB', 'TCEA3', 'ID3', 'RCAN3', 'LDLRAP1', 'STMN1', 'ZNF683', 'IFI6']\n"
     ]
    }
   ],
   "source": [
    "# Boolean mask of HVGs\n",
    "hvg_mask = rna_adata.var[\"highly_variable\"].values\n",
    "\n",
    "# Names of the top HVGs\n",
    "hvg_genes = rna_adata.var_names[hvg_mask].tolist()\n",
    "print(f\"Selected {len(hvg_genes)} highly variable genes.\")\n",
    "print(hvg_genes[:20])  # peek at first few\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6663cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 161764 × 2000\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
      "    uns: 'neighbors', 'hvg'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'PCs', 'SPCA'\n",
      "    layers: 'log1p'\n",
      "    obsp: 'distances'\n"
     ]
    }
   ],
   "source": [
    "# Optional: make a HVG-only AnnData for modeling\n",
    "rna_adata_hvg = rna_adata[:, hvg_mask].copy()\n",
    "print(rna_adata_hvg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02dd9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ADT AnnData object\n",
    "adt_adata = sc.read_h5ad(\"../data/Hao_ADT_data.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf7bbdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 161764 × 228\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'APCA'\n",
      "[[1.95916424 0.86914159 1.48523314 ... 0.55307644 1.04608444 1.72565693]\n",
      " [0.4322284  1.01422751 0.79594998 ... 0.66587988 0.85514588 1.37971736]\n",
      " [0.61381759 1.30390619 0.75610373 ... 0.6874892  0.75610373 1.04246048]\n",
      " ...\n",
      " [1.50685426 0.54914608 1.0656108  ... 0.25674036 0.25674036 1.16168749]\n",
      " [1.63537843 0.32520632 1.19570797 ... 0.45519093 0.51435145 2.10485699]\n",
      " [1.58503    0.64458811 1.61118685 ... 0.416905   0.23002318 2.07731277]]\n",
      "0.0\n",
      "8.589231339100166\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(adt_adata)\n",
    "print(adt_adata.X)\n",
    "print(adt_adata.X.min())\n",
    "print(adt_adata.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ae26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ADT data to raw counts and use a NB or ZINB decoder in model training and save the current .X counts to\n",
    "# .layers['log1p']\n",
    "adt_adata.layers['log1p'] = adt_adata.X\n",
    "adt_adata.X = adt_adata.raw.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88604153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 161764 × 228\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'APCA'\n",
      "    layers: 'log1p'\n",
      "  (0, 0)\t66.0\n",
      "  (0, 1)\t15.0\n",
      "  (0, 2)\t37.0\n",
      "  (0, 3)\t142.0\n",
      "  (0, 4)\t4.0\n",
      "  (0, 5)\t3.0\n",
      "  (0, 6)\t242.0\n",
      "  (0, 7)\t9.0\n",
      "  (0, 8)\t752.0\n",
      "  (0, 9)\t14.0\n",
      "  (0, 10)\t5.0\n",
      "  (0, 11)\t3.0\n",
      "  (0, 13)\t20.0\n",
      "  (0, 14)\t8.0\n",
      "  (0, 15)\t4.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 17)\t5.0\n",
      "  (0, 18)\t4.0\n",
      "  (0, 19)\t2.0\n",
      "  (0, 20)\t5.0\n",
      "  (0, 21)\t28.0\n",
      "  (0, 22)\t1.0\n",
      "  (0, 23)\t12.0\n",
      "  (0, 24)\t479.0\n",
      "  (0, 25)\t6.0\n",
      "  :\t:\n",
      "  (161763, 203)\t3.0\n",
      "  (161763, 204)\t2.0\n",
      "  (161763, 205)\t3.0\n",
      "  (161763, 206)\t34.0\n",
      "  (161763, 207)\t149.0\n",
      "  (161763, 208)\t38.0\n",
      "  (161763, 209)\t23.0\n",
      "  (161763, 210)\t79.0\n",
      "  (161763, 211)\t3.0\n",
      "  (161763, 212)\t109.0\n",
      "  (161763, 213)\t13.0\n",
      "  (161763, 214)\t5.0\n",
      "  (161763, 215)\t1.0\n",
      "  (161763, 216)\t11.0\n",
      "  (161763, 217)\t9.0\n",
      "  (161763, 218)\t7.0\n",
      "  (161763, 219)\t21.0\n",
      "  (161763, 220)\t2.0\n",
      "  (161763, 221)\t7.0\n",
      "  (161763, 222)\t5.0\n",
      "  (161763, 223)\t6.0\n",
      "  (161763, 224)\t7.0\n",
      "  (161763, 225)\t4.0\n",
      "  (161763, 226)\t2.0\n",
      "  (161763, 227)\t54.0\n",
      "0.0\n",
      "42866.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(adt_adata)\n",
    "print(adt_adata.X)\n",
    "print(adt_adata.X.min())\n",
    "print(adt_adata.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d6aa53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "def get_X_values(adata, n_cells: int = 5000):\n",
    "    \"\"\"\n",
    "    Return flattened values from adata.X (optionally subsampled over cells).\n",
    "    Uses only non-zero entries if X is sparse.\n",
    "    \"\"\"\n",
    "    X = adata.X\n",
    "\n",
    "    # Optional subsampling over cells (rows)\n",
    "    if n_cells is not None and adata.n_obs > n_cells:\n",
    "        idx = np.random.choice(adata.n_obs, size=n_cells, replace=False)\n",
    "        X = X[idx]\n",
    "\n",
    "    if sparse.issparse(X):\n",
    "        vals = X.data  # nonzero values\n",
    "    else:\n",
    "        vals = np.asarray(X).ravel()\n",
    "\n",
    "    # Remove zeros explicitly (just to focus on count/ADT magnitude)\n",
    "    vals = vals[vals > 0]\n",
    "    return vals\n",
    "\n",
    "def plot_X_distribution(rna_adata, adt_adata, n_cells: int = 5000):\n",
    "    rna_vals = get_X_values(rna_adata, n_cells=n_cells)\n",
    "    adt_vals = get_X_values(adt_adata, n_cells=n_cells)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # --- RNA raw ---\n",
    "    axes[0, 0].hist(rna_vals, bins=100, alpha=0.8)\n",
    "    axes[0, 0].set_title(\"RNA .X nonzero values (raw)\")\n",
    "    axes[0, 0].set_xlabel(\"Value\")\n",
    "    axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # --- RNA log10 ---\n",
    "    axes[0, 1].hist(np.log10(rna_vals + 1e-8), bins=100, alpha=0.8)\n",
    "    axes[0, 1].set_title(\"RNA .X nonzero values (log10)\")\n",
    "    axes[0, 1].set_xlabel(\"log10(value)\")\n",
    "    axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # --- ADT raw ---\n",
    "    axes[1, 0].hist(adt_vals, bins=100, alpha=0.8)\n",
    "    axes[1, 0].set_title(\"ADT .X nonzero values (raw)\")\n",
    "    axes[1, 0].set_xlabel(\"Value\")\n",
    "    axes[1, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # --- ADT log10 ---\n",
    "    axes[1, 1].hist(np.log10(adt_vals + 1e-8), bins=100, alpha=0.8)\n",
    "    axes[1, 1].set_title(\"ADT .X nonzero values (log10)\")\n",
    "    axes[1, 1].set_xlabel(\"log10(value)\")\n",
    "    axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call it:\n",
    "plot_X_distribution(rna_adata, adt_adata, n_cells=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ded069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AnnData dictionary for both objects\n",
    "adata_by_mod = {\n",
    "    \"rna\": rna_adata_hvg,\n",
    "    \"adt\": adt_adata,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7337ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bipartite matching code\n",
    "'''\n",
    "import scanpy as sc\n",
    "\n",
    "# Assume you have two *unpaired* datasets of the same tissue:\n",
    "#   rna_adata_unpaired, adt_adata_unpaired\n",
    "\n",
    "# You can do separate PCAs (simple version):\n",
    "sc.pp.pca(rna_adata_unpaired, n_comps=50)\n",
    "sc.pp.pca(adt_adata_unpaired, n_comps=50)\n",
    "\n",
    "# This creates:\n",
    "#   rna_adata_unpaired.obsm[\"X_pca\"]\n",
    "#   adt_adata_unpaired.obsm[\"X_pca\"]\n",
    "'''\n",
    "\n",
    "'''\n",
    "from univi.matching import bipartite_match_adata\n",
    "\n",
    "matched_A, matched_B = bipartite_match_adata(\n",
    "    adata_A=rna_adata_unpaired,\n",
    "    adata_B=adt_adata_unpaired,\n",
    "    emb_key=\"X_pca\",      # or another embedding key\n",
    "    metric=\"euclidean\",   # can also try \"cosine\"\n",
    "    max_cells=20000,      # or smaller/larger depending on memory\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "print(len(matched_A), len(matched_B))   # same length = number of pseudo-pairs\n",
    "'''\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "# subset\n",
    "rna_matched = rna_adata_unpaired[matched_A].copy()\n",
    "adt_matched = adt_adata_unpaired[matched_B].copy()\n",
    "\n",
    "# give them a shared index for MultiModalDataset\n",
    "n_pairs = rna_matched.n_obs\n",
    "new_index = np.array([f\"pair_{i}\" for i in range(n_pairs)], dtype=str)\n",
    "\n",
    "rna_matched.obs_names = new_index\n",
    "adt_matched.obs_names = new_index\n",
    "'''\n",
    "\n",
    "'''\n",
    "adata_by_mod = {\n",
    "    \"rna\": rna_matched,\n",
    "    \"adt\": adt_matched,\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset by cell types for balanced model training and reduced computational expense and save to different adata\n",
    "# objects for later use of the remaining cells as a test set so as not to re-use the training cells after training\n",
    "'''\n",
    "# With bipartite matching approach\n",
    "#import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "#from univi.data import MultiModalDataset\n",
    "\n",
    "# matched indices into the *original* adatas\n",
    "# matched_rna_idx, matched_adt_idx from bipartite_match_adata(...)\n",
    "n_matched = len(matched_rna_idx)\n",
    "assert n_matched == len(matched_adt_idx)\n",
    "\n",
    "# 1) build matched AnnData views (for clarity)\n",
    "rna_matched = rna_adata[matched_rna_idx].copy()\n",
    "adt_matched = adt_adata[matched_adt_idx].copy()\n",
    "\n",
    "# 2) build dataset from matched adatas\n",
    "adata_by_mod = {\"rna\": rna_matched, \"adt\": adt_matched}\n",
    "full_dataset = MultiModalDataset(\n",
    "    adata_dict=adata_by_mod,\n",
    "    X_key=\"X\",                # or your chosen layer / key\n",
    "    device=train_cfg.device,  # cpu or cuda\n",
    ")\n",
    "\n",
    "# 3) make train/val/test indices in *matched space* (0..n_matched-1)\n",
    "indices = np.arange(n_matched)\n",
    "rng = np.random.default_rng(train_cfg.seed)\n",
    "rng.shuffle(indices)\n",
    "\n",
    "frac_train = 0.8\n",
    "frac_val   = 0.1\n",
    "n_train = int(frac_train * n_matched)\n",
    "n_val   = int(frac_val * n_matched)\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "val_idx   = indices[n_train:n_train + n_val]\n",
    "test_idx  = indices[n_train + n_val:]\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "\n",
    "# 4) build dataloaders\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset   = Subset(full_dataset, val_idx)\n",
    "test_dataset  = Subset(full_dataset, test_idx)  # for later eval\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "\n",
    "\n",
    "# global indices (into original rna_adata / adt_adata)\n",
    "rna_train_global = matched_rna_idx[train_idx]\n",
    "rna_val_global   = matched_rna_idx[val_idx]\n",
    "rna_test_global  = matched_rna_idx[test_idx]\n",
    "\n",
    "adt_train_global = matched_adt_idx[train_idx]\n",
    "adt_val_global   = matched_adt_idx[val_idx]\n",
    "adt_test_global  = matched_adt_idx[test_idx]\n",
    "\n",
    "\n",
    "def init_split_column(adata, col=\"univi_split\"):\n",
    "    if col not in adata.obs.columns:\n",
    "        adata.obs[col] = \"unused\"\n",
    "\n",
    "# initialize\n",
    "init_split_column(rna_adata, \"univi_split\")\n",
    "init_split_column(adt_adata, \"univi_split\")\n",
    "\n",
    "# everything that was matched at all\n",
    "rna_adata.obs.loc[matched_rna_idx, \"univi_split\"] = \"matched\"\n",
    "adt_adata.obs.loc[matched_adt_idx, \"univi_split\"] = \"matched\"\n",
    "\n",
    "# override for train / val / test\n",
    "rna_adata.obs.loc[rna_train_global, \"univi_split\"] = \"train\"\n",
    "rna_adata.obs.loc[rna_val_global,   \"univi_split\"] = \"val\"\n",
    "rna_adata.obs.loc[rna_test_global,  \"univi_split\"] = \"test\"\n",
    "\n",
    "adt_adata.obs.loc[adt_train_global, \"univi_split\"] = \"train\"\n",
    "adt_adata.obs.loc[adt_val_global,   \"univi_split\"] = \"val\"\n",
    "adt_adata.obs.loc[adt_test_global,  \"univi_split\"] = \"test\"\n",
    "\n",
    "\n",
    "rna_train_adata = rna_adata[rna_adata.obs[\"univi_split\"] == \"train\"].copy()\n",
    "rna_val_adata   = rna_adata[rna_adata.obs[\"univi_split\"] == \"val\"].copy()\n",
    "rna_test_adata  = rna_adata[rna_adata.obs[\"univi_split\"] == \"test\"].copy()\n",
    "rna_unused      = rna_adata[rna_adata.obs[\"univi_split\"] == \"unused\"].copy()\n",
    "\n",
    "adt_train_adata = adt_adata[adt_adata.obs[\"univi_split\"] == \"train\"].copy()\n",
    "adt_val_adata   = adt_adata[adt_adata.obs[\"univi_split\"] == \"val\"].copy()\n",
    "adt_test_adata  = adt_adata[adt_adata.obs[\"univi_split\"] == \"test\"].copy()\n",
    "adt_unused      = adt_adata[adt_adata.obs[\"univi_split\"] == \"unused\"].copy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ebac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without bipartite matching approach\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from univi.data import MultiModalDataset\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 0. Sanity check: RNA / ADT are already aligned\n",
    "# --------------------------------------------------\n",
    "assert rna_adata.n_obs == adt_adata.n_obs, \"RNA and ADT have different #cells\"\n",
    "assert np.array_equal(rna_adata.obs_names, adt_adata.obs_names), (\n",
    "    \"RNA and ADT obs_names are not aligned – align them first.\"\n",
    ")\n",
    "\n",
    "n_cells = rna_adata.n_obs\n",
    "print(f\"Total paired cells: {n_cells}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Build full MultiModalDataset from aligned adatas\n",
    "# --------------------------------------------------\n",
    "adata_by_mod = {\"rna\": rna_adata, \"adt\": adt_adata}\n",
    "\n",
    "full_dataset = MultiModalDataset(\n",
    "    adata_dict=adata_by_mod,\n",
    "    X_key=\"X\",                # or your chosen layer / key\n",
    "    device=train_cfg.device,  # \"cpu\" or \"cuda\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Make train / val / test splits on 0..n_cells-1\n",
    "# --------------------------------------------------\n",
    "indices = np.arange(n_cells)\n",
    "rng = np.random.default_rng(train_cfg.seed)\n",
    "rng.shuffle(indices)\n",
    "\n",
    "frac_train = 0.8\n",
    "frac_val   = 0.1\n",
    "\n",
    "n_train = int(frac_train * n_cells)\n",
    "n_val   = int(frac_val   * n_cells)\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "val_idx   = indices[n_train:n_train + n_val]\n",
    "test_idx  = indices[n_train + n_val:]\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Build Dataset subsets + DataLoaders\n",
    "# --------------------------------------------------\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset   = Subset(full_dataset, val_idx)\n",
    "test_dataset  = Subset(full_dataset, test_idx)  # for later eval\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Annotate splits in the *original* AnnData objects\n",
    "# --------------------------------------------------\n",
    "def init_split_column(adata, col=\"univi_split\"):\n",
    "    if col not in adata.obs.columns:\n",
    "        adata.obs[col] = \"unused\"\n",
    "\n",
    "init_split_column(rna_adata, \"univi_split\")\n",
    "init_split_column(adt_adata, \"univi_split\")\n",
    "\n",
    "# everything starts as \"unused\"\n",
    "rna_adata.obs[\"univi_split\"] = \"unused\"\n",
    "adt_adata.obs[\"univi_split\"] = \"unused\"\n",
    "\n",
    "# mark train / val / test using the global indices directly\n",
    "rna_adata.obs.iloc[train_idx, rna_adata.obs.columns.get_loc(\"univi_split\")] = \"train\"\n",
    "rna_adata.obs.iloc[val_idx,   rna_adata.obs.columns.get_loc(\"univi_split\")] = \"val\"\n",
    "rna_adata.obs.iloc[test_idx,  rna_adata.obs.columns.get_loc(\"univi_split\")] = \"test\"\n",
    "\n",
    "adt_adata.obs.iloc[train_idx, adt_adata.obs.columns.get_loc(\"univi_split\")] = \"train\"\n",
    "adt_adata.obs.iloc[val_idx,   adt_adata.obs.columns.get_loc(\"univi_split\")] = \"val\"\n",
    "adt_adata.obs.iloc[test_idx,  adt_adata.obs.columns.get_loc(\"univi_split\")] = \"test\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Optional: split AnnData objects for convenience\n",
    "# --------------------------------------------------\n",
    "rna_train_adata = rna_adata[rna_adata.obs[\"univi_split\"] == \"train\"].copy()\n",
    "rna_val_adata   = rna_adata[rna_adata.obs[\"univi_split\"] == \"val\"].copy()\n",
    "rna_test_adata  = rna_adata[rna_adata.obs[\"univi_split\"] == \"test\"].copy()\n",
    "rna_unused      = rna_adata[rna_adata.obs[\"univi_split\"] == \"unused\"].copy()\n",
    "\n",
    "adt_train_adata = adt_adata[adt_adata.obs[\"univi_split\"] == \"train\"].copy()\n",
    "adt_val_adata   = adt_adata[adt_adata.obs[\"univi_split\"] == \"val\"].copy()\n",
    "adt_test_adata  = adt_adata[adt_adata.obs[\"univi_split\"] == \"test\"].copy()\n",
    "adt_unused      = adt_adata[adt_adata.obs[\"univi_split\"] == \"unused\"].copy()\n",
    "\n",
    "print(\n",
    "    \"RNA split sizes:\",\n",
    "    {k: v.n_obs for k, v in dict(\n",
    "        train=rna_train_adata,\n",
    "        val=rna_val_adata,\n",
    "        test=rna_test_adata,\n",
    "        unused=rna_unused,\n",
    "    ).items()},\n",
    ")\n",
    "print(\n",
    "    \"ADT split sizes:\",\n",
    "    {k: v.n_obs for k, v in dict(\n",
    "        train=adt_train_adata,\n",
    "        val=adt_val_adata,\n",
    "        test=adt_test_adata,\n",
    "        unused=adt_unused,\n",
    "    ).items()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f59139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rna': AnnData object with n_obs × n_vars = 161764 × 2000\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
      "    uns: 'neighbors', 'hvg'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'PCs', 'SPCA'\n",
      "    layers: 'log1p'\n",
      "    obsp: 'distances', 'adt': AnnData object with n_obs × n_vars = 161764 × 228\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'APCA'\n",
      "    layers: 'log1p'}\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(adata_by_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0a38ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3. Build ModalityConfig list\n",
    "# -------------------------\n",
    "modality_cfgs = []\n",
    "for m in data_cfg[\"modalities\"]:\n",
    "    name = m[\"name\"]\n",
    "    adata = adata_by_mod[name]\n",
    "\n",
    "    hidden = m.get(\"hidden_dims\", model_cfg[\"hidden_dims_default\"])\n",
    "\n",
    "    modality_cfgs.append(\n",
    "        ModalityConfig(\n",
    "            name=name,\n",
    "            input_dim=int(adata.n_vars),\n",
    "            encoder_hidden=hidden,\n",
    "            decoder_hidden=hidden,\n",
    "            likelihood=m[\"likelihood\"],  # e.g. \"nb\" for RNA, \"gaussian\" for ADT\n",
    "        )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4211036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModalityConfig(name='rna', input_dim=2000, encoder_hidden=[512, 256], decoder_hidden=[512, 256], likelihood='nb'), ModalityConfig(name='adt', input_dim=228, encoder_hidden=[128, 64], decoder_hidden=[128, 64], likelihood='nb')]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(modality_cfgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22325d32",
   "metadata": {},
   "source": [
    "#### Initialize model and data via dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76d731a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. UniVI model config\n",
    "# -------------------------\n",
    "univi_cfg = UniVIConfig(\n",
    "    latent_dim=model_cfg.get(\"latent_dim\", 32),\n",
    "    modalities=modality_cfgs,\n",
    "    beta=model_cfg.get(\"beta\", 1.0),\n",
    "    gamma=model_cfg.get(\"gamma\", 1.0),\n",
    "    encoder_dropout=model_cfg.get(\"dropout\", 0.0),\n",
    "    encoder_batchnorm=model_cfg.get(\"batchnorm\", True),\n",
    "    kl_anneal_start=model_cfg.get(\"kl_anneal_start\", 0),\n",
    "    kl_anneal_end=model_cfg.get(\"kl_anneal_end\", 0),\n",
    "    align_anneal_start=model_cfg.get(\"align_anneal_start\", 0),\n",
    "    align_anneal_end=model_cfg.get(\"align_anneal_end\", 0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0abe8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniVIConfig(latent_dim=40, modalities=[ModalityConfig(name='rna', input_dim=2000, encoder_hidden=[512, 256], decoder_hidden=[512, 256], likelihood='nb'), ModalityConfig(name='adt', input_dim=228, encoder_hidden=[128, 64], decoder_hidden=[128, 64], likelihood='nb')], beta=20.0, gamma=80.0, encoder_dropout=0.1, decoder_dropout=0.0, encoder_batchnorm=True, decoder_batchnorm=False, kl_anneal_start=10, kl_anneal_end=50, align_anneal_start=10, align_anneal_end=50)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(univi_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38da5a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_cfg = TrainingConfig(\\n    seed=0,\\n    batch_size=256,\\n    num_workers=0,\\n    n_epochs=200,\\n    lr=1e-3,\\n    weight_decay=1e-4,\\n    grad_clip=5.0,\\n    device=device,\\n    log_every=10,\\n    early_stopping=True,\\n    patience=20,\\n    min_delta=0.0,\\n)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5. Training config\n",
    "# -------------------------\n",
    "train_cfg = TrainingConfig(\n",
    "    n_epochs=train_cfg_json[\"n_epochs\"],\n",
    "    batch_size=train_cfg_json[\"batch_size\"],\n",
    "    lr=train_cfg_json[\"lr\"],\n",
    "    weight_decay=train_cfg_json.get(\"weight_decay\", 0.0),\n",
    "    device=train_cfg_json.get(\"device\", \"cuda\"),\n",
    "    log_every=train_cfg_json.get(\"log_every\", 5),\n",
    "    grad_clip=train_cfg_json.get(\"grad_clip\", None),\n",
    "    num_workers=train_cfg_json.get(\"num_workers\", 0),\n",
    "    seed=train_cfg_json.get(\"seed\", 0),\n",
    "    early_stopping=train_cfg_json.get(\"early_stopping\", True),\n",
    "    patience=train_cfg_json.get(\"patience\", 20),\n",
    "    min_delta=train_cfg_json.get(\"min_delta\", 0.0),\n",
    ")\n",
    "\n",
    "\n",
    "# Or specify training config by hand like so:\n",
    "'''\n",
    "train_cfg = TrainingConfig(\n",
    "    seed=0,\n",
    "    batch_size=256,\n",
    "    num_workers=0,\n",
    "    n_epochs=200,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    grad_clip=5.0,\n",
    "    device=device,\n",
    "    log_every=10,\n",
    "    early_stopping=True,\n",
    "    patience=20,\n",
    "    min_delta=0.0,\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4690fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingConfig: TrainingConfig(n_epochs=200, batch_size=256, lr=0.001, weight_decay=0.0001, device='cpu', log_every=1, grad_clip=5.0, num_workers=0, seed=42, early_stopping=True, patience=20, min_delta=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"TrainingConfig:\", train_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "323c0c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cells in dataset: 161764\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6. Build Dataset + DataLoaders\n",
    "# -------------------------\n",
    "set_seed(train_cfg.seed)\n",
    "\n",
    "dataset = MultiModalDataset(\n",
    "    adata_dict=adata_by_mod,\n",
    "    X_key=\"X\",\n",
    "    device=train_cfg.device,\n",
    ")\n",
    "\n",
    "print(\"Total cells in dataset:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8582914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cells: 129411, Val cells: 32353\n"
     ]
    }
   ],
   "source": [
    "# Old training and validation splitting data loading code:\n",
    "'''\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# fraction of cells to use for training\n",
    "train_fraction = 0.8   # or whatever you like\n",
    "\n",
    "n_cells = len(dataset)\n",
    "indices = np.random.permutation(n_cells)\n",
    "\n",
    "n_train = int(train_fraction * n_cells)\n",
    "train_idx = indices[:n_train]\n",
    "val_idx   = indices[n_train:]\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset   = Subset(dataset, val_idx)\n",
    "\n",
    "print(f\"Train cells: {len(train_dataset)}, Val cells: {len(val_dataset)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1025ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04866014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-17 22:19:44,197] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-17 22:19:44,198] [UniVITrainer] [INFO]   n_epochs: 200\n",
      "[2025-11-17 22:19:44,199] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-17 22:19:44,200] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-17 22:19:44,200] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-17 22:19:44,201] [UniVITrainer] [INFO]   device: cpu\n",
      "[2025-11-17 22:19:44,201] [UniVITrainer] [INFO]   log_every: 1\n",
      "[2025-11-17 22:19:44,201] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-17 22:19:44,202] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-17 22:19:44,202] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-17 22:19:44,202] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-17 22:19:44,203] [UniVITrainer] [INFO]   patience: 20\n",
      "[2025-11-17 22:19:44,203] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 7. Instantiate model + trainer\n",
    "# -------------------------\n",
    "model = UniVIMultiModalVAE(univi_cfg).to(train_cfg.device)\n",
    "\n",
    "trainer = UniVITrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    train_cfg=train_cfg,\n",
    "    device=train_cfg.device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ba81b",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cfc253d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0303a9f6d88a4050926e3fb390077d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-17 22:21:45,254] [UniVITrainer] [INFO] [Epoch 001] Train loss: 1212012.0922 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:22:07,181] [UniVITrainer] [INFO] [Epoch 001] Val loss: 500522.0297 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:22:07,201] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 500522.0297\n",
      "[2025-11-17 22:24:09,955] [UniVITrainer] [INFO] [Epoch 002] Train loss: 371852.1813 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:24:32,405] [UniVITrainer] [INFO] [Epoch 002] Val loss: 411047.7219 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:24:32,425] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 411047.7219\n",
      "[2025-11-17 22:26:36,090] [UniVITrainer] [INFO] [Epoch 003] Train loss: 326039.5093 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:26:58,348] [UniVITrainer] [INFO] [Epoch 003] Val loss: 380991.0741 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:26:58,368] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 380991.0741\n",
      "[2025-11-17 22:31:25,864] [UniVITrainer] [INFO] [Epoch 004] Train loss: 293792.1574 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:31:48,956] [UniVITrainer] [INFO] [Epoch 004] Val loss: 346955.2776 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:31:48,977] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 346955.2776\n",
      "[2025-11-17 22:36:46,980] [UniVITrainer] [INFO] [Epoch 005] Train loss: 266168.5110 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:37:10,054] [UniVITrainer] [INFO] [Epoch 005] Val loss: 301623.6920 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:37:10,074] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 301623.6920\n",
      "[2025-11-17 22:41:57,841] [UniVITrainer] [INFO] [Epoch 006] Train loss: 236514.4916 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:42:20,531] [UniVITrainer] [INFO] [Epoch 006] Val loss: 253695.0669 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:42:20,549] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 253695.0669\n",
      "[2025-11-17 22:47:06,585] [UniVITrainer] [INFO] [Epoch 007] Train loss: 220663.9884 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:47:33,343] [UniVITrainer] [INFO] [Epoch 007] Val loss: 224595.7195 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:47:33,370] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 224595.7195\n",
      "[2025-11-17 22:52:22,412] [UniVITrainer] [INFO] [Epoch 008] Train loss: 198930.1300 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:52:45,772] [UniVITrainer] [INFO] [Epoch 008] Val loss: 217065.6926 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:52:45,792] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 217065.6926\n",
      "[2025-11-17 22:57:50,419] [UniVITrainer] [INFO] [Epoch 009] Train loss: 185313.4414 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:58:18,127] [UniVITrainer] [INFO] [Epoch 009] Val loss: 194121.1253 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 22:58:18,142] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 194121.1253\n",
      "[2025-11-17 23:03:22,353] [UniVITrainer] [INFO] [Epoch 010] Train loss: 178959.5518 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 23:03:52,583] [UniVITrainer] [INFO] [Epoch 010] Val loss: 192207.1693 (beta=0.000, gamma=0.000)\n",
      "[2025-11-17 23:03:52,607] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 192207.1693\n",
      "[2025-11-17 23:09:07,789] [UniVITrainer] [INFO] [Epoch 011] Train loss: 198191.6674 (beta=0.500, gamma=2.000)\n",
      "[2025-11-17 23:09:39,992] [UniVITrainer] [INFO] [Epoch 011] Val loss: 188032.6509 (beta=0.500, gamma=2.000)\n",
      "[2025-11-17 23:09:40,011] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 188032.6509\n",
      "[2025-11-17 23:14:56,554] [UniVITrainer] [INFO] [Epoch 012] Train loss: 189573.2871 (beta=1.000, gamma=4.000)\n",
      "[2025-11-17 23:15:25,638] [UniVITrainer] [INFO] [Epoch 012] Val loss: 182488.0713 (beta=1.000, gamma=4.000)\n",
      "[2025-11-17 23:15:25,659] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 182488.0713\n",
      "[2025-11-17 23:20:43,856] [UniVITrainer] [INFO] [Epoch 013] Train loss: 181276.0119 (beta=1.500, gamma=6.000)\n",
      "[2025-11-17 23:21:17,099] [UniVITrainer] [INFO] [Epoch 013] Val loss: 174315.7354 (beta=1.500, gamma=6.000)\n",
      "[2025-11-17 23:21:17,120] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 174315.7354\n",
      "[2025-11-17 23:26:37,092] [UniVITrainer] [INFO] [Epoch 014] Train loss: 173218.2168 (beta=2.000, gamma=8.000)\n",
      "[2025-11-17 23:27:08,735] [UniVITrainer] [INFO] [Epoch 014] Val loss: 169053.0290 (beta=2.000, gamma=8.000)\n",
      "[2025-11-17 23:27:08,756] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 169053.0290\n",
      "[2025-11-17 23:32:45,244] [UniVITrainer] [INFO] [Epoch 015] Train loss: 167441.7429 (beta=2.500, gamma=10.000)\n",
      "[2025-11-17 23:33:16,123] [UniVITrainer] [INFO] [Epoch 015] Val loss: 163402.8636 (beta=2.500, gamma=10.000)\n",
      "[2025-11-17 23:33:16,145] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 163402.8636\n",
      "[2025-11-17 23:38:42,285] [UniVITrainer] [INFO] [Epoch 016] Train loss: 164586.6942 (beta=3.000, gamma=12.000)\n",
      "[2025-11-17 23:39:17,177] [UniVITrainer] [INFO] [Epoch 016] Val loss: 161481.7765 (beta=3.000, gamma=12.000)\n",
      "[2025-11-17 23:39:17,201] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 161481.7765\n",
      "[2025-11-17 23:45:07,580] [UniVITrainer] [INFO] [Epoch 017] Train loss: 161052.2539 (beta=3.500, gamma=14.000)\n",
      "[2025-11-17 23:45:41,461] [UniVITrainer] [INFO] [Epoch 017] Val loss: 164740.2674 (beta=3.500, gamma=14.000)\n",
      "[2025-11-17 23:51:28,311] [UniVITrainer] [INFO] [Epoch 018] Train loss: 155319.8857 (beta=4.000, gamma=16.000)\n",
      "[2025-11-17 23:52:01,619] [UniVITrainer] [INFO] [Epoch 018] Val loss: 176327.1223 (beta=4.000, gamma=16.000)\n",
      "[2025-11-17 23:57:42,849] [UniVITrainer] [INFO] [Epoch 019] Train loss: 155404.2045 (beta=4.500, gamma=18.000)\n",
      "[2025-11-17 23:58:16,445] [UniVITrainer] [INFO] [Epoch 019] Val loss: 153987.4779 (beta=4.500, gamma=18.000)\n",
      "[2025-11-17 23:58:16,463] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 153987.4779\n",
      "[2025-11-18 00:04:06,265] [UniVITrainer] [INFO] [Epoch 020] Train loss: 151577.3568 (beta=5.000, gamma=20.000)\n",
      "[2025-11-18 00:04:40,279] [UniVITrainer] [INFO] [Epoch 020] Val loss: 157697.9929 (beta=5.000, gamma=20.000)\n",
      "[2025-11-18 00:10:18,837] [UniVITrainer] [INFO] [Epoch 021] Train loss: 150735.5794 (beta=5.500, gamma=22.000)\n",
      "[2025-11-18 00:10:51,193] [UniVITrainer] [INFO] [Epoch 021] Val loss: 152647.2300 (beta=5.500, gamma=22.000)\n",
      "[2025-11-18 00:10:51,215] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 152647.2300\n",
      "[2025-11-18 00:16:22,342] [UniVITrainer] [INFO] [Epoch 022] Train loss: 148007.4548 (beta=6.000, gamma=24.000)\n",
      "[2025-11-18 00:16:58,749] [UniVITrainer] [INFO] [Epoch 022] Val loss: 172603.9332 (beta=6.000, gamma=24.000)\n",
      "[2025-11-18 00:22:53,661] [UniVITrainer] [INFO] [Epoch 023] Train loss: 145664.2741 (beta=6.500, gamma=26.000)\n",
      "[2025-11-18 00:23:26,231] [UniVITrainer] [INFO] [Epoch 023] Val loss: 148659.0567 (beta=6.500, gamma=26.000)\n",
      "[2025-11-18 00:23:26,253] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 148659.0567\n",
      "[2025-11-18 00:29:09,719] [UniVITrainer] [INFO] [Epoch 024] Train loss: 144974.3770 (beta=7.000, gamma=28.000)\n",
      "[2025-11-18 00:29:42,808] [UniVITrainer] [INFO] [Epoch 024] Val loss: 147265.9906 (beta=7.000, gamma=28.000)\n",
      "[2025-11-18 00:29:42,828] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 147265.9906\n",
      "[2025-11-18 00:35:06,239] [UniVITrainer] [INFO] [Epoch 025] Train loss: 143276.4959 (beta=7.500, gamma=30.000)\n",
      "[2025-11-18 00:35:35,763] [UniVITrainer] [INFO] [Epoch 025] Val loss: 146085.7314 (beta=7.500, gamma=30.000)\n",
      "[2025-11-18 00:35:35,784] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 146085.7314\n",
      "[2025-11-18 00:40:48,784] [UniVITrainer] [INFO] [Epoch 026] Train loss: 141720.6236 (beta=8.000, gamma=32.000)\n",
      "[2025-11-18 00:41:18,573] [UniVITrainer] [INFO] [Epoch 026] Val loss: 149098.9721 (beta=8.000, gamma=32.000)\n",
      "[2025-11-18 00:46:29,784] [UniVITrainer] [INFO] [Epoch 027] Train loss: 140897.7040 (beta=8.500, gamma=34.000)\n",
      "[2025-11-18 00:46:59,380] [UniVITrainer] [INFO] [Epoch 027] Val loss: 138235.5143 (beta=8.500, gamma=34.000)\n",
      "[2025-11-18 00:46:59,401] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 138235.5143\n",
      "[2025-11-18 00:52:24,543] [UniVITrainer] [INFO] [Epoch 028] Train loss: 138419.8467 (beta=9.000, gamma=36.000)\n",
      "[2025-11-18 00:52:54,836] [UniVITrainer] [INFO] [Epoch 028] Val loss: 136124.2537 (beta=9.000, gamma=36.000)\n",
      "[2025-11-18 00:52:54,859] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 136124.2537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-18 00:58:18,889] [UniVITrainer] [INFO] [Epoch 029] Train loss: 137184.0991 (beta=9.500, gamma=38.000)\n",
      "[2025-11-18 00:58:48,567] [UniVITrainer] [INFO] [Epoch 029] Val loss: 133644.0602 (beta=9.500, gamma=38.000)\n",
      "[2025-11-18 00:58:48,588] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 133644.0602\n",
      "[2025-11-18 01:04:11,760] [UniVITrainer] [INFO] [Epoch 030] Train loss: 134955.8971 (beta=10.000, gamma=40.000)\n",
      "[2025-11-18 01:04:41,480] [UniVITrainer] [INFO] [Epoch 030] Val loss: 126146.9100 (beta=10.000, gamma=40.000)\n",
      "[2025-11-18 01:04:41,504] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 126146.9100\n",
      "[2025-11-18 01:09:59,199] [UniVITrainer] [INFO] [Epoch 031] Train loss: 136163.8108 (beta=10.500, gamma=42.000)\n",
      "[2025-11-18 01:10:27,950] [UniVITrainer] [INFO] [Epoch 031] Val loss: 123395.7131 (beta=10.500, gamma=42.000)\n",
      "[2025-11-18 01:10:27,971] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 123395.7131\n",
      "[2025-11-18 01:15:43,355] [UniVITrainer] [INFO] [Epoch 032] Train loss: 135997.6878 (beta=11.000, gamma=44.000)\n",
      "[2025-11-18 01:16:15,850] [UniVITrainer] [INFO] [Epoch 032] Val loss: 138312.6284 (beta=11.000, gamma=44.000)\n",
      "[2025-11-18 01:21:27,358] [UniVITrainer] [INFO] [Epoch 033] Train loss: 133463.7445 (beta=11.500, gamma=46.000)\n",
      "[2025-11-18 01:21:57,924] [UniVITrainer] [INFO] [Epoch 033] Val loss: 122051.9088 (beta=11.500, gamma=46.000)\n",
      "[2025-11-18 01:21:57,947] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 122051.9088\n",
      "[2025-11-18 01:27:17,508] [UniVITrainer] [INFO] [Epoch 034] Train loss: 130478.9889 (beta=12.000, gamma=48.000)\n",
      "[2025-11-18 01:27:48,237] [UniVITrainer] [INFO] [Epoch 034] Val loss: 117212.9387 (beta=12.000, gamma=48.000)\n",
      "[2025-11-18 01:27:48,257] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 117212.9387\n",
      "[2025-11-18 01:33:04,629] [UniVITrainer] [INFO] [Epoch 035] Train loss: 128294.4243 (beta=12.500, gamma=50.000)\n",
      "[2025-11-18 01:33:34,392] [UniVITrainer] [INFO] [Epoch 035] Val loss: 107663.2829 (beta=12.500, gamma=50.000)\n",
      "[2025-11-18 01:33:34,412] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 107663.2829\n",
      "[2025-11-18 01:38:40,804] [UniVITrainer] [INFO] [Epoch 036] Train loss: 130955.8973 (beta=13.000, gamma=52.000)\n",
      "[2025-11-18 01:39:10,447] [UniVITrainer] [INFO] [Epoch 036] Val loss: 108390.8604 (beta=13.000, gamma=52.000)\n",
      "[2025-11-18 01:44:25,601] [UniVITrainer] [INFO] [Epoch 037] Train loss: 130349.8259 (beta=13.500, gamma=54.000)\n",
      "[2025-11-18 01:45:01,112] [UniVITrainer] [INFO] [Epoch 037] Val loss: 103893.5590 (beta=13.500, gamma=54.000)\n",
      "[2025-11-18 01:45:01,131] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 103893.5590\n",
      "[2025-11-18 01:50:20,774] [UniVITrainer] [INFO] [Epoch 038] Train loss: 129678.3115 (beta=14.000, gamma=56.000)\n",
      "[2025-11-18 01:50:52,273] [UniVITrainer] [INFO] [Epoch 038] Val loss: 103944.0336 (beta=14.000, gamma=56.000)\n",
      "[2025-11-18 01:56:15,039] [UniVITrainer] [INFO] [Epoch 039] Train loss: 127013.9001 (beta=14.500, gamma=58.000)\n",
      "[2025-11-18 01:56:45,711] [UniVITrainer] [INFO] [Epoch 039] Val loss: 104674.4056 (beta=14.500, gamma=58.000)\n",
      "[2025-11-18 02:01:53,605] [UniVITrainer] [INFO] [Epoch 040] Train loss: 126542.4747 (beta=15.000, gamma=60.000)\n",
      "[2025-11-18 02:02:23,136] [UniVITrainer] [INFO] [Epoch 040] Val loss: 105582.1984 (beta=15.000, gamma=60.000)\n",
      "[2025-11-18 02:07:47,510] [UniVITrainer] [INFO] [Epoch 041] Train loss: 127115.9166 (beta=15.500, gamma=62.000)\n",
      "[2025-11-18 02:08:19,569] [UniVITrainer] [INFO] [Epoch 041] Val loss: 102771.0490 (beta=15.500, gamma=62.000)\n",
      "[2025-11-18 02:08:19,591] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 102771.0490\n",
      "[2025-11-18 02:13:58,808] [UniVITrainer] [INFO] [Epoch 042] Train loss: 127390.4583 (beta=16.000, gamma=64.000)\n",
      "[2025-11-18 02:14:30,561] [UniVITrainer] [INFO] [Epoch 042] Val loss: 102602.3494 (beta=16.000, gamma=64.000)\n",
      "[2025-11-18 02:14:30,577] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 102602.3494\n",
      "[2025-11-18 02:20:05,897] [UniVITrainer] [INFO] [Epoch 043] Train loss: 126239.0924 (beta=16.500, gamma=66.000)\n",
      "[2025-11-18 02:20:40,952] [UniVITrainer] [INFO] [Epoch 043] Val loss: 100470.8515 (beta=16.500, gamma=66.000)\n",
      "[2025-11-18 02:20:40,976] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 100470.8515\n",
      "[2025-11-18 02:26:14,685] [UniVITrainer] [INFO] [Epoch 044] Train loss: 127037.3895 (beta=17.000, gamma=68.000)\n",
      "[2025-11-18 02:26:48,532] [UniVITrainer] [INFO] [Epoch 044] Val loss: 99331.3804 (beta=17.000, gamma=68.000)\n",
      "[2025-11-18 02:26:48,557] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 99331.3804\n",
      "[2025-11-18 02:32:32,284] [UniVITrainer] [INFO] [Epoch 045] Train loss: 124546.7253 (beta=17.500, gamma=70.000)\n",
      "[2025-11-18 02:33:05,896] [UniVITrainer] [INFO] [Epoch 045] Val loss: 103341.6311 (beta=17.500, gamma=70.000)\n",
      "[2025-11-18 02:38:46,861] [UniVITrainer] [INFO] [Epoch 046] Train loss: 124569.3734 (beta=18.000, gamma=72.000)\n",
      "[2025-11-18 02:39:19,444] [UniVITrainer] [INFO] [Epoch 046] Val loss: 100317.0436 (beta=18.000, gamma=72.000)\n",
      "[2025-11-18 02:44:53,000] [UniVITrainer] [INFO] [Epoch 047] Train loss: 124858.5207 (beta=18.500, gamma=74.000)\n",
      "[2025-11-18 02:45:27,386] [UniVITrainer] [INFO] [Epoch 047] Val loss: 103715.2753 (beta=18.500, gamma=74.000)\n",
      "[2025-11-18 02:50:56,735] [UniVITrainer] [INFO] [Epoch 048] Train loss: 123473.6315 (beta=19.000, gamma=76.000)\n",
      "[2025-11-18 02:51:28,242] [UniVITrainer] [INFO] [Epoch 048] Val loss: 100897.0144 (beta=19.000, gamma=76.000)\n",
      "[2025-11-18 02:56:59,728] [UniVITrainer] [INFO] [Epoch 049] Train loss: 125466.8666 (beta=19.500, gamma=78.000)\n",
      "[2025-11-18 02:57:32,601] [UniVITrainer] [INFO] [Epoch 049] Val loss: 102802.3354 (beta=19.500, gamma=78.000)\n",
      "[2025-11-18 03:02:54,177] [UniVITrainer] [INFO] [Epoch 050] Train loss: 125924.3228 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:03:26,823] [UniVITrainer] [INFO] [Epoch 050] Val loss: 100081.4941 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:08:52,749] [UniVITrainer] [INFO] [Epoch 051] Train loss: 123538.7406 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:09:23,053] [UniVITrainer] [INFO] [Epoch 051] Val loss: 118205.3030 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:14:46,403] [UniVITrainer] [INFO] [Epoch 052] Train loss: 122274.9571 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:15:17,016] [UniVITrainer] [INFO] [Epoch 052] Val loss: 102153.3820 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:20:36,146] [UniVITrainer] [INFO] [Epoch 053] Train loss: 122395.4919 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:21:07,755] [UniVITrainer] [INFO] [Epoch 053] Val loss: 98082.0781 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:21:07,778] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 98082.0781\n",
      "[2025-11-18 03:26:42,131] [UniVITrainer] [INFO] [Epoch 054] Train loss: 124003.3021 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:27:14,954] [UniVITrainer] [INFO] [Epoch 054] Val loss: 99069.4548 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:33:02,584] [UniVITrainer] [INFO] [Epoch 055] Train loss: 121403.2895 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:33:40,900] [UniVITrainer] [INFO] [Epoch 055] Val loss: 94308.2467 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:33:40,923] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 94308.2467\n",
      "[2025-11-18 03:39:31,040] [UniVITrainer] [INFO] [Epoch 056] Train loss: 121049.5094 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:40:02,790] [UniVITrainer] [INFO] [Epoch 056] Val loss: 100536.4267 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:45:23,941] [UniVITrainer] [INFO] [Epoch 057] Train loss: 119537.3454 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:45:55,464] [UniVITrainer] [INFO] [Epoch 057] Val loss: 94525.4910 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:51:24,070] [UniVITrainer] [INFO] [Epoch 058] Train loss: 119937.5509 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:51:54,549] [UniVITrainer] [INFO] [Epoch 058] Val loss: 107317.4959 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:57:39,991] [UniVITrainer] [INFO] [Epoch 059] Train loss: 119429.7776 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:58:19,603] [UniVITrainer] [INFO] [Epoch 059] Val loss: 91788.7815 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 03:58:19,628] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 91788.7815\n",
      "[2025-11-18 04:03:48,208] [UniVITrainer] [INFO] [Epoch 060] Train loss: 119185.2318 (beta=20.000, gamma=80.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-18 04:04:23,320] [UniVITrainer] [INFO] [Epoch 060] Val loss: 97263.8522 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:09:52,279] [UniVITrainer] [INFO] [Epoch 061] Train loss: 119616.1609 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:10:24,540] [UniVITrainer] [INFO] [Epoch 061] Val loss: 101332.5813 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:16:36,725] [UniVITrainer] [INFO] [Epoch 062] Train loss: 117959.4756 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:17:11,202] [UniVITrainer] [INFO] [Epoch 062] Val loss: 92591.3947 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:22:52,726] [UniVITrainer] [INFO] [Epoch 063] Train loss: 119262.8423 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:23:27,465] [UniVITrainer] [INFO] [Epoch 063] Val loss: 101327.8063 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:29:23,810] [UniVITrainer] [INFO] [Epoch 064] Train loss: 120779.9011 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:29:59,730] [UniVITrainer] [INFO] [Epoch 064] Val loss: 94580.1476 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:35:58,733] [UniVITrainer] [INFO] [Epoch 065] Train loss: 118441.0058 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:36:34,010] [UniVITrainer] [INFO] [Epoch 065] Val loss: 101291.6219 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:42:39,867] [UniVITrainer] [INFO] [Epoch 066] Train loss: 117950.0097 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:43:16,585] [UniVITrainer] [INFO] [Epoch 066] Val loss: 94265.0619 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:49:16,541] [UniVITrainer] [INFO] [Epoch 067] Train loss: 118030.6718 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:49:51,625] [UniVITrainer] [INFO] [Epoch 067] Val loss: 92404.6752 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:55:38,134] [UniVITrainer] [INFO] [Epoch 068] Train loss: 118604.2574 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 04:56:15,703] [UniVITrainer] [INFO] [Epoch 068] Val loss: 93333.5548 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:02:10,951] [UniVITrainer] [INFO] [Epoch 069] Train loss: 116888.3521 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:02:46,051] [UniVITrainer] [INFO] [Epoch 069] Val loss: 95443.3602 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:09:00,031] [UniVITrainer] [INFO] [Epoch 070] Train loss: 118895.8511 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:09:35,691] [UniVITrainer] [INFO] [Epoch 070] Val loss: 96714.9167 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:15:38,529] [UniVITrainer] [INFO] [Epoch 071] Train loss: 116854.4679 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:16:20,432] [UniVITrainer] [INFO] [Epoch 071] Val loss: 99666.3585 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:22:33,307] [UniVITrainer] [INFO] [Epoch 072] Train loss: 115745.6678 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:23:09,891] [UniVITrainer] [INFO] [Epoch 072] Val loss: 96280.4122 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:29:18,855] [UniVITrainer] [INFO] [Epoch 073] Train loss: 116370.3257 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:29:53,848] [UniVITrainer] [INFO] [Epoch 073] Val loss: 98529.1077 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:36:05,600] [UniVITrainer] [INFO] [Epoch 074] Train loss: 114582.1934 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:36:40,071] [UniVITrainer] [INFO] [Epoch 074] Val loss: 90826.3972 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:36:40,097] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 90826.3972\n",
      "[2025-11-18 05:42:40,298] [UniVITrainer] [INFO] [Epoch 075] Train loss: 115352.1806 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:43:16,944] [UniVITrainer] [INFO] [Epoch 075] Val loss: 94062.1102 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:49:45,774] [UniVITrainer] [INFO] [Epoch 076] Train loss: 116513.9200 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:50:24,708] [UniVITrainer] [INFO] [Epoch 076] Val loss: 92786.9102 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:56:58,033] [UniVITrainer] [INFO] [Epoch 077] Train loss: 113704.4306 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 05:57:35,157] [UniVITrainer] [INFO] [Epoch 077] Val loss: 96284.8494 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:03:48,176] [UniVITrainer] [INFO] [Epoch 078] Train loss: 115181.3003 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:04:31,591] [UniVITrainer] [INFO] [Epoch 078] Val loss: 97986.0556 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:10:35,379] [UniVITrainer] [INFO] [Epoch 079] Train loss: 115418.5598 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:11:13,909] [UniVITrainer] [INFO] [Epoch 079] Val loss: 90793.7956 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:11:13,931] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 90793.7956\n",
      "[2025-11-18 06:17:34,660] [UniVITrainer] [INFO] [Epoch 080] Train loss: 113229.3904 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:18:20,005] [UniVITrainer] [INFO] [Epoch 080] Val loss: 88838.8895 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:18:20,032] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 88838.8895\n",
      "[2025-11-18 06:24:48,578] [UniVITrainer] [INFO] [Epoch 081] Train loss: 113209.8301 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:25:25,977] [UniVITrainer] [INFO] [Epoch 081] Val loss: 91095.3375 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:31:45,298] [UniVITrainer] [INFO] [Epoch 082] Train loss: 115325.4262 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:32:24,104] [UniVITrainer] [INFO] [Epoch 082] Val loss: 88481.0336 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:32:24,123] [UniVITrainer] [INFO] [Epoch 082] New best val loss: 88481.0336\n",
      "[2025-11-18 06:38:54,385] [UniVITrainer] [INFO] [Epoch 083] Train loss: 113087.4177 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:39:33,516] [UniVITrainer] [INFO] [Epoch 083] Val loss: 90312.1101 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:46:00,486] [UniVITrainer] [INFO] [Epoch 084] Train loss: 113856.4281 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:46:43,230] [UniVITrainer] [INFO] [Epoch 084] Val loss: 92557.5158 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:53:24,702] [UniVITrainer] [INFO] [Epoch 085] Train loss: 113107.6705 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 06:54:04,099] [UniVITrainer] [INFO] [Epoch 085] Val loss: 98532.8990 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:00:36,526] [UniVITrainer] [INFO] [Epoch 086] Train loss: 111719.2172 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:01:19,083] [UniVITrainer] [INFO] [Epoch 086] Val loss: 86915.3457 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:01:19,107] [UniVITrainer] [INFO] [Epoch 086] New best val loss: 86915.3457\n",
      "[2025-11-18 07:08:08,484] [UniVITrainer] [INFO] [Epoch 087] Train loss: 113289.7836 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:08:49,892] [UniVITrainer] [INFO] [Epoch 087] Val loss: 87953.7375 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:15:22,021] [UniVITrainer] [INFO] [Epoch 088] Train loss: 114439.3801 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:16:01,461] [UniVITrainer] [INFO] [Epoch 088] Val loss: 90448.2968 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:22:41,166] [UniVITrainer] [INFO] [Epoch 089] Train loss: 111636.8612 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:23:21,068] [UniVITrainer] [INFO] [Epoch 089] Val loss: 95582.5738 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:30:14,022] [UniVITrainer] [INFO] [Epoch 090] Train loss: 112992.6740 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:30:54,279] [UniVITrainer] [INFO] [Epoch 090] Val loss: 88139.3201 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:37:52,734] [UniVITrainer] [INFO] [Epoch 091] Train loss: 115544.9187 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:38:33,445] [UniVITrainer] [INFO] [Epoch 091] Val loss: 108882.4070 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:45:37,544] [UniVITrainer] [INFO] [Epoch 092] Train loss: 110389.6149 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:46:17,187] [UniVITrainer] [INFO] [Epoch 092] Val loss: 87806.5014 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:53:01,877] [UniVITrainer] [INFO] [Epoch 093] Train loss: 113036.3361 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:53:41,995] [UniVITrainer] [INFO] [Epoch 093] Val loss: 86725.3012 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 07:53:42,017] [UniVITrainer] [INFO] [Epoch 093] New best val loss: 86725.3012\n",
      "[2025-11-18 08:00:34,750] [UniVITrainer] [INFO] [Epoch 094] Train loss: 112892.5637 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:01:17,451] [UniVITrainer] [INFO] [Epoch 094] Val loss: 84473.5291 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:01:17,476] [UniVITrainer] [INFO] [Epoch 094] New best val loss: 84473.5291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-18 08:08:07,941] [UniVITrainer] [INFO] [Epoch 095] Train loss: 111245.9830 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:08:49,387] [UniVITrainer] [INFO] [Epoch 095] Val loss: 94354.4930 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:15:54,255] [UniVITrainer] [INFO] [Epoch 096] Train loss: 110418.4212 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:16:33,257] [UniVITrainer] [INFO] [Epoch 096] Val loss: 97676.4908 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:23:13,275] [UniVITrainer] [INFO] [Epoch 097] Train loss: 112974.3509 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:24:00,984] [UniVITrainer] [INFO] [Epoch 097] Val loss: 85619.6776 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:30:53,850] [UniVITrainer] [INFO] [Epoch 098] Train loss: 110912.4053 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:31:36,525] [UniVITrainer] [INFO] [Epoch 098] Val loss: 89765.3433 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:38:26,726] [UniVITrainer] [INFO] [Epoch 099] Train loss: 112196.0495 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:39:06,454] [UniVITrainer] [INFO] [Epoch 099] Val loss: 87228.8618 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:46:05,112] [UniVITrainer] [INFO] [Epoch 100] Train loss: 111580.7713 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:46:46,794] [UniVITrainer] [INFO] [Epoch 100] Val loss: 85696.7026 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:54:42,892] [UniVITrainer] [INFO] [Epoch 101] Train loss: 110660.4821 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 08:55:28,782] [UniVITrainer] [INFO] [Epoch 101] Val loss: 89271.6415 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:02:25,786] [UniVITrainer] [INFO] [Epoch 102] Train loss: 110133.6385 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:03:07,344] [UniVITrainer] [INFO] [Epoch 102] Val loss: 91898.8360 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:09:58,601] [UniVITrainer] [INFO] [Epoch 103] Train loss: 107979.2465 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:10:43,922] [UniVITrainer] [INFO] [Epoch 103] Val loss: 83086.6930 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:10:43,942] [UniVITrainer] [INFO] [Epoch 103] New best val loss: 83086.6930\n",
      "[2025-11-18 09:17:57,143] [UniVITrainer] [INFO] [Epoch 104] Train loss: 108696.0014 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:18:37,073] [UniVITrainer] [INFO] [Epoch 104] Val loss: 92588.6396 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:33:21,946] [UniVITrainer] [INFO] [Epoch 105] Train loss: 109500.5003 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:34:48,463] [UniVITrainer] [INFO] [Epoch 105] Val loss: 83450.0692 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:47:53,195] [UniVITrainer] [INFO] [Epoch 106] Train loss: 110699.1526 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 09:49:21,251] [UniVITrainer] [INFO] [Epoch 106] Val loss: 84336.5234 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 10:03:29,140] [UniVITrainer] [INFO] [Epoch 107] Train loss: 107020.9508 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 10:04:52,524] [UniVITrainer] [INFO] [Epoch 107] Val loss: 90237.7491 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 10:19:46,190] [UniVITrainer] [INFO] [Epoch 108] Train loss: 109341.8171 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 10:21:23,580] [UniVITrainer] [INFO] [Epoch 108] Val loss: 92829.5703 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 10:37:20,084] [UniVITrainer] [INFO] [Epoch 109] Train loss: 107743.0040 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 10:39:16,668] [UniVITrainer] [INFO] [Epoch 109] Val loss: 88329.1454 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 10:58:30,638] [UniVITrainer] [INFO] [Epoch 110] Train loss: 107924.7979 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 11:00:36,682] [UniVITrainer] [INFO] [Epoch 110] Val loss: 86345.9138 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 11:18:44,727] [UniVITrainer] [INFO] [Epoch 111] Train loss: 106683.7161 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 11:20:40,818] [UniVITrainer] [INFO] [Epoch 111] Val loss: 84562.7170 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 11:40:05,856] [UniVITrainer] [INFO] [Epoch 112] Train loss: 109297.0236 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 11:41:57,030] [UniVITrainer] [INFO] [Epoch 112] Val loss: 87129.6888 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 12:00:43,477] [UniVITrainer] [INFO] [Epoch 113] Train loss: 107418.5038 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 12:02:34,421] [UniVITrainer] [INFO] [Epoch 113] Val loss: 87763.4303 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 12:20:08,855] [UniVITrainer] [INFO] [Epoch 114] Train loss: 107169.8367 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 12:21:52,043] [UniVITrainer] [INFO] [Epoch 114] Val loss: 93777.8617 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 12:39:26,186] [UniVITrainer] [INFO] [Epoch 115] Train loss: 107928.1588 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 12:41:18,007] [UniVITrainer] [INFO] [Epoch 115] Val loss: 84921.1883 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 13:00:54,364] [UniVITrainer] [INFO] [Epoch 116] Train loss: 105917.5846 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 13:02:43,743] [UniVITrainer] [INFO] [Epoch 116] Val loss: 86139.1105 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 13:22:28,933] [UniVITrainer] [INFO] [Epoch 117] Train loss: 106695.0870 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 13:24:15,238] [UniVITrainer] [INFO] [Epoch 117] Val loss: 89145.5010 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 13:43:46,805] [UniVITrainer] [INFO] [Epoch 118] Train loss: 105078.4448 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 13:45:45,846] [UniVITrainer] [INFO] [Epoch 118] Val loss: 85406.5548 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 14:13:25,254] [UniVITrainer] [INFO] [Epoch 119] Train loss: 105469.4841 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 14:15:41,946] [UniVITrainer] [INFO] [Epoch 119] Val loss: 88450.6676 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 14:39:46,268] [UniVITrainer] [INFO] [Epoch 120] Train loss: 104685.8842 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 14:41:38,781] [UniVITrainer] [INFO] [Epoch 120] Val loss: 85171.7441 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 15:01:31,397] [UniVITrainer] [INFO] [Epoch 121] Train loss: 104839.1794 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 15:03:15,985] [UniVITrainer] [INFO] [Epoch 121] Val loss: 83970.1562 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 15:19:29,788] [UniVITrainer] [INFO] [Epoch 122] Train loss: 105684.3198 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 15:21:15,334] [UniVITrainer] [INFO] [Epoch 122] Val loss: 87737.5877 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 15:31:28,662] [UniVITrainer] [INFO] [Epoch 123] Train loss: 105929.5741 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 15:32:13,215] [UniVITrainer] [INFO] [Epoch 123] Val loss: 82967.3473 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 15:32:13,240] [UniVITrainer] [INFO] [Epoch 123] New best val loss: 82967.3473\n",
      "[2025-11-18 15:50:37,856] [UniVITrainer] [INFO] [Epoch 124] Train loss: 105243.3791 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 15:52:30,836] [UniVITrainer] [INFO] [Epoch 124] Val loss: 90272.2305 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 16:13:45,319] [UniVITrainer] [INFO] [Epoch 125] Train loss: 103860.5139 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 16:15:35,581] [UniVITrainer] [INFO] [Epoch 125] Val loss: 84296.9025 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 16:25:45,785] [UniVITrainer] [INFO] [Epoch 126] Train loss: 104013.8123 (beta=20.000, gamma=80.000)\n",
      "[2025-11-18 16:26:29,593] [UniVITrainer] [INFO] [Epoch 126] Val loss: 97063.0502 (beta=20.000, gamma=80.000)\n",
      "Exception ignored in: <generator object tqdm_notebook.__iter__ at 0x7f5ab2c23120>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/tqdm/notebook.py\", line 263, in __iter__\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/tqdm/notebook.py\", line 180, in display\n",
      "    rtext.value = right\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/traitlets/traitlets.py\", line 715, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/traitlets/traitlets.py\", line 704, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/traitlets/traitlets.py\", line 1374, in _notify_trait\n",
      "    self.notify_change(\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/ipywidgets/widgets/widget.py\", line 685, in notify_change\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/ipywidgets/widgets/widget.py\", line 554, in send_state\n",
      "    self._send(msg, buffers=buffers)\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/ipywidgets/widgets/widget.py\", line 817, in _send\n",
      "    self.comm.send(data=msg, buffers=buffers)\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/ipykernel/comm/comm.py\", line 137, in send\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/ipykernel/comm/comm.py\", line 71, in _publish_msg\n",
      "    self.kernel.session.send(\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/jupyter_client/session.py\", line 844, in send\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/jupyter_client/session.py\", line 729, in serialize\n",
      "    self.pack(msg[\"header\"]),\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/jupyter_client/session.py\", line 99, in json_packer\n",
      "    return json.dumps(\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/json/__init__.py\", line 234, in dumps\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/json/encoder.py\", line 199, in encode\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/home/groups/precepts/ashforda/anaconda3/envs/manifold_alignment/lib/python3.9/site-packages/jupyter_client/jsonutil.py\", line 112, in json_default\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 8. Train!\n",
    "# -------------------------\n",
    "history = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fad59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "os.makedirs(\"../saved_models\", exist_ok=True)\n",
    "\n",
    "# after training\n",
    "#history = trainer.fit()\n",
    "\n",
    "# trainer.model already has the best weights (because we restored best_state_dict)\n",
    "ckpt_path = \"../saved_models/univi_hao_training_all_best.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": trainer.model.state_dict(),\n",
    "        \"univi_cfg\": asdict(univi_cfg),\n",
    "        \"best_epoch\": trainer.best_epoch,\n",
    "        \"best_val_loss\": trainer.best_val_loss,\n",
    "    },\n",
    "    ckpt_path,\n",
    ")\n",
    "print(\"Saved best model to:\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later to reload model\n",
    "'''\n",
    "import torch\n",
    "from univi.config import UniVIConfig, ModalityConfig\n",
    "from univi.models.univi import UniVIMultiModalVAE\n",
    "\n",
    "device = \"cpu\"  # or \"cuda\" if available\n",
    "\n",
    "ckpt = torch.load(\n",
    "    \"../saved_models/univi_hao_training_all_best.pt\",\n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "# ---- Rebuild UniVIConfig, making sure modalities are ModalityConfig objects ----\n",
    "cfg_dict = ckpt[\"univi_cfg\"]\n",
    "\n",
    "# If this is an OmegaConf object or similar, make sure it's a plain dict\n",
    "try:\n",
    "    from omegaconf import DictConfig, OmegaConf\n",
    "    if isinstance(cfg_dict, DictConfig):\n",
    "        cfg_dict = OmegaConf.to_container(cfg_dict, resolve=True)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Now rehydrate each modality\n",
    "modalities = [ModalityConfig(**m) for m in cfg_dict[\"modalities\"]]\n",
    "cfg_dict = {**cfg_dict, \"modalities\": modalities}\n",
    "\n",
    "univi_cfg_loaded = UniVIConfig(**cfg_dict)\n",
    "\n",
    "# ---- Rebuild model + load weights ----\n",
    "model_loaded = UniVIMultiModalVAE(univi_cfg_loaded).to(device)\n",
    "model_loaded.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "print(\"Best epoch was:\", ckpt.get(\"best_epoch\"), \"val loss =\", ckpt.get(\"best_val_loss\"))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3880ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66caaae",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "\n",
    "from univi import evaluation as univi_eval\n",
    "# from univi import plotting as univi_plot  # only needed if you still want their saver\n",
    "\n",
    "# -------------------------\n",
    "# 1. Encode latent embeddings\n",
    "# -------------------------\n",
    "z_rna = trainer.encode_modality(rna_test_adata, modality=\"rna\")\n",
    "z_adt = trainer.encode_modality(adt_test_adata, modality=\"adt\")\n",
    "\n",
    "rna_test_adata.obsm[\"X_univi\"] = z_rna\n",
    "adt_test_adata.obsm[\"X_univi\"] = z_adt\n",
    "\n",
    "# -------------------------\n",
    "# 2. FOSCTTM (global alignment)\n",
    "# -------------------------\n",
    "foscttm = univi_eval.compute_foscttm(z_rna, z_adt)\n",
    "print(f\"FOSCTTM (rna vs adt): {foscttm:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Modality mixing in joint embedding\n",
    "# -------------------------\n",
    "Z_joint = np.concatenate([z_rna, z_adt], axis=0)\n",
    "modality_labels = np.array(\n",
    "    [\"rna\"] * z_rna.shape[0] + [\"adt\"] * z_adt.shape[0]\n",
    ")\n",
    "\n",
    "mixing_score = univi_eval.compute_modality_mixing(\n",
    "    Z_joint,\n",
    "    modality_labels,\n",
    "    k=20,\n",
    ")\n",
    "print(f\"Modality mixing score (k=20): {mixing_score:.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. Label transfer (ADT → RNA)\n",
    "# -------------------------\n",
    "labels_rna = rna_test_adata.obs[\"celltype.l2\"].astype(str).values\n",
    "labels_adt = adt_test_adata.obs[\"celltype.l2\"].astype(str).values\n",
    "\n",
    "pred_rna_from_adt, acc_rna, cm_rna = univi_eval.label_transfer_knn(\n",
    "    Z_source=z_adt,\n",
    "    labels_source=labels_adt,\n",
    "    Z_target=z_rna,\n",
    "    labels_target=labels_rna,\n",
    "    k=15,\n",
    ")\n",
    "\n",
    "print(f\"Label transfer accuracy (ADT → RNA, k=15): {acc_rna:.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4a. Confusion matrix plot (show)\n",
    "# -------------------------\n",
    "uniq_labels = np.unique(labels_rna)\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(\n",
    "    cm_rna,\n",
    "    annot=False,\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=uniq_labels,\n",
    "    yticklabels=uniq_labels,\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    ")\n",
    "plt.xlabel(\"Predicted (ADT → RNA)\")\n",
    "plt.ylabel(\"True (RNA)\")\n",
    "plt.title(\"ADT → RNA label transfer confusion matrix\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: also save with univi_plot if you like\n",
    "# from univi import plotting as univi_plot\n",
    "# univi_plot.plot_confusion_matrix(\n",
    "#     cm_rna,\n",
    "#     labels=uniq_labels,\n",
    "#     title=\"ADT → RNA label transfer\",\n",
    "#     savepath=\"../figures/citeseq_univi_label_transfer_cm.png\",\n",
    "# )\n",
    "\n",
    "# -------------------------\n",
    "# 5. UMAP visualization (biological structure)\n",
    "#    on the UniVI latent space\n",
    "# -------------------------\n",
    "\n",
    "# Make copies and tag modality\n",
    "rna_tmp = rna_test_adata.copy()\n",
    "adt_tmp = adt_test_adata.copy()\n",
    "rna_tmp.obs[\"modality\"] = \"rna\"\n",
    "adt_tmp.obs[\"modality\"] = \"adt\"\n",
    "\n",
    "# Concatenate\n",
    "combined = rna_tmp.concatenate(\n",
    "    adt_tmp,\n",
    "    join=\"outer\",\n",
    "    batch_key=\"concat_batch\",\n",
    "    batch_categories=[\"rna\", \"adt\"],\n",
    "    index_unique=None,\n",
    ")\n",
    "\n",
    "# Ensure stacked X_univi matches the concat order\n",
    "combined.obsm[\"X_univi\"] = np.vstack([\n",
    "    rna_test_adata.obsm[\"X_univi\"],\n",
    "    adt_test_adata.obsm[\"X_univi\"],\n",
    "])\n",
    "\n",
    "# Neighbors/UMAP on UniVI embedding\n",
    "sc.pp.neighbors(combined, use_rep=\"X_univi\", n_neighbors=30)\n",
    "sc.tl.umap(combined)\n",
    "\n",
    "# Show UMAP with two panels: modality and celltype.l2\n",
    "sc.pl.umap(\n",
    "    combined,\n",
    "    color=[\"modality\", \"celltype.l2\"],\n",
    "    wspace=0.4,\n",
    "    size=10,\n",
    "    alpha=0.7,\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "# Optional: save figure via scanpy\n",
    "# sc.pl.umap(\n",
    "#     combined,\n",
    "#     color=[\"modality\", \"celltype.l2\"],\n",
    "#     wspace=0.4,\n",
    "#     size=10,\n",
    "#     alpha=0.7,\n",
    "#     save=\"_citeseq_univi_umap.png\",\n",
    "# )\n",
    "\n",
    "# -------------------------\n",
    "# 6. Optional: cross-modal reconstruction metrics (RNA → ADT)\n",
    "# -------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_rna = rna_test_adata.X\n",
    "    if sp.issparse(X_rna):\n",
    "        X_rna = X_rna.toarray()\n",
    "    X_rna_t = torch.as_tensor(X_rna, dtype=torch.float32, device=trainer.device)\n",
    "\n",
    "    xhat_adt_list = []\n",
    "    batch_size = 512\n",
    "    for start in range(0, X_rna_t.shape[0], batch_size):\n",
    "        xb = X_rna_t[start:start + batch_size]\n",
    "        mu_dict, logvar_dict = model.encode_modalities({\"rna\": xb})\n",
    "        mu_z, logvar_z = model.mixture_of_experts(mu_dict, logvar_dict)\n",
    "        xhat_dict = model.decode_modalities(mu_z)\n",
    "        xhat_adt_list.append(xhat_dict[\"adt\"].cpu().numpy())\n",
    "\n",
    "    xhat_adt = np.vstack(xhat_adt_list)\n",
    "\n",
    "# compare to observed ADT\n",
    "X_adt = adt_test_adata.X\n",
    "if sp.issparse(X_adt):\n",
    "    X_adt = X_adt.toarray()\n",
    "\n",
    "mse_feat = univi_eval.mse_per_feature(X_adt, xhat_adt)\n",
    "corr_feat = univi_eval.pearson_corr_per_feature(X_adt, xhat_adt)\n",
    "\n",
    "print(f\"Mean ADT MSE (RNA→ADT): {mse_feat.mean():.4f}\")\n",
    "print(f\"Mean ADT Pearson r (RNA→ADT): {corr_feat.mean():.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6a. Histogram of feature-wise Pearson r\n",
    "# -------------------------\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(corr_feat, bins=30)\n",
    "plt.xlabel(\"Pearson r (per ADT feature)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"RNA→ADT reconstruction: feature-wise correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db147ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode new cells to latent embeddings\n",
    "z_rna = univi_eval.encode_adata(model, rna_adata, modality=\"rna\", device=train_cfg.device)\n",
    "z_adt = univi_eval.encode_adata(model, adt_adata, modality=\"adt\", device=train_cfg.device)\n",
    "\n",
    "rna_adata.obsm[\"X_univi\"] = z_rna\n",
    "adt_adata.obsm[\"X_univi\"] = z_adt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross modal generation from one modality to another: RNA -> ADT example\n",
    "Xhat_adt_from_rna = univi_eval.cross_modal_predict(\n",
    "    model,\n",
    "    adata_src=rna_adata,\n",
    "    src_mod=\"rna\",\n",
    "    tgt_mod=\"adt\",\n",
    "    device=train_cfg.device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising using the decoders\n",
    "univi_eval.denoise_adata(model, rna_adata, modality=\"rna\", device=train_cfg.device)\n",
    "univi_eval.denoise_adata(model, adt_adata, modality=\"adt\", device=train_cfg.device)\n",
    "\n",
    "# you now have rna_adata.layers[\"univi_denoised\"] etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from the latent space per cell type\n",
    "\n",
    "# fit per-celltype Gaussians in latent space\n",
    "labels_rna = rna_adata.obs[\"celltype.l2\"].astype(str).values\n",
    "Z_rna = rna_adata.obsm[\"X_univi\"]  # from encode_adata earlier\n",
    "\n",
    "gauss_by_ct = univi_eval.fit_latent_gaussians_by_label(Z_rna, labels_rna)\n",
    "\n",
    "# define how many samples per cell type\n",
    "spec = {\n",
    "    \"CD4 Naive T\": 1000,\n",
    "    \"CD8 Effector T\": 1000,\n",
    "    \"Memory B\": 500,\n",
    "}\n",
    "\n",
    "z_samp_by_ct = univi_eval.sample_from_latent_gaussians(gauss_by_ct, spec, random_state=42)\n",
    "\n",
    "# decode to desired modality\n",
    "def decode_latent_samples(model, z_samp_by_ct, modality: str, device: str = \"cpu\"):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    with torch.no_grad():\n",
    "        for lab, Z in z_samp_by_ct.items():\n",
    "            z_t = torch.as_tensor(Z, dtype=torch.float32, device=device)\n",
    "            xhat_dict = model.decode_modalities(z_t)\n",
    "            out[lab] = xhat_dict[modality].cpu().numpy()\n",
    "    return out\n",
    "\n",
    "synthetic_adt_by_ct = decode_latent_samples(model, z_samp_by_ct, modality=\"adt\", device=train_cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33fe662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e38040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifold_alignment",
   "language": "python",
   "name": "manifold_alignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
