{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f083334",
   "metadata": {},
   "source": [
    "# UniVI – Figure 9 analysis (robust v1, no leakage)\n",
    "\n",
    "This notebook builds the Figure 8 missing-modality / overlap curve **without train–test leakage** and with **loss_mode='v1'**.\n",
    "\n",
    "Key idea for v1 training with missing modalities. We enforce mini-batches with paired and mini-batches with unimodal data with a deterministic mask + grouped batch sampler. The unimodal data VAEs are treated as unimodal VAEs during training with only self-reconstruction and the beta-weighted prior KL divergence term (no KL-align term included in these mini-batches).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 0) Imports / versions ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Mapping, Tuple, List, Sequence\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, Sampler\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# UniVI\n",
    "import univi as uv\n",
    "from univi import UniVIMultiModalVAE, ModalityConfig, UniVIConfig, TrainingConfig\n",
    "from univi.trainer import UniVITrainer\n",
    "from univi.data import MultiModalDataset, align_paired_obs_names\n",
    "\n",
    "print(\"torch:\", \"v\" + torch.__version__)\n",
    "print(\"scanpy:\", \"v\" + sc.__version__)\n",
    "print(\"univi:\", \"v\" + uv.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f61c4-5591-4b9f-ae1c-2b5895cd023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"figure.dpi\"] = 300        # inline display dpi\n",
    "mpl.rcParams[\"savefig.dpi\"] = 300       # default save dpi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd37c08",
   "metadata": {},
   "source": [
    "## 1) Preprocessing without leakage\n",
    "\n",
    "We fit RNA HVGs + normalization and ATAC TF-IDF/LSI on the training split only, then apply the exact transforms to val/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632074dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "@dataclass\n",
    "class RNAFit:\n",
    "    hvg: List[str]\n",
    "    counts_layer: str = \"counts\"\n",
    "    target_sum: float = 1e4\n",
    "    out_layer: str = \"log1p\"\n",
    "\n",
    "@dataclass\n",
    "class ATACFit:\n",
    "    counts_layer: str = \"counts\"\n",
    "    tfidf: TfidfTransformer = None\n",
    "    svd: TruncatedSVD = None\n",
    "    scaler: StandardScaler = None\n",
    "    drop_first: bool = True\n",
    "\n",
    "def _ensure_layer(adata: ad.AnnData, layer: str):\n",
    "    if layer not in adata.layers:\n",
    "        adata.layers[layer] = adata.X.copy()\n",
    "\n",
    "def fit_rna_on_train(\n",
    "    rna_train: ad.AnnData,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    n_hvg: int = 2000,\n",
    "    target_sum: float = 1e4,\n",
    "    out_layer: str = \"log1p\",\n",
    "    hvg_flavor: str = \"seurat_v3\",\n",
    ") -> RNAFit:\n",
    "    rna_train = rna_train.copy()\n",
    "    _ensure_layer(rna_train, counts_layer)\n",
    "\n",
    "    tmp = rna_train.copy()\n",
    "    tmp.X = tmp.layers[counts_layer]\n",
    "    try:\n",
    "        sc.pp.highly_variable_genes(tmp, n_top_genes=int(n_hvg), flavor=hvg_flavor)\n",
    "    except Exception:\n",
    "        # fallback that doesn't require scikit-misc\n",
    "        sc.pp.highly_variable_genes(tmp, n_top_genes=int(n_hvg), flavor=\"cell_ranger\")\n",
    "\n",
    "    hvg = tmp.var_names[tmp.var[\"highly_variable\"].to_numpy()].tolist()\n",
    "    return RNAFit(hvg=hvg, counts_layer=counts_layer, target_sum=float(target_sum), out_layer=str(out_layer))\n",
    "\n",
    "def apply_rna_fit(rna: ad.AnnData, fit: RNAFit) -> ad.AnnData:\n",
    "    _ensure_layer(rna, fit.counts_layer)\n",
    "    a = rna[:, fit.hvg].copy()\n",
    "    a.layers[fit.out_layer] = a.layers[fit.counts_layer].copy()\n",
    "    sc.pp.normalize_total(a, target_sum=float(fit.target_sum), layer=fit.out_layer)\n",
    "    sc.pp.log1p(a, layer=fit.out_layer)\n",
    "    a.X = a.layers[fit.out_layer]\n",
    "    return a\n",
    "\n",
    "def fit_atac_on_train(\n",
    "    atac_train: ad.AnnData,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    n_lsi: int = 50,\n",
    "    seed: int = 0,\n",
    "    tfidf_norm: str = \"l2\",\n",
    "    smooth_idf: bool = True,\n",
    "    sublinear_tf: bool = False,\n",
    "    drop_first: bool = True,\n",
    ") -> ATACFit:\n",
    "    atac_train = atac_train.copy()\n",
    "    _ensure_layer(atac_train, counts_layer)\n",
    "    X = atac_train.layers[counts_layer]\n",
    "    if not sp.issparse(X):\n",
    "        X = sp.csr_matrix(X)\n",
    "\n",
    "    tfidf = TfidfTransformer(norm=tfidf_norm, use_idf=True, smooth_idf=bool(smooth_idf), sublinear_tf=bool(sublinear_tf))\n",
    "    Xt = tfidf.fit_transform(X)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=int(n_lsi), random_state=int(seed))\n",
    "    Z = svd.fit_transform(Xt)\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    scaler.fit(Z)\n",
    "\n",
    "    return ATACFit(counts_layer=counts_layer, tfidf=tfidf, svd=svd, scaler=scaler, drop_first=bool(drop_first))\n",
    "\n",
    "def apply_atac_fit(atac: ad.AnnData, fit: ATACFit) -> ad.AnnData:\n",
    "    _ensure_layer(atac, fit.counts_layer)\n",
    "    X = atac.layers[fit.counts_layer]\n",
    "    if not sp.issparse(X):\n",
    "        X = sp.csr_matrix(X)\n",
    "\n",
    "    Xt = fit.tfidf.transform(X)\n",
    "    Z  = fit.svd.transform(Xt)\n",
    "    Z  = fit.scaler.transform(Z).astype(np.float32, copy=False)\n",
    "\n",
    "    if fit.drop_first and Z.shape[1] >= 2:\n",
    "        Z = Z[:, 1:]\n",
    "\n",
    "    return ad.AnnData(\n",
    "        X=Z,\n",
    "        obs=atac.obs.copy(),\n",
    "        var=pd.DataFrame(index=[f\"LSI_{i}\" for i in range(1, Z.shape[1] + 1)]),\n",
    "    )\n",
    "\n",
    "def preprocess_multiome_splits_fit_apply(\n",
    "    rna_train: ad.AnnData, atac_train: ad.AnnData,\n",
    "    rna_val: ad.AnnData,   atac_val: ad.AnnData,\n",
    "    rna_test: ad.AnnData,  atac_test: ad.AnnData,\n",
    "    *,\n",
    "    rna_counts_layer: str = \"counts\",\n",
    "    atac_counts_layer: str = \"counts\",\n",
    "    n_hvg: int = 2000,\n",
    "    target_sum: float = 1e4,\n",
    "    n_lsi: int = 50,\n",
    "    seed: int = 0,\n",
    ") -> Tuple[ad.AnnData, ad.AnnData, ad.AnnData, ad.AnnData, ad.AnnData, ad.AnnData, RNAFit, ATACFit]:\n",
    "\n",
    "    for a in (rna_train, rna_val, rna_test):\n",
    "        _ensure_layer(a, rna_counts_layer)\n",
    "    for a in (atac_train, atac_val, atac_test):\n",
    "        _ensure_layer(a, atac_counts_layer)\n",
    "\n",
    "    rna_fit = fit_rna_on_train(rna_train, counts_layer=rna_counts_layer, n_hvg=n_hvg, target_sum=target_sum)\n",
    "    atac_fit = fit_atac_on_train(atac_train, counts_layer=atac_counts_layer, n_lsi=n_lsi, seed=seed)\n",
    "\n",
    "    rna_tr_pp = apply_rna_fit(rna_train, rna_fit)\n",
    "    rna_va_pp = apply_rna_fit(rna_val,   rna_fit)\n",
    "    rna_te_pp = apply_rna_fit(rna_test,  rna_fit)\n",
    "\n",
    "    atac_tr_lsi = apply_atac_fit(atac_train, atac_fit)\n",
    "    atac_va_lsi = apply_atac_fit(atac_val,   atac_fit)\n",
    "    atac_te_lsi = apply_atac_fit(atac_test,  atac_fit)\n",
    "\n",
    "    return (rna_tr_pp, atac_tr_lsi,\n",
    "            rna_va_pp, atac_va_lsi,\n",
    "            rna_te_pp, atac_te_lsi,\n",
    "            rna_fit, atac_fit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0896365",
   "metadata": {},
   "source": [
    "### Provide your splits\n",
    "\n",
    "Define `rna_train/val/test` and `atac_train/val/test` (paired and in the same order within each split), then run preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514bf90-3ef2-42d7-b3fd-0d0deb2fb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_raw = sc.read_h5ad(\"./data/10x_Genomics_Multiome_data/10x-Multiome-Pbmc10k-RNA.h5ad\")\n",
    "atac_raw = sc.read_h5ad(\"./data/10x_Genomics_Multiome_data/10x-Multiome-Pbmc10k-ATAC.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7a7de-355d-4330-8a61-338cc61c80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_raw.layers['counts'] = rna_raw.X\n",
    "atac_raw.layers['counts'] = atac_raw.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d34686-12b7-4a3d-b3fa-d418a1a2ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from univi.data import align_paired_obs_names\n",
    "\n",
    "adata_dict_raw = {\"rna\": rna_raw, \"atac\": atac_raw}\n",
    "adata_dict_raw = align_paired_obs_names(adata_dict_raw)\n",
    "\n",
    "rna_raw = adata_dict_raw[\"rna\"]\n",
    "atac_raw = adata_dict_raw[\"atac\"]\n",
    "\n",
    "print(rna_raw.n_obs, atac_raw.n_obs)\n",
    "assert (rna_raw.obs_names == atac_raw.obs_names).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87892a-7a60-4df4-b252-296b7ce858b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Sanity checks ----\n",
    "print(rna_raw)\n",
    "print(\"rna X min/max:\", float(rna_raw.X.min()), float(rna_raw.X.max()))\n",
    "print(atac_raw)\n",
    "print(\"atac X min/max:\", float(atac_raw.X.min()), float(atac_raw.X.max()))\n",
    "\n",
    "# Recommended: store raw counts in .layers['counts'] if not already there.\n",
    "def ensure_counts_layer(adata: ad.AnnData, layer: str = \"counts\") -> None:\n",
    "    if layer not in adata.layers:\n",
    "        adata.layers[layer] = adata.X.copy()\n",
    "\n",
    "ensure_counts_layer(rna_raw, \"counts\")\n",
    "ensure_counts_layer(atac_raw, \"counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80754e4-ddc3-48ee-989c-7a901220dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2) Create splits (skip if you already have rna_train/val/test etc) ----\n",
    "if \"rna_train\" not in globals():\n",
    "    rng = np.random.default_rng(0)\n",
    "    n = rna_raw.n_obs\n",
    "    idx = np.arange(n)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n_tr = int(0.8 * n)\n",
    "    n_va = int(0.1 * n)\n",
    "    tr_idx = idx[:n_tr]\n",
    "    va_idx = idx[n_tr:n_tr+n_va]\n",
    "    te_idx = idx[n_tr+n_va:]\n",
    "\n",
    "    rna_train, rna_val, rna_test = rna_raw[tr_idx].copy(), rna_raw[va_idx].copy(), rna_raw[te_idx].copy()\n",
    "    atac_train, atac_val, atac_test = atac_raw[tr_idx].copy(), atac_raw[va_idx].copy(), atac_raw[te_idx].copy()\n",
    "\n",
    "print(\"splits:\", rna_train.n_obs, rna_val.n_obs, rna_test.n_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130a06d-16ad-4100-8a70-e75698ad361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3) Preprocessing: fit on train only, apply to splits ----\n",
    "\n",
    "@dataclass\n",
    "class RNAFit:\n",
    "    hvg: List[str]\n",
    "    counts_layer: str = \"counts\"\n",
    "    target_sum: float = 1e4\n",
    "    out_layer: str = \"log1p\"\n",
    "\n",
    "@dataclass\n",
    "class ATACFit:\n",
    "    counts_layer: str = \"counts\"\n",
    "    tfidf: Any = None\n",
    "    svd: Any = None\n",
    "    scaler: Any = None\n",
    "    n_lsi: int = 101\n",
    "    drop_first: bool = True\n",
    "\n",
    "def _to_csr(X):\n",
    "    return X if sp.issparse(X) else sp.csr_matrix(X)\n",
    "\n",
    "def fit_rna_on_train(\n",
    "    rna_train: ad.AnnData,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    n_hvg: int = 2000,\n",
    "    target_sum: float = 1e4,\n",
    "    out_layer: str = \"log1p\",\n",
    ") -> RNAFit:\n",
    "    rna = rna_train.copy()\n",
    "    if counts_layer not in rna.layers:\n",
    "        rna.layers[counts_layer] = rna.X.copy()\n",
    "\n",
    "    # Build a stable working layer for HVG selection\n",
    "    rna.layers[out_layer] = _to_csr(rna.layers[counts_layer]).copy()\n",
    "    sc.pp.normalize_total(rna, target_sum=float(target_sum), layer=out_layer)\n",
    "    sc.pp.log1p(rna, layer=out_layer)\n",
    "\n",
    "    # Prefer seurat_v3 if available; fall back cleanly if skmisc missing.\n",
    "    tried = []\n",
    "    for flavor in [\"seurat_v3\", \"cell_ranger\", \"seurat\"]:\n",
    "        try:\n",
    "            tried.append(flavor)\n",
    "            sc.pp.highly_variable_genes(rna, n_top_genes=int(n_hvg), flavor=flavor, layer=out_layer)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "            continue\n",
    "    else:\n",
    "        raise RuntimeError(f\"HVG selection failed. Tried: {tried}. Last error: {last}\")\n",
    "\n",
    "    hvg = rna.var_names[rna.var[\"highly_variable\"].to_numpy()].tolist()\n",
    "    if len(hvg) == 0:\n",
    "        raise RuntimeError(\"No HVGs selected; check that RNA layer values are sane.\")\n",
    "    return RNAFit(hvg=hvg, counts_layer=counts_layer, target_sum=float(target_sum), out_layer=out_layer)\n",
    "\n",
    "def apply_rna_fit(rna: ad.AnnData, fit: RNAFit) -> ad.AnnData:\n",
    "    a = rna[:, fit.hvg].copy()\n",
    "    if fit.counts_layer not in a.layers:\n",
    "        a.layers[fit.counts_layer] = a.X.copy()\n",
    "\n",
    "    a.layers[fit.out_layer] = _to_csr(a.layers[fit.counts_layer]).copy()\n",
    "    sc.pp.normalize_total(a, target_sum=float(fit.target_sum), layer=fit.out_layer)\n",
    "    sc.pp.log1p(a, layer=fit.out_layer)\n",
    "\n",
    "    a.X = a.layers[fit.out_layer].copy()\n",
    "    return a\n",
    "\n",
    "def fit_atac_on_train(\n",
    "    atac_train: ad.AnnData,\n",
    "    *,\n",
    "    counts_layer: str = \"counts\",\n",
    "    n_lsi: int = 50,\n",
    "    seed: int = 0,\n",
    ") -> ATACFit:\n",
    "    a = atac_train.copy()\n",
    "    if counts_layer not in a.layers:\n",
    "        a.layers[counts_layer] = a.X.copy()\n",
    "\n",
    "    X = _to_csr(a.layers[counts_layer])\n",
    "\n",
    "    tfidf = TfidfTransformer(norm=\"l2\", use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "    X_t = tfidf.fit_transform(X)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=int(n_lsi), random_state=int(seed))\n",
    "    Z = svd.fit_transform(X_t)\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    Zs = scaler.fit_transform(Z).astype(np.float32, copy=False)\n",
    "\n",
    "    return ATACFit(counts_layer=counts_layer, tfidf=tfidf, svd=svd, scaler=scaler, n_lsi=int(n_lsi), drop_first=True)\n",
    "\n",
    "def apply_atac_fit(atac: ad.AnnData, fit: ATACFit) -> ad.AnnData:\n",
    "    a = atac.copy()\n",
    "    if fit.counts_layer not in a.layers:\n",
    "        a.layers[fit.counts_layer] = a.X.copy()\n",
    "\n",
    "    X = _to_csr(a.layers[fit.counts_layer])\n",
    "    Xt = fit.tfidf.transform(X)\n",
    "    Z = fit.svd.transform(Xt)\n",
    "    Z = fit.scaler.transform(Z).astype(np.float32, copy=False)\n",
    "\n",
    "    if fit.drop_first and Z.shape[1] >= 2:\n",
    "        Z = Z[:, 1:]\n",
    "\n",
    "    atac_lsi = ad.AnnData(\n",
    "        X=Z,\n",
    "        obs=a.obs.copy(),\n",
    "        var=pd.DataFrame(index=[f\"LSI_{i}\" for i in range(1, Z.shape[1] + 1)]),\n",
    "    )\n",
    "    return atac_lsi\n",
    "\n",
    "def preprocess_multiome_splits_fit_apply(\n",
    "    rna_train: ad.AnnData, atac_train: ad.AnnData,\n",
    "    rna_val: ad.AnnData,   atac_val: ad.AnnData,\n",
    "    rna_test: ad.AnnData,  atac_test: ad.AnnData,\n",
    "    *,\n",
    "    rna_counts_layer=\"counts\",\n",
    "    atac_counts_layer=\"counts\",\n",
    "    n_hvg=2000,\n",
    "    target_sum=1e4,\n",
    "    n_lsi=101,\n",
    "    seed=0,\n",
    "):\n",
    "    rna_fit = fit_rna_on_train(\n",
    "        rna_train, counts_layer=rna_counts_layer, n_hvg=n_hvg, target_sum=target_sum\n",
    "    )\n",
    "    atac_fit = fit_atac_on_train(\n",
    "        atac_train, counts_layer=atac_counts_layer, n_lsi=n_lsi, seed=seed\n",
    "    )\n",
    "\n",
    "    rna_tr = apply_rna_fit(rna_train, rna_fit)\n",
    "    rna_va = apply_rna_fit(rna_val, rna_fit)\n",
    "    rna_te = apply_rna_fit(rna_test, rna_fit)\n",
    "\n",
    "    atac_tr = apply_atac_fit(atac_train, atac_fit)\n",
    "    atac_va = apply_atac_fit(atac_val, atac_fit)\n",
    "    atac_te = apply_atac_fit(atac_test, atac_fit)\n",
    "\n",
    "    return (rna_tr, atac_tr, rna_va, atac_va, rna_te, atac_te, rna_fit, atac_fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737edb3-613e-4947-988c-b1d63092ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- run preprocessing ----\n",
    "(rna_tr_pp, atac_tr_lsi,\n",
    " rna_va_pp, atac_va_lsi,\n",
    " rna_te_pp, atac_te_lsi,\n",
    " rna_fit, atac_fit) = preprocess_multiome_splits_fit_apply(\n",
    "    rna_train, atac_train,\n",
    "    rna_val, atac_val,\n",
    "    rna_test, atac_test,\n",
    "    rna_counts_layer=\"counts\",\n",
    "    atac_counts_layer=\"counts\",\n",
    "    n_hvg=2000,\n",
    "    target_sum=1e4,\n",
    "    n_lsi=101,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"RNA HVG:\", rna_tr_pp.n_vars, \"ATAC LSI dims:\", atac_tr_lsi.n_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd6207",
   "metadata": {},
   "source": [
    "## 2) Build paired dataset + indices + labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a843df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_dict = {\n",
    "    \"rna\":  rna_tr_pp.concatenate(rna_va_pp, rna_te_pp, batch_key=None),\n",
    "    \"atac\": atac_tr_lsi.concatenate(atac_va_lsi, atac_te_lsi, batch_key=None),\n",
    "}\n",
    "adata_dict = align_paired_obs_names(adata_dict)\n",
    "\n",
    "dataset = MultiModalDataset(adata_dict=adata_dict, X_key=\"X\", device=None)\n",
    "\n",
    "n_tr = rna_tr_pp.n_obs\n",
    "n_va = rna_va_pp.n_obs\n",
    "n_te = rna_te_pp.n_obs\n",
    "\n",
    "train_idx = np.arange(0, n_tr)\n",
    "val_idx   = np.arange(n_tr, n_tr + n_va)\n",
    "test_idx  = np.arange(n_tr + n_va, n_tr + n_va + n_te)\n",
    "\n",
    "label_key = \"cell_type\"  # change if needed\n",
    "y_all = adata_dict[\"rna\"].obs[label_key].to_numpy()\n",
    "\n",
    "print(\"dataset n:\", len(dataset), \"train/val/test:\", n_tr, n_va, n_te)\n",
    "print(\"labels:\", pd.Series(y_all).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8a192-2419-49e5-b56c-df893e2f2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# --- 1) make integer labels (stable mapping) ---\n",
    "label_key = \"cell_type\"\n",
    "y_str = adata_dict[\"rna\"].obs[label_key].astype(str).to_numpy()\n",
    "\n",
    "uniq = np.unique(y_str)\n",
    "label_to_int = {lab: i for i, lab in enumerate(uniq)}\n",
    "y_int = np.array([label_to_int[x] for x in y_str], dtype=np.int64)\n",
    "\n",
    "print(\"n labels:\", len(uniq))\n",
    "print(\"example mapping:\", list(label_to_int.items())[:5])\n",
    "\n",
    "# --- 2) wrapper dataset that injects labels into each sample dict ---\n",
    "class LabeledMultiModalDataset(Dataset):\n",
    "    def __init__(self, base_ds, y_int, y_str=None):\n",
    "        self.base = base_ds\n",
    "        self.y_int = np.asarray(y_int, dtype=np.int64)\n",
    "        self.y_str = None if y_str is None else np.asarray(y_str, dtype=object)\n",
    "        if len(self.base) != len(self.y_int):\n",
    "            raise ValueError(f\"len(base)={len(self.base)} != len(y_int)={len(self.y_int)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.base[i]\n",
    "        # base is expected to yield dict-like\n",
    "        if not isinstance(x, dict):\n",
    "            # if it yields (dict, ...) take first\n",
    "            if isinstance(x, (tuple, list)) and len(x) > 0 and isinstance(x[0], dict):\n",
    "                x = x[0]\n",
    "            else:\n",
    "                raise TypeError(f\"Expected dict or (dict,...); got {type(x)}\")\n",
    "\n",
    "        x = dict(x)\n",
    "        x[\"y\"] = torch.tensor(int(self.y_int[i]), dtype=torch.long)\n",
    "        if self.y_str is not None:\n",
    "            x[\"cell_type\"] = self.y_str[i]  # NOTE: strings won't collate unless you handle them\n",
    "        return x\n",
    "\n",
    "# --- 3) replace dataset with labeled version (use y_int only for safety) ---\n",
    "dataset = LabeledMultiModalDataset(dataset, y_int=y_int, y_str=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe9432",
   "metadata": {},
   "source": [
    "## 3) v1-safe loaders for missing modalities\n",
    "\n",
    "In v1, a batch cannot mix RNA-only and ATAC-only items (or you hit the `stack expects each tensor to be equal size` crash). We fix this by:\n",
    "\n",
    "1. Creating a **deterministic** per-item group assignment for the training split (paired vs unimodal).\n",
    "2. Using a **grouped batch sampler** so each batch contains items from a single group.\n",
    "\n",
    "Validation and test loaders remain fully paired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset wrappers\n",
    "# -----------------------------\n",
    "class IndexedDataset(Dataset):\n",
    "    \"\"\"Wrap base_dataset + an index list, and return (x_dict, global_id).\"\"\"\n",
    "    def __init__(self, base_dataset, indices):\n",
    "        self.base = base_dataset\n",
    "        self.indices = np.asarray(indices, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.indices.shape[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        gi = int(self.indices[int(i)])\n",
    "        item = self.base[gi]\n",
    "\n",
    "        if isinstance(item, dict):\n",
    "            x = item\n",
    "        elif isinstance(item, (tuple, list)) and len(item) >= 1 and isinstance(item[0], dict):\n",
    "            x = item[0]\n",
    "        else:\n",
    "            raise TypeError(f\"Expected base_dataset to yield dict or (dict, ...), got {type(item)}\")\n",
    "\n",
    "        return dict(x), gi\n",
    "\n",
    "\n",
    "class DeterministicMaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Mask exactly one modality for non-anchor rows based on groups.\n",
    "      groups==0: paired anchor (keep both)\n",
    "      groups!=0: unpaired (DROP drop_modality key)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds: Dataset, groups: np.ndarray, *, drop_modality: str):\n",
    "        self.base = base_ds\n",
    "        self.groups = np.asarray(groups, dtype=np.int64)\n",
    "        self.drop_modality = str(drop_modality)\n",
    "        if len(self.base) != len(self.groups):\n",
    "            raise ValueError(f\"base_ds len={len(self.base)} != groups len={len(self.groups)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, gid = self.base[i]\n",
    "        g = int(self.groups[int(i)])\n",
    "\n",
    "        x = dict(x)\n",
    "        if g != 0:\n",
    "            x.pop(self.drop_modality, None)  # IMPORTANT: remove key, don't set None\n",
    "        return x, gid\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Collate: homogeneous key-set guaranteed by sampler + mask dataset\n",
    "# -----------------------------\n",
    "def collate_xdict_with_idx(batch):\n",
    "    \"\"\"Collate list[(x_dict, gid)] into (dict[str, Tensor], gids Tensor).\"\"\"\n",
    "    if len(batch) == 0:\n",
    "        raise ValueError(\"Empty batch\")\n",
    "\n",
    "    gids = torch.as_tensor([b[1] for b in batch], dtype=torch.long)\n",
    "\n",
    "    keys0 = set(batch[0][0].keys())\n",
    "    for x, _ in batch[1:]:\n",
    "        if set(x.keys()) != keys0:\n",
    "            raise RuntimeError(\n",
    "                \"Non-homogeneous batch detected (mixed modality presence within a batch). \"\n",
    "                \"This breaks UniVI v1 MoE assumptions. Fix sampler/grouping.\"\n",
    "            )\n",
    "\n",
    "    out = {}\n",
    "    for k in keys0:\n",
    "        out[k] = torch.stack([x[k] for x, _ in batch], dim=0)\n",
    "\n",
    "    return out, gids\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Sampler: paired-driven homogeneous batches with unpaired interleaving\n",
    "# -----------------------------\n",
    "class InterleavedGroupedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Paired-driven homogeneous batch sampler (with safe tail behavior).\n",
    "\n",
    "    Backbone: yields paired (group==0) batches.\n",
    "    Interleave: yields unpaired (group!=0) batches such that the expected number of\n",
    "    unpaired batches per paired batch ~= unpaired_per_paired.\n",
    "\n",
    "    Always yields homogeneous batches (all paired OR all unpaired).\n",
    "\n",
    "    Tail fix:\n",
    "      - If paired_idx < batch_size, we DO NOT error.\n",
    "        Instead, if oversample_paired=True, we run with n_paired_batches=1 and\n",
    "        generate paired batches by sampling WITH replacement.\n",
    "      - If oversample_paired=False and paired_idx < batch_size, we fall back to\n",
    "        yielding only unpaired batches (still homogeneous). This keeps things running,\n",
    "        but you lose paired anchors in that extreme.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        groups: np.ndarray,\n",
    "        batch_size: int,\n",
    "        *,\n",
    "        seed: int = 0,\n",
    "        drop_last: bool = True,\n",
    "        unpaired_per_paired: float = 1.0,\n",
    "        oversample_paired: bool = True,\n",
    "        oversample_unpaired: bool = True,\n",
    "    ):\n",
    "        self.groups = np.asarray(groups, dtype=np.int64)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.seed = int(seed)\n",
    "        self.drop_last = bool(drop_last)\n",
    "\n",
    "        upp = float(unpaired_per_paired)\n",
    "        if not np.isfinite(upp) or upp < 0:\n",
    "            raise ValueError(f\"unpaired_per_paired must be finite and >=0, got {unpaired_per_paired}\")\n",
    "        self.unpaired_per_paired = upp\n",
    "\n",
    "        self.oversample_paired = bool(oversample_paired)\n",
    "        self.oversample_unpaired = bool(oversample_unpaired)\n",
    "\n",
    "        self.paired_idx = np.where(self.groups == 0)[0].astype(np.int64)\n",
    "        self.unpaired_idx = np.where(self.groups != 0)[0].astype(np.int64)\n",
    "\n",
    "        if len(self.paired_idx) == 0:\n",
    "            raise ValueError(\"No paired samples (group==0). Need at least one anchor sample.\")\n",
    "\n",
    "        self.only_paired = (len(self.unpaired_idx) == 0)\n",
    "\n",
    "        # Can we make at least one full paired batch without replacement?\n",
    "        self.can_full_paired = (len(self.paired_idx) >= self.batch_size)\n",
    "\n",
    "        # Paired-driven epoch length:\n",
    "        # - normal: floor(P / B)\n",
    "        # - tail: if P < B and oversample_paired=True => force 1 paired batch (replacement)\n",
    "        # - tail: if P < B and oversample_paired=False => 0 paired batches (we'll yield only unpaired)\n",
    "        if self.can_full_paired:\n",
    "            self.n_paired_batches = len(self.paired_idx) // self.batch_size\n",
    "        else:\n",
    "            self.n_paired_batches = 1 if self.oversample_paired else 0\n",
    "\n",
    "        if self.only_paired and self.n_paired_batches <= 0:\n",
    "            # Only paired data exists but too small and oversample_paired=False => cannot proceed.\n",
    "            raise ValueError(\n",
    "                \"Paired-only training split has fewer samples than batch_size and oversample_paired=False.\"\n",
    "            )\n",
    "\n",
    "        # Expected total batches (for progress bars / DataLoader length)\n",
    "        if self.only_paired:\n",
    "            self.n_unpaired_expected = 0\n",
    "        else:\n",
    "            self.n_unpaired_expected = int(round(self.n_paired_batches * self.unpaired_per_paired))\n",
    "\n",
    "        # If we have no paired batches (tail + oversample_paired=False), define length from unpaired\n",
    "        if self.n_paired_batches == 0:\n",
    "            # define an epoch as all full unpaired batches if possible; else 1 replacement batch if allowed\n",
    "            if len(self.unpaired_idx) >= self.batch_size:\n",
    "                self.n_unpaired_backbone = len(self.unpaired_idx) // self.batch_size\n",
    "            else:\n",
    "                self.n_unpaired_backbone = 1 if self.oversample_unpaired else 0\n",
    "\n",
    "            if self.n_unpaired_backbone <= 0:\n",
    "                raise ValueError(\"Not enough samples to form even one batch at this batch_size.\")\n",
    "\n",
    "            self.n_batches = int(self.n_unpaired_backbone)\n",
    "        else:\n",
    "            self.n_batches = int(self.n_paired_batches + self.n_unpaired_expected)\n",
    "\n",
    "        self.n_batches = max(int(self.n_batches), 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.n_batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "\n",
    "        paired_pool = self.paired_idx.copy()\n",
    "        rng.shuffle(paired_pool)\n",
    "        p_ptr = 0\n",
    "\n",
    "        unpaired_pool = self.unpaired_idx.copy()\n",
    "        if len(unpaired_pool) > 0:\n",
    "            rng.shuffle(unpaired_pool)\n",
    "        u_ptr = 0\n",
    "\n",
    "        def pop_full_no_wrap(pool, ptr):\n",
    "            end = ptr + self.batch_size\n",
    "            if end <= len(pool):\n",
    "                return pool[ptr:end], end\n",
    "            return None, ptr\n",
    "\n",
    "        def pop_full_with_replacement(pool):\n",
    "            if len(pool) == 0:\n",
    "                return None\n",
    "            idx = rng.integers(0, len(pool), size=self.batch_size)\n",
    "            return pool[idx]\n",
    "\n",
    "        def next_paired_batch():\n",
    "            nonlocal p_ptr, paired_pool\n",
    "            # If pool is smaller than batch_size, only replacement sampling can work\n",
    "            if len(paired_pool) < self.batch_size:\n",
    "                return pop_full_with_replacement(paired_pool) if self.oversample_paired else None\n",
    "\n",
    "            pb, p_ptr2 = pop_full_no_wrap(paired_pool, p_ptr)\n",
    "            if pb is None:\n",
    "                if not self.oversample_paired:\n",
    "                    return None\n",
    "                rng.shuffle(paired_pool)\n",
    "                p_ptr = 0\n",
    "                pb, p_ptr2 = pop_full_no_wrap(paired_pool, p_ptr)\n",
    "                if pb is None:\n",
    "                    return None\n",
    "            p_ptr = p_ptr2\n",
    "            return pb\n",
    "\n",
    "        def next_unpaired_batch():\n",
    "            nonlocal u_ptr, unpaired_pool\n",
    "            # If pool is smaller than batch_size, only replacement sampling can work\n",
    "            if len(unpaired_pool) < self.batch_size:\n",
    "                return pop_full_with_replacement(unpaired_pool) if self.oversample_unpaired else None\n",
    "\n",
    "            ub, u_ptr2 = pop_full_no_wrap(unpaired_pool, u_ptr)\n",
    "            if ub is not None:\n",
    "                u_ptr = u_ptr2\n",
    "                return ub\n",
    "\n",
    "            if not self.oversample_unpaired:\n",
    "                return None\n",
    "\n",
    "            rng.shuffle(unpaired_pool)\n",
    "            u_ptr = 0\n",
    "            ub, u_ptr2 = pop_full_no_wrap(unpaired_pool, u_ptr)\n",
    "            if ub is None:\n",
    "                return None\n",
    "            u_ptr = u_ptr2\n",
    "            return ub\n",
    "\n",
    "        # Case A: only paired data\n",
    "        if self.only_paired:\n",
    "            for _ in range(self.n_paired_batches):\n",
    "                pb = next_paired_batch()\n",
    "                if pb is None:\n",
    "                    break\n",
    "                yield pb.tolist()\n",
    "            return\n",
    "\n",
    "        # Case B: no paired batches in this epoch (tail + oversample_paired=False)\n",
    "        if self.n_paired_batches == 0:\n",
    "            # define epoch as unpaired backbone only\n",
    "            # (still homogeneous; you just don't see paired anchors)\n",
    "            # yield all full unpaired batches; if too small, yield one replacement batch (if allowed)\n",
    "            if len(unpaired_pool) >= self.batch_size:\n",
    "                n_unpaired_backbone = len(unpaired_pool) // self.batch_size\n",
    "                for _ in range(n_unpaired_backbone):\n",
    "                    ub = next_unpaired_batch()\n",
    "                    if ub is None:\n",
    "                        break\n",
    "                    yield ub.tolist()\n",
    "            else:\n",
    "                ub = next_unpaired_batch()\n",
    "                if ub is not None:\n",
    "                    yield ub.tolist()\n",
    "            return\n",
    "\n",
    "        # Case C: mixed (paired-driven backbone, interleave unpaired)\n",
    "        carry = 0.0\n",
    "        for _ in range(self.n_paired_batches):\n",
    "            pb = next_paired_batch()\n",
    "            if pb is None:\n",
    "                break\n",
    "            yield pb.tolist()\n",
    "\n",
    "            carry += self.unpaired_per_paired\n",
    "            while carry >= 1.0:\n",
    "                carry -= 1.0\n",
    "                ub = next_unpaired_batch()\n",
    "                if ub is None:\n",
    "                    break\n",
    "                yield ub.tolist()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# main loader builder\n",
    "# -----------------------------\n",
    "def make_loaders_with_overlap_v1(\n",
    "    base_dataset,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    test_idx,\n",
    "    *,\n",
    "    overlap_fraction: float,\n",
    "    drop_modality: str,\n",
    "    seed: int,\n",
    "    batch_size: int = 256,\n",
    "    num_workers: int = 0,\n",
    "    oversample_paired: bool = True,\n",
    "):\n",
    "    \"\"\"overlap_fraction applies ONLY to training. val/test remain paired.\"\"\"\n",
    "    p = float(overlap_fraction)\n",
    "    if not (0.0 <= p <= 1.0):\n",
    "        raise ValueError(f\"overlap_fraction must be in [0,1], got {p}\")\n",
    "\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    # --- train subset ---\n",
    "    train_indexed = IndexedDataset(base_dataset, indices=train_idx)\n",
    "    N = len(train_indexed)\n",
    "    if N == 0:\n",
    "        raise ValueError(\"Empty train split\")\n",
    "\n",
    "    n_paired = int(round(p * N))\n",
    "    n_paired = max(min(n_paired, N), 0)\n",
    "\n",
    "    perm = rng.permutation(N)\n",
    "    paired_local = perm[:n_paired]\n",
    "\n",
    "    groups = np.ones(N, dtype=np.int64)  # 1 = unpaired\n",
    "    groups[paired_local] = 0             # 0 = paired anchor\n",
    "\n",
    "    train_masked = DeterministicMaskDataset(\n",
    "        train_indexed, groups=groups, drop_modality=str(drop_modality)\n",
    "    )\n",
    "\n",
    "    n_paired_real = int((groups == 0).sum())\n",
    "    n_unpaired_real = int((groups != 0).sum())\n",
    "\n",
    "    ratio_unpaired_per_paired = (n_unpaired_real / max(n_paired_real, 1))\n",
    "    upp_sampling = float(ratio_unpaired_per_paired)  # scale to true split (no clipping)\n",
    "\n",
    "    print(\n",
    "        f\"[overlap={p:g}] batch_size={int(batch_size)} \"\n",
    "        f\"train_paired={n_paired_real} train_unpaired={n_unpaired_real} \"\n",
    "        f\"(unpaired_per_paired={ratio_unpaired_per_paired:.6f}, sampler_upp={upp_sampling:.6f})\"\n",
    "    )\n",
    "\n",
    "    # Train loader: homogeneous batches enforced by sampler (when unpaired exists)\n",
    "    if n_unpaired_real == 0:\n",
    "        train_loader = DataLoader(\n",
    "            train_masked,\n",
    "            batch_size=int(batch_size),\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=int(num_workers),\n",
    "            collate_fn=collate_xdict_with_idx,\n",
    "        )\n",
    "    else:\n",
    "        batch_sampler = InterleavedGroupedBatchSampler(\n",
    "            groups=groups,\n",
    "            batch_size=int(batch_size),\n",
    "            seed=int(seed),\n",
    "            drop_last=True,\n",
    "            unpaired_per_paired=upp_sampling,\n",
    "            oversample_paired=bool(oversample_paired),\n",
    "            oversample_unpaired=True,\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_masked,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=int(num_workers),\n",
    "            collate_fn=collate_xdict_with_idx,\n",
    "        )\n",
    "\n",
    "    # --- val/test: always paired, no masking ---\n",
    "    val_ds = IndexedDataset(base_dataset, indices=val_idx)\n",
    "    test_ds = IndexedDataset(base_dataset, indices=test_idx)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=int(batch_size),\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=int(num_workers),\n",
    "        collate_fn=collate_xdict_with_idx,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=int(batch_size),\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=int(num_workers),\n",
    "        collate_fn=collate_xdict_with_idx,\n",
    "    )\n",
    "\n",
    "    info = {\n",
    "        \"train_paired\": n_paired_real,\n",
    "        \"train_unpaired\": n_unpaired_real,\n",
    "        \"unpaired_per_paired\": float(ratio_unpaired_per_paired),\n",
    "        \"sampler_unpaired_per_paired\": float(upp_sampling),\n",
    "    }\n",
    "    return train_loader, val_loader, test_loader, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d9223-c557-4f33-8faa-9d30786377c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, info = make_loaders_with_overlap_v1(\n",
    "    base_dataset=dataset,     # <- change if your dataset var is named differently\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    overlap_fraction=1.0,     # example: 100% paired anchors\n",
    "    drop_modality=\"atac\",     # or \"rna\"\n",
    "    seed=0,\n",
    "    batch_size=256,\n",
    "    num_workers=0,\n",
    "    oversample_paired=True,\n",
    ")\n",
    "\n",
    "info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecdf901-1d4a-4099-bfa0-119e00fdf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"val_idx n =\", len(val_idx))\n",
    "print(\"test_idx n =\", len(test_idx))\n",
    "\n",
    "def _get_xdict_from_batch(batch):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "      - (x_dict, gids) from collate_xdict_with_idx\n",
    "      - {\"rna\":..., \"atac\":...} dict batches\n",
    "      - {\"x_dict\": {...}} style (just in case)\n",
    "    \"\"\"\n",
    "    # Most common in your notebook: (x_dict, gids)\n",
    "    if isinstance(batch, (tuple, list)) and len(batch) >= 1 and isinstance(batch[0], dict):\n",
    "        return batch[0]\n",
    "\n",
    "    # Direct dict batch\n",
    "    if isinstance(batch, dict):\n",
    "        if \"x_dict\" in batch and isinstance(batch[\"x_dict\"], dict):\n",
    "            return batch[\"x_dict\"]\n",
    "        return batch\n",
    "\n",
    "    raise TypeError(f\"Unknown batch type: {type(batch)}\")\n",
    "\n",
    "def count_paired(loader, n_batches=20, rna_key=\"rna\", atac_key=\"atac\"):\n",
    "    paired = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "\n",
    "        x = _get_xdict_from_batch(batch)\n",
    "\n",
    "        has_rna  = (rna_key in x) and (x[rna_key] is not None)\n",
    "        has_atac = (atac_key in x) and (x[atac_key] is not None)\n",
    "\n",
    "        # if tensors exist but are empty, treat as missing\n",
    "        try:\n",
    "            if has_rna and hasattr(x[rna_key], \"shape\") and x[rna_key].shape[0] == 0:\n",
    "                has_rna = False\n",
    "            if has_atac and hasattr(x[atac_key], \"shape\") and x[atac_key].shape[0] == 0:\n",
    "                has_atac = False\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        paired += int(has_rna and has_atac)\n",
    "        total += 1\n",
    "\n",
    "    return paired, total\n",
    "\n",
    "vp, vt = count_paired(val_loader, n_batches=20)\n",
    "tp, tt = count_paired(test_loader, n_batches=20)\n",
    "print(f\"val paired batches ~ {vp}/{vt}\")\n",
    "print(f\"test paired batches ~ {tp}/{tt}\")\n",
    "\n",
    "# Optional: show what keys you actually have\n",
    "xb, gids = next(iter(val_loader))\n",
    "print(\"val batch keys:\", list(xb.keys()) if isinstance(xb, dict) else type(xb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ccc91",
   "metadata": {},
   "source": [
    "## 4) Model + training (v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adcaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from univi import UniVIMultiModalVAE, ModalityConfig, UniVIConfig, TrainingConfig\n",
    "from univi.trainer import UniVITrainer\n",
    "\n",
    "# -----------------------------\n",
    "# Config builders (UNCHANGED schedule)\n",
    "# -----------------------------\n",
    "def make_univi_cfg(rna_dim: int, atac_dim: int) -> UniVIConfig:\n",
    "    return UniVIConfig(\n",
    "        latent_dim=30,\n",
    "        beta=1.25,\n",
    "        gamma=4.35,\n",
    "        encoder_dropout=0.10,\n",
    "        decoder_dropout=0.05,\n",
    "        encoder_batchnorm=True,\n",
    "        decoder_batchnorm=False,\n",
    "        #kl_anneal_start=50,\n",
    "        #kl_anneal_end=105,\n",
    "        #align_anneal_start=75,\n",
    "        #align_anneal_end=140,\n",
    "        kl_anneal_start=0,\n",
    "        kl_anneal_end=60,\n",
    "        align_anneal_start=25,\n",
    "        align_anneal_end=85,\n",
    "        modalities=[\n",
    "            ModalityConfig(\n",
    "                name=\"rna\",\n",
    "                input_dim=int(rna_dim),\n",
    "                encoder_hidden=[512, 256, 128],\n",
    "                decoder_hidden=[128, 256, 512],\n",
    "                likelihood=\"gaussian\",\n",
    "            ),\n",
    "            ModalityConfig(\n",
    "                name=\"atac\",\n",
    "                input_dim=int(atac_dim),\n",
    "                encoder_hidden=[128, 64],\n",
    "                decoder_hidden=[64, 128],\n",
    "                likelihood=\"gaussian\",\n",
    "            ),\n",
    "        ],\n",
    "        class_heads=[],\n",
    "    )\n",
    "\n",
    "def make_train_cfg(device, *, early_stopping=True, patience=100, min_delta=0.0) -> TrainingConfig:\n",
    "    return TrainingConfig(\n",
    "        n_epochs=5000,\n",
    "        batch_size=256,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        device=device,\n",
    "        log_every=100,\n",
    "        grad_clip=5.0,\n",
    "        early_stopping=bool(early_stopping),\n",
    "        patience=int(patience),\n",
    "        min_delta=float(min_delta),\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Device selection\n",
    "# -----------------------------\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# Trainer compat\n",
    "# -----------------------------\n",
    "def _build_univi_trainer_compat(*, model, train_cfg, train_loader, val_loader):\n",
    "    sig = inspect.signature(UniVITrainer.__init__)\n",
    "    params = sig.parameters\n",
    "    kwargs = {}\n",
    "\n",
    "    if \"model\" in params:\n",
    "        kwargs[\"model\"] = model\n",
    "    if \"train_loader\" in params:\n",
    "        kwargs[\"train_loader\"] = train_loader\n",
    "    if \"val_loader\" in params:\n",
    "        kwargs[\"val_loader\"] = val_loader\n",
    "\n",
    "    for cand in (\"train_cfg\", \"training_cfg\", \"cfg_train\", \"config\", \"cfg\"):\n",
    "        if cand in params:\n",
    "            kwargs[cand] = train_cfg\n",
    "            return UniVITrainer(**kwargs)\n",
    "\n",
    "    for k, v in vars(train_cfg).items():\n",
    "        if k in params:\n",
    "            kwargs[k] = v\n",
    "\n",
    "    return UniVITrainer(**kwargs)\n",
    "\n",
    "def _trainer_fit_compat(trainer, train_loader, val_loader):\n",
    "    try:\n",
    "        return trainer.fit()\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        return trainer.fit(train_loader=train_loader, val_loader=val_loader)\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return trainer.fit(train_loader, val_loader)\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    model: torch.nn.Module\n",
    "    warmup_epochs: int\n",
    "    best_epoch_all: int | None\n",
    "    best_val_all: float | None\n",
    "    best_epoch_postwarmup: int | None\n",
    "    best_val_postwarmup: float | None\n",
    "\n",
    "def _extract_val_history(trainer):\n",
    "    \"\"\"Best-effort: pull a list of validation losses (floats) if trainer exposes it.\"\"\"\n",
    "    for cand in (\"val_losses\", \"val_loss_history\", \"history_val\", \"val_history\", \"history\"):\n",
    "        if not hasattr(trainer, cand):\n",
    "            continue\n",
    "        h = getattr(trainer, cand)\n",
    "        if isinstance(h, dict):\n",
    "            # common pattern: {\"train_loss\":[...], \"val_loss\":[...]}\n",
    "            for kk in (\"val_loss\", \"val\", \"val_losses\"):\n",
    "                if kk in h and isinstance(h[kk], (list, tuple)) and len(h[kk]) > 0:\n",
    "                    h2 = h[kk]\n",
    "                    if isinstance(h2[0], (float, int, np.floating, np.integer)):\n",
    "                        return [float(x) for x in h2]\n",
    "        if isinstance(h, (list, tuple)) and len(h) > 0 and isinstance(h[0], (float, int, np.floating, np.integer)):\n",
    "            return [float(x) for x in h]\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Training wrapper: do NOT shift anneals; only *report* best post-warmup.\n",
    "# Also: prevent early stopping from firing before warmup by inflating patience.\n",
    "# -----------------------------\n",
    "def train_one_overlap_v1(\n",
    "    *,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    seed: int = 42,\n",
    "    device=device,\n",
    "    loss_mode: str = \"v1\",\n",
    "    v1_recon: str = \"avg\",\n",
    "    warmup_epochs: int = 50,   # <-- only affects reporting + inflated patience\n",
    "    patience: int = 5000,\n",
    "    min_delta: float = 0.0,\n",
    "):\n",
    "    torch.manual_seed(int(seed))\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "    rna_dim = int(rna_tr_pp.n_vars)\n",
    "    atac_dim = int(atac_tr_lsi.n_vars)\n",
    "\n",
    "    univi_cfg = make_univi_cfg(rna_dim=rna_dim, atac_dim=atac_dim)\n",
    "\n",
    "    w = int(max(warmup_epochs, 0))\n",
    "\n",
    "    # IMPORTANT: do not stop before warmup.\n",
    "    # We can't easily override UniVITrainer's internal ES across versions,\n",
    "    # so we make the trainer \"too patient\" to early-stop during warmup.\n",
    "    patience_eff = int(patience) + w\n",
    "\n",
    "    train_cfg = make_train_cfg(\n",
    "        device=device,\n",
    "        early_stopping=True,\n",
    "        #patience=patience_eff,\n",
    "        patience=patience,\n",
    "        min_delta=float(min_delta),\n",
    "    )\n",
    "\n",
    "    model = UniVIMultiModalVAE(\n",
    "        univi_cfg,\n",
    "        loss_mode=str(loss_mode),\n",
    "        v1_recon=str(v1_recon),\n",
    "    ).to(device)\n",
    "\n",
    "    trainer = _build_univi_trainer_compat(\n",
    "        model=model,\n",
    "        train_cfg=train_cfg,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "    )\n",
    "\n",
    "    # some versions expect these attributes\n",
    "    if not hasattr(trainer, \"train_loader\"):\n",
    "        trainer.train_loader = train_loader\n",
    "    if not hasattr(trainer, \"val_loader\"):\n",
    "        trainer.val_loader = val_loader\n",
    "\n",
    "    _trainer_fit_compat(trainer, train_loader=train_loader, val_loader=val_loader)\n",
    "\n",
    "    # --- best overall (trainer-reported if available) ---\n",
    "    best_epoch_all = getattr(trainer, \"best_epoch\", None)\n",
    "    best_val_all = getattr(trainer, \"best_val_loss\", None)\n",
    "    if best_val_all is None:\n",
    "        best_val_all = getattr(trainer, \"best_val\", None)\n",
    "\n",
    "    # --- best post-warmup (best-effort) ---\n",
    "    best_epoch_pw = None\n",
    "    best_val_pw = None\n",
    "\n",
    "    hist = _extract_val_history(trainer)\n",
    "    if hist is not None and len(hist) > w:\n",
    "        sub = hist[w:]              # epochs are 1-indexed in logs; hist[0] is epoch 1\n",
    "        j = int(np.argmin(sub))\n",
    "        best_epoch_pw = w + j + 1\n",
    "        best_val_pw = float(sub[j])\n",
    "    else:\n",
    "        # fallback: only accept trainer's best if it is after warmup\n",
    "        if isinstance(best_epoch_all, (int, np.integer)) and int(best_epoch_all) >= max(w, 1):\n",
    "            best_epoch_pw = int(best_epoch_all)\n",
    "            best_val_pw = float(best_val_all) if best_val_all is not None else None\n",
    "\n",
    "    return TrainResult(\n",
    "        model=model,\n",
    "        warmup_epochs=w,\n",
    "        best_epoch_all=(int(best_epoch_all) if best_epoch_all is not None else None),\n",
    "        best_val_all=(float(best_val_all) if best_val_all is not None else None),\n",
    "        best_epoch_postwarmup=best_epoch_pw,\n",
    "        best_val_postwarmup=best_val_pw,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d9049",
   "metadata": {},
   "source": [
    "## 5) Encoding + metrics (alignment + label transfer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8652a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ============================================================\n",
    "# Robust latent extraction for UniVI (handles many return layouts)\n",
    "# + stable label coding across splits via id_to_int\n",
    "# ============================================================\n",
    "\n",
    "def _to_numpy_ids(gids):\n",
    "    \"\"\"Convert gids to a 1D int64 numpy array.\"\"\"\n",
    "    if gids is None:\n",
    "        return None\n",
    "    if torch.is_tensor(gids):\n",
    "        return gids.detach().cpu().numpy().astype(np.int64, copy=False)\n",
    "    return np.asarray(gids, dtype=np.int64)\n",
    "\n",
    "def _build_label_encoder(y_all):\n",
    "    \"\"\"\n",
    "    Returns (y_raw, id_to_int_or_None)\n",
    "    - If y_all is numeric -> y_raw numeric, id_to_int None.\n",
    "    - If y_all is object/string -> y_raw as str array, id_to_int mapping over ALL unique labels in y_all.\n",
    "    \"\"\"\n",
    "    y_raw = np.asarray(y_all)\n",
    "    kind = y_raw.dtype.kind\n",
    "    if kind in (\"i\", \"u\", \"f\", \"b\"):\n",
    "        return y_raw, None\n",
    "\n",
    "    y_raw = y_raw.astype(str)\n",
    "    uniq = np.unique(y_raw)\n",
    "    id_to_int = {lab: i for i, lab in enumerate(uniq.tolist())}\n",
    "    return y_raw, id_to_int\n",
    "\n",
    "def _encode_labels_for_gids(y_raw, gids_np, *, id_to_int=None):\n",
    "    \"\"\"\n",
    "    Encode labels for the given gids.\n",
    "    If y_raw is numeric -> returns numeric torch tensor.\n",
    "    If y_raw is str/object -> returns torch.long codes (using provided id_to_int, extending if needed).\n",
    "    \"\"\"\n",
    "    y_batch = y_raw[gids_np]\n",
    "    y_batch_arr = np.asarray(y_batch)\n",
    "    kind = y_batch_arr.dtype.kind\n",
    "\n",
    "    if kind in (\"i\", \"u\", \"b\"):\n",
    "        return torch.as_tensor(y_batch_arr.astype(np.int64, copy=False), dtype=torch.long), id_to_int\n",
    "    if kind == \"f\":\n",
    "        return torch.as_tensor(y_batch_arr.astype(np.float32, copy=False)), id_to_int\n",
    "\n",
    "    # object/string\n",
    "    y_batch_str = y_batch_arr.astype(str)\n",
    "\n",
    "    if id_to_int is None:\n",
    "        uniq = np.unique(y_batch_str)\n",
    "        id_to_int = {lab: i for i, lab in enumerate(uniq.tolist())}\n",
    "\n",
    "    codes = np.empty(y_batch_str.shape[0], dtype=np.int64)\n",
    "    next_code = (max(id_to_int.values()) + 1) if len(id_to_int) else 0\n",
    "    for i, lab in enumerate(y_batch_str):\n",
    "        if lab not in id_to_int:\n",
    "            id_to_int[lab] = next_code\n",
    "            next_code += 1\n",
    "        codes[i] = id_to_int[lab]\n",
    "\n",
    "    return torch.as_tensor(codes, dtype=torch.long), id_to_int\n",
    "\n",
    "def _unwrap_model_output(out):\n",
    "    \"\"\"\n",
    "    UniVI sometimes returns:\n",
    "      - dict\n",
    "      - (loss, dict)\n",
    "      - (dict, extras...)\n",
    "      - tensor\n",
    "    We want the dict/tensor that contains embeddings/latents.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(out) or isinstance(out, dict):\n",
    "        return out\n",
    "    if isinstance(out, (tuple, list)) and len(out) > 0:\n",
    "        for item in out:\n",
    "            if isinstance(item, dict) or torch.is_tensor(item):\n",
    "                return item\n",
    "        return out[0]\n",
    "    return out\n",
    "\n",
    "def _smart_call(fn, x_dict):\n",
    "    \"\"\"\n",
    "    Call fn with whichever kwarg name it seems to want.\n",
    "    Tries: x_dict / batch / x / inputs ; else positional.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sig = inspect.signature(fn)\n",
    "        params = sig.parameters\n",
    "        for name in (\"x_dict\", \"batch\", \"x\", \"inputs\"):\n",
    "            if name in params:\n",
    "                return fn(**{name: x_dict})\n",
    "        return fn(x_dict)\n",
    "    except TypeError:\n",
    "        return fn(x_dict)\n",
    "\n",
    "def _call_univi_encoder(model, x_dict):\n",
    "    \"\"\"\n",
    "    Try encode() first, then forward().\n",
    "    Uses signature-aware calling and unwraps tuple outputs.\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"encode\") and callable(getattr(model, \"encode\")):\n",
    "        out = _smart_call(model.encode, x_dict)\n",
    "        out = _unwrap_model_output(out)\n",
    "        if isinstance(out, (dict, torch.Tensor)):\n",
    "            return out\n",
    "    out = _smart_call(model, x_dict)\n",
    "    return _unwrap_model_output(out)\n",
    "\n",
    "def _first_tensor(*xs):\n",
    "    for x in xs:\n",
    "        if torch.is_tensor(x):\n",
    "            return x\n",
    "    return None\n",
    "\n",
    "def _find_tensors_anywhere(obj, *, path=\"\"):\n",
    "    \"\"\"Recursively collect tensors from nested dict/list/tuple structures.\"\"\"\n",
    "    out = []\n",
    "    if torch.is_tensor(obj):\n",
    "        out.append((path or \"tensor\", obj))\n",
    "    elif isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            out.extend(_find_tensors_anywhere(v, path=f\"{path}.{k}\" if path else str(k)))\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        for i, v in enumerate(obj):\n",
    "            out.extend(_find_tensors_anywhere(v, path=f\"{path}[{i}]\" if path else f\"[{i}]\"))\n",
    "    return out\n",
    "\n",
    "def _get_path(obj, path_tuple):\n",
    "    cur = obj\n",
    "    for k in path_tuple:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return None\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def _extract_latent(enc, which: str):\n",
    "    \"\"\"\n",
    "    which in {\"rna\",\"atac\",\"fused\"}.\n",
    "    Searches many common UniVI layouts/keys and falls back to any 2D tensor found.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(enc):\n",
    "        return enc if which == \"fused\" else None\n",
    "\n",
    "    if not isinstance(enc, dict):\n",
    "        return None\n",
    "\n",
    "    # 1) flat keys\n",
    "    v = _first_tensor(\n",
    "        enc.get(f\"mu_{which}\", None),\n",
    "        enc.get(f\"z_{which}\", None),\n",
    "        enc.get(f\"latent_{which}\", None),\n",
    "        enc.get(f\"emb_{which}\", None),\n",
    "        enc.get(f\"embedding_{which}\", None),\n",
    "    )\n",
    "    if v is not None:\n",
    "        return v\n",
    "\n",
    "    # fused often uses generic keys\n",
    "    if which == \"fused\":\n",
    "        v = _first_tensor(\n",
    "            enc.get(\"mu\", None), enc.get(\"z\", None), enc.get(\"mean\", None),\n",
    "            enc.get(\"latent\", None), enc.get(\"embedding\", None),\n",
    "            enc.get(\"mu_shared\", None), enc.get(\"z_shared\", None),\n",
    "            enc.get(\"mu_joint\", None), enc.get(\"z_joint\", None),\n",
    "        )\n",
    "        if v is not None:\n",
    "            return v\n",
    "\n",
    "    # 2) nested containers\n",
    "    for container in (\"latents\", \"latent\", \"posterior\", \"post\", \"qz\", \"q\", \"enc\", \"encode\", \"outputs\"):\n",
    "        sub = enc.get(container, None)\n",
    "        if isinstance(sub, dict):\n",
    "            v = _first_tensor(\n",
    "                sub.get(f\"mu_{which}\", None),\n",
    "                sub.get(f\"z_{which}\", None),\n",
    "                sub.get(which, None) if torch.is_tensor(sub.get(which, None)) else None,\n",
    "            )\n",
    "            if v is not None:\n",
    "                return v\n",
    "\n",
    "            if which in sub and isinstance(sub[which], dict):\n",
    "                vv = _first_tensor(\n",
    "                    sub[which].get(\"mu\", None),\n",
    "                    sub[which].get(\"z\", None),\n",
    "                    sub[which].get(\"mean\", None),\n",
    "                    sub[which].get(\"latent\", None),\n",
    "                    sub[which].get(\"embedding\", None),\n",
    "                )\n",
    "                if vv is not None:\n",
    "                    return vv\n",
    "\n",
    "    # 3) top-level modality dict: enc[\"rna\"]={\"mu\":...}\n",
    "    if which in enc and isinstance(enc[which], dict):\n",
    "        vv = _first_tensor(\n",
    "            enc[which].get(\"mu\", None),\n",
    "            enc[which].get(\"z\", None),\n",
    "            enc[which].get(\"mean\", None),\n",
    "            enc[which].get(\"latent\", None),\n",
    "            enc[which].get(\"embedding\", None),\n",
    "        )\n",
    "        if vv is not None:\n",
    "            return vv\n",
    "\n",
    "    # 4) known nested paths\n",
    "    for p in [\n",
    "        (\"posterior\", which, \"mu\"),\n",
    "        (\"posterior\", which, \"z\"),\n",
    "        (\"latents\", which, \"mu\"),\n",
    "        (\"latents\", which, \"z\"),\n",
    "        (\"qz\", which, \"mu\"),\n",
    "        (\"qz\", which, \"z\"),\n",
    "    ]:\n",
    "        sub = _get_path(enc, p[:-1])\n",
    "        if isinstance(sub, dict):\n",
    "            vv = sub.get(p[-1], None)\n",
    "            if torch.is_tensor(vv):\n",
    "                return vv\n",
    "\n",
    "    # 5) last resort: any tensor anywhere (prefer 2D)\n",
    "    tensors = _find_tensors_anywhere(enc)\n",
    "    if tensors:\n",
    "        for _, t in tensors:\n",
    "            if t.ndim == 2:\n",
    "                return t\n",
    "        return tensors[0][1]\n",
    "\n",
    "    return None\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_embeddings_with_labels(\n",
    "    model,\n",
    "    loader,\n",
    "    *,\n",
    "    device,\n",
    "    y_all,\n",
    "    id_to_int=None,\n",
    "    require_mus=True,\n",
    "    debug_first_batch=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Expects loader batches: (x_dict, gids)\n",
    "\n",
    "    Returns dict with:\n",
    "      mu_rna/mu_atac/mu_fused  (each may be None if modality absent),\n",
    "      y (torch), gids (torch.long),\n",
    "      id_to_int (stable mapping for string labels)\n",
    "    \"\"\"\n",
    "\n",
    "    def _encode_one(model, x_sub: dict):\n",
    "        enc = _call_univi_encoder(model, x_sub)\n",
    "        return _unwrap_model_output(enc)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # global label handling (stable mapping)\n",
    "    y_raw, global_map = _build_label_encoder(y_all)\n",
    "    kind = np.asarray(y_raw).dtype.kind\n",
    "    use_map = None\n",
    "    if kind not in (\"i\", \"u\", \"f\", \"b\"):\n",
    "        use_map = id_to_int if id_to_int is not None else global_map\n",
    "\n",
    "    mu_rna_all, mu_atac_all, mu_fused_all = [], [], []\n",
    "    y_list, gids_list = [], []\n",
    "\n",
    "    for b_ix, batch in enumerate(loader):\n",
    "        if not (isinstance(batch, (tuple, list)) and len(batch) == 2 and isinstance(batch[0], dict)):\n",
    "            raise RuntimeError(\n",
    "                \"encode_embeddings_with_labels expects batches like (x_dict, gids). \"\n",
    "                \"Use collate_xdict_with_idx and IndexedDataset.\"\n",
    "            )\n",
    "\n",
    "        x_dict, gids = batch\n",
    "        gids_np = _to_numpy_ids(gids)\n",
    "        if gids_np is None:\n",
    "            raise RuntimeError(\"gids is None; loader must provide integer indices for label lookup.\")\n",
    "\n",
    "        # move tensors to device (keep only tensor modalities + anything else model might accept)\n",
    "        x_full = {}\n",
    "        for k, v in x_dict.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            x_full[k] = v.to(device) if torch.is_tensor(v) else v\n",
    "\n",
    "        # --- (A) fused: encode with whatever is present (often both) ---\n",
    "        enc_full = _encode_one(model, x_full)\n",
    "\n",
    "        # --- (B) rna-only / atac-only encodes: FORCE modality-specific outputs ---\n",
    "        enc_rna = None\n",
    "        if \"rna\" in x_full and torch.is_tensor(x_full[\"rna\"]):\n",
    "            enc_rna = _encode_one(model, {\"rna\": x_full[\"rna\"]})\n",
    "\n",
    "        enc_atac = None\n",
    "        if \"atac\" in x_full and torch.is_tensor(x_full[\"atac\"]):\n",
    "            enc_atac = _encode_one(model, {\"atac\": x_full[\"atac\"]})\n",
    "\n",
    "        if debug_first_batch and b_ix == 0:\n",
    "            def _keys(obj):\n",
    "                return sorted(obj.keys()) if isinstance(obj, dict) else [str(type(obj))]\n",
    "            print(\"[latent-debug] enc_full keys:\", _keys(enc_full))\n",
    "            if enc_rna is not None:  print(\"[latent-debug] enc_rna  keys:\", _keys(enc_rna))\n",
    "            if enc_atac is not None: print(\"[latent-debug] enc_atac keys:\", _keys(enc_atac))\n",
    "\n",
    "        # Extract fused first (from full)\n",
    "        mu_fused = _extract_latent(enc_full, \"fused\")\n",
    "\n",
    "        # Extract modality-specific from forced single-modality encodes.\n",
    "        # If UniVI still returns generic keys, _extract_latent(...,\"fused\") fallback won’t help,\n",
    "        # so we also try \"fused\" as last resort.\n",
    "        mu_rna = None\n",
    "        if enc_rna is not None:\n",
    "            mu_rna = _extract_latent(enc_rna, \"rna\")\n",
    "            if mu_rna is None:\n",
    "                mu_rna = _extract_latent(enc_rna, \"fused\")\n",
    "\n",
    "        mu_atac = None\n",
    "        if enc_atac is not None:\n",
    "            mu_atac = _extract_latent(enc_atac, \"atac\")\n",
    "            if mu_atac is None:\n",
    "                mu_atac = _extract_latent(enc_atac, \"fused\")\n",
    "\n",
    "        if torch.is_tensor(mu_rna):   mu_rna_all.append(mu_rna.detach().cpu())\n",
    "        if torch.is_tensor(mu_atac):  mu_atac_all.append(mu_atac.detach().cpu())\n",
    "        if torch.is_tensor(mu_fused): mu_fused_all.append(mu_fused.detach().cpu())\n",
    "\n",
    "        y_tensor, use_map = _encode_labels_for_gids(y_raw, gids_np, id_to_int=use_map)\n",
    "        y_list.append(y_tensor.detach().cpu())\n",
    "        gids_list.append(torch.as_tensor(gids_np, dtype=torch.long))\n",
    "\n",
    "    def _cat(xs):\n",
    "        return torch.cat(xs, dim=0) if len(xs) else None\n",
    "\n",
    "    mu_rna_out   = _cat(mu_rna_all)\n",
    "    mu_atac_out  = _cat(mu_atac_all)\n",
    "    mu_fused_out = _cat(mu_fused_all)\n",
    "    y_out        = _cat(y_list)\n",
    "    gids_out     = _cat(gids_list)\n",
    "\n",
    "    if require_mus and (mu_rna_out is None or mu_atac_out is None) and (mu_fused_out is None):\n",
    "        raise RuntimeError(\n",
    "            \"Latent extraction failed: could not obtain modality-specific latents \"\n",
    "            \"(mu_rna/mu_atac) nor a fused latent. Run with debug_first_batch=True.\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"mu_rna\": mu_rna_out,\n",
    "        \"mu_atac\": mu_atac_out,\n",
    "        \"mu_fused\": mu_fused_out,\n",
    "        \"y\": y_out,\n",
    "        \"y_rna\": y_out,\n",
    "        \"y_atac\": y_out,\n",
    "        \"y_fused\": y_out,\n",
    "        \"gids\": gids_out,\n",
    "        \"id_to_int\": use_map,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2216596",
   "metadata": {},
   "source": [
    "## 6) Figure 8 runner (missing-modality curve + label transfer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---- knobs ----\n",
    "\n",
    "def choose_batch_size(overlap: float, batch_size_default: int) -> int:\n",
    "    p = float(overlap)\n",
    "    bs0 = int(batch_size_default)\n",
    "    if p >= 0.05:\n",
    "        return bs0\n",
    "    if bs0 >= 256:\n",
    "        return 128\n",
    "    if bs0 >= 128:\n",
    "        return 64\n",
    "    return bs0\n",
    "\n",
    "def choose_unpaired_per_paired(overlap: float) -> float:\n",
    "    \"\"\"\n",
    "    Heuristic: as overlap shrinks, keep relatively more paired anchors.\n",
    "    Returns unpaired_per_paired in [0.25, 8.0].\n",
    "    \"\"\"\n",
    "    p = max(float(overlap), 1e-6)\n",
    "    val = 1.0 / np.sqrt(p / 0.10)     # p=0.10 -> 1, p=0.01 -> 3.16\n",
    "    unpaired_per_paired = 1.0 / val   # invert to get fewer unpaired when p small\n",
    "    return float(np.clip(unpaired_per_paired, 0.25, 8.0))\n",
    "\n",
    "# ---- dataset wrappers ----\n",
    "\n",
    "class IndexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a base dataset + an index list and ALWAYS return an integer gid.\n",
    "\n",
    "    Output format:\n",
    "      - (x_dict, gid_int)                       if base yields dict\n",
    "      - (x_dict, gid_int, *extras_from_base)    if base yields (dict, ...)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, indices):\n",
    "        self.base = base_dataset\n",
    "        self.indices = np.asarray(indices, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.indices.shape[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        gid = int(self.indices[int(i)])  # global integer index into base_dataset\n",
    "        item = self.base[gid]\n",
    "\n",
    "        if isinstance(item, dict):\n",
    "            return dict(item), gid\n",
    "\n",
    "        if isinstance(item, (tuple, list)) and len(item) >= 1 and isinstance(item[0], dict):\n",
    "            x_dict = dict(item[0])\n",
    "            extras = tuple(item[1:])\n",
    "            return (x_dict, gid, *extras)\n",
    "\n",
    "        raise TypeError(f\"Base dataset must yield dict or (dict, ...), got: {type(item)}\")\n",
    "\n",
    "class DeterministicMaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Deterministically drops one modality for unpaired rows according to groups.\n",
    "      groups==0: paired (keep all modalities)\n",
    "      groups!=0: unpaired (drop `drop_modality` -> set to None)\n",
    "    Assumes base yields (x_dict, gid, ...) after IndexDataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, *, groups, drop_modality: str):\n",
    "        self.base = base_dataset\n",
    "        self.groups = np.asarray(groups, dtype=np.int64)\n",
    "        self.drop_modality = str(drop_modality)\n",
    "\n",
    "        if len(self.base) != len(self.groups):\n",
    "            raise ValueError(f\"base_dataset len={len(self.base)} != groups len={len(self.groups)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        item = self.base[i]\n",
    "\n",
    "        if isinstance(item, dict):\n",
    "            x_dict = dict(item)\n",
    "            rest = ()\n",
    "        elif isinstance(item, (tuple, list)) and len(item) >= 1 and isinstance(item[0], dict):\n",
    "            x_dict = dict(item[0])\n",
    "            rest = tuple(item[1:])\n",
    "        else:\n",
    "            raise TypeError(f\"Expected dataset to yield dict or (dict, ...), got {type(item)}\")\n",
    "\n",
    "        g = int(self.groups[int(i)])\n",
    "        if g != 0:\n",
    "            x_dict[self.drop_modality] = None\n",
    "\n",
    "        return (x_dict, *rest) if rest else x_dict\n",
    "\n",
    "# ---- collate ----\n",
    "\n",
    "def collate_xdict_with_idx(batch):\n",
    "    \"\"\"\n",
    "    Collate items that are either:\n",
    "      - x_dict\n",
    "      - (x_dict, gid)\n",
    "      - (x_dict, gid, ...)\n",
    "\n",
    "    Produces:\n",
    "      - out_xdict\n",
    "      - gids (torch.int64) if present\n",
    "      - extras (stacked tensors) if present\n",
    "    \"\"\"\n",
    "    x_list = []\n",
    "    rest_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        if isinstance(item, dict):\n",
    "            x_list.append(item)\n",
    "            rest_list.append(())\n",
    "        else:\n",
    "            x_list.append(item[0])\n",
    "            rest_list.append(tuple(item[1:]))\n",
    "\n",
    "    # union of keys\n",
    "    keys = set()\n",
    "    for x in x_list:\n",
    "        keys |= set(x.keys())\n",
    "\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        vals = [x.get(k, None) for x in x_list]\n",
    "\n",
    "        if all(v is None for v in vals):\n",
    "            out[k] = None\n",
    "            continue\n",
    "\n",
    "        # If any None present, you are mixing paired+unpaired in same batch -> fail fast.\n",
    "        if any(v is None for v in vals):\n",
    "            raise RuntimeError(\n",
    "                f\"Within-batch mixed presence for modality '{k}'. \"\n",
    "                f\"Sampler must keep paired/unpaired separate per-batch.\"\n",
    "            )\n",
    "\n",
    "        v0 = vals[0]\n",
    "        if torch.is_tensor(v0):\n",
    "            out[k] = torch.stack(vals, dim=0)\n",
    "        else:\n",
    "            out[k] = torch.as_tensor(np.stack(vals, axis=0))\n",
    "\n",
    "    # no gids/extras\n",
    "    if len(rest_list[0]) == 0:\n",
    "        return out\n",
    "\n",
    "    # first rest item is gid by IndexDataset\n",
    "    gids = torch.as_tensor([int(r[0]) for r in rest_list], dtype=torch.int64)\n",
    "\n",
    "    # if no extras, return (x, gids)\n",
    "    if len(rest_list[0]) == 1:\n",
    "        return out, gids\n",
    "\n",
    "    # extras: stack each column if possible, else return as list\n",
    "    extras_cols = list(zip(*[r[1:] for r in rest_list]))\n",
    "    extras_out = []\n",
    "    for col in extras_cols:\n",
    "        try:\n",
    "            extras_out.append(torch.as_tensor(col))\n",
    "        except Exception:\n",
    "            extras_out.append(list(col))\n",
    "\n",
    "    return (out, gids, *extras_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed8fc1-3bc3-4f8f-b1bc-f42c97999ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot UMAPs of latents (rna / atac / fused), colored by cell type ----\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def _as_numpy_2d(x):\n",
    "    import torch\n",
    "    if x is None:\n",
    "        return None\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    return x\n",
    "\n",
    "def _get_label_strings(enc, y_all):\n",
    "    \"\"\"\n",
    "    Prefer true string labels from y_all[gids] if gids is present.\n",
    "    Falls back to integer y in enc.\n",
    "    \"\"\"\n",
    "    gids = enc.get(\"gids\", None)\n",
    "    if gids is not None:\n",
    "        gids = _as_numpy_2d(gids).reshape(-1).astype(np.int64)\n",
    "        labs = np.asarray(y_all)[gids]\n",
    "        return labs.astype(str)\n",
    "\n",
    "    y = enc.get(\"y_fused\", enc.get(\"y_rna\", enc.get(\"y\", None)))\n",
    "    if y is None:\n",
    "        return None\n",
    "    return _as_numpy_2d(y).reshape(-1).astype(str)\n",
    "\n",
    "def _umap_embed(X, *, seed=0, n_neighbors=15, min_dist=0.3, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Returns 2D embedding via UMAP; falls back to PCA if umap-learn isn't installed.\n",
    "    \"\"\"\n",
    "    X = _as_numpy_2d(X)\n",
    "    if X is None:\n",
    "        return None\n",
    "    try:\n",
    "        import umap\n",
    "        reducer = umap.UMAP(\n",
    "            n_neighbors=int(n_neighbors),\n",
    "            min_dist=float(min_dist),\n",
    "            metric=str(metric),\n",
    "            random_state=int(seed),\n",
    "        )\n",
    "        return reducer.fit_transform(X)\n",
    "    except Exception:\n",
    "        from sklearn.decomposition import PCA\n",
    "        return PCA(n_components=2, random_state=int(seed)).fit_transform(X)\n",
    "\n",
    "def _prep_label_mapping(labels, *, sort_legend_by=\"count\"):\n",
    "    \"\"\"\n",
    "    Build ONE stable label->int mapping + ordered label list.\n",
    "    This prevents colors from changing between panels.\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        return None, None, None, None\n",
    "\n",
    "    labels = np.asarray(labels).astype(str)\n",
    "    uniq, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    if sort_legend_by == \"count\":\n",
    "        order = np.argsort(-counts)  # descending\n",
    "    else:\n",
    "        order = np.argsort(uniq.astype(str))\n",
    "\n",
    "    uniq = uniq[order]\n",
    "    counts = counts[order]\n",
    "\n",
    "    lab_to_int = {lab: i for i, lab in enumerate(uniq.tolist())}\n",
    "    c = np.array([lab_to_int[lab] for lab in labels], dtype=int)\n",
    "\n",
    "    # Use a discrete cmap with fixed number of categories\n",
    "    cmap = plt.get_cmap(\"tab20\", max(len(uniq), 1))\n",
    "    return labels, uniq, counts, (lab_to_int, c, cmap)\n",
    "\n",
    "def _legend_handles(uniq, counts, cmap, *, legend_max=25):\n",
    "    if uniq is None or counts is None:\n",
    "        return None\n",
    "\n",
    "    n_cat = len(uniq)\n",
    "    if legend_max is None or legend_max <= 0 or n_cat == 0:\n",
    "        return None\n",
    "\n",
    "    top_n = int(min(legend_max, n_cat))\n",
    "    handles = []\n",
    "    for i in range(top_n):\n",
    "        lab = uniq[i]\n",
    "        col = cmap(i)\n",
    "        handles.append(\n",
    "            Line2D(\n",
    "                [0], [0],\n",
    "                marker=\"o\",\n",
    "                color=\"none\",\n",
    "                markerfacecolor=col,\n",
    "                markeredgecolor=\"none\",\n",
    "                markersize=6,\n",
    "                label=f\"{lab} (n={counts[i]})\",\n",
    "            )\n",
    "        )\n",
    "    return handles\n",
    "\n",
    "def plot_latent_umap(\n",
    "    X,\n",
    "    labels,\n",
    "    *,\n",
    "    title=\"UMAP\",\n",
    "    seed=0,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.3,\n",
    "    point_size=5,\n",
    "    alpha=0.8,\n",
    "    legend_max=25,\n",
    "    sort_legend_by=\"count\",  # \"count\" or \"name\"\n",
    "):\n",
    "    X = _as_numpy_2d(X)\n",
    "    if X is None:\n",
    "        print(f\"[skip] {title}: X is None\")\n",
    "        return\n",
    "\n",
    "    emb = _umap_embed(X, seed=seed, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    if emb is None:\n",
    "        print(f\"[skip] {title}: embedding failed\")\n",
    "        return\n",
    "\n",
    "    labels = None if labels is None else np.asarray(labels).astype(str)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    if labels is None:\n",
    "        plt.scatter(emb[:, 0], emb[:, 1], s=point_size, alpha=alpha)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"UMAP1\"); plt.ylabel(\"UMAP2\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    # Stable mapping for this plot call\n",
    "    _, uniq, counts, packed = _prep_label_mapping(labels, sort_legend_by=sort_legend_by)\n",
    "    _, c, cmap = packed\n",
    "\n",
    "    plt.scatter(\n",
    "        emb[:, 0], emb[:, 1],\n",
    "        c=c,\n",
    "        cmap=cmap,\n",
    "        s=point_size,\n",
    "        alpha=alpha,\n",
    "        linewidths=0,\n",
    "    )\n",
    "\n",
    "    handles = _legend_handles(uniq, counts, cmap, legend_max=legend_max)\n",
    "    if handles is not None:\n",
    "        plt.legend(handles=handles, bbox_to_anchor=(1.02, 1), loc=\"upper left\", frameon=False)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP1\"); plt.ylabel(\"UMAP2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_latent_umaps(enc, *, y_all, seed=0, n_neighbors=15, min_dist=0.3, point_size=5, alpha=0.8):\n",
    "    labels = _get_label_strings(enc, y_all)\n",
    "\n",
    "    plot_latent_umap(\n",
    "        enc.get(\"mu_rna\", None),\n",
    "        labels,\n",
    "        title=\"RNA latent (mu_rna)\",\n",
    "        seed=seed,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        point_size=point_size,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "\n",
    "    plot_latent_umap(\n",
    "        enc.get(\"mu_atac\", None),\n",
    "        labels,\n",
    "        title=\"ATAC latent (mu_atac)\",\n",
    "        seed=seed,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        point_size=point_size,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "\n",
    "    plot_latent_umap(\n",
    "        enc.get(\"mu_fused\", None),\n",
    "        labels,\n",
    "        title=\"Fused latent (mu_fused)\",\n",
    "        seed=seed,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        point_size=point_size,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "\n",
    "def save_umaps_for_enc(\n",
    "    enc,\n",
    "    *,\n",
    "    y_all,\n",
    "    out_png,\n",
    "    seed=0,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.3,\n",
    "    point_size=8,\n",
    "    alpha=0.75,\n",
    "    dpi=450,                 # NEW default: higher quality, but safe\n",
    "    legend_max=25,            # NEW default legend size for saved figs\n",
    "    sort_legend_by=\"count\",   # keep consistent with interactive\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves a single figure with 3 subplots (RNA/ATAC/Fused) colored by cell type.\n",
    "    Uses ONE stable label->color mapping across all panels.\n",
    "    \"\"\"\n",
    "    labels = _get_label_strings(enc, y_all)\n",
    "\n",
    "    # precompute stable mapping ONCE for consistent colors across panels\n",
    "    labs = None if labels is None else np.asarray(labels).astype(str)\n",
    "    if labs is not None:\n",
    "        _, uniq, counts, packed = _prep_label_mapping(labs, sort_legend_by=sort_legend_by)\n",
    "        _, c_all, cmap = packed\n",
    "        legend_handles = _legend_handles(uniq, counts, cmap, legend_max=legend_max)\n",
    "    else:\n",
    "        c_all, cmap, legend_handles = None, None, None\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 5), dpi=dpi)\n",
    "\n",
    "    def _plot_into(ax, X, title):\n",
    "        X = _as_numpy_2d(X)\n",
    "        if X is None:\n",
    "            ax.set_title(f\"{title} (missing)\")\n",
    "            ax.axis(\"off\")\n",
    "            return\n",
    "\n",
    "        emb = _umap_embed(X, seed=seed, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "        if emb is None:\n",
    "            ax.set_title(f\"{title} (embed failed)\")\n",
    "            ax.axis(\"off\")\n",
    "            return\n",
    "\n",
    "        if labs is None or c_all is None or cmap is None:\n",
    "            ax.scatter(emb[:, 0], emb[:, 1], s=point_size, alpha=alpha, linewidths=0)\n",
    "            ax.set_title(title)\n",
    "            return\n",
    "\n",
    "        ax.scatter(\n",
    "            emb[:, 0], emb[:, 1],\n",
    "            c=c_all,\n",
    "            cmap=cmap,\n",
    "            s=point_size,\n",
    "            alpha=alpha,\n",
    "            linewidths=0,\n",
    "        )\n",
    "        ax.set_title(title)\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "    _plot_into(ax1, enc.get(\"mu_rna\", None),   \"RNA latent (mu_rna)\")\n",
    "    _plot_into(ax2, enc.get(\"mu_atac\", None),  \"ATAC latent (mu_atac)\")\n",
    "    _plot_into(ax3, enc.get(\"mu_fused\", None), \"Fused latent (mu_fused)\")\n",
    "\n",
    "    for ax in (ax1, ax2, ax3):\n",
    "        ax.set_xlabel(\"UMAP1\")\n",
    "        ax.set_ylabel(\"UMAP2\")\n",
    "\n",
    "    # put legend on the right, shared across panels\n",
    "    if legend_handles is not None:\n",
    "        fig.legend(\n",
    "            handles=legend_handles,\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1.01, 0.5),\n",
    "            frameon=False,\n",
    "            title=f\"Cell types (top {min(int(legend_max), len(legend_handles))})\",\n",
    "        )\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_dir = os.path.dirname(out_png)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    fig.savefig(out_png, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def save_paired_overlay_umap(\n",
    "    enc,\n",
    "    *,\n",
    "    y_all,\n",
    "    out_png,\n",
    "    seed=0,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.3,\n",
    "    point_size=8,\n",
    "    alpha=0.75,\n",
    "    dpi=450,\n",
    "    legend_max=25,\n",
    "    sort_legend_by=\"count\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves a 1x2 figure where RNA and ATAC points are embedded TOGETHER in one UMAP:\n",
    "      - Left: colored by cell type (stable mapping, shared legend)\n",
    "      - Right: colored by modality (RNA vs ATAC)\n",
    "    This is the most direct “are modalities aligned?” visual.\n",
    "\n",
    "    Requires enc[\"mu_rna\"] and enc[\"mu_atac\"].\n",
    "    Assumes rows are paired; the cell_type labels are duplicated for RNA and ATAC.\n",
    "    \"\"\"\n",
    "    mu_rna = enc.get(\"mu_rna\", None)\n",
    "    mu_atac = enc.get(\"mu_atac\", None)\n",
    "\n",
    "    Xr = _as_numpy_2d(mu_rna)\n",
    "    Xa = _as_numpy_2d(mu_atac)\n",
    "\n",
    "    if Xr is None or Xa is None:\n",
    "        # still save a stub figure so the pipeline doesn't break\n",
    "        fig = plt.figure(figsize=(12, 5), dpi=dpi)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"Paired overlay UMAP (missing mu_rna or mu_atac)\")\n",
    "        out_dir = os.path.dirname(out_png)\n",
    "        if out_dir:\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "        fig.savefig(out_png, dpi=dpi, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    if Xr.shape[0] != Xa.shape[0]:\n",
    "        raise ValueError(f\"mu_rna and mu_atac must have same n for paired overlay: {Xr.shape[0]} vs {Xa.shape[0]}\")\n",
    "\n",
    "    # labels from gids if present, otherwise fallback inside your helper\n",
    "    labels = _get_label_strings(enc, y_all)\n",
    "    labs = None if labels is None else np.asarray(labels).astype(str)\n",
    "\n",
    "    # Concatenate data: [RNA; ATAC]\n",
    "    X = np.concatenate([Xr, Xa], axis=0)\n",
    "\n",
    "    # Duplicate labels for the two modalities\n",
    "    if labs is not None:\n",
    "        labs2 = np.concatenate([labs, labs], axis=0)\n",
    "        _, uniq, counts, packed = _prep_label_mapping(labs2, sort_legend_by=sort_legend_by)\n",
    "        _, c_all, cmap = packed\n",
    "        legend_handles = _legend_handles(uniq, counts, cmap, legend_max=legend_max)\n",
    "    else:\n",
    "        labs2, c_all, cmap, legend_handles = None, None, None, None\n",
    "\n",
    "    modality = np.array([\"RNA\"] * Xr.shape[0] + [\"ATAC\"] * Xa.shape[0], dtype=object)\n",
    "\n",
    "    emb = _umap_embed(X, seed=seed, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    if emb is None:\n",
    "        fig = plt.figure(figsize=(12, 5), dpi=dpi)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"Paired overlay UMAP (embed failed)\")\n",
    "        out_dir = os.path.dirname(out_png)\n",
    "        if out_dir:\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "        fig.savefig(out_png, dpi=dpi, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 6), dpi=dpi)\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    # --- Panel 1: colored by cell type ---\n",
    "    if labs2 is None or c_all is None or cmap is None:\n",
    "        ax1.scatter(emb[:, 0], emb[:, 1], s=point_size, alpha=alpha, linewidths=0)\n",
    "        ax1.set_title(\"Paired overlay (RNA+ATAC) — no labels\")\n",
    "    else:\n",
    "        ax1.scatter(\n",
    "            emb[:, 0], emb[:, 1],\n",
    "            c=c_all,\n",
    "            cmap=cmap,\n",
    "            s=point_size,\n",
    "            alpha=alpha,\n",
    "            linewidths=0,\n",
    "        )\n",
    "        ax1.set_title(\"Paired overlay (RNA+ATAC) — colored by cell type\")\n",
    "\n",
    "    # --- Panel 2: colored by modality ---\n",
    "    # fixed 2-category palette without relying on global styles\n",
    "    mod_to_int = {\"RNA\": 0, \"ATAC\": 1}\n",
    "    mod_c = np.array([mod_to_int[m] for m in modality], dtype=int)\n",
    "    mod_cmap = plt.get_cmap(\"Set1\", 2)\n",
    "\n",
    "    ax2.scatter(\n",
    "        emb[:, 0], emb[:, 1],\n",
    "        c=mod_c,\n",
    "        cmap=mod_cmap,\n",
    "        s=point_size,\n",
    "        alpha=alpha,\n",
    "        linewidths=0,\n",
    "    )\n",
    "    ax2.set_title(\"Paired overlay (RNA+ATAC) — colored by modality\")\n",
    "\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_xlabel(\"UMAP1\")\n",
    "        ax.set_ylabel(\"UMAP2\")\n",
    "\n",
    "    # shared cell-type legend (panel 1)\n",
    "    if legend_handles is not None:\n",
    "        fig.legend(\n",
    "            handles=legend_handles,\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1.01, 0.5),\n",
    "            frameon=False,\n",
    "            title=f\"Cell types (top {min(int(legend_max), len(legend_handles))})\",\n",
    "        )\n",
    "\n",
    "    # modality legend on ax2\n",
    "    mod_handles = [\n",
    "        Line2D([0], [0], marker=\"o\", color=\"none\", markerfacecolor=mod_cmap(0), markersize=6, label=\"RNA\"),\n",
    "        Line2D([0], [0], marker=\"o\", color=\"none\", markerfacecolor=mod_cmap(1), markersize=6, label=\"ATAC\"),\n",
    "    ]\n",
    "    ax2.legend(handles=mod_handles, loc=\"upper right\", frameon=False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_dir = os.path.dirname(out_png)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    fig.savefig(out_png, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb7371-f0a3-4602-8ee4-c9651099f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# alignment metrics: FOSCTTM + Recall@K (paired rows)\n",
    "# -----------------------------\n",
    "\n",
    "def _as_numpy_2d(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    return x.astype(np.float32, copy=False)\n",
    "\n",
    "def _as_numpy_1d(y):\n",
    "    if y is None:\n",
    "        return None\n",
    "    if torch.is_tensor(y):\n",
    "        y = y.detach().cpu().numpy()\n",
    "    y = np.asarray(y)\n",
    "    return y.reshape(-1)\n",
    "\n",
    "def _pairwise_sq_dists(X, Y):\n",
    "    X = _as_numpy_2d(X)\n",
    "    Y = _as_numpy_2d(Y)\n",
    "    x2 = np.sum(X * X, axis=1, keepdims=True)\n",
    "    y2 = np.sum(Y * Y, axis=1, keepdims=True).T\n",
    "    D = x2 + y2 - 2.0 * (X @ Y.T)\n",
    "    np.maximum(D, 0.0, out=D)\n",
    "    return D\n",
    "\n",
    "def foscttm(X, Y, *, symmetric=True):\n",
    "    \"\"\"\n",
    "    FOSCTTM (Fraction of Samples Closer Than the True Match).\n",
    "    Assumes paired rows: X[i] matches Y[i]. Lower is better.\n",
    "    If symmetric=True, averages X->Y and Y->X.\n",
    "    \"\"\"\n",
    "    D = _pairwise_sq_dists(X, Y)\n",
    "    n = D.shape[0]\n",
    "    diag = np.diag(D)\n",
    "\n",
    "    denom = (n - 1) if n > 1 else 1\n",
    "    frac_xy = (D < diag[:, None]).sum(axis=1) / denom\n",
    "\n",
    "    if not symmetric:\n",
    "        return float(frac_xy.mean())\n",
    "\n",
    "    frac_yx = (D < diag[None, :]).sum(axis=0) / denom\n",
    "    return float(0.5 * (frac_xy.mean() + frac_yx.mean()))\n",
    "\n",
    "def recall_at_k(X, Y, *, ks=(1, 10, 25, 50, 100), symmetric=True):\n",
    "    \"\"\"\n",
    "    Recall@K for paired matching. Hit if true match is within top-K nearest.\n",
    "    If symmetric=True, averages X->Y and Y->X.\n",
    "    \"\"\"\n",
    "    D = _pairwise_sq_dists(X, Y)\n",
    "    n = D.shape[0]\n",
    "    ks = [int(k) for k in ks if int(k) >= 1]\n",
    "\n",
    "    order_xy = np.argsort(D, axis=1)\n",
    "    pos_xy = np.empty(n, dtype=np.int32)\n",
    "    for i in range(n):\n",
    "        pos_xy[i] = np.where(order_xy[i] == i)[0][0]\n",
    "\n",
    "    if symmetric:\n",
    "        order_yx = np.argsort(D.T, axis=1)\n",
    "        pos_yx = np.empty(n, dtype=np.int32)\n",
    "        for i in range(n):\n",
    "            pos_yx[i] = np.where(order_yx[i] == i)[0][0]\n",
    "\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        k_eff = min(k, n)\n",
    "        r_xy = float((pos_xy < k_eff).mean())\n",
    "        if not symmetric:\n",
    "            out[k] = r_xy\n",
    "        else:\n",
    "            r_yx = float((pos_yx < k_eff).mean())\n",
    "            out[k] = 0.5 * (r_xy + r_yx)\n",
    "    return out\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def label_transfer_knn(X_src, y_src, X_tgt, y_tgt, *, k: int = 5):\n",
    "    \"\"\"\n",
    "    Train kNN on (X_src, y_src) and predict labels on X_tgt.\n",
    "    Returns {\"acc\": ..., \"macro_f1\": ...}.\n",
    "\n",
    "    Assumes y_* are already integer-coded (recommended).\n",
    "    \"\"\"\n",
    "    X_src = _as_numpy_2d(X_src)\n",
    "    X_tgt = _as_numpy_2d(X_tgt)\n",
    "    y_src = _as_numpy_1d(y_src)\n",
    "    y_tgt = _as_numpy_1d(y_tgt)\n",
    "\n",
    "    if X_src is None or X_tgt is None or y_src is None or y_tgt is None:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "\n",
    "    if X_src.shape[0] != y_src.shape[0] or X_tgt.shape[0] != y_tgt.shape[0]:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "\n",
    "    if X_src.shape[0] < 2 or X_tgt.shape[0] < 2:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "\n",
    "    # k must be <= n_src\n",
    "    k_eff = int(min(max(1, k), X_src.shape[0]))\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=k_eff, weights=\"distance\", metric=\"euclidean\")\n",
    "    clf.fit(X_src, y_src)\n",
    "    y_pred = clf.predict(X_tgt)\n",
    "\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_tgt, y_pred)),\n",
    "        \"macro_f1\": float(f1_score(y_tgt, y_pred, average=\"macro\")),\n",
    "    }\n",
    "    \n",
    "# -----------------------------\n",
    "# small helpers\n",
    "# -----------------------------\n",
    "\n",
    "def _get_model_and_training_info(train_result):\n",
    "    \"\"\"Accept either model or TrainResult-like object.\"\"\"\n",
    "    model = getattr(train_result, \"model\", train_result)\n",
    "    info = {}\n",
    "    for k in (\"warmup_epochs\", \"best_epoch_all\",\"best_val_all\", \"best_epoch_postwarmup\",\"best_val_postwarmup\"):\n",
    "        if hasattr(train_result, k):\n",
    "            v = getattr(train_result, k)\n",
    "            if v is not None:\n",
    "                info[k] = int(v) if (\"epoch\" in k or \"warmup\" in k) else float(v)\n",
    "    return model, info\n",
    "\n",
    "def _compute_fused_latent(enc, *, fuse_mode=\"avg\"):\n",
    "    \"\"\"\n",
    "    Return fused latent for clustering metrics on paired samples.\n",
    "    If mu_fused exists, use it; else fuse rna/atac.\n",
    "    \"\"\"\n",
    "    if enc is None:\n",
    "        return None\n",
    "    if enc.get(\"mu_fused\", None) is not None:\n",
    "        return enc[\"mu_fused\"]\n",
    "\n",
    "    mu_rna = enc.get(\"mu_rna\", None)\n",
    "    mu_atac = enc.get(\"mu_atac\", None)\n",
    "    if mu_rna is None or mu_atac is None:\n",
    "        return None\n",
    "\n",
    "    if fuse_mode == \"concat\":\n",
    "        if torch.is_tensor(mu_rna) and torch.is_tensor(mu_atac):\n",
    "            return torch.cat([mu_rna, mu_atac], dim=1)\n",
    "        return np.concatenate([_as_numpy_2d(mu_rna), _as_numpy_2d(mu_atac)], axis=1)\n",
    "\n",
    "    # default avg\n",
    "    if torch.is_tensor(mu_rna) and torch.is_tensor(mu_atac):\n",
    "        return 0.5 * (mu_rna + mu_atac)\n",
    "    return 0.5 * (_as_numpy_2d(mu_rna) + _as_numpy_2d(mu_atac))\n",
    "\n",
    "def _filter_none_labels(X, y):\n",
    "    X = _as_numpy_2d(X)\n",
    "    y = _as_numpy_1d(y)\n",
    "    if X is None or y is None:\n",
    "        return None, None\n",
    "    mask = np.array([v is not None for v in y], dtype=bool)\n",
    "    if mask.sum() < 2:\n",
    "        return None, None\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "def _safe_silhouette(X, labels):\n",
    "    \"\"\"\n",
    "    Strict silhouette: returns NaN unless all clusters have >=2 samples and >=2 clusters exist.\n",
    "    \"\"\"\n",
    "    labels = _as_numpy_1d(labels)\n",
    "    X = _as_numpy_2d(X)\n",
    "    if X is None or labels is None:\n",
    "        return np.nan\n",
    "\n",
    "    uniq, counts = np.unique(labels, return_counts=True)\n",
    "    if len(uniq) < 2 or np.any(counts < 2):\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        return float(silhouette_score(X, labels))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _safe_silhouette_drop_small_classes(X, labels, *, min_per_class: int = 2, min_total: int = 10):\n",
    "    \"\"\"\n",
    "    More forgiving silhouette for y_true:\n",
    "      - drops labels that have < min_per_class samples\n",
    "      - returns (silhouette, n_used, frac_used)\n",
    "    \"\"\"\n",
    "    labels = _as_numpy_1d(labels)\n",
    "    X = _as_numpy_2d(X)\n",
    "    if X is None or labels is None:\n",
    "        return np.nan, 0, 0.0\n",
    "\n",
    "    uniq, counts = np.unique(labels, return_counts=True)\n",
    "    keep_labs = set(uniq[counts >= int(min_per_class)].tolist())\n",
    "    if len(keep_labs) < 2:\n",
    "        return np.nan, 0, 0.0\n",
    "\n",
    "    keep = np.array([lab in keep_labs for lab in labels], dtype=bool)\n",
    "    n_used = int(keep.sum())\n",
    "    if n_used < int(min_total):\n",
    "        return np.nan, n_used, float(n_used / max(len(labels), 1))\n",
    "\n",
    "    Xk = X[keep]\n",
    "    yk = labels[keep]\n",
    "\n",
    "    # still need >=2 samples per remaining class (should hold, but double-check)\n",
    "    uniq2, counts2 = np.unique(yk, return_counts=True)\n",
    "    if len(uniq2) < 2 or np.any(counts2 < 2):\n",
    "        return np.nan, n_used, float(n_used / max(len(labels), 1))\n",
    "\n",
    "    try:\n",
    "        sil = float(silhouette_score(Xk, yk))\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "\n",
    "    return sil, n_used, float(n_used / max(len(labels), 1))\n",
    "\n",
    "def _cluster_metrics_on_latent(X, y_true, *, n_clusters=None, kmeans_seed=0):\n",
    "    \"\"\"\n",
    "    Clustering & separation metrics on latent.\n",
    "    - KMeans metrics use all samples (if k>=2).\n",
    "    - SIL_true is computed robustly by dropping tiny classes (otherwise it's often NaN on splits).\n",
    "    \"\"\"\n",
    "    X = _as_numpy_2d(X)\n",
    "    y_true = _as_numpy_1d(y_true)\n",
    "\n",
    "    if X is None or y_true is None:\n",
    "        return {}\n",
    "\n",
    "    X, y_true = _filter_none_labels(X, y_true)\n",
    "    if X is None or y_true is None:\n",
    "        return {\n",
    "            \"n_labels\": 0, \"kmeans_k\": 0,\n",
    "            \"ARI\": np.nan, \"NMI\": np.nan,\n",
    "            \"SIL_kmeans\": np.nan, \"SIL_true\": np.nan,\n",
    "            \"SIL_true_n\": 0, \"SIL_true_frac\": 0.0,\n",
    "            \"CH_kmeans\": np.nan, \"DB_kmeans\": np.nan,\n",
    "        }\n",
    "\n",
    "    uniq = np.unique(y_true)\n",
    "    k = int(n_clusters) if n_clusters is not None else int(len(uniq))\n",
    "\n",
    "    # SIL_true: robust version (drop singleton labels)\n",
    "    sil_true, sil_true_n, sil_true_frac = _safe_silhouette_drop_small_classes(X, y_true)\n",
    "\n",
    "    if k < 2:\n",
    "        return {\n",
    "            \"n_labels\": int(len(uniq)),\n",
    "            \"kmeans_k\": int(k),\n",
    "            \"ARI\": np.nan,\n",
    "            \"NMI\": np.nan,\n",
    "            \"SIL_kmeans\": np.nan,\n",
    "            \"SIL_true\": float(sil_true),\n",
    "            \"SIL_true_n\": int(sil_true_n),\n",
    "            \"SIL_true_frac\": float(sil_true_frac),\n",
    "            \"CH_kmeans\": np.nan,\n",
    "            \"DB_kmeans\": np.nan,\n",
    "        }\n",
    "\n",
    "    km = KMeans(n_clusters=k, n_init=20, random_state=int(kmeans_seed))\n",
    "    y_pred = km.fit_predict(X)\n",
    "\n",
    "    out = {\n",
    "        \"n_labels\": int(len(uniq)),\n",
    "        \"kmeans_k\": int(k),\n",
    "        \"ARI\": float(adjusted_rand_score(y_true, y_pred)),\n",
    "        \"NMI\": float(normalized_mutual_info_score(y_true, y_pred)),\n",
    "        \"SIL_kmeans\": _safe_silhouette(X, y_pred),\n",
    "        \"SIL_true\": float(sil_true),\n",
    "        \"SIL_true_n\": int(sil_true_n),\n",
    "        \"SIL_true_frac\": float(sil_true_frac),\n",
    "    }\n",
    "    try:\n",
    "        out[\"CH_kmeans\"] = float(calinski_harabasz_score(X, y_pred))\n",
    "    except Exception:\n",
    "        out[\"CH_kmeans\"] = np.nan\n",
    "    try:\n",
    "        out[\"DB_kmeans\"] = float(davies_bouldin_score(X, y_pred))\n",
    "    except Exception:\n",
    "        out[\"DB_kmeans\"] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "def first_not_none(*vals):\n",
    "    for v in vals:\n",
    "        if v is not None:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _lt_or_nans(X_src, y_src, X_tgt, y_tgt, *, k):\n",
    "    \"\"\"\n",
    "    Safe wrapper around label_transfer_knn.\n",
    "    Returns dict with acc/macro_f1 always present.\n",
    "    \"\"\"\n",
    "    X_src = _as_numpy_2d(X_src)\n",
    "    X_tgt = _as_numpy_2d(X_tgt)\n",
    "    y_src = _as_numpy_1d(y_src)\n",
    "    y_tgt = _as_numpy_1d(y_tgt)\n",
    "\n",
    "    if X_src is None or X_tgt is None or y_src is None or y_tgt is None:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "    if X_src.shape[0] != y_src.shape[0] or X_tgt.shape[0] != y_tgt.shape[0]:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "    if X_src.shape[0] < 2 or X_tgt.shape[0] < 2:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "    if len(np.unique(y_src)) < 2:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "\n",
    "    kk = int(min(k, max(1, X_src.shape[0] - 1)))\n",
    "    try:\n",
    "        out = label_transfer_knn(X_src, y_src, X_tgt, y_tgt, k=kk)\n",
    "        return {\"acc\": float(out.get(\"acc\", np.nan)), \"macro_f1\": float(out.get(\"macro_f1\", np.nan))}\n",
    "    except Exception:\n",
    "        return {\"acc\": np.nan, \"macro_f1\": np.nan}\n",
    "\n",
    "# -----------------------------\n",
    "# main runner\n",
    "# -----------------------------\n",
    "\n",
    "def run_fig8_missing_modality_curve_v1(\n",
    "    overlap_grid=(1.0, 0.975, 0.95, 0.925, 0.9, 0.85, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.025, 0.02, 0.0175, 0.0150, 0.0125, 0.01, 0.005, 0.001),\n",
    "    *,\n",
    "    seed: int = 42,\n",
    "    drop_modality: str = \"atac\",\n",
    "    k_knn: int = 3,\n",
    "    batch_size: int = 256,\n",
    "    v1_recon: str = \"avg\",\n",
    "    adaptive_batch_size: bool = True,\n",
    "    oversample_paired: bool = True,\n",
    "    fuse_mode: str = \"avg\",\n",
    "    n_clusters: int | None = None,\n",
    "    kmeans_seed: int = 0,\n",
    "    warmup_epochs: int = 50,\n",
    "    patience: int = 100,\n",
    "    min_delta: float = 0.0,\n",
    "    allow_unpaired_test: bool = True,\n",
    "\n",
    "    # --- NEW: UMAP saving controls ---\n",
    "    umap_out_dir: str | None = None,\n",
    "    umap_use: str = \"test\",                 # \"test\" or \"val\"\n",
    "    save_umaps_every: int = 1,\n",
    "    save_umaps_if_overlap_leq: float | None = None,  # e.g. 0.1 to only save tail\n",
    "    umap_seed: int | None = None,\n",
    "    umap_n_neighbors: int = 15,\n",
    "    umap_min_dist: float = 0.3,\n",
    "    umap_point_size: int = 6,\n",
    "    umap_alpha: float = 0.75,\n",
    "    umap_dpi: int = 450,\n",
    "    umap_legend_max: int = 25,\n",
    "    umap_sort_legend_by: str = \"count\",\n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    if umap_seed is None:\n",
    "        umap_seed = int(seed)\n",
    "\n",
    "    for i, overlap in enumerate(overlap_grid):\n",
    "        p = float(overlap)\n",
    "        bs_use = choose_batch_size(p, batch_size) if adaptive_batch_size else int(batch_size)\n",
    "        print(f\"[overlap={p:g}] batch_size={bs_use}\")\n",
    "\n",
    "        train_loader, val_loader, test_loader, comp = make_loaders_with_overlap_v1(\n",
    "            dataset, train_idx, val_idx, test_idx,\n",
    "            overlap_fraction=p,\n",
    "            drop_modality=str(drop_modality),\n",
    "            seed=int(seed),\n",
    "            batch_size=int(bs_use),\n",
    "            oversample_paired=bool(oversample_paired),\n",
    "        )\n",
    "\n",
    "        train_result = train_one_overlap_v1(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            seed=int(seed),\n",
    "            loss_mode=\"v1\",\n",
    "            v1_recon=str(v1_recon),\n",
    "            warmup_epochs=int(warmup_epochs),\n",
    "            patience=int(patience),\n",
    "            min_delta=float(min_delta),\n",
    "        )\n",
    "\n",
    "        model, train_info = _get_model_and_training_info(train_result)\n",
    "\n",
    "        # --- encode VAL first (stable mapping), then TEST ---\n",
    "        enc_val = encode_embeddings_with_labels(\n",
    "            model, val_loader, device=device, y_all=y_all, id_to_int=None\n",
    "        )\n",
    "        enc_test = encode_embeddings_with_labels(\n",
    "            model, test_loader, device=device, y_all=y_all, id_to_int=enc_val.get(\"id_to_int\", None)\n",
    "        )\n",
    "\n",
    "        # --- NEW: save UMAPs ---\n",
    "        if umap_out_dir is not None:\n",
    "            do_every = (int(save_umaps_every) <= 1) or (i % int(save_umaps_every) == 0)\n",
    "            do_thresh = (save_umaps_if_overlap_leq is None) or (p <= float(save_umaps_if_overlap_leq))\n",
    "            if do_every and do_thresh:\n",
    "                enc_plot = enc_test if str(umap_use).lower() == \"test\" else enc_val\n",
    "\n",
    "                tag = f\"drop-{drop_modality}__overlap-{p:.6g}__bs-{int(bs_use)}\"\n",
    "                out1 = os.path.join(umap_out_dir, f\"{tag}__single_latents.png\")\n",
    "                out2 = os.path.join(umap_out_dir, f\"{tag}__paired_overlay.png\")\n",
    "\n",
    "                # 3-panel (RNA/ATAC/Fused) colored by cell type\n",
    "                save_umaps_for_enc(\n",
    "                    enc_plot,\n",
    "                    y_all=y_all,\n",
    "                    out_png=out1,\n",
    "                    seed=int(umap_seed),\n",
    "                    n_neighbors=int(umap_n_neighbors),\n",
    "                    min_dist=float(umap_min_dist),\n",
    "                    point_size=int(umap_point_size),\n",
    "                    alpha=float(umap_alpha),\n",
    "                    dpi=int(umap_dpi),\n",
    "                    legend_max=int(umap_legend_max),\n",
    "                    sort_legend_by=str(umap_sort_legend_by),\n",
    "                )\n",
    "\n",
    "                # overlay (RNA+ATAC embedded together) colored by cell type + modality\n",
    "                save_paired_overlay_umap(\n",
    "                    enc_plot,\n",
    "                    y_all=y_all,\n",
    "                    out_png=out2,\n",
    "                    seed=int(umap_seed),\n",
    "                    n_neighbors=int(umap_n_neighbors),\n",
    "                    min_dist=float(umap_min_dist),\n",
    "                    point_size=int(umap_point_size),\n",
    "                    alpha=float(umap_alpha),\n",
    "                    dpi=int(umap_dpi),\n",
    "                    legend_max=int(umap_legend_max),\n",
    "                    sort_legend_by=str(umap_sort_legend_by),\n",
    "                )\n",
    "\n",
    "        mu_rna_test  = enc_test.get(\"mu_rna\", None)\n",
    "        mu_atac_test = enc_test.get(\"mu_atac\", None)\n",
    "\n",
    "        y_test = first_not_none(enc_test.get(\"y_fused\"), enc_test.get(\"y_rna\"), enc_test.get(\"y_atac\"))\n",
    "        y_test_np = _as_numpy_1d(y_test)\n",
    "\n",
    "        row = {\n",
    "            \"overlap_fraction\": p,\n",
    "            \"batch_size\": int(bs_use),\n",
    "            \"drop_modality\": str(drop_modality),\n",
    "            \"v1_recon\": str(v1_recon),\n",
    "            \"fuse_mode\": str(fuse_mode),\n",
    "            **comp,\n",
    "            **train_info,\n",
    "        }\n",
    "\n",
    "        # --- alignment metrics on paired test ---\n",
    "        if mu_rna_test is None or mu_atac_test is None:\n",
    "            if not allow_unpaired_test:\n",
    "                raise RuntimeError(\"Test loader should be paired; missing mu_rna or mu_atac.\")\n",
    "            row[\"FOSCTTM\"] = np.nan\n",
    "            for kk in (1, 10, 25, 50, 100):\n",
    "                row[f\"Recall@{kk}\"] = np.nan\n",
    "        else:\n",
    "            X_rna  = _as_numpy_2d(mu_rna_test)\n",
    "            X_atac = _as_numpy_2d(mu_atac_test)\n",
    "            row[\"FOSCTTM\"] = float(foscttm(X_rna, X_atac))\n",
    "            recs = recall_at_k(X_rna, X_atac, ks=(1, 10, 25, 50, 100))\n",
    "            for kk, v in recs.items():\n",
    "                row[f\"Recall@{kk}\"] = float(v)\n",
    "\n",
    "        # --- label transfer: VAL -> TEST ---\n",
    "        lt_r2a = _lt_or_nans(\n",
    "            enc_val.get(\"mu_rna\"),  enc_val.get(\"y_rna\"),\n",
    "            enc_test.get(\"mu_atac\"), enc_test.get(\"y_atac\"),\n",
    "            k=int(k_knn),\n",
    "        )\n",
    "        lt_a2r = _lt_or_nans(\n",
    "            enc_val.get(\"mu_atac\"), enc_val.get(\"y_atac\"),\n",
    "            enc_test.get(\"mu_rna\"),  enc_test.get(\"y_rna\"),\n",
    "            k=int(k_knn),\n",
    "        )\n",
    "        row[\"LT_RNA2ATAC_acc\"] = float(lt_r2a[\"acc\"])\n",
    "        row[\"LT_RNA2ATAC_macroF1\"] = float(lt_r2a[\"macro_f1\"])\n",
    "        row[\"LT_ATAC2RNA_acc\"] = float(lt_a2r[\"acc\"])\n",
    "        row[\"LT_ATAC2RNA_macroF1\"] = float(lt_a2r[\"macro_f1\"])\n",
    "\n",
    "        # --- clustering/separation metrics on fused latent (test) ---\n",
    "        mu_fused = _compute_fused_latent(enc_test, fuse_mode=str(fuse_mode))\n",
    "        row.update(_cluster_metrics_on_latent(mu_fused, y_test_np, n_clusters=n_clusters, kmeans_seed=kmeans_seed))\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b965381-c3ad-4490-84ba-175b85858d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_drop_atac_fig8 = run_fig8_missing_modality_curve_v1(\n",
    "    drop_modality=\"atac\",\n",
    "    warmup_epochs=0,\n",
    "    fuse_mode=\"moe\",\n",
    "    umap_out_dir=\"./ablation_umaps_drop_atac\",\n",
    "    #save_umaps_if_overlap_leq=0.10,   # only save when <= 10% paired\n",
    "    save_umaps_if_overlap_leq=1.00,\n",
    "    save_umaps_every=1,\n",
    "    umap_point_size=5,\n",
    "    umap_alpha=0.75,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd25c1-6def-49f7-8cbb-a897ac475e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_drop_atac_fig8 = run_fig8_missing_modality_curve_v1(drop_modality=\"atac\", warmup_epochs=0, fuse_mode=\"avg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50b3f8-bbbd-41f2-9bc9-2f2dde57823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_atac_fig8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c9300-a41f-4da6-8222-a4092e2af683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_drop_rna_fig8 = run_fig8_missing_modality_curve_v1(\n",
    "    drop_modality=\"rna\",\n",
    "    warmup_epochs=0,\n",
    "    fuse_mode=\"moe\",\n",
    "    umap_out_dir=\"./results/figure_9_paired_ablation_outputs-2-10-2026/ablation_umaps_drop_rna\",\n",
    "    #save_umaps_if_overlap_leq=0.10,   # only save when <= 10% paired\n",
    "    save_umaps_if_overlap_leq=1.00,\n",
    "    save_umaps_every=1,\n",
    "    umap_point_size=8,\n",
    "    umap_alpha=0.75,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e77e1-c14d-4ef5-a228-51b3c659df15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_drop_rna_fig8  = run_fig8_missing_modality_curve_v1(drop_modality=\"rna\", warmup_epochs=0, fuse_mode=\"avg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf27122-e71f-4e49-86f1-eb97943f9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_rna_fig8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b4264-5cf8-42d7-ab29-79ad261e3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a TSV in the current working directory\n",
    "df_drop_rna_fig8.to_csv(\"./results/figure_9_paired_ablation_outputs-2-10-2026/df_paired_ablation_rna.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# (optional) also save a gzipped TSV\n",
    "#df_drop_rna_fig8.to_csv(\"df_drop_rna_fig8.tsv.gz\", sep=\"\\t\", index=False, compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914aaeeb-18d5-4d89-b019-b41bde0d3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a TSV in the current working directory\n",
    "df_drop_atac_fig8.to_csv(\"./results/figure_9_paired_ablation_outputs-2-10-2026/df_paired_ablation_atac.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# (optional) also save a gzipped TSV\n",
    "#df_drop_atac_fig8.to_csv(\"df_drop_rna_fig8.tsv.gz\", sep=\"\\t\", index=False, compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77066e",
   "metadata": {},
   "source": [
    "## 7) Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5c12f-a614-433a-9440-a2e59721a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    # Figure size (inches)\n",
    "    \"figure.figsize\": (12, 10),\n",
    "\n",
    "    # DPI: displayed in notebooks vs saved files\n",
    "    \"figure.dpi\": 300,      # notebook display\n",
    "    \"savefig.dpi\": 300,     # saved output\n",
    "\n",
    "    # Fonts\n",
    "    \"font.size\": 11,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"legend.fontsize\": 10,\n",
    "\n",
    "    # Lines/markers\n",
    "    \"lines.linewidth\": 1.6,\n",
    "    \"lines.markersize\": 5,\n",
    "\n",
    "    # Layout + export\n",
    "    \"figure.autolayout\": False,     # if True, similar to tight_layout each time\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"savefig.pad_inches\": 0.02,\n",
    "})\n",
    "\n",
    "#mpl.rcdefaults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c650b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_fig8(df: pd.DataFrame, *, save_prefix: str | None = None):\n",
    "    df = df.sort_values(\"overlap_fraction\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(df[\"overlap_fraction\"], df[\"FOSCTTM\"], marker=\"o\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xlabel(\"Paired overlap fraction in training\")\n",
    "    plt.ylabel(\"FOSCTTM (lower better)\")\n",
    "    plt.title(\"Figure 8A: alignment vs missing modality\")\n",
    "    if save_prefix:\n",
    "        plt.savefig(f\"{save_prefix}_foscttm.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(df[\"overlap_fraction\"], df[\"LT_RNA2ATAC_acc\"], marker=\"o\", label=\"RNA→ATAC acc\")\n",
    "    plt.plot(df[\"overlap_fraction\"], df[\"LT_ATAC2RNA_acc\"], marker=\"o\", label=\"ATAC→RNA acc\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xlabel(\"Paired overlap fraction in training\")\n",
    "    plt.ylabel(\"Label transfer accuracy\")\n",
    "    plt.title(\"Figure 8B: label transfer vs missing modality\")\n",
    "    plt.legend()\n",
    "    if save_prefix:\n",
    "        plt.savefig(f\"{save_prefix}_label_transfer.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6dc33-25f2-433f-ada6-aa5d2abed4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig8(df_drop_atac_fig8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd6918-6523-41f1-933e-276ee39f937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig8(df_drop_rna_fig8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106051f1-88b1-4043-95e3-1a089815bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig8_recall(df: pd.DataFrame, ks=(1,10,25,50,100), save_prefix=None):\n",
    "    df = df.sort_values(\"overlap_fraction\")\n",
    "    plt.figure()\n",
    "    for k in ks:\n",
    "        col = f\"Recall@{k}\"\n",
    "        if col in df.columns:\n",
    "            plt.plot(df[\"overlap_fraction\"], df[col], marker=\"o\", label=col)\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xlabel(\"Paired overlap fraction in training\")\n",
    "    plt.ylabel(\"Recall@K (higher better)\")\n",
    "    plt.title(\"Figure 8: cross-modal retrieval vs missing modality\")\n",
    "    plt.legend()\n",
    "    if save_prefix:\n",
    "        plt.savefig(f\"{save_prefix}_recall.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251a141-0287-462c-b209-f2c835321863",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig8_recall(df_drop_atac_fig8)\n",
    "#plot_fig8_recall(df_fig8, save_prefix=\"./figures/fig8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63470cdc-51e5-48dc-98ec-b2e2d1e3a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig8_recall(df_drop_rna_fig8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0284e0-daff-4ebb-863a-c434cefac6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig8_labeltransfer_f1(df: pd.DataFrame, save_prefix=None):\n",
    "    df = df.sort_values(\"overlap_fraction\")\n",
    "    plt.figure()\n",
    "    if \"LT_RNA2ATAC_macroF1\" in df.columns:\n",
    "        plt.plot(df[\"overlap_fraction\"], df[\"LT_RNA2ATAC_macroF1\"], marker=\"o\", label=\"RNA→ATAC macroF1\")\n",
    "    if \"LT_ATAC2RNA_macroF1\" in df.columns:\n",
    "        plt.plot(df[\"overlap_fraction\"], df[\"LT_ATAC2RNA_macroF1\"], marker=\"o\", label=\"ATAC→RNA macroF1\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xlabel(\"Paired overlap fraction in training\")\n",
    "    plt.ylabel(\"Label transfer macroF1\")\n",
    "    plt.title(\"Figure 8: label transfer macroF1 vs missing modality\")\n",
    "    plt.legend()\n",
    "    if save_prefix:\n",
    "        plt.savefig(f\"{save_prefix}_label_transfer_macroF1.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99de228-e6c2-45ca-bd3b-f07f2ff8117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig8_labeltransfer_f1(df_drop_atac_fig8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e2cc7-daa4-47b0-a5c2-43212171b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig8_labeltransfer_f1(df_drop_rna_fig8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539ae8a-139b-4711-979d-f98598c8aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_band(df_all, ycol, title, savepath=None):\n",
    "    g = df_all.groupby(\"overlap_fraction\")[ycol]\n",
    "    x = np.array(sorted(df_all[\"overlap_fraction\"].unique()))\n",
    "    mean = np.array([g.get_group(v).mean() for v in x])\n",
    "    std  = np.array([g.get_group(v).std(ddof=1) for v in x])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, mean, marker=\"o\")\n",
    "    plt.fill_between(x, mean-std, mean+std, alpha=0.2)\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xlabel(\"Paired overlap fraction in training\")\n",
    "    plt.ylabel(ycol)\n",
    "    plt.title(title)\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73678d9f-1a9b-40a8-9cad-267b42938e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_with_band(df_all, \"FOSCTTM\", \"Fig 8A: FOSCTTM vs overlap (mean±sd)\", \"./figures/fig8_foscttm_band.png\")\n",
    "#plot_with_band(df_all, \"LT_RNA2ATAC_acc\", \"Fig 8B: RNA→ATAC acc vs overlap (mean±sd)\", \"./figures/fig8_lt_r2a_band.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd70f2-8c55-493b-9229-3f20956caf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Band plot helpers\n",
    "# -----------------------------\n",
    "\n",
    "def _ensure_dir(path: str | None):\n",
    "    if path is None:\n",
    "        return\n",
    "    d = os.path.dirname(path)\n",
    "    if d:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def _infer_metric_cols(df: pd.DataFrame, *, x_col=\"overlap_fraction\", exclude=()):\n",
    "    \"\"\"\n",
    "    Heuristic: numeric cols excluding x_col and other known metadata.\n",
    "    \"\"\"\n",
    "    exclude = set(exclude) | {x_col}\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    # drop obvious bookkeeping columns if present\n",
    "    drop_like = {\n",
    "        \"seed\", \"fold\", \"run\", \"repeat\",\n",
    "        \"batch_size\",\n",
    "        \"warmup_epochs\", \"best_epoch_all\", \"best_val_all\",\n",
    "        \"best_epoch_postwarmup\", \"best_val_postwarmup\",\n",
    "        \"n_labels\", \"kmeans_k\", \"SIL_true_n\"\n",
    "    }\n",
    "    exclude |= drop_like.intersection(num_cols)\n",
    "    return [c for c in num_cols if c not in exclude]\n",
    "\n",
    "def summarize_for_band(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    *,\n",
    "    x_col: str = \"overlap_fraction\",\n",
    "    group_cols: tuple[str, ...] = (),\n",
    "    agg: str = \"sd\",          # \"sd\" | \"sem\" | \"quantile\"\n",
    "    q: tuple[float, float] = (0.25, 0.75),\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a tidy summary DataFrame with columns:\n",
    "      [*group_cols, x_col, mean, lo, hi, n]\n",
    "    where lo/hi are the band bounds.\n",
    "    \"\"\"\n",
    "    cols = [x_col, metric, *group_cols]\n",
    "    d = df.loc[:, [c for c in cols if c in df.columns]].copy()\n",
    "    d = d.dropna(subset=[x_col, metric])\n",
    "\n",
    "    if d.empty:\n",
    "        return pd.DataFrame(columns=[*group_cols, x_col, \"mean\", \"lo\", \"hi\", \"n\"])\n",
    "\n",
    "    gb_keys = [*group_cols, x_col] if group_cols else [x_col]\n",
    "    g = d.groupby(gb_keys, dropna=False)[metric]\n",
    "\n",
    "    if agg == \"quantile\":\n",
    "        lo_q, hi_q = q\n",
    "        out = g.agg(\n",
    "            mean=\"mean\",\n",
    "            lo=lambda s: float(s.quantile(lo_q)),\n",
    "            hi=lambda s: float(s.quantile(hi_q)),\n",
    "            n=\"count\",\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        out = g.agg(mean=\"mean\", sd=\"std\", n=\"count\").reset_index()\n",
    "        out[\"sd\"] = out[\"sd\"].fillna(0.0)\n",
    "\n",
    "        if agg == \"sem\":\n",
    "            out[\"err\"] = out[\"sd\"] / np.sqrt(np.maximum(out[\"n\"].to_numpy(), 1))\n",
    "        elif agg == \"sd\":\n",
    "            out[\"err\"] = out[\"sd\"]\n",
    "        else:\n",
    "            raise ValueError(f\"agg must be one of ['sd','sem','quantile'], got {agg!r}\")\n",
    "\n",
    "        out[\"lo\"] = out[\"mean\"] - out[\"err\"]\n",
    "        out[\"hi\"] = out[\"mean\"] + out[\"err\"]\n",
    "        out = out.drop(columns=[\"sd\", \"err\"])\n",
    "\n",
    "    # sort nicely for plotting\n",
    "    out = out.sort_values([*group_cols, x_col] if group_cols else [x_col]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def plot_with_band(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    title: str,\n",
    "    out_png: str | None = None,\n",
    "    *,\n",
    "    x_col: str = \"overlap_fraction\",\n",
    "    group_cols: tuple[str, ...] = (),\n",
    "    agg: str = \"sd\",                # \"sd\" | \"sem\" | \"quantile\"\n",
    "    q: tuple[float, float] = (0.25, 0.75),\n",
    "    xlabel: str | None = None,\n",
    "    ylabel: str | None = None,\n",
    "    logx: bool = False,\n",
    "    invert_x: bool = True,          # overlap often decreases left->right; set False if you don’t want that\n",
    "    ylim: tuple[float, float] | None = None,\n",
    "    figsize=(12, 10),\n",
    "    dpi: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single-metric band plot. If group_cols given, draws one line per group.\n",
    "    \"\"\"\n",
    "    summ = summarize_for_band(df, metric, x_col=x_col, group_cols=group_cols, agg=agg, q=q)\n",
    "    if summ.empty:\n",
    "        raise ValueError(f\"No data to plot for metric {metric!r} (after dropping NaNs).\")\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    if group_cols:\n",
    "        # Each unique group gets a line\n",
    "        group_df = summ.groupby(list(group_cols), dropna=False)\n",
    "        for gkey, sub in group_df:\n",
    "            label = \" | \".join([f\"{c}={v}\" for c, v in zip(group_cols, (gkey if isinstance(gkey, tuple) else (gkey,)))])\n",
    "            x = sub[x_col].to_numpy()\n",
    "            m = sub[\"mean\"].to_numpy()\n",
    "            lo = sub[\"lo\"].to_numpy()\n",
    "            hi = sub[\"hi\"].to_numpy()\n",
    "\n",
    "            ax.plot(x, m, label=label)\n",
    "            ax.fill_between(x, lo, hi, alpha=0.2)\n",
    "    else:\n",
    "        x = summ[x_col].to_numpy()\n",
    "        m = summ[\"mean\"].to_numpy()\n",
    "        lo = summ[\"lo\"].to_numpy()\n",
    "        hi = summ[\"hi\"].to_numpy()\n",
    "\n",
    "        ax.plot(x, m)\n",
    "        ax.fill_between(x, lo, hi, alpha=0.2)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel or x_col)\n",
    "    ax.set_ylabel(ylabel or metric)\n",
    "\n",
    "    if logx:\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "    if invert_x:\n",
    "        ax.invert_xaxis()\n",
    "\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    if group_cols:\n",
    "        ax.legend(frameon=False, fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if out_png is not None:\n",
    "        _ensure_dir(out_png)\n",
    "        plt.savefig(out_png, dpi=dpi)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return summ  # handy if you want to inspect the aggregated numbers\n",
    "\n",
    "def plot_many_metrics_with_band(\n",
    "    df: pd.DataFrame,\n",
    "    metrics: list[str] | None = None,\n",
    "    out_dir: str | None = \"./figures/fig8_bands\",\n",
    "    *,\n",
    "    x_col: str = \"overlap_fraction\",\n",
    "    group_cols: tuple[str, ...] = (),\n",
    "    agg: str = \"sd\",\n",
    "    q: tuple[float, float] = (0.25, 0.75),\n",
    "    invert_x: bool = True,\n",
    "    title_prefix: str = \"\",\n",
    "    exclude_cols=(),\n",
    "):\n",
    "    \"\"\"\n",
    "    If out_dir is None: show plots in notebook (no saving).\n",
    "    Else: save one PNG per metric into out_dir.\n",
    "    \"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = _infer_metric_cols(df, x_col=x_col, exclude=exclude_cols)\n",
    "\n",
    "    if out_dir is not None:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    summaries = {}\n",
    "    for met in metrics:\n",
    "        out_png = None if out_dir is None else os.path.join(out_dir, f\"{met}__band.png\")\n",
    "        title = f\"{title_prefix}{met} vs {x_col} ({agg})\"\n",
    "        summ = plot_with_band(\n",
    "            df, met, title, out_png,\n",
    "            x_col=x_col, group_cols=group_cols, agg=agg, q=q,\n",
    "            invert_x=invert_x,\n",
    "        )\n",
    "        summaries[met] = summ\n",
    "\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996ac2e-39e6-436e-a965-60c680bae210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your two explicit calls (mean ± sd over repeats)\n",
    "'''\n",
    "plot_with_band(\n",
    "    df_all, \"FOSCTTM\",\n",
    "    \"Fig 8A: FOSCTTM vs overlap (mean±sd)\",\n",
    "    out_png=None,\n",
    "    agg=\"sd\",\n",
    ")\n",
    "\n",
    "plot_with_band(\n",
    "    df_all, \"LT_RNA2ATAC_acc\",\n",
    "    \"Fig 8B: RNA→ATAC acc vs overlap (mean±sd)\",\n",
    "    out_png=None,\n",
    "    agg=\"sd\",\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f29e69-9615-4b7b-91ff-41279f67b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One PNG per metric, autodetected from numeric columns\n",
    "'''\n",
    "plot_many_metrics_with_band(\n",
    "    df_all,\n",
    "    #out_dir=\"./figures/fig8_all_metric_bands\",\n",
    "    out_dir=None,    \n",
    "    agg=\"sem\",               # or \"sd\" or \"quantile\"\n",
    "    #agg=\"sd\",\n",
    "    invert_x=True,\n",
    "    title_prefix=\"Fig8: \",\n",
    "    exclude_cols=(\"overlap_fraction\", \"train_paired\", \"train_unpaired\", \"unpaired_per_paired\", \"sampler_unpaired_per_paired\", \"SIL_true_frac\",),  # optional\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a78a242-ac81-4b49-8d0f-96a5c6914afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do different data splits per seed and see how that effects the eval results\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_dataset_for_split_seed(\n",
    "    *,\n",
    "    rna_raw,\n",
    "    atac_raw,\n",
    "    label_key: str,\n",
    "    split_seed: int,\n",
    "    train_frac: float = 0.8,\n",
    "    val_frac: float = 0.1,\n",
    "    # preprocessing params\n",
    "    rna_counts_layer: str = \"counts\",\n",
    "    atac_counts_layer: str = \"counts\",\n",
    "    n_hvg: int = 2000,\n",
    "    target_sum: float = 1e4,\n",
    "    n_lsi: int = 101,\n",
    "    preprocess_seed: int | None = None,  # if None, use split_seed\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      dataset, train_idx, val_idx, test_idx, y_all, (rna_tr_pp, atac_tr_lsi)  # last pair for dim lookups\n",
    "    \"\"\"\n",
    "    if preprocess_seed is None:\n",
    "        preprocess_seed = int(split_seed)\n",
    "\n",
    "    # --- 1) resample splits ---\n",
    "    rng = np.random.default_rng(int(split_seed))\n",
    "    n = int(rna_raw.n_obs)\n",
    "    idx = np.arange(n, dtype=np.int64)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n_tr = int(round(train_frac * n))\n",
    "    n_va = int(round(val_frac * n))\n",
    "    n_tr = max(1, min(n_tr, n - 2))\n",
    "    n_va = max(1, min(n_va, n - n_tr - 1))\n",
    "    n_te = n - n_tr - n_va\n",
    "\n",
    "    tr_idx = idx[:n_tr]\n",
    "    va_idx = idx[n_tr:n_tr + n_va]\n",
    "    te_idx = idx[n_tr + n_va:]\n",
    "\n",
    "    rna_train, rna_val, rna_test = rna_raw[tr_idx].copy(), rna_raw[va_idx].copy(), rna_raw[te_idx].copy()\n",
    "    atac_train, atac_val, atac_test = atac_raw[tr_idx].copy(), atac_raw[va_idx].copy(), atac_raw[te_idx].copy()\n",
    "\n",
    "    # --- 2) fit preprocessing on TRAIN only; apply to val/test ---\n",
    "    (rna_tr_pp, atac_tr_lsi,\n",
    "     rna_va_pp, atac_va_lsi,\n",
    "     rna_te_pp, atac_te_lsi,\n",
    "     rna_fit, atac_fit) = preprocess_multiome_splits_fit_apply(\n",
    "        rna_train, atac_train,\n",
    "        rna_val,   atac_val,\n",
    "        rna_test,  atac_test,\n",
    "        rna_counts_layer=rna_counts_layer,\n",
    "        atac_counts_layer=atac_counts_layer,\n",
    "        n_hvg=int(n_hvg),\n",
    "        target_sum=float(target_sum),\n",
    "        n_lsi=int(n_lsi),\n",
    "        seed=int(preprocess_seed),\n",
    "    )\n",
    "\n",
    "    # --- 3) rebuild dataset exactly like your current notebook does ---\n",
    "    adata_dict = {\n",
    "        \"rna\":  rna_tr_pp.concatenate(rna_va_pp, rna_te_pp, batch_key=None),\n",
    "        \"atac\": atac_tr_lsi.concatenate(atac_va_lsi, atac_te_lsi, batch_key=None),\n",
    "    }\n",
    "    adata_dict = align_paired_obs_names(adata_dict)\n",
    "\n",
    "    base_ds = MultiModalDataset(adata_dict=adata_dict, X_key=\"X\", device=None)\n",
    "\n",
    "    # integer labels for metrics / encoding\n",
    "    y_str = adata_dict[\"rna\"].obs[label_key].astype(str).to_numpy()\n",
    "    uniq = np.unique(y_str)\n",
    "    label_to_int = {lab: i for i, lab in enumerate(uniq)}\n",
    "    y_int = np.array([label_to_int[x] for x in y_str], dtype=np.int64)\n",
    "\n",
    "    dataset_labeled = LabeledMultiModalDataset(base_ds, y_int=y_int, y_str=None)\n",
    "\n",
    "    # indices into the concatenated dataset\n",
    "    train_idx = np.arange(0, n_tr, dtype=np.int64)\n",
    "    val_idx   = np.arange(n_tr, n_tr + n_va, dtype=np.int64)\n",
    "    test_idx  = np.arange(n_tr + n_va, n_tr + n_va + n_te, dtype=np.int64)\n",
    "\n",
    "    # y_all used by encode_embeddings_with_labels in your runner\n",
    "    y_all = y_int  # keep it int-coded and stable inside this split\n",
    "\n",
    "    return dataset_labeled, train_idx, val_idx, test_idx, y_all, (rna_tr_pp, atac_tr_lsi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc00126-9edb-46fa-adb6-aecc0557f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def _ensure_parent_dir(path: str):\n",
    "    d = os.path.dirname(os.path.abspath(path))\n",
    "    if d and not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def _append_tsv_union(path: str, df: pd.DataFrame, *, append: bool):\n",
    "    _ensure_parent_dir(path)\n",
    "    if append and os.path.exists(path):\n",
    "        existing = pd.read_csv(path, sep=\"\\t\")\n",
    "        all_cols = sorted(set(existing.columns).union(df.columns))\n",
    "        existing2 = existing.reindex(columns=all_cols)\n",
    "        df2 = df.reindex(columns=all_cols)\n",
    "        out = pd.concat([existing2, df2], axis=0, ignore_index=True)\n",
    "        out.to_csv(path, sep=\"\\t\", index=False)\n",
    "    else:\n",
    "        df.to_csv(path, sep=\"\\t\", index=False)\n",
    "\n",
    "def run_fig8_resample_splits_to_tsv(\n",
    "    *,\n",
    "    rna_raw,\n",
    "    atac_raw,\n",
    "    label_key: str = \"cell_type\",\n",
    "    split_seeds=(0, 1, 2, 3, 4),\n",
    "    model_seeds=(0,),   # <-- tuple/list\n",
    "    overlap_grid=(1.0, 0.9, 0.8, 0.7),\n",
    "    patience: int = 50,\n",
    "    fuse_mode: str = \"moe\",\n",
    "    drop_modality: str = \"atac\",\n",
    "    # preprocessing params\n",
    "    n_hvg: int = 2000,\n",
    "    n_lsi: int = 101,\n",
    "    target_sum: float = 1e4,\n",
    "\n",
    "    # output controls\n",
    "    out_tsv: str | None = None,\n",
    "    out_meta_tsv: str | None = None,\n",
    "    append: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Works with your CURRENT run_fig8_missing_modality_curve_v1 (no TSV args needed).\n",
    "    Produces the same metrics, then writes TSV(s) from this wrapper.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "\n",
    "    # globals expected by run_fig8_missing_modality_curve_v1\n",
    "    global dataset, train_idx, val_idx, test_idx, y_all, rna_tr_pp, atac_tr_lsi\n",
    "\n",
    "    # optional: write one row per (split_seed, model_seed) with run knobs\n",
    "    meta_rows = []\n",
    "\n",
    "    for split_seed in split_seeds:\n",
    "        ds, tr, va, te, y, (rna_tr, atac_tr) = build_dataset_for_split_seed(\n",
    "            rna_raw=rna_raw,\n",
    "            atac_raw=atac_raw,\n",
    "            label_key=label_key,\n",
    "            split_seed=int(split_seed),\n",
    "            n_hvg=int(n_hvg),\n",
    "            n_lsi=int(n_lsi),\n",
    "            target_sum=float(target_sum),\n",
    "        )\n",
    "\n",
    "        dataset     = ds\n",
    "        train_idx   = tr\n",
    "        val_idx     = va\n",
    "        test_idx    = te\n",
    "        y_all       = y\n",
    "        rna_tr_pp   = rna_tr\n",
    "        atac_tr_lsi = atac_tr\n",
    "\n",
    "        for model_seed in model_seeds:\n",
    "            # call your existing curve runner (no TSV kwargs!)\n",
    "            d = run_fig8_missing_modality_curve_v1(\n",
    "                overlap_grid=overlap_grid,\n",
    "                patience=int(patience),\n",
    "                fuse_mode=str(fuse_mode),\n",
    "                drop_modality=str(drop_modality),\n",
    "                seed=int(model_seed),\n",
    "            )\n",
    "\n",
    "            d[\"split_seed\"] = int(split_seed)\n",
    "            d[\"model_seed\"] = int(model_seed)\n",
    "            all_dfs.append(d)\n",
    "\n",
    "            meta_rows.append({\n",
    "                \"split_seed\": int(split_seed),\n",
    "                \"model_seed\": int(model_seed),\n",
    "                \"label_key\": str(label_key),\n",
    "                \"drop_modality\": str(drop_modality),\n",
    "                \"fuse_mode\": str(fuse_mode),\n",
    "                \"patience\": int(patience),\n",
    "                \"n_hvg\": int(n_hvg),\n",
    "                \"n_lsi\": int(n_lsi),\n",
    "                \"target_sum\": float(target_sum),\n",
    "                \"n_overlaps\": int(len(overlap_grid)),\n",
    "            })\n",
    "\n",
    "            # save this run's rows incrementally (optional but nice)\n",
    "            if out_tsv is not None:\n",
    "                _append_tsv_union(out_tsv, d, append=append and os.path.exists(out_tsv))\n",
    "\n",
    "    df_all = pd.concat(all_dfs, ignore_index=True) if len(all_dfs) else pd.DataFrame()\n",
    "\n",
    "    if out_meta_tsv is not None:\n",
    "        meta_df = pd.DataFrame(meta_rows)\n",
    "        _append_tsv_union(out_meta_tsv, meta_df, append=append and os.path.exists(out_meta_tsv))\n",
    "\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe248e-e99e-4e00-87bd-7074251ee569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./results/figure_9_paired_ablation_reproducibility\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62898fa6-b19f-49e6-ac32-4b4df8f99818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_seeds = list(range(20))\n",
    "\n",
    "df_all_atac_cv = run_fig8_resample_splits_to_tsv(\n",
    "    rna_raw=rna_raw,\n",
    "    atac_raw=atac_raw,\n",
    "    label_key=\"cell_type\",\n",
    "    split_seeds=split_seeds,\n",
    "    model_seeds=(0,),\n",
    "    overlap_grid=(1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.075, 0.05, 0.045, 0.04, 0.035, 0.03, 0.025, 0.02, 0.0175, 0.015, 0.0125, 0.01, 0.0075, 0.005),\n",
    "    patience=50,\n",
    "    fuse_mode=\"moe\",\n",
    "    drop_modality=\"atac\",\n",
    "    n_hvg=2000,\n",
    "    n_lsi=101,\n",
    "    target_sum=1e4,\n",
    "    out_tsv=\"./results/figure_9_paired_ablation_outputs-2-10-2026/fig8_curve__drop_atac__all_seeds.tsv\",\n",
    "    out_meta_tsv=\"./results/figure_9_paired_ablation_outputs-2-10-2026/fig8_curve__drop_atac__all_seeds__meta.tsv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9434a-8b8c-478e-82fe-fe2717cb5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_atac_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289a0a7-1f1d-4d0c-a992-30de78df726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One PNG per metric, autodetected from numeric columns\n",
    "plot_many_metrics_with_band(\n",
    "    df_all_atac_cv,\n",
    "    #out_dir=\"./figures/fig8_all_20cv_metric_bands_quantile_agg\",\n",
    "    out_dir=None,    \n",
    "    agg=\"quantile\",               # or \"sd\" or \"quantile\"\n",
    "    #agg=\"sd\",\n",
    "    #agg=\"sem\",\n",
    "    invert_x=True,\n",
    "    title_prefix=\"Fig8: \",\n",
    "    exclude_cols=(\"overlap_fraction\", \"train_paired\", \"train_unpaired\", \"unpaired_per_paired\", \"sampler_unpaired_per_paired\", \"SIL_true_frac\",),  # optional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddafcb-3075-4744-aac7-aec0b7955219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca5a3e-b0e6-4665-ab07-69a2a65c2032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_rna_cv = run_fig8_resample_splits_to_tsv(\n",
    "    rna_raw=rna_raw,\n",
    "    atac_raw=atac_raw,\n",
    "    label_key=\"cell_type\",\n",
    "    split_seeds=split_seeds,\n",
    "    model_seeds=(0,),\n",
    "    overlap_grid=(1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.075, 0.05, 0.045, 0.04, 0.035, 0.03, 0.025, 0.02, 0.0175, 0.015, 0.0125, 0.01, 0.0075, 0.005),\n",
    "    patience=50,\n",
    "    fuse_mode=\"moe\",\n",
    "    drop_modality=\"rna\",\n",
    "    n_hvg=2000,\n",
    "    n_lsi=101,\n",
    "    target_sum=1e4,\n",
    "    out_tsv=\"./results/figure_9_paired_ablation_outputs-2-10-2026/fig8_curve__drop_rna__all_seeds.tsv\",\n",
    "    out_meta_tsv=\"./results/figure_9_paired_ablation_outputs-2-10-2026/fig8_curve__drop_rna__all_seeds__meta.tsv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d610c-585c-414e-a6c3-234b4759fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_rna_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2fd3e9-5cc3-4b0c-9ebf-fe013fb29c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One PNG per metric, autodetected from numeric columns\n",
    "plot_many_metrics_with_band(\n",
    "    df_all_rna_cv,\n",
    "    #out_dir=\"./figures/fig8_all_20cv_metric_bands_quantile_agg\",\n",
    "    out_dir=None,    \n",
    "    agg=\"quantile\",               # or \"sd\" or \"quantile\"\n",
    "    #agg=\"sd\",\n",
    "    #agg=\"sem\",\n",
    "    invert_x=True,\n",
    "    title_prefix=\"Fig8: \",\n",
    "    exclude_cols=(\"overlap_fraction\", \"train_paired\", \"train_unpaired\", \"unpaired_per_paired\", \"sampler_unpaired_per_paired\", \"SIL_true_frac\",),  # optional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ea310-4ad1-4dde-9327-782d9af2ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2ea06-be1f-4fd2-9b33-4e98e3217f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e7a3e-e204-41f9-948f-5371d21aac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f50b3e1-1335-44df-b531-328b2f7106ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e65e51-f28c-4ac7-bffd-633572ede1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3baa88d-fcc3-4d40-b6d5-2d3aba0f82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03c60f-7613-4ff9-8ead-316229beda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d07da0-3df8-4e0c-a76a-c8febbd00d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf199e0b-6b1f-41d3-b4ce-d5f345807616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e847c-4c28-46bb-9c62-e6b626e9d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e243d45-ea8a-4b9c-b95f-3b2212f24eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af9c4c-ffa2-4fff-b440-d5d6209ceef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f79e6c-69e1-4c28-8525-53ef16660d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4b61a-272f-465c-8cf2-88774d344d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45b4ef-2cfd-47c6-a78d-ec2351f687e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3472a-8f88-4c3e-acb2-371aa8cdb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c4bc4-f1e1-47fa-9f19-a2967e4efaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad681a-028a-4aa8-be59-5bf317d9f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a755931-7bbd-421c-a349-be23ae882315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a57c11b-31ee-460e-b119-0b0aeda7d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40c90c-8593-461a-ba20-e4a80f88fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a043ec-e663-47a3-8067-f683db3ad112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2a8ed-0a1a-4249-a055-0c7a8e32df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58539cf1-5bb1-4b79-b641-aec6d7766f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f1d91-39b6-465d-9f16-3dab7e79cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610eae6-9384-4c2b-9e6a-4da02f500e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a87a44-0606-406a-8760-ab605957a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34c88f-2e5d-48d2-ad73-21790c3a1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b9b22-3061-45dc-9192-1e85b2c6477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59ba27-0660-49f3-bcfa-84daee83affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144983b-ccef-408b-83f7-3b594c54a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b2441-5bb8-4247-adac-3b1777d9bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4279e4-8491-4938-93da-d1891570118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400891e-37c4-4461-83fa-70607dca49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (UniVI v0.3.9)",
   "language": "python",
   "name": "univi_v0.3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
