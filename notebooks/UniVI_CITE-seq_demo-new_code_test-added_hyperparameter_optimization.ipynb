{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc2305c",
   "metadata": {},
   "source": [
    "# UniVI CITE-seq data integration demonstration/tutorial - Subsetting training data by celltype test\n",
    "\n",
    "Andrew Ashford, Pathways + Omics Group, Oregon Health & Science University - 11/18/2025\n",
    "\n",
    "This Jupyter Notebook will be used to outline the training steps for a UniVI model using human PBMC CITE-seq data. This is a copy of the other version used to test new code while the other one is running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3861d19",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf6fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b6eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 0. Wire up package import\n",
    "# -------------------------git status\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from univi import (\n",
    "    UniVIMultiModalVAE,\n",
    "    ModalityConfig,\n",
    "    UniVIConfig,\n",
    "    TrainingConfig,\n",
    "    matching,\n",
    ")\n",
    "from univi.data import MultiModalDataset\n",
    "from univi.trainer import UniVITrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c989bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.4.1+cu121\n",
      "torch.version.cuda: 12.1\n",
      "CUDA available: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb267f1",
   "metadata": {},
   "source": [
    "#### Read in and preprocess data as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56f3294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/groups/precepts/ashforda/anaconda3/envs/UniVI_working_environment_v2/lib/python3.8/site-packages/anndata/compat/__init__.py:229: FutureWarning: Moving element from .uns['neighbors']['distances'] to .obsp['distances'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2. Load AnnData objects\n",
    "# -------------------------\n",
    "# Load RNA AnnData object\n",
    "rna_adata = sc.read_h5ad(\"../data/Hao_CITE-seq_data/Hao_RNA_data.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b021733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 161764 × 20729\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features'\n",
      "    uns: 'neighbors'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'PCs', 'SPCA'\n",
      "    obsp: 'distances'\n",
      "  (0, 1)\t0.6931471805599453\n",
      "  (0, 19)\t0.6931471805599453\n",
      "  (0, 23)\t0.6931471805599453\n",
      "  (0, 28)\t0.6931471805599453\n",
      "  (0, 30)\t1.3862943611198906\n",
      "  (0, 48)\t0.6931471805599453\n",
      "  (0, 49)\t1.3862943611198906\n",
      "  (0, 54)\t0.6931471805599453\n",
      "  (0, 57)\t1.0986122886681096\n",
      "  (0, 63)\t0.6931471805599453\n",
      "  (0, 82)\t2.1972245773362196\n",
      "  (0, 98)\t0.6931471805599453\n",
      "  (0, 99)\t0.6931471805599453\n",
      "  (0, 111)\t1.791759469228055\n",
      "  (0, 127)\t0.6931471805599453\n",
      "  (0, 138)\t0.6931471805599453\n",
      "  (0, 149)\t0.6931471805599453\n",
      "  (0, 151)\t1.6094379124341003\n",
      "  (0, 157)\t0.6931471805599453\n",
      "  (0, 159)\t0.6931471805599453\n",
      "  (0, 160)\t0.6931471805599453\n",
      "  (0, 161)\t0.6931471805599453\n",
      "  (0, 162)\t0.6931471805599453\n",
      "  (0, 170)\t1.0986122886681096\n",
      "  (0, 182)\t0.6931471805599453\n",
      "  :\t:\n",
      "  (161763, 16110)\t1.6094379124341003\n",
      "  (161763, 16113)\t1.6094379124341003\n",
      "  (161763, 16130)\t0.6931471805599453\n",
      "  (161763, 16158)\t1.3862943611198906\n",
      "  (161763, 16159)\t1.6094379124341003\n",
      "  (161763, 16160)\t2.1972245773362196\n",
      "  (161763, 16163)\t0.6931471805599453\n",
      "  (161763, 16184)\t3.5263605246161616\n",
      "  (161763, 16185)\t3.5553480614894135\n",
      "  (161763, 16186)\t4.605170185988092\n",
      "  (161763, 16187)\t4.290459441148391\n",
      "  (161763, 16188)\t0.6931471805599453\n",
      "  (161763, 16189)\t4.174387269895637\n",
      "  (161763, 16190)\t4.330733340286331\n",
      "  (161763, 16191)\t3.9889840465642745\n",
      "  (161763, 16193)\t3.4657359027997265\n",
      "  (161763, 16194)\t2.302585092994046\n",
      "  (161763, 16196)\t3.9512437185814275\n",
      "  (161763, 16350)\t0.6931471805599453\n",
      "  (161763, 16585)\t0.6931471805599453\n",
      "  (161763, 16688)\t0.6931471805599453\n",
      "  (161763, 17511)\t0.6931471805599453\n",
      "  (161763, 17866)\t1.3862943611198906\n",
      "  (161763, 18249)\t0.6931471805599453\n",
      "  (161763, 18702)\t0.6931471805599453\n",
      "0.0\n",
      "9.049467146388098\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(rna_adata)\n",
    "print(rna_adata.X)\n",
    "print(rna_adata.X.min())\n",
    "print(rna_adata.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28dc1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the RNA counts to raw counts so they're not log-normalized and use ZINB or NB as the decoder distribution\n",
    "# for model training\n",
    "rna_adata.layers['stored_norm'] = rna_adata.X\n",
    "rna_adata.layers['counts'] = rna_adata.raw.X\n",
    "rna_adata.X = rna_adata.layers['counts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ebce9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNA shape after HVG selection: (161764, 20729)\n"
     ]
    }
   ],
   "source": [
    "# Norm the RNA data\n",
    "# Library-size normalize (CPM-ish, per cell)\n",
    "sc.pp.normalize_total(\n",
    "    rna_adata,\n",
    "    target_sum=1e4,    # standard in scRNA\n",
    "    inplace=True\n",
    ")   # rna_adata.X becomes normalized counts\n",
    "\n",
    "# Log-transform (this is your \"log1p\" expression)\n",
    "sc.pp.log1p(rna_adata)  # now X = log1p(norm counts)\n",
    "\n",
    "# Keep a clean copy of log1p-normalized values\n",
    "rna_adata.layers[\"log1p\"] = rna_adata.X.copy()\n",
    "\n",
    "print(\"RNA shape after HVG selection:\", rna_adata.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fbb913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.9428929810682248\n",
      "  (0, 19)\t0.9428929810682248\n",
      "  (0, 23)\t0.9428929810682248\n",
      "  (0, 28)\t0.9428929810682248\n",
      "  (0, 30)\t1.7408510757332776\n",
      "  (0, 48)\t0.9428929810682248\n",
      "  (0, 49)\t1.7408510757332776\n",
      "  (0, 54)\t0.9428929810682248\n",
      "  (0, 57)\t1.41943804993247\n",
      "  (0, 63)\t0.9428929810682248\n",
      "  (0, 82)\t2.6055880701758563\n",
      "  (0, 98)\t0.9428929810682248\n",
      "  (0, 99)\t0.9428929810682248\n",
      "  (0, 111)\t2.1789463883292677\n",
      "  (0, 127)\t0.9428929810682248\n",
      "  (0, 138)\t0.9428929810682248\n",
      "  (0, 149)\t0.9428929810682248\n",
      "  (0, 151)\t1.983700234470687\n",
      "  (0, 157)\t0.9428929810682248\n",
      "  (0, 159)\t0.9428929810682248\n",
      "  (0, 160)\t0.9428929810682248\n",
      "  (0, 161)\t0.9428929810682248\n",
      "  (0, 162)\t0.9428929810682248\n",
      "  (0, 170)\t1.41943804993247\n",
      "  (0, 182)\t0.9428929810682248\n",
      "  :\t:\n",
      "  (161763, 16110)\t1.7371448862545538\n",
      "  (161763, 16113)\t1.7371448862545538\n",
      "  (161763, 16130)\t0.7748538943739998\n",
      "  (161763, 16158)\t1.5064800733436712\n",
      "  (161763, 16159)\t1.7371448862545538\n",
      "  (161763, 16160)\t2.338164580793208\n",
      "  (161763, 16163)\t0.7748538943739998\n",
      "  (161763, 16184)\t3.6793107063878114\n",
      "  (161763, 16185)\t3.708421030117792\n",
      "  (161763, 16186)\t4.7609529028742035\n",
      "  (161763, 16187)\t4.445703077921809\n",
      "  (161763, 16188)\t0.7748538943739998\n",
      "  (161763, 16189)\t4.3293850756835885\n",
      "  (161763, 16190)\t4.486055808081522\n",
      "  (161763, 16191)\t4.1435247419971\n",
      "  (161763, 16193)\t3.6184174357487\n",
      "  (161763, 16194)\t2.445166980076903\n",
      "  (161763, 16196)\t4.105680496149983\n",
      "  (161763, 16350)\t0.7748538943739998\n",
      "  (161763, 16585)\t0.7748538943739998\n",
      "  (161763, 16688)\t0.7748538943739998\n",
      "  (161763, 17511)\t0.7748538943739998\n",
      "  (161763, 17866)\t1.5064800733436712\n",
      "  (161763, 18249)\t0.7748538943739998\n",
      "  (161763, 18702)\t0.7748538943739998\n",
      "0.0\n",
      "8.859009894880954\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(rna_adata.X)\n",
    "print(rna_adata.X.min())\n",
    "print(rna_adata.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "826afdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your counts are in rna_adata.X (raw or log-normalized), this is fine:\n",
    "sc.pp.highly_variable_genes(\n",
    "    rna_adata,\n",
    "    layer='log1p',\n",
    "    n_top_genes=2000,\n",
    "    flavor=\"seurat\",   # or \"cell_ranger\" / \"seurat_v3\"\n",
    "    inplace=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfbc7459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2000 highly variable genes.\n",
      "['HES4', 'ISG15', 'TNFRSF18', 'TNFRSF4', 'MIB2', 'MMP23B', 'TNFRSF25', 'CA6', 'RBP7', 'DRAXIN', 'AGTRAP', 'DHRS3', 'PADI4', 'UBXN10-AS1', 'CAMK2N1', 'CDA', 'AL031005.1', 'C1QA', 'C1QC', 'C1QB']\n"
     ]
    }
   ],
   "source": [
    "# Boolean mask of HVGs\n",
    "hvg_mask = rna_adata.var[\"highly_variable\"].values\n",
    "\n",
    "# Names of the top HVGs\n",
    "hvg_genes = rna_adata.var_names[hvg_mask].tolist()\n",
    "print(f\"Selected {len(hvg_genes)} highly variable genes.\")\n",
    "print(hvg_genes[:20])  # peek at first few\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b832a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 161764 × 2000\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
      "    uns: 'neighbors', 'log1p', 'hvg'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'PCs', 'SPCA'\n",
      "    layers: 'stored_norm', 'counts', 'log1p'\n",
      "    obsp: 'distances'\n"
     ]
    }
   ],
   "source": [
    "# Optional: make a HVG-only AnnData for modeling\n",
    "rna_adata_hvg = rna_adata[:, hvg_mask].copy()\n",
    "print(rna_adata_hvg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ff7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, scale the RNA data\n",
    "# Z-scale for Gaussian decoder (center per gene, unit variance; clipping to avoid crazy outliers)\n",
    "sc.pp.scale(\n",
    "    rna_adata_hvg,\n",
    "    zero_center=True\n",
    "    #zero_center=True,\n",
    "    #max_value=10\n",
    ")   # rna_adata.X now ~ N(0,1) per gene (on HVGs only)\n",
    "\n",
    "# Optional: keep a copy of scaled values\n",
    "rna_adata.layers[\"scaled\"] = rna_adata.X.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02383b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set rna_adata_hvg.X to Z-scaled values\n",
    "rna_adata.X = rna_adata.layers[\"scaled\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f3e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ADT AnnData object\n",
    "adt_adata = sc.read_h5ad(\"../data/Hao_CITE-seq_data/Hao_ADT_data.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4278182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 161764 × 228\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'APCA'\n",
      "[[1.95916424 0.86914159 1.48523314 ... 0.55307644 1.04608444 1.72565693]\n",
      " [0.4322284  1.01422751 0.79594998 ... 0.66587988 0.85514588 1.37971736]\n",
      " [0.61381759 1.30390619 0.75610373 ... 0.6874892  0.75610373 1.04246048]\n",
      " ...\n",
      " [1.50685426 0.54914608 1.0656108  ... 0.25674036 0.25674036 1.16168749]\n",
      " [1.63537843 0.32520632 1.19570797 ... 0.45519093 0.51435145 2.10485699]\n",
      " [1.58503    0.64458811 1.61118685 ... 0.416905   0.23002318 2.07731277]]\n",
      "0.0\n",
      "8.589231339100166\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(adt_adata)\n",
    "print(adt_adata.X)\n",
    "print(adt_adata.X.min())\n",
    "print(adt_adata.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d3d8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ADT data to raw counts and use a NB or ZINB decoder in model training and save the current .X counts to\n",
    "# .layers['log1p']\n",
    "adt_adata.layers['stored_norm'] = adt_adata.X\n",
    "adt_adata.layers['counts'] = adt_adata.raw.X\n",
    "adt_adata.X = adt_adata.layers['counts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f202408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and optionally scale ADT data using CLR norming\n",
    "# --------------------------------------------------\n",
    "# ADT: CLR normalize from raw counts\n",
    "#   - raw counts in adt_adata.layers['counts'] and in .X\n",
    "#   - Hao-normalized values currently in adt_adata.layers['stored_norm']\n",
    "# --------------------------------------------------\n",
    "\n",
    "# log1p\n",
    "#X_log = np.log1p(adt_adata)\n",
    "\n",
    "#import scanpy as sc\n",
    "\n",
    "sc.pp.log1p(adt_adata)  # modifies adt_adata.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87b92ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log = adt_adata.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b9d404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLR: subtract per-cell mean log expression (equivalent to ln( count_ij / geometric_mean_i ))\n",
    "row_means = X_log.mean(axis=1)\n",
    "X_clr = X_log - row_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f89d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADT CLR shape: (161764, 228)\n"
     ]
    }
   ],
   "source": [
    "# Store CLR in layers and set as current X\n",
    "adt_adata.layers[\"clr\"] = X_clr\n",
    "adt_adata.X = X_clr.copy()\n",
    "\n",
    "print(\"ADT CLR shape:\", adt_adata.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "065522d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/groups/precepts/ashforda/anaconda3/envs/UniVI_working_environment_v2/lib/python3.8/site-packages/sklearn/utils/validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/groups/precepts/ashforda/anaconda3/envs/UniVI_working_environment_v2/lib/python3.8/site-packages/sklearn/utils/validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADT CLR scaled for Gaussian decoder, shape: (161764, 228)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Optional: Z-scale CLR values for Gaussian decoder\n",
    "#   (per-protein mean 0, var 1 across cells)\n",
    "#   If you stick with Gaussian decoders for ADT too, DO this.\n",
    "#   If you use NB/ZINB for ADT, DON'T.\n",
    "# --------------------------------------------------\n",
    "scaler_adt = StandardScaler(with_mean=True, with_std=True)\n",
    "X_clr_scaled = scaler_adt.fit_transform(X_clr)\n",
    "\n",
    "adt_adata.layers[\"clr_scaled\"] = X_clr_scaled\n",
    "adt_adata.X = X_clr_scaled\n",
    "print(\"ADT CLR scaled for Gaussian decoder, shape:\", adt_adata.X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2ed5442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 161764 × 228\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features'\n",
      "    uns: 'log1p'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'APCA'\n",
      "    layers: 'stored_norm', 'counts', 'clr', 'clr_scaled'\n",
      "[[ 1.75353838  0.63154435  0.77162124 ... -0.30293594  1.15547398\n",
      "   0.79655139]\n",
      " [-0.69662312  1.12293289 -0.95481186 ...  0.24935615  0.62156007\n",
      "   0.27990605]\n",
      " [-0.27562413  1.91766876 -1.06597148 ...  0.34149149  0.30626632\n",
      "  -0.28328269]\n",
      " ...\n",
      " [ 1.17244265 -0.40020108 -0.20187065 ... -1.64015216 -1.83246206\n",
      "  -0.07285185]\n",
      " [ 1.33230182 -1.59675792  0.08586175 ... -0.74620268 -0.70590279\n",
      "   1.34267206]\n",
      " [ 1.27350976 -0.07507278  1.07045903 ... -0.82878852 -2.08149672\n",
      "   1.30990852]]\n",
      "-9.157326331250525\n",
      "15.32049290481086\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(adt_adata)\n",
    "print(adt_adata.X)\n",
    "print(adt_adata.X.min())\n",
    "print(adt_adata.X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64cb9ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "def get_X_values(adata, n_cells: int = 5000):\n",
    "    \"\"\"\n",
    "    Return flattened values from adata.X (optionally subsampled over cells).\n",
    "    Uses only non-zero entries if X is sparse.\n",
    "    \"\"\"\n",
    "    X = adata.X\n",
    "\n",
    "    # Optional subsampling over cells (rows)\n",
    "    if n_cells is not None and adata.n_obs > n_cells:\n",
    "        idx = np.random.choice(adata.n_obs, size=n_cells, replace=False)\n",
    "        X = X[idx]\n",
    "\n",
    "    if sparse.issparse(X):\n",
    "        vals = X.data  # nonzero values\n",
    "    else:\n",
    "        vals = np.asarray(X).ravel()\n",
    "\n",
    "    # Remove zeros explicitly (just to focus on count/ADT magnitude)\n",
    "    vals = vals[vals > 0]\n",
    "    return vals\n",
    "\n",
    "def plot_X_distribution(rna_adata, adt_adata, n_cells: int = 5000):\n",
    "    rna_vals = get_X_values(rna_adata, n_cells=n_cells)\n",
    "    adt_vals = get_X_values(adt_adata, n_cells=n_cells)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # --- RNA raw ---\n",
    "    axes[0, 0].hist(rna_vals, bins=100, alpha=0.8)\n",
    "    axes[0, 0].set_title(\"RNA .X nonzero values (raw)\")\n",
    "    axes[0, 0].set_xlabel(\"Value\")\n",
    "    axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # --- RNA log10 ---\n",
    "    axes[0, 1].hist(np.log10(rna_vals + 1e-8), bins=100, alpha=0.8)\n",
    "    axes[0, 1].set_title(\"RNA .X nonzero values (log10)\")\n",
    "    axes[0, 1].set_xlabel(\"log10(value)\")\n",
    "    axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # --- ADT raw ---\n",
    "    axes[1, 0].hist(adt_vals, bins=100, alpha=0.8)\n",
    "    axes[1, 0].set_title(\"ADT .X nonzero values (raw)\")\n",
    "    axes[1, 0].set_xlabel(\"Value\")\n",
    "    axes[1, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # --- ADT log10 ---\n",
    "    axes[1, 1].hist(np.log10(adt_vals + 1e-8), bins=100, alpha=0.8)\n",
    "    axes[1, 1].set_title(\"ADT .X nonzero values (log10)\")\n",
    "    axes[1, 1].set_xlabel(\"log10(value)\")\n",
    "    axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call it:\n",
    "plot_X_distribution(rna_adata_hvg, adt_adata, n_cells=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b70058e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 161764 × 2000\n",
      "    obs: 'nCount_ADT', 'nFeature_ADT', 'nCount_RNA', 'nFeature_RNA', 'orig.ident', 'lane', 'donor', 'time', 'celltype.l1', 'celltype.l2', 'celltype.l3', 'Phase', 'nCount_SCT', 'nFeature_SCT'\n",
      "    var: 'features', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n",
      "    uns: 'neighbors', 'log1p', 'hvg'\n",
      "    obsm: 'X_apca', 'X_aumap', 'X_pca', 'X_spca', 'X_umap', 'X_wnn.umap'\n",
      "    varm: 'PCs', 'SPCA'\n",
      "    layers: 'stored_norm', 'counts', 'log1p'\n",
      "    obsp: 'distances'\n",
      "{'NK', 'CD8 T', 'DC', 'other T', 'B', 'CD4 T', 'Mono', 'other'}\n",
      "Mono       49010\n",
      "CD4 T      41001\n",
      "CD8 T      25469\n",
      "NK         18664\n",
      "B          13800\n",
      "other T     6789\n",
      "DC          3589\n",
      "other       3442\n",
      "Name: celltype.l1, dtype: int64\n",
      "{'B naive', 'cDC2', 'cDC1', 'Treg', 'Doublet', 'CD8 TCM', 'CD8 Naive', 'Plasmablast', 'ASDC', 'CD4 Naive', 'NK Proliferating', 'CD4 Proliferating', 'CD14 Mono', 'NK_CD56bright', 'CD16 Mono', 'CD4 CTL', 'MAIT', 'dnT', 'B intermediate', 'pDC', 'NK', 'gdT', 'CD4 TEM', 'HSPC', 'ILC', 'CD8 TEM', 'CD8 Proliferating', 'CD4 TCM', 'Platelet', 'Eryth', 'B memory'}\n",
      "CD14 Mono            42690\n",
      "CD4 Naive            17479\n",
      "NK                   17173\n",
      "CD4 TCM              14889\n",
      "CD8 TEM              11727\n",
      "CD8 Naive            10768\n",
      "B naive               7718\n",
      "CD16 Mono             6320\n",
      "CD4 TEM               4282\n",
      "gdT                   3649\n",
      "B memory              3285\n",
      "CD8 TCM               2883\n",
      "MAIT                  2784\n",
      "Treg                  2507\n",
      "cDC2                  2501\n",
      "B intermediate        2431\n",
      "Platelet              2293\n",
      "CD4 CTL               1736\n",
      "NK_CD56bright          943\n",
      "pDC                    861\n",
      "Doublet                605\n",
      "NK Proliferating       548\n",
      "Plasmablast            366\n",
      "dnT                    356\n",
      "HSPC                   329\n",
      "cDC1                   151\n",
      "ILC                    132\n",
      "CD4 Proliferating      108\n",
      "CD8 Proliferating       91\n",
      "Eryth                   83\n",
      "ASDC                    76\n",
      "Name: celltype.l2, dtype: int64\n",
      "{'CD4 TEM_2', 'gdT_4', 'CD8 TEM_2', 'cDC1', 'CD4 TEM_4', 'CD8 TEM_1', 'CD4 TCM_1', 'Doublet', 'NK_1', 'CD8 Naive', 'CD8 TEM_4', 'ASDC_pDC', 'Plasmablast', 'CD4 Naive', 'NK Proliferating', 'CD4 Proliferating', 'CD14 Mono', 'NK_CD56bright', 'CD4 TCM_2', 'CD16 Mono', 'cDC2_2', 'gdT_2', 'dnT_1', 'CD4 CTL', 'MAIT', 'gdT_3', 'B naive lambda', 'gdT_1', 'B memory kappa', 'pDC', 'CD8 TEM_3', 'CD8 TEM_5', 'HSPC', 'CD8 TCM_2', 'ILC', 'B intermediate lambda', 'B naive kappa', 'CD8 TCM_3', 'dnT_2', 'B memory lambda', 'CD8 TEM_6', 'NK_3', 'CD8 TCM_1', 'CD8 Naive_2', 'CD8 Proliferating', 'CD4 TCM_3', 'NK_4', 'cDC2_1', 'CD4 TEM_1', 'Treg Naive', 'Platelet', 'Plasma', 'NK_2', 'CD4 TEM_3', 'Eryth', 'B intermediate kappa', 'Treg Memory', 'ASDC_mDC'}\n",
      "CD14 Mono                42690\n",
      "CD4 Naive                17479\n",
      "CD8 Naive                10478\n",
      "NK_2                      9418\n",
      "CD4 TCM_1                 8141\n",
      "CD16 Mono                 6320\n",
      "CD4 TCM_3                 6155\n",
      "B naive kappa             4852\n",
      "NK_1                      4126\n",
      "CD8 TEM_4                 3504\n",
      "B naive lambda            2866\n",
      "CD8 TEM_1                 2786\n",
      "MAIT                      2784\n",
      "CD8 TEM_2                 2435\n",
      "Platelet                  2293\n",
      "NK_3                      2152\n",
      "CD4 TEM_3                 2044\n",
      "B memory kappa            2037\n",
      "CD8 TEM_5                 1973\n",
      "CD4 CTL                   1736\n",
      "cDC2_2                    1729\n",
      "CD4 TEM_1                 1706\n",
      "gdT_1                     1633\n",
      "NK_4                      1477\n",
      "B intermediate lambda     1331\n",
      "CD8 TCM_2                 1322\n",
      "Treg Naive                1295\n",
      "B memory lambda           1248\n",
      "Treg Memory               1212\n",
      "B intermediate kappa      1100\n",
      "NK_CD56bright              943\n",
      "CD8 TCM_1                  929\n",
      "pDC                        861\n",
      "cDC2_1                     772\n",
      "gdT_2                      769\n",
      "gdT_3                      724\n",
      "CD8 TEM_6                  636\n",
      "CD8 TCM_3                  632\n",
      "Doublet                    605\n",
      "CD4 TCM_2                  593\n",
      "NK Proliferating           548\n",
      "gdT_4                      523\n",
      "CD4 TEM_2                  452\n",
      "CD8 TEM_3                  393\n",
      "HSPC                       329\n",
      "Plasma                     318\n",
      "CD8 Naive_2                290\n",
      "dnT_1                      189\n",
      "dnT_2                      167\n",
      "cDC1                       151\n",
      "ILC                        132\n",
      "CD4 Proliferating          108\n",
      "CD8 Proliferating           91\n",
      "Eryth                       83\n",
      "CD4 TEM_4                   80\n",
      "Plasmablast                 48\n",
      "ASDC_mDC                    40\n",
      "ASDC_pDC                    36\n",
      "Name: celltype.l3, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rna_adata_hvg)\n",
    "print(set(rna_adata_hvg.obs['celltype.l1']))\n",
    "print(rna_adata_hvg.obs['celltype.l1'].value_counts())\n",
    "print(set(rna_adata_hvg.obs['celltype.l2']))\n",
    "print(rna_adata_hvg.obs['celltype.l2'].value_counts())\n",
    "print(set(rna_adata_hvg.obs['celltype.l3']))\n",
    "print(rna_adata_hvg.obs['celltype.l3'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c17c7",
   "metadata": {},
   "source": [
    "#### Code to optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44ed9d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# UniVI hyperparameter search (10x Multiome PBMC)\n",
    "# ==============================================\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from univi.config import UniVIConfig, ModalityConfig, TrainingConfig\n",
    "from univi.data import MultiModalDataset\n",
    "from univi.models.univi import UniVIMultiModalVAE\n",
    "from univi.trainer import UniVITrainer\n",
    "from univi import evaluation as univi_eval\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 0. Assumes you already have:\n",
    "#    - rna, adt : AnnData objects\n",
    "#      * rna.layers[\"counts\"] = raw RNA counts\n",
    "#      * adt.X\n",
    "#      * rna.obs[\"cell_type\"] with 19 PBMC cell types\n",
    "#      * obs_names aligned: rna.obs_names == adt.obs_names\n",
    "# ------------------------------------------------------\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec8ec2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced pool: 22649 cells\n",
      "  Train: 18119\n",
      "  Val  : 2264\n",
      "  Test : 2266\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 1. Balanced cell-type train/val/test split\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def make_balanced_split(\n",
    "    adata,\n",
    "    celltype_key=\"celltype.l1\",\n",
    "    max_per_type=1000,\n",
    "    frac_train=0.8,\n",
    "    frac_val=0.1,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns train_idx, val_idx, test_idx (indices into adata.obs_names),\n",
    "    using at most max_per_type cells per cell type.\n",
    "    \"\"\"\n",
    "    labels = adata.obs[celltype_key].astype(str).values\n",
    "    cell_types = np.unique(labels)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pooled_idx = []\n",
    "\n",
    "    for ct in cell_types:\n",
    "        idx_ct = np.where(labels == ct)[0]\n",
    "        if len(idx_ct) == 0:\n",
    "            continue\n",
    "        rng.shuffle(idx_ct)\n",
    "        pooled_idx.append(idx_ct[:max_per_type])\n",
    "\n",
    "    pooled_idx = np.concatenate(pooled_idx)\n",
    "    rng.shuffle(pooled_idx)\n",
    "\n",
    "    n = pooled_idx.shape[0]\n",
    "    n_train = int(frac_train * n)\n",
    "    n_val = int(frac_val * n)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_idx = pooled_idx[:n_train]\n",
    "    val_idx = pooled_idx[n_train:n_train + n_val]\n",
    "    test_idx = pooled_idx[n_train + n_val:]\n",
    "\n",
    "    print(f\"Balanced pool: {n} cells\")\n",
    "    print(f\"  Train: {len(train_idx)}\")\n",
    "    print(f\"  Val  : {len(val_idx)}\")\n",
    "    print(f\"  Test : {len(test_idx)}\")\n",
    "\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "\n",
    "train_idx, val_idx, test_idx = make_balanced_split(\n",
    "    rna_adata_hvg,\n",
    "    celltype_key=\"celltype.l2\",\n",
    "    max_per_type=1000,\n",
    "    frac_train=0.8,\n",
    "    frac_val=0.1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Build view-specific AnnDatas for the split (so we can reuse them)\n",
    "rna_train = rna_adata_hvg[train_idx].copy()\n",
    "rna_val   = rna_adata_hvg[val_idx].copy()\n",
    "rna_test  = rna_adata_hvg[test_idx].copy()\n",
    "\n",
    "adt_train = adt_adata[train_idx].copy()\n",
    "adt_val   = adt_adata[val_idx].copy()\n",
    "adt_test  = adt_adata[test_idx].copy()\n",
    "\n",
    "# Consistency checks\n",
    "assert np.all(rna_train.obs_names == adt_train.obs_names)\n",
    "assert np.all(rna_val.obs_names   == adt_val.obs_names)\n",
    "assert np.all(rna_test.obs_names  == adt_test.obs_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a115d650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Val cells in dataset: 20383\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 2. Build a base MultiModalDataset (train+val only)\n",
    "#    – we will reuse this across all hyperparameters.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "adata_trainval = {\n",
    "    \"rna\": rna_adata_hvg[ np.concatenate([train_idx, val_idx]) ].copy(),\n",
    "    \"adt\": adt_adata[np.concatenate([train_idx, val_idx]) ].copy(),\n",
    "}\n",
    "\n",
    "# To keep mapping from dataset indices back to AnnData rows\n",
    "trainval_obs_names = adata_trainval[\"rna\"].obs_names.to_numpy()\n",
    "\n",
    "dataset = MultiModalDataset(\n",
    "    adata_dict=adata_trainval,\n",
    "    X_key=\"X\",          # rna.X, adt.X\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "n_cells_tv = dataset.n_cells\n",
    "print(\"Train+Val cells in dataset:\", n_cells_tv)\n",
    "\n",
    "# Remap train/val indices into [0..n_cells_tv)\n",
    "name_to_pos = {name: i for i, name in enumerate(trainval_obs_names)}\n",
    "train_idx_ds = np.array([name_to_pos[n] for n in rna_train.obs_names])\n",
    "val_idx_ds   = np.array([name_to_pos[n] for n in rna_val.obs_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af20cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 3. Hyperparameter search space (arch + regularization)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from univi import evaluation as univi_eval\n",
    "\n",
    "# Architecture options; feel free to tweak / add\n",
    "rna_arch_options = [\n",
    "    {\"name\": \"rna_med2\",  \"enc\": [512, 256],         \"dec\": [256, 512]},\n",
    "    {\"name\": \"rna_wide2\", \"enc\": [1024, 512],        \"dec\": [512, 1024]},\n",
    "    {\"name\": \"rna_wide3\", \"enc\": [1024, 512, 256],   \"dec\": [256, 512, 1024]},\n",
    "]\n",
    "\n",
    "adt_arch_options = [\n",
    "    {\"name\": \"adt_small2\", \"enc\": [128, 64],      \"dec\": [64, 128]},\n",
    "    {\"name\": \"adt_med2\",   \"enc\": [256, 128],     \"dec\": [128, 256]},\n",
    "]\n",
    "\n",
    "search_space = {\n",
    "    \"latent_dim\":       [20, 32, 40, 50, 64, 82, 120],\n",
    "    \"beta\":             [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0, 110.0, 120.0, 160.0, 200.0],\n",
    "    \"gamma\":            [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0, 110.0, 120.0, 160.0, 200.0],\n",
    "    \"lr\":               [1e-3, 5e-4],\n",
    "    \"weight_decay\":     [1e-4, 1e-5],\n",
    "    \"encoder_dropout\":  [0.0, 0.1],\n",
    "    \"decoder_batchnorm\":[False, True],\n",
    "    \"rna_arch\":         rna_arch_options,\n",
    "    \"adt_arch\":         adt_arch_options,\n",
    "}\n",
    "\n",
    "MAX_CONFIGS = 120  # how many random configs to try\n",
    "\n",
    "\n",
    "def iter_hparam_configs(space_dict, max_configs=MAX_CONFIGS, seed=0):\n",
    "    \"\"\"\n",
    "    Random sampler over the hyperparameter space.\n",
    "    Each config independently samples a value for each key.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    keys = list(space_dict.keys())\n",
    "    for _ in range(max_configs):\n",
    "        hp = {}\n",
    "        for k in keys:\n",
    "            options = space_dict[k]\n",
    "            idx = rng.integers(len(options))\n",
    "            hp[k] = options[idx]\n",
    "        yield hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a693c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 4. Helper to build UniVI + TrainingConfig from hparams\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def build_univi_and_train_cfg(hp):\n",
    "    \"\"\"\n",
    "    hp: dict with keys from search_space, including rna_arch / adt_arch.\n",
    "    \"\"\"\n",
    "    latent_dim        = hp[\"latent_dim\"]\n",
    "    beta              = hp[\"beta\"]\n",
    "    gamma             = hp[\"gamma\"]\n",
    "    lr                = hp[\"lr\"]\n",
    "    weight_decay      = hp[\"weight_decay\"]\n",
    "    encoder_dropout   = hp[\"encoder_dropout\"]\n",
    "    decoder_batchnorm = hp[\"decoder_batchnorm\"]\n",
    "\n",
    "    rna_arch  = hp[\"rna_arch\"]\n",
    "    adt_arch = hp[\"adt_arch\"]\n",
    "\n",
    "    # ---- modality configs ----\n",
    "    mod_rna = ModalityConfig(\n",
    "        name=\"rna\",\n",
    "        input_dim=rna_adata_hvg.n_vars,      # using all genes; swap to HVGs if you prefer\n",
    "        encoder_hidden=rna_arch[\"enc\"],\n",
    "        decoder_hidden=rna_arch[\"dec\"],\n",
    "        likelihood=\"gaussian\",\n",
    "    )\n",
    "    mod_adt = ModalityConfig(\n",
    "        name=\"adt\",\n",
    "        input_dim=adt_adata.X.shape[1],  # LSI dimensionality\n",
    "        encoder_hidden=adt_arch[\"enc\"],\n",
    "        decoder_hidden=adt_arch[\"dec\"],\n",
    "        likelihood=\"gaussian\",\n",
    "    )\n",
    "\n",
    "    univi_cfg = UniVIConfig(\n",
    "        latent_dim=latent_dim,\n",
    "        beta=beta,\n",
    "        gamma=gamma,\n",
    "        encoder_dropout=encoder_dropout,\n",
    "        decoder_dropout=0.0,\n",
    "        encoder_batchnorm=True,\n",
    "        decoder_batchnorm=decoder_batchnorm,\n",
    "        #kl_anneal_start=5,\n",
    "        kl_anneal_start=0,\n",
    "        #kl_anneal_end=50,\n",
    "        kl_anneal_end=0,\n",
    "        #align_anneal_start=5,\n",
    "        align_anneal_start=0,\n",
    "        #align_anneal_end=50,\n",
    "        align_anneal_end=0,\n",
    "        modalities=[mod_rna, mod_adt],\n",
    "    )\n",
    "\n",
    "    train_cfg = TrainingConfig(\n",
    "        n_epochs=100,          # shorter for search; increase for final training\n",
    "        batch_size=256,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        device=device,\n",
    "        log_every=5,\n",
    "        grad_clip=5.0,\n",
    "        num_workers=0,\n",
    "        seed=42,\n",
    "        early_stopping=True,\n",
    "        patience=15,\n",
    "        min_delta=0.0,\n",
    "    )\n",
    "\n",
    "    return univi_cfg, train_cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e1e69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 5. Train + evaluate one hyperparameter configuration\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def evaluate_config(hp, config_id):\n",
    "    \"\"\"\n",
    "    Train a UniVI model with hyperparameters hp, evaluate on val set.\n",
    "    Returns a dict with metrics + hp.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[Config {config_id}] Hyperparameters:\")\n",
    "    pretty_hp = {\n",
    "        **{k: v for k, v in hp.items() if k not in (\"rna_arch\", \"adt_arch\")},\n",
    "        \"rna_arch\": hp[\"rna_arch\"][\"name\"],\n",
    "        \"adt_arch\": hp[\"adt_arch\"][\"name\"],\n",
    "    }\n",
    "    print(json.dumps(pretty_hp, indent=2))\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    univi_cfg, train_cfg = build_univi_and_train_cfg(hp)\n",
    "\n",
    "    model = UniVIMultiModalVAE(univi_cfg).to(device)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_ds = Subset(dataset, train_idx_ds)\n",
    "    val_ds   = Subset(dataset, val_idx_ds)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=train_cfg.num_workers,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=train_cfg.num_workers,\n",
    "    )\n",
    "\n",
    "    trainer = UniVITrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        train_cfg=train_cfg,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    t0 = time.time()\n",
    "    history = trainer.fit()\n",
    "    t1 = time.time()\n",
    "\n",
    "    best_val      = float(min(trainer.history[\"val_loss\"]))\n",
    "    final_train   = float(trainer.history[\"train_loss\"][-1])\n",
    "    final_beta    = float(trainer.history[\"beta\"][-1])\n",
    "    final_gamma   = float(trainer.history[\"gamma\"][-1])\n",
    "    elapsed_min   = (t1 - t0) / 60.0\n",
    "\n",
    "    # Latent embeddings for validation cells\n",
    "    z_rna_val  = trainer.encode_modality(rna_val,  modality=\"rna\")\n",
    "    z_adt_val = trainer.encode_modality(adt_val, modality=\"adt\")\n",
    "\n",
    "    fos = float(univi_eval.compute_foscttm(z_rna_val, z_adt_val))\n",
    "\n",
    "    labels_rna  = rna_val.obs[\"celltype.l2\"].astype(str).values\n",
    "    labels_adt = adt_val.obs[\"celltype.l2\"].astype(str).values\n",
    "\n",
    "    _, acc_val, _ = univi_eval.label_transfer_knn(\n",
    "        Z_source=z_adt_val,\n",
    "        labels_source=labels_adt,\n",
    "        Z_target=z_rna_val,\n",
    "        labels_target=labels_rna,\n",
    "        k=15,\n",
    "    )\n",
    "    acc_val = float(acc_val)\n",
    "\n",
    "    result = {\n",
    "        \"config_id\": config_id,\n",
    "        \"best_val_loss\": best_val,\n",
    "        \"final_train_loss\": final_train,\n",
    "        \"final_beta\": final_beta,\n",
    "        \"final_gamma\": final_gamma,\n",
    "        \"foscttm_val\": fos,\n",
    "        \"label_transfer_val\": acc_val,\n",
    "        \"minutes\": elapsed_min,\n",
    "        \"history\": history,\n",
    "        \"hp\": deepcopy(hp),\n",
    "    }\n",
    "\n",
    "    print(f\"[Config {config_id}] Done in {elapsed_min:.1f} min\")\n",
    "    print(f\"  best_val_loss              = {best_val:.3f}\")\n",
    "    print(f\"  FOSCTTM (RNA vs ADT, val) = {fos:.4f}\")\n",
    "    print(f\"  Label transfer (ADT→RNA)  = {acc_val:.3f}\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6535a76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Config 1] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 100.0,\n",
      "  \"gamma\": 80.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 20:36:57,274] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 20:36:57,277] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 20:36:57,277] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 20:36:57,278] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-19 20:36:57,279] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-19 20:36:57,279] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 20:36:57,281] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 20:36:57,282] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 20:36:57,282] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 20:36:57,284] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 20:36:57,285] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 20:36:57,285] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 20:36:57,287] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca103f18b794e18819e2302884b1830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 20:37:09,261] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4120.6952 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:37:10,226] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3737.1243 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:37:10,413] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3737.1243\n",
      "[2025-11-19 20:37:18,092] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3698.8551\n",
      "[2025-11-19 20:37:26,844] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3696.0965\n",
      "[2025-11-19 20:37:35,775] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3675.7431\n",
      "[2025-11-19 20:37:43,718] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3661.7942 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:37:44,653] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3651.2184 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:37:44,763] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3651.2184\n",
      "[2025-11-19 20:37:54,131] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3608.7258\n",
      "[2025-11-19 20:38:10,827] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3607.0625\n",
      "[2025-11-19 20:38:18,810] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3590.2021\n",
      "[2025-11-19 20:38:27,223] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3556.2196 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:38:28,166] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3571.1755 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:38:28,181] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3571.1755\n",
      "[2025-11-19 20:38:37,575] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3555.4658\n",
      "[2025-11-19 20:38:46,196] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3547.1975\n",
      "[2025-11-19 20:38:56,364] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3471.9427\n",
      "[2025-11-19 20:39:05,636] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3467.2971\n",
      "[2025-11-19 20:39:13,686] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3412.3416 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:39:14,670] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3448.8817 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:39:14,789] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3448.8817\n",
      "[2025-11-19 20:39:23,609] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3431.6369\n",
      "[2025-11-19 20:39:51,628] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3430.4966\n",
      "[2025-11-19 20:40:00,049] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3380.3597 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:40:01,015] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3413.8647 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:40:01,123] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3413.8647\n",
      "[2025-11-19 20:40:19,810] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3411.7935\n",
      "[2025-11-19 20:40:27,771] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3409.8409\n",
      "[2025-11-19 20:40:37,186] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3404.3860\n",
      "[2025-11-19 20:40:45,474] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3365.2321 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:40:46,449] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3412.0158 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:41:05,137] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3402.8874\n",
      "[2025-11-19 20:41:23,700] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3393.6021\n",
      "[2025-11-19 20:41:32,095] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3353.3771 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:41:32,516] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3396.2837 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:41:51,038] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3384.4619\n",
      "[2025-11-19 20:42:00,001] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3358.4778\n",
      "[2025-11-19 20:42:09,332] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3352.5585\n",
      "[2025-11-19 20:42:17,717] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3290.0093 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:42:18,699] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3349.3374 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:42:18,731] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3349.3374\n",
      "[2025-11-19 20:42:46,921] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3343.7222\n",
      "[2025-11-19 20:42:54,679] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3341.4886\n",
      "[2025-11-19 20:43:00,314] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3278.2904 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:43:01,204] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3342.1168 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:43:27,156] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3338.0821\n",
      "[2025-11-19 20:43:44,562] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3268.7061 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:43:44,764] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3336.8072 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:43:44,877] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3336.8072\n",
      "[2025-11-19 20:44:03,439] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3327.2628\n",
      "[2025-11-19 20:44:29,362] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3263.3222 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:44:29,918] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3331.6072 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:44:56,691] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3327.2060\n",
      "[2025-11-19 20:45:05,113] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3324.3951\n",
      "[2025-11-19 20:45:13,458] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3248.4233 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:45:14,445] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3325.3695 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:45:23,984] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3321.9837\n",
      "[2025-11-19 20:45:42,695] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3321.8184\n",
      "[2025-11-19 20:46:00,078] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3239.9806 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:46:00,913] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3314.0170 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:46:00,934] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3314.0170\n",
      "[2025-11-19 20:46:45,626] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3234.3237 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:46:46,583] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3322.4024 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:47:13,948] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3312.4311\n",
      "[2025-11-19 20:47:23,326] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3312.2844\n",
      "[2025-11-19 20:47:31,575] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3227.7058 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:47:32,549] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3313.8897 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:47:41,809] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3312.0708\n",
      "[2025-11-19 20:47:50,983] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3309.6987\n",
      "[2025-11-19 20:48:00,289] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3306.6985\n",
      "[2025-11-19 20:48:10,581] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3301.7295\n",
      "[2025-11-19 20:48:24,456] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3215.2017 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:48:25,411] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3298.6758 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:48:25,546] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 3298.6758\n",
      "[2025-11-19 20:48:43,037] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3293.7719\n",
      "[2025-11-19 20:49:07,269] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3203.7252 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:49:08,283] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3297.5047 (beta=100.000, gamma=80.000)\n",
      "[2025-11-19 20:49:08,304] [UniVITrainer] [INFO] Restored best model from epoch 77 (val loss = 3293.7719)\n",
      "[2025-11-19 20:49:09,391] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 20:49:09,392] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 20:49:09,393] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 20:49:09,394] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-19 20:49:09,394] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-19 20:49:09,395] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 20:49:09,396] [UniVITrainer] [INFO]   log_every: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 20:49:09,396] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 20:49:09,397] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 20:49:09,398] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 20:49:09,399] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 20:49:09,399] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 20:49:09,400] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 1] Done in 12.2 min\n",
      "  best_val_loss              = 3293.772\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0876\n",
      "  Label transfer (ADT→RNA)  = 0.439\n",
      "--> New best config (id=1) with score=2489.309\n",
      "\n",
      "================================================================================\n",
      "[Config 2] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 100.0,\n",
      "  \"gamma\": 160.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1a17f3f1454160808c0622f7833bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 20:49:17,988] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4921.8898 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:49:18,987] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3910.7335 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:49:19,024] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3910.7335\n",
      "[2025-11-19 20:49:28,751] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3784.5032\n",
      "[2025-11-19 20:49:37,578] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3704.6890\n",
      "[2025-11-19 20:49:47,013] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3669.8503\n",
      "[2025-11-19 20:49:55,477] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3676.9580 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:49:56,478] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3631.5667 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:49:56,516] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3631.5667\n",
      "[2025-11-19 20:50:06,040] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3612.0977\n",
      "[2025-11-19 20:50:15,748] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3549.8950\n",
      "[2025-11-19 20:50:25,529] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3512.0036\n",
      "[2025-11-19 20:50:35,337] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3495.6124\n",
      "[2025-11-19 20:50:44,103] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3480.2386 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:50:45,120] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3492.2916 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:50:45,161] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3492.2916\n",
      "[2025-11-19 20:50:54,931] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3481.2579\n",
      "[2025-11-19 20:51:03,413] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3468.9266\n",
      "[2025-11-19 20:51:13,072] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3462.1057\n",
      "[2025-11-19 20:51:22,686] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3445.8860\n",
      "[2025-11-19 20:51:31,280] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3434.9477 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:51:32,254] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3455.5153 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:51:51,869] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3433.9829\n",
      "[2025-11-19 20:52:01,643] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3429.3898\n",
      "[2025-11-19 20:52:11,458] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3407.8630\n",
      "[2025-11-19 20:52:20,148] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3369.8185 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:52:21,146] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3396.7349 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:52:21,212] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3396.7349\n",
      "[2025-11-19 20:52:40,655] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3393.7222\n",
      "[2025-11-19 20:52:50,468] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3378.5866\n",
      "[2025-11-19 20:53:08,094] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3340.2726 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:53:08,943] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3380.6959 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:53:18,440] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3370.3996\n",
      "[2025-11-19 20:53:46,768] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3363.9721\n",
      "[2025-11-19 20:53:55,134] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3314.4196 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:53:56,152] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3360.1071 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:53:56,192] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3360.1071\n",
      "[2025-11-19 20:54:13,356] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3349.6412\n",
      "[2025-11-19 20:54:32,691] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3347.5975\n",
      "[2025-11-19 20:54:41,162] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3295.0997 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:54:42,091] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3345.0341 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:54:42,119] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3345.0341\n",
      "[2025-11-19 20:55:00,422] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3343.4149\n",
      "[2025-11-19 20:55:19,126] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3342.8945\n",
      "[2025-11-19 20:55:27,673] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3288.0903 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:55:28,654] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3340.7453 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:55:28,730] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3340.7453\n",
      "[2025-11-19 20:55:46,927] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3332.0018\n",
      "[2025-11-19 20:56:06,300] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3323.7293\n",
      "[2025-11-19 20:56:13,331] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3276.6700 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:56:14,336] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3331.0378 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:56:33,430] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3314.0835\n",
      "[2025-11-19 20:56:59,987] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3259.2739 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:57:00,954] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3312.3312 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:57:00,981] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3312.3312\n",
      "[2025-11-19 20:57:29,567] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3309.3927\n",
      "[2025-11-19 20:57:47,724] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3248.2579 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:57:48,729] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3310.2448 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:57:58,414] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3302.8687\n",
      "[2025-11-19 20:58:17,783] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3299.3662\n",
      "[2025-11-19 20:58:27,504] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3297.8542\n",
      "[2025-11-19 20:58:35,704] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3226.0892 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:58:36,685] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3291.7764 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:58:36,888] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3291.7764\n",
      "[2025-11-19 20:59:01,079] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3212.1367 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:59:01,420] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3284.4530 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:59:01,443] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3284.4530\n",
      "[2025-11-19 20:59:06,941] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3277.5168\n",
      "[2025-11-19 20:59:08,879] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3275.4666\n",
      "[2025-11-19 20:59:10,563] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3200.8397 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:59:10,683] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3274.6571 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:59:10,698] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3274.6571\n",
      "[2025-11-19 20:59:12,526] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3273.8331\n",
      "[2025-11-19 20:59:17,958] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3271.1764\n",
      "[2025-11-19 20:59:19,640] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3189.2817 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:59:19,776] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3271.8949 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:59:25,420] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3269.9521\n",
      "[2025-11-19 20:59:27,286] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3266.7032\n",
      "[2025-11-19 20:59:28,972] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3181.5789 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:59:29,091] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3266.3654 (beta=100.000, gamma=160.000)\n",
      "[2025-11-19 20:59:29,107] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 3266.3654\n",
      "[2025-11-19 20:59:29,119] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 3266.3654)\n",
      "[2025-11-19 20:59:29,888] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 20:59:29,889] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 20:59:29,890] [UniVITrainer] [INFO]   batch_size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 20:59:29,890] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-19 20:59:29,891] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-19 20:59:29,892] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 20:59:29,893] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 20:59:29,893] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 20:59:29,894] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 20:59:29,895] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 20:59:29,895] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 20:59:29,896] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 20:59:29,897] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 2] Done in 10.3 min\n",
      "  best_val_loss              = 3266.365\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0814\n",
      "  Label transfer (ADT→RNA)  = 0.545\n",
      "--> New best config (id=2) with score=2286.814\n",
      "\n",
      "================================================================================\n",
      "[Config 3] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 50,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 40.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6e6003e7194eb4b327688425d1b8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 20:59:31,593] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4051.2663 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 20:59:31,710] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3818.5564 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 20:59:31,740] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3818.5564\n",
      "[2025-11-19 20:59:33,518] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3811.0805\n",
      "[2025-11-19 20:59:35,402] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3806.5312\n",
      "[2025-11-19 20:59:37,248] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3788.8880\n",
      "[2025-11-19 20:59:38,923] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3722.8993 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 20:59:39,042] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3710.0714 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 20:59:39,114] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3710.0714\n",
      "[2025-11-19 20:59:42,907] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3703.1022\n",
      "[2025-11-19 20:59:44,700] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3696.2572\n",
      "[2025-11-19 20:59:48,187] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3682.8084 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 20:59:48,320] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3696.4118 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 20:59:52,072] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3693.3019\n",
      "[2025-11-19 20:59:53,866] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3688.0065\n",
      "[2025-11-19 20:59:55,729] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3685.2821\n",
      "[2025-11-19 20:59:57,392] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3665.1952 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 20:59:57,512] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3675.9893 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 20:59:57,528] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3675.9893\n",
      "[2025-11-19 20:59:59,321] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3670.2971\n",
      "[2025-11-19 21:00:03,058] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3664.1530\n",
      "[2025-11-19 21:00:04,943] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3648.9806\n",
      "[2025-11-19 21:00:06,623] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3595.2438 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:06,744] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3630.1399 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:06,816] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3630.1399\n",
      "[2025-11-19 21:00:08,620] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3606.5842\n",
      "[2025-11-19 21:00:10,408] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3533.2728\n",
      "[2025-11-19 21:00:14,030] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3516.6316\n",
      "[2025-11-19 21:00:15,697] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3476.7123 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:15,817] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3505.2044 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:15,845] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3505.2044\n",
      "[2025-11-19 21:00:23,199] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3500.4088\n",
      "[2025-11-19 21:00:25,103] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3456.2671 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:25,235] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3486.1648 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:25,250] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3486.1648\n",
      "[2025-11-19 21:00:30,770] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3484.9311\n",
      "[2025-11-19 21:00:34,274] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3448.5392 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:34,409] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3488.5777 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:36,268] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3484.0727\n",
      "[2025-11-19 21:00:39,825] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3476.0534\n",
      "[2025-11-19 21:00:43,235] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3441.2451 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:43,353] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3492.4192 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:52,386] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3434.2752 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:52,503] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3479.3044 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:00:57,975] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3475.3338\n",
      "[2025-11-19 21:01:01,556] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3424.6617 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:01,673] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3487.5050 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:07,349] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3465.3454\n",
      "[2025-11-19 21:01:10,812] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3417.7222 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:10,940] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3477.7641 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:18,275] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3461.1680\n",
      "[2025-11-19 21:01:19,933] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3417.1594 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:20,049] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3470.9419 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:23,687] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3460.3504\n",
      "[2025-11-19 21:01:29,126] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3403.9478 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:29,258] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3465.0603 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:34,620] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3456.3548\n",
      "[2025-11-19 21:01:38,096] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3401.9353 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:38,213] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3459.3041 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:43,804] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3455.4036\n",
      "[2025-11-19 21:01:47,500] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3396.2340 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:47,618] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3459.7586 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:54,982] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3454.2684\n",
      "[2025-11-19 21:01:56,692] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3394.3836 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:56,810] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3454.4490 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 21:01:56,820] [UniVITrainer] [INFO] Restored best model from epoch 79 (val loss = 3454.2684)\n",
      "[2025-11-19 21:01:57,601] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 21:01:57,602] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 21:01:57,603] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 21:01:57,603] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-19 21:01:57,604] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-19 21:01:57,604] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 21:01:57,605] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 21:01:57,605] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 21:01:57,606] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 21:01:57,607] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 21:01:57,607] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 21:01:57,608] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 21:01:57,609] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 3] Done in 2.4 min\n",
      "  best_val_loss              = 3454.268\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1360\n",
      "  Label transfer (ADT→RNA)  = 0.326\n",
      "\n",
      "================================================================================\n",
      "[Config 4] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 20,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 100.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf7b16dcf274ab88eafcdd5a32e9fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 21:01:59,308] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3994.6742 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:01:59,446] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3774.9646 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:01:59,515] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3774.9646\n",
      "[2025-11-19 21:02:01,310] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3689.7656\n",
      "[2025-11-19 21:02:03,195] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3666.0013\n",
      "[2025-11-19 21:02:04,957] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3637.0606\n",
      "[2025-11-19 21:02:06,602] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3612.5582 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:06,721] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3620.6554 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:06,731] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3620.6554\n",
      "[2025-11-19 21:02:08,643] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3543.8454\n",
      "[2025-11-19 21:02:10,470] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3510.5729\n",
      "[2025-11-19 21:02:12,229] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3474.4729\n",
      "[2025-11-19 21:02:14,042] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3458.3837\n",
      "[2025-11-19 21:02:15,668] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3419.5123 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:15,786] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3450.9608 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:15,796] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3450.9608\n",
      "[2025-11-19 21:02:17,579] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3432.2252\n",
      "[2025-11-19 21:02:19,336] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3423.4962\n",
      "[2025-11-19 21:02:21,221] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3415.4131\n",
      "[2025-11-19 21:02:23,064] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3406.2469\n",
      "[2025-11-19 21:02:24,685] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3363.9845 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:24,818] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3406.4459 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:26,824] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3403.1972\n",
      "[2025-11-19 21:02:30,363] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3399.1155\n",
      "[2025-11-19 21:02:33,768] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3338.3887 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:33,886] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3397.9561 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:33,903] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3397.9561\n",
      "[2025-11-19 21:02:35,785] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3386.2274\n",
      "[2025-11-19 21:02:37,571] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3380.5751\n",
      "[2025-11-19 21:02:39,310] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3380.2713\n",
      "[2025-11-19 21:02:42,731] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3318.9206 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:42,851] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3378.0747 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:42,925] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3378.0747\n",
      "[2025-11-19 21:02:46,646] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3374.2927\n",
      "[2025-11-19 21:02:50,319] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3369.9554\n",
      "[2025-11-19 21:02:51,985] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3308.1565 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:52,106] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3366.5851 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:02:52,114] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3366.5851\n",
      "[2025-11-19 21:02:54,051] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3363.2791\n",
      "[2025-11-19 21:02:57,733] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3357.3275\n",
      "[2025-11-19 21:02:59,524] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3356.9504\n",
      "[2025-11-19 21:03:01,166] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3292.8170 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:01,297] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3358.9887 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:04,966] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3356.5043\n",
      "[2025-11-19 21:03:06,965] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3354.5395\n",
      "[2025-11-19 21:03:08,716] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3347.9865\n",
      "[2025-11-19 21:03:10,352] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3284.1480 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:10,486] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3360.3115 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:17,101] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3344.3735\n",
      "[2025-11-19 21:03:20,673] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3273.4912 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:20,797] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3348.3655 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:24,307] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3339.2620\n",
      "[2025-11-19 21:03:29,451] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3263.5257 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:29,569] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3345.0553 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:31,471] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3334.2400\n",
      "[2025-11-19 21:03:35,733] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3330.7635\n",
      "[2025-11-19 21:03:39,694] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3251.0236 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:39,812] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3326.0586 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:39,830] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3326.0586\n",
      "[2025-11-19 21:03:41,605] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3324.4478\n",
      "[2025-11-19 21:03:45,146] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3317.3920\n",
      "[2025-11-19 21:03:48,942] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3228.8382 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:49,060] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3315.2146 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:49,070] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3315.2146\n",
      "[2025-11-19 21:03:52,658] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3313.5050\n",
      "[2025-11-19 21:03:54,435] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3312.4571\n",
      "[2025-11-19 21:03:56,270] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3304.6884\n",
      "[2025-11-19 21:03:57,924] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3217.3009 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:03:58,043] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3312.0397 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:04:05,331] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3302.7807\n",
      "[2025-11-19 21:04:06,999] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3210.4457 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:04:07,119] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3304.4110 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:04:08,903] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3301.7454\n",
      "[2025-11-19 21:04:12,467] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3299.5741\n",
      "[2025-11-19 21:04:16,011] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3202.7244 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:04:16,135] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3295.5625 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:04:16,155] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 3295.5625\n",
      "[2025-11-19 21:04:25,147] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3194.8727 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:04:25,400] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3303.0998 (beta=120.000, gamma=100.000)\n",
      "[2025-11-19 21:04:25,420] [UniVITrainer] [INFO] Restored best model from epoch 75 (val loss = 3295.5625)\n",
      "[2025-11-19 21:04:26,930] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 21:04:26,933] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 21:04:26,934] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 21:04:26,934] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-19 21:04:26,935] [UniVITrainer] [INFO]   weight_decay: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 21:04:26,936] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 21:04:26,936] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 21:04:26,937] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 21:04:26,937] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 21:04:26,942] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 21:04:26,943] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 21:04:26,944] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 21:04:26,944] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 4] Done in 2.5 min\n",
      "  best_val_loss              = 3295.563\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0790\n",
      "  Label transfer (ADT→RNA)  = 0.544\n",
      "\n",
      "================================================================================\n",
      "[Config 5] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 20,\n",
      "  \"beta\": 40.0,\n",
      "  \"gamma\": 60.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42297adb2e14909b268445a5df4f90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 21:04:29,636] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3790.4829 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:04:29,923] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3638.4968 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:04:30,021] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3638.4968\n",
      "[2025-11-19 21:04:32,180] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3504.4288\n",
      "[2025-11-19 21:04:34,195] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3501.5591\n",
      "[2025-11-19 21:04:35,982] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3375.4449\n",
      "[2025-11-19 21:04:37,554] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3294.9312 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:04:37,684] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3304.0532 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:04:37,778] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3304.0532\n",
      "[2025-11-19 21:04:39,544] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3282.2763\n",
      "[2025-11-19 21:04:41,361] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3272.7938\n",
      "[2025-11-19 21:04:43,153] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3240.5001\n",
      "[2025-11-19 21:04:45,054] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3224.4718\n",
      "[2025-11-19 21:04:46,574] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3175.3020 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:04:46,704] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3220.7912 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:04:46,799] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3220.7912\n",
      "[2025-11-19 21:04:48,562] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3204.2945\n",
      "[2025-11-19 21:04:50,360] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3190.7677\n",
      "[2025-11-19 21:04:52,046] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3182.7085\n",
      "[2025-11-19 21:04:53,730] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3172.0243\n",
      "[2025-11-19 21:04:55,301] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3115.6236 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:04:55,420] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3172.4903 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:04:57,186] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3159.9262\n",
      "[2025-11-19 21:04:58,944] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3157.9362\n",
      "[2025-11-19 21:05:00,679] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3151.5845\n",
      "[2025-11-19 21:05:03,937] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3084.8919 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:04,055] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3141.5894 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:04,064] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3141.5894\n",
      "[2025-11-19 21:05:05,767] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3138.1710\n",
      "[2025-11-19 21:05:07,519] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3137.8278\n",
      "[2025-11-19 21:05:09,176] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3136.9830\n",
      "[2025-11-19 21:05:10,925] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3128.4556\n",
      "[2025-11-19 21:05:12,792] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3051.8099 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:12,925] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3127.1419 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:12,995] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3127.1419\n",
      "[2025-11-19 21:05:14,667] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3125.1947\n",
      "[2025-11-19 21:05:16,339] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3119.5378\n",
      "[2025-11-19 21:05:18,048] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3116.0342\n",
      "[2025-11-19 21:05:21,313] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3033.6058 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:21,429] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3112.5904 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:21,506] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3112.5904\n",
      "[2025-11-19 21:05:24,976] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3107.9691\n",
      "[2025-11-19 21:05:28,402] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3106.2598\n",
      "[2025-11-19 21:05:29,966] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3018.4195 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:30,084] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3106.9373 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:31,765] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3104.5564\n",
      "[2025-11-19 21:05:33,445] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3098.3123\n",
      "[2025-11-19 21:05:36,860] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3097.9584\n",
      "[2025-11-19 21:05:38,533] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3002.3360 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:38,665] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3095.2286 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:38,684] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3095.2286\n",
      "[2025-11-19 21:05:40,470] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3086.1964\n",
      "[2025-11-19 21:05:42,216] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3080.4883\n",
      "[2025-11-19 21:05:43,886] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3077.9780\n",
      "[2025-11-19 21:05:47,127] [UniVITrainer] [INFO] [Epoch 045] Train loss: 2965.4331 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:47,242] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3072.0012 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:47,258] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3072.0012\n",
      "[2025-11-19 21:05:48,972] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3070.8685\n",
      "[2025-11-19 21:05:52,742] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3069.5754\n",
      "[2025-11-19 21:05:56,013] [UniVITrainer] [INFO] [Epoch 050] Train loss: 2951.7288 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:56,131] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3068.3387 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:05:56,141] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3068.3387\n",
      "[2025-11-19 21:05:59,516] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3061.3992\n",
      "[2025-11-19 21:06:04,459] [UniVITrainer] [INFO] [Epoch 055] Train loss: 2938.3693 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:06:04,591] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3064.2402 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:06:06,388] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3060.7891\n",
      "[2025-11-19 21:06:08,110] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3060.1354\n",
      "[2025-11-19 21:06:09,829] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3059.5743\n",
      "[2025-11-19 21:06:11,570] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3056.6157\n",
      "[2025-11-19 21:06:13,109] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2923.4017 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:06:13,226] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3058.3933 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:06:19,094] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3055.4798\n",
      "[2025-11-19 21:06:27,903] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3047.3834\n",
      "[2025-11-19 21:06:52,369] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2911.3876 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:06:53,320] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3047.4499 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:07:02,033] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3045.5256\n",
      "[2025-11-19 21:07:09,980] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3041.9559\n",
      "[2025-11-19 21:07:35,605] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2898.8369 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:07:36,568] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3042.6333 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:07:45,879] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3039.4666\n",
      "[2025-11-19 21:08:03,134] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3039.1248\n",
      "[2025-11-19 21:08:12,590] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3038.1704\n",
      "[2025-11-19 21:08:21,041] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2890.2232 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:08:22,026] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3046.8302 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:08:40,769] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3034.0675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 21:09:06,921] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2875.5038 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:09:07,921] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3038.9723 (beta=40.000, gamma=60.000)\n",
      "[2025-11-19 21:09:07,939] [UniVITrainer] [INFO] Restored best model from epoch 77 (val loss = 3034.0675)\n",
      "[2025-11-19 21:09:08,791] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 21:09:08,792] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 21:09:08,794] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 21:09:08,795] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-19 21:09:08,796] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-19 21:09:08,797] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 21:09:08,798] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 21:09:08,799] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 21:09:08,800] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 21:09:08,801] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 21:09:08,802] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 21:09:08,804] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 21:09:08,805] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 5] Done in 4.7 min\n",
      "  best_val_loss              = 3034.068\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0453\n",
      "  Label transfer (ADT→RNA)  = 0.674\n",
      "--> New best config (id=5) with score=1894.050\n",
      "\n",
      "================================================================================\n",
      "[Config 6] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 80.0,\n",
      "  \"gamma\": 100.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27fd73c6c544d7dac6ef47453696f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 21:09:17,307] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4120.9928 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:09:18,338] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3733.6638 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:09:18,393] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3733.6638\n",
      "[2025-11-19 21:09:27,971] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3687.8507\n",
      "[2025-11-19 21:09:46,450] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3629.9714\n",
      "[2025-11-19 21:09:54,888] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3615.4207 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:09:55,904] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3594.1487 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:09:55,963] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3594.1487\n",
      "[2025-11-19 21:10:05,220] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3567.3396\n",
      "[2025-11-19 21:10:14,680] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3551.8025\n",
      "[2025-11-19 21:10:24,412] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3504.4646\n",
      "[2025-11-19 21:10:33,907] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3439.6484\n",
      "[2025-11-19 21:10:41,317] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3422.4781 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:10:41,950] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3419.0161 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:10:41,990] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3419.0161\n",
      "[2025-11-19 21:11:01,249] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3401.8316\n",
      "[2025-11-19 21:11:17,555] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3389.2798\n",
      "[2025-11-19 21:11:24,961] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3373.5451 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:11:25,597] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3393.4351 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:11:35,263] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3379.7529\n",
      "[2025-11-19 21:11:44,856] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3370.1891\n",
      "[2025-11-19 21:12:03,135] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3365.6115\n",
      "[2025-11-19 21:12:11,592] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3336.5081 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:12:12,443] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3374.6182 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:12:21,959] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3359.7692\n",
      "[2025-11-19 21:12:48,941] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3358.0929\n",
      "[2025-11-19 21:12:57,601] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3319.6006 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:12:58,415] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3366.0381 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:13:05,089] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3353.7998\n",
      "[2025-11-19 21:13:24,399] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3353.5455\n",
      "[2025-11-19 21:13:33,360] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3345.0564\n",
      "[2025-11-19 21:13:40,831] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3309.3752 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:13:41,836] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3344.4025 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:13:42,022] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3344.4025\n",
      "[2025-11-19 21:13:51,084] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3337.5108\n",
      "[2025-11-19 21:14:26,945] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3297.5365 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:14:27,960] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3340.3779 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:14:37,415] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3335.2791\n",
      "[2025-11-19 21:14:47,098] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3332.6033\n",
      "[2025-11-19 21:14:56,508] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3331.2179\n",
      "[2025-11-19 21:15:06,191] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3326.1699\n",
      "[2025-11-19 21:15:14,650] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3262.0146 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:15:15,538] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3300.8640 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:15:15,611] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3300.8640\n",
      "[2025-11-19 21:15:24,272] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3290.2794\n",
      "[2025-11-19 21:15:43,127] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3287.8755\n",
      "[2025-11-19 21:16:00,269] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3235.8665 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:16:01,282] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3285.0611 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:16:01,340] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3285.0611\n",
      "[2025-11-19 21:16:10,867] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3281.2276\n",
      "[2025-11-19 21:16:28,795] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3280.1311\n",
      "[2025-11-19 21:16:38,532] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3274.5623\n",
      "[2025-11-19 21:16:47,183] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3221.0105 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:16:48,195] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3276.8612 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:17:06,296] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3268.4615\n",
      "[2025-11-19 21:17:16,126] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3260.3502\n",
      "[2025-11-19 21:17:25,529] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3259.4269\n",
      "[2025-11-19 21:17:34,158] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3199.1563 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:17:35,138] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3251.5699 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:17:35,327] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3251.5699\n",
      "[2025-11-19 21:18:22,354] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3192.6694 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:18:23,381] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3255.3527 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:18:31,729] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3248.0026\n",
      "[2025-11-19 21:18:49,215] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3244.7336\n",
      "[2025-11-19 21:19:05,760] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3186.5134 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:19:06,790] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3248.8723 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:19:24,520] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3239.6249\n",
      "[2025-11-19 21:19:50,744] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3178.7550 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:19:51,756] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3246.0668 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:20:16,863] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3239.1623\n",
      "[2025-11-19 21:20:34,607] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3172.7167 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:20:35,544] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3241.2028 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:20:52,488] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3162.9877 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:20:52,701] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3235.4967 (beta=80.000, gamma=100.000)\n",
      "[2025-11-19 21:20:52,872] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 3235.4967\n",
      "[2025-11-19 21:20:52,900] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 3235.4967)\n",
      "[2025-11-19 21:20:53,722] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 21:20:53,723] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 21:20:53,724] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 21:20:53,730] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-19 21:20:53,734] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-19 21:20:53,735] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 21:20:53,742] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 21:20:53,743] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 21:20:53,744] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 21:20:53,744] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 21:20:53,745] [UniVITrainer] [INFO]   early_stopping: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 21:20:53,746] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 21:20:53,746] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 6] Done in 11.7 min\n",
      "  best_val_loss              = 3235.497\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0713\n",
      "  Label transfer (ADT→RNA)  = 0.525\n",
      "\n",
      "================================================================================\n",
      "[Config 7] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 60.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b307100160884d379b335f5fdf50dc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 21:21:02,005] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4568.1330 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:21:02,987] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3882.4693 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:21:03,095] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3882.4693\n",
      "[2025-11-19 21:21:12,207] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3757.5891\n",
      "[2025-11-19 21:21:21,195] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3740.3283\n",
      "[2025-11-19 21:21:30,682] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3698.5479\n",
      "[2025-11-19 21:21:39,121] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3695.2325 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:21:40,079] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3676.1011 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:21:40,119] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3676.1011\n",
      "[2025-11-19 21:21:49,447] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3674.3773\n",
      "[2025-11-19 21:21:58,625] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3661.8582\n",
      "[2025-11-19 21:22:14,296] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3635.9543\n",
      "[2025-11-19 21:22:22,147] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3626.8728 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:22:23,131] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3620.5893 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:22:23,365] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3620.5893\n",
      "[2025-11-19 21:22:32,518] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3554.5541\n",
      "[2025-11-19 21:22:42,283] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3537.7386\n",
      "[2025-11-19 21:22:51,919] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3525.8003\n",
      "[2025-11-19 21:23:02,907] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3515.7349\n",
      "[2025-11-19 21:23:11,607] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3500.8026 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:23:12,606] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3514.9132 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:23:12,632] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3514.9132\n",
      "[2025-11-19 21:23:18,285] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3500.3312\n",
      "[2025-11-19 21:23:54,300] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3486.5550 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:23:55,296] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3497.4754 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:23:55,335] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3497.4754\n",
      "[2025-11-19 21:24:04,885] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3492.8904\n",
      "[2025-11-19 21:24:13,651] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3489.3337\n",
      "[2025-11-19 21:24:41,200] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3466.6225 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:24:42,207] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3485.4210 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:24:42,240] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3485.4210\n",
      "[2025-11-19 21:24:51,887] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3476.1130\n",
      "[2025-11-19 21:25:28,514] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3447.2810 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:25:29,476] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3467.2090 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:25:29,598] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3467.2090\n",
      "[2025-11-19 21:26:03,800] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3465.8124\n",
      "[2025-11-19 21:26:21,461] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3435.8290 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:26:22,320] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3474.6295 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:26:50,593] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3459.0254\n",
      "[2025-11-19 21:27:05,600] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3426.2072 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:27:06,010] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3463.8848 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:27:34,043] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3457.0667\n",
      "[2025-11-19 21:27:49,746] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3423.7254 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:27:50,087] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3461.6485 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:28:31,680] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3425.4616 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:28:32,681] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3455.2171 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:28:32,935] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3455.2171\n",
      "[2025-11-19 21:29:19,218] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3415.9983 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:29:20,269] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3455.2479 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:29:50,740] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3452.0419\n",
      "[2025-11-19 21:30:00,973] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3427.3099\n",
      "[2025-11-19 21:30:10,277] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3382.2640 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:30:11,324] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3423.0413 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:30:11,511] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3423.0413\n",
      "[2025-11-19 21:30:30,392] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3420.4504\n",
      "[2025-11-19 21:31:00,225] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3378.3551 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:31:01,285] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3423.8838 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:31:11,353] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3419.6655\n",
      "[2025-11-19 21:31:41,991] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3410.7255\n",
      "[2025-11-19 21:31:51,131] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3367.2152 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:31:52,214] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3417.1837 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:32:30,150] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3362.4090 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:32:31,193] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3413.6107 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:32:40,018] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3405.6910\n",
      "[2025-11-19 21:32:54,484] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3402.1087\n",
      "[2025-11-19 21:33:01,624] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3352.8319 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:33:02,075] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3400.5129 (beta=160.000, gamma=60.000)\n",
      "[2025-11-19 21:33:02,326] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 3400.5129\n",
      "[2025-11-19 21:33:02,359] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 3400.5129)\n",
      "[2025-11-19 21:33:04,064] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 21:33:04,078] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 21:33:04,078] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 21:33:04,079] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-19 21:33:04,079] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-19 21:33:04,080] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 21:33:04,081] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 21:33:04,081] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 21:33:04,082] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 21:33:04,082] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 21:33:04,083] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 21:33:04,083] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 21:33:04,084] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 7] Done in 12.1 min\n",
      "  best_val_loss              = 3400.513\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1442\n",
      "  Label transfer (ADT→RNA)  = 0.385\n",
      "\n",
      "================================================================================\n",
      "[Config 8] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 20.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92d709c8d2645178a49d5c0f281d34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 21:33:09,803] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4154.6033 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:33:10,833] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3847.5930 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:33:10,956] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3847.5930\n",
      "[2025-11-19 21:33:19,338] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3750.5962\n",
      "[2025-11-19 21:33:28,500] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3710.5123\n",
      "[2025-11-19 21:33:37,491] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3693.3379\n",
      "[2025-11-19 21:33:44,994] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3672.4833 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:33:45,983] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3659.6388 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:33:46,098] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3659.6388\n",
      "[2025-11-19 21:33:54,036] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3607.0760\n",
      "[2025-11-19 21:34:02,427] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3580.0319\n",
      "[2025-11-19 21:34:10,875] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3579.3592\n",
      "[2025-11-19 21:34:19,723] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3574.0235\n",
      "[2025-11-19 21:34:24,783] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3549.9878 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:34:25,790] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3552.1702 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:34:25,809] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3552.1702\n",
      "[2025-11-19 21:34:44,231] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3542.5486\n",
      "[2025-11-19 21:34:54,045] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3539.1608\n",
      "[2025-11-19 21:35:02,813] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3521.3498\n",
      "[2025-11-19 21:35:11,549] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3503.8489 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:35:12,575] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3525.0391 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:35:22,620] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3502.3459\n",
      "[2025-11-19 21:35:30,303] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3490.1443\n",
      "[2025-11-19 21:35:39,264] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3479.4570\n",
      "[2025-11-19 21:35:46,753] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3472.2700\n",
      "[2025-11-19 21:35:55,434] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3449.1028 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:35:56,405] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3476.8551 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:36:05,937] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3464.6092\n",
      "[2025-11-19 21:36:20,550] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3457.4202\n",
      "[2025-11-19 21:36:37,039] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3414.9605 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:36:38,040] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3456.3414 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:36:38,164] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3456.3414\n",
      "[2025-11-19 21:36:47,966] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3442.8174\n",
      "[2025-11-19 21:37:09,606] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3442.2589\n",
      "[2025-11-19 21:37:16,475] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3400.4340 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:37:17,440] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3442.6918 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:37:33,941] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3437.4628\n",
      "[2025-11-19 21:37:40,381] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3430.5047\n",
      "[2025-11-19 21:37:55,490] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3387.5725 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:37:56,169] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3432.7385 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:38:21,434] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3427.5919\n",
      "[2025-11-19 21:38:30,018] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3417.7317\n",
      "[2025-11-19 21:38:36,960] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3378.4932 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:38:37,834] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3427.6207 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:39:14,043] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3373.0449 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:39:14,344] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3423.3933 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:39:26,754] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3415.8934\n",
      "[2025-11-19 21:39:31,467] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3361.7239 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:39:31,975] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3420.0892 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:39:53,554] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3407.1825\n",
      "[2025-11-19 21:40:08,505] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3352.7953 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:40:09,411] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3400.8203 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:40:09,536] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3400.8203\n",
      "[2025-11-19 21:40:35,530] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3398.2527\n",
      "[2025-11-19 21:40:53,467] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3344.5779 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:40:54,451] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3403.8209 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:41:22,876] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3397.3480\n",
      "[2025-11-19 21:41:40,148] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3337.6933 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:41:40,285] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3396.7731 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:41:40,412] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3396.7731\n",
      "[2025-11-19 21:42:16,775] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3393.8690\n",
      "[2025-11-19 21:42:25,472] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3338.5244 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:42:26,397] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3404.8728 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:43:17,577] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3329.1808 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:43:19,278] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3392.7693 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:43:19,296] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 3392.7693\n",
      "[2025-11-19 21:43:50,538] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3388.5634\n",
      "[2025-11-19 21:44:34,604] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3329.1139 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:44:36,300] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3396.0891 (beta=160.000, gamma=20.000)\n",
      "[2025-11-19 21:44:36,335] [UniVITrainer] [INFO] Restored best model from epoch 77 (val loss = 3388.5634)\n",
      "[2025-11-19 21:44:37,260] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 21:44:37,261] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 21:44:37,263] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 21:44:37,265] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-19 21:44:37,269] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-19 21:44:37,270] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 21:44:37,275] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 21:44:37,275] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 21:44:37,281] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 21:44:37,282] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 21:44:37,283] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 21:44:37,283] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 21:44:37,284] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 8] Done in 11.5 min\n",
      "  best_val_loss              = 3388.563\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1326\n",
      "  Label transfer (ADT→RNA)  = 0.417\n",
      "\n",
      "================================================================================\n",
      "[Config 9] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 60.0,\n",
      "  \"gamma\": 100.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bb23a121f44f1a842c752489e2e00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 21:44:51,119] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4182.9913 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:44:52,819] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3768.9328 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:44:52,994] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3768.9328\n",
      "[2025-11-19 21:45:08,450] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3625.8758\n",
      "[2025-11-19 21:45:24,354] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3549.7591\n",
      "[2025-11-19 21:45:38,568] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3489.4772\n",
      "[2025-11-19 21:45:52,728] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3456.4965 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:45:54,429] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3415.5974 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:45:54,605] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3415.5974\n",
      "[2025-11-19 21:46:10,319] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3387.2606\n",
      "[2025-11-19 21:46:26,338] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3363.3028\n",
      "[2025-11-19 21:46:42,321] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3337.3278\n",
      "[2025-11-19 21:46:58,219] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3325.8009\n",
      "[2025-11-19 21:47:12,392] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3288.6500 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:47:13,697] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3311.3697 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:47:13,881] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3311.3697\n",
      "[2025-11-19 21:47:29,927] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3303.7585\n",
      "[2025-11-19 21:47:40,696] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3295.5470\n",
      "[2025-11-19 21:47:50,665] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3292.9977\n",
      "[2025-11-19 21:47:59,202] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3271.2248\n",
      "[2025-11-19 21:48:08,090] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3231.5266 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:48:08,226] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3264.0250 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:48:08,466] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3264.0250\n",
      "[2025-11-19 21:48:18,207] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3252.9295\n",
      "[2025-11-19 21:48:23,633] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3251.2412\n",
      "[2025-11-19 21:48:32,471] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3239.2805\n",
      "[2025-11-19 21:48:41,367] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3227.8643\n",
      "[2025-11-19 21:48:48,944] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3190.2824 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:48:49,955] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3236.9170 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:48:58,096] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3223.8545\n",
      "[2025-11-19 21:49:05,358] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3220.1491\n",
      "[2025-11-19 21:49:16,822] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3214.2128\n",
      "[2025-11-19 21:49:31,874] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3213.9501\n",
      "[2025-11-19 21:49:45,359] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3164.1140 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:49:46,864] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3211.7888 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:49:47,076] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3211.7888\n",
      "[2025-11-19 21:50:02,396] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3200.9864\n",
      "[2025-11-19 21:50:33,111] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3187.0044\n",
      "[2025-11-19 21:51:01,415] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3127.8577 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:51:03,086] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3186.9617 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:51:03,242] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3186.9617\n",
      "[2025-11-19 21:51:34,170] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3182.2943\n",
      "[2025-11-19 21:52:03,079] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3181.0191\n",
      "[2025-11-19 21:52:15,036] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3109.2903 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:52:16,703] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3179.6108 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:52:16,861] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3179.6108\n",
      "[2025-11-19 21:52:30,914] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3174.0553\n",
      "[2025-11-19 21:52:46,346] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3164.6034\n",
      "[2025-11-19 21:53:31,414] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3093.7241 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:53:33,076] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3164.8840 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:54:18,639] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3162.5575\n",
      "[2025-11-19 21:54:47,029] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3080.8001 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:54:48,679] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3154.0156 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:54:48,849] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3154.0156\n",
      "[2025-11-19 21:55:59,383] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3070.0154 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:56:01,025] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3155.3582 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:56:16,357] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3148.8226\n",
      "[2025-11-19 21:56:46,770] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3146.4510\n",
      "[2025-11-19 21:57:15,691] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3056.1720 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:57:17,351] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3147.5202 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:58:01,774] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3144.7665\n",
      "[2025-11-19 21:58:17,160] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3143.5935\n",
      "[2025-11-19 21:58:31,176] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3047.4319 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:58:32,144] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3146.4416 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:58:47,687] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3140.6209\n",
      "[2025-11-19 21:59:02,438] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3140.2424\n",
      "[2025-11-19 21:59:33,513] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3139.8433\n",
      "[2025-11-19 21:59:47,205] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3037.6617 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 21:59:48,840] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3140.6772 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 22:00:04,504] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3138.5162\n",
      "[2025-11-19 22:00:20,145] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3137.3123\n",
      "[2025-11-19 22:00:33,747] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3134.1971\n",
      "[2025-11-19 22:01:01,732] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3027.1880 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 22:01:03,369] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3135.4187 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 22:01:33,005] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3131.2659\n",
      "[2025-11-19 22:01:47,778] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3129.6092\n",
      "[2025-11-19 22:02:01,379] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3127.6978\n",
      "[2025-11-19 22:02:14,786] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3019.0063 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 22:02:16,427] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3129.5677 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 22:02:45,128] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3127.2644\n",
      "[2025-11-19 22:03:25,417] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3010.1990 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 22:03:27,048] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3128.2166 (beta=60.000, gamma=100.000)\n",
      "[2025-11-19 22:03:27,081] [UniVITrainer] [INFO] Restored best model from epoch 77 (val loss = 3127.2644)\n",
      "[2025-11-19 22:03:27,874] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 22:03:27,875] [UniVITrainer] [INFO]   n_epochs: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 22:03:27,876] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 22:03:27,881] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-19 22:03:27,886] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-19 22:03:27,887] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 22:03:27,892] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 22:03:27,893] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 22:03:27,894] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 22:03:27,894] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 22:03:27,895] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 22:03:27,896] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 22:03:27,896] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 9] Done in 18.8 min\n",
      "  best_val_loss              = 3127.264\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0602\n",
      "  Label transfer (ADT→RNA)  = 0.630\n",
      "\n",
      "================================================================================\n",
      "[Config 10] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 40.0,\n",
      "  \"gamma\": 40.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71add978bb4453baebaa1f6a2a47d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 22:03:41,646] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4040.9055 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:03:43,282] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3696.9795 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:03:43,435] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3696.9795\n",
      "[2025-11-19 22:03:58,285] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3638.6482\n",
      "[2025-11-19 22:04:13,227] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3546.7933\n",
      "[2025-11-19 22:04:27,155] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3516.9342\n",
      "[2025-11-19 22:04:39,916] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3517.8928 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:04:41,549] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3498.3003 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:04:41,623] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3498.3003\n",
      "[2025-11-19 22:04:56,846] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3479.1486\n",
      "[2025-11-19 22:05:10,569] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3446.0097\n",
      "[2025-11-19 22:05:26,070] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3419.2821\n",
      "[2025-11-19 22:05:41,235] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3396.8265\n",
      "[2025-11-19 22:05:52,367] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3326.2618 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:05:54,001] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3308.9285 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:05:54,152] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3308.9285\n",
      "[2025-11-19 22:06:08,329] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3281.0206\n",
      "[2025-11-19 22:06:23,785] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3271.6970\n",
      "[2025-11-19 22:06:39,158] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3262.9686\n",
      "[2025-11-19 22:06:54,495] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3260.0340\n",
      "[2025-11-19 22:07:08,235] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3218.3577 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:07:09,867] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3251.2069 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:07:09,912] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3251.2069\n",
      "[2025-11-19 22:07:24,188] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3234.8962\n",
      "[2025-11-19 22:07:54,779] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3220.3256\n",
      "[2025-11-19 22:08:08,830] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3216.2431\n",
      "[2025-11-19 22:08:21,949] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3175.5698 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:08:23,626] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3206.8467 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:08:23,798] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3206.8467\n",
      "[2025-11-19 22:08:38,064] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3198.8234\n",
      "[2025-11-19 22:08:53,272] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3193.1699\n",
      "[2025-11-19 22:09:08,730] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3186.5656\n",
      "[2025-11-19 22:09:23,781] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3182.6573\n",
      "[2025-11-19 22:09:37,782] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3140.5604 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:09:39,477] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3179.6310 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:09:39,496] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3179.6310\n",
      "[2025-11-19 22:09:53,986] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3176.3242\n",
      "[2025-11-19 22:10:09,441] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3172.2075\n",
      "[2025-11-19 22:10:24,816] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3165.6670\n",
      "[2025-11-19 22:10:40,465] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3162.8256\n",
      "[2025-11-19 22:10:53,474] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3117.2135 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:10:55,168] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3169.1711 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:11:10,628] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3162.3337\n",
      "[2025-11-19 22:11:26,523] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3148.8384\n",
      "[2025-11-19 22:11:42,001] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3138.4393\n",
      "[2025-11-19 22:11:56,943] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3125.9399\n",
      "[2025-11-19 22:12:10,925] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3066.8868 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:12:12,617] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3118.7362 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:12:12,701] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3118.7362\n",
      "[2025-11-19 22:12:28,546] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3117.8332\n",
      "[2025-11-19 22:13:00,355] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3113.1281\n",
      "[2025-11-19 22:13:15,455] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3111.3260\n",
      "[2025-11-19 22:13:28,089] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3039.4413 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:13:29,737] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3111.3091 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:13:29,905] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3111.3091\n",
      "[2025-11-19 22:13:45,152] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3105.5840\n",
      "[2025-11-19 22:14:42,376] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3028.3082 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:14:44,022] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3117.1125 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:14:59,406] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3103.8897\n",
      "[2025-11-19 22:15:14,266] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3099.2159\n",
      "[2025-11-19 22:15:57,452] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3012.2701 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:15:59,096] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3098.2574 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:15:59,244] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3098.2574\n",
      "[2025-11-19 22:16:27,977] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3095.4623\n",
      "[2025-11-19 22:16:43,361] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3094.7480\n",
      "[2025-11-19 22:16:58,647] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3088.6205\n",
      "[2025-11-19 22:17:12,460] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3000.7112 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:17:13,919] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3094.1867 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:17:29,230] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3088.0406\n",
      "[2025-11-19 22:17:44,846] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3086.7033\n",
      "[2025-11-19 22:18:00,265] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3083.9189\n",
      "[2025-11-19 22:18:28,060] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2988.9729 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:18:29,299] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3084.0399 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:18:44,717] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3079.6200\n",
      "[2025-11-19 22:19:30,633] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3076.7034\n",
      "[2025-11-19 22:19:44,445] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2973.5636 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:19:46,085] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3076.0883 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:19:46,251] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3076.0883\n",
      "[2025-11-19 22:19:59,449] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3074.7077\n",
      "[2025-11-19 22:20:27,077] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3066.0512\n",
      "[2025-11-19 22:20:41,506] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2960.1403 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:20:41,644] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3075.3617 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:21:22,468] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2951.7656 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:21:23,454] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3067.1734 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:21:33,365] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3060.8650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 22:22:10,253] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2945.4263 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:22:11,295] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3062.6328 (beta=40.000, gamma=40.000)\n",
      "[2025-11-19 22:22:11,322] [UniVITrainer] [INFO] Restored best model from epoch 76 (val loss = 3060.8650)\n",
      "[2025-11-19 22:22:12,135] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 22:22:12,136] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 22:22:12,137] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 22:22:12,138] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-19 22:22:12,138] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-19 22:22:12,139] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 22:22:12,139] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 22:22:12,140] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 22:22:12,140] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 22:22:12,141] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 22:22:12,142] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 22:22:12,143] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 22:22:12,143] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 10] Done in 18.7 min\n",
      "  best_val_loss              = 3060.865\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0585\n",
      "  Label transfer (ADT→RNA)  = 0.589\n",
      "\n",
      "================================================================================\n",
      "[Config 11] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 40.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846a5243fd86420eb7cd63828e756bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 22:22:20,782] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4013.0373 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:22:21,791] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3811.9967 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:22:21,822] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3811.9967\n",
      "[2025-11-19 22:22:31,490] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3807.3843\n",
      "[2025-11-19 22:22:41,418] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3806.2687\n",
      "[2025-11-19 22:22:51,330] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3805.8027\n",
      "[2025-11-19 22:22:58,261] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3802.3739 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:22:59,297] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3805.6487 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:22:59,330] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3805.6487\n",
      "[2025-11-19 22:23:18,507] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3805.2903\n",
      "[2025-11-19 22:23:28,222] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3805.2816\n",
      "[2025-11-19 22:23:36,956] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3805.2381\n",
      "[2025-11-19 22:23:45,799] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3798.2531 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:23:46,858] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3805.2026 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:23:46,870] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3805.2026\n",
      "[2025-11-19 22:23:57,032] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3804.8949\n",
      "[2025-11-19 22:24:06,697] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3802.6023\n",
      "[2025-11-19 22:24:16,678] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3736.1756\n",
      "[2025-11-19 22:24:26,608] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3706.5917\n",
      "[2025-11-19 22:24:32,989] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3691.6941 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:24:33,997] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3694.9509 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:24:34,017] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3694.9509\n",
      "[2025-11-19 22:24:42,924] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3688.6019\n",
      "[2025-11-19 22:24:58,636] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3686.0908\n",
      "[2025-11-19 22:25:05,898] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3679.0903\n",
      "[2025-11-19 22:25:13,122] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3664.8799 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:25:14,175] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3677.5982 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:25:14,292] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3677.5982\n",
      "[2025-11-19 22:25:21,465] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3677.0267\n",
      "[2025-11-19 22:25:49,675] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3667.0464\n",
      "[2025-11-19 22:25:58,587] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3654.0418 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:25:59,253] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3662.3426 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:25:59,366] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3662.3426\n",
      "[2025-11-19 22:26:36,510] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3660.5135\n",
      "[2025-11-19 22:26:43,834] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3639.4768 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:26:44,298] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3655.3137 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:26:44,353] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3655.3137\n",
      "[2025-11-19 22:26:53,427] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3652.7176\n",
      "[2025-11-19 22:27:02,977] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3651.4197\n",
      "[2025-11-19 22:27:11,360] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3646.8647\n",
      "[2025-11-19 22:27:18,406] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3628.8642\n",
      "[2025-11-19 22:27:27,127] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3604.6519 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:27:28,132] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3615.2243 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:27:28,188] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3615.2243\n",
      "[2025-11-19 22:27:35,637] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3614.6138\n",
      "[2025-11-19 22:27:42,856] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3609.7148\n",
      "[2025-11-19 22:27:52,347] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3598.8132\n",
      "[2025-11-19 22:28:00,894] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3593.7694\n",
      "[2025-11-19 22:28:09,793] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3565.8691 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:28:10,802] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3594.5229 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:28:30,092] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3584.5059\n",
      "[2025-11-19 22:29:00,060] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3553.0681 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:29:01,073] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3583.9166 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:29:01,163] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3583.9166\n",
      "[2025-11-19 22:29:09,566] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3577.8985\n",
      "[2025-11-19 22:29:36,641] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3577.8253\n",
      "[2025-11-19 22:29:45,110] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3547.4421 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:29:46,182] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3572.6706 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:29:46,240] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3572.6706\n",
      "[2025-11-19 22:30:53,057] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3545.2295 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:30:53,842] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3568.6121 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:30:53,942] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3568.6121\n",
      "[2025-11-19 22:31:23,488] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3567.7501\n",
      "[2025-11-19 22:31:50,637] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3546.4367\n",
      "[2025-11-19 22:32:04,483] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3497.8364 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:32:06,186] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3512.4699 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:32:06,301] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3512.4699\n",
      "[2025-11-19 22:32:19,244] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3505.0665\n",
      "[2025-11-19 22:32:32,950] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3501.5834\n",
      "[2025-11-19 22:32:47,381] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3494.9735\n",
      "[2025-11-19 22:33:00,878] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3492.5853\n",
      "[2025-11-19 22:33:14,605] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3454.9155 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:33:16,290] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3492.6352 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:33:29,446] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3491.3478\n",
      "[2025-11-19 22:33:45,065] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3489.4421\n",
      "[2025-11-19 22:34:00,175] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3479.7884\n",
      "[2025-11-19 22:34:27,987] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3438.3905 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:34:29,674] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3475.4435 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:34:29,773] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3475.4435\n",
      "[2025-11-19 22:35:15,156] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3473.0869\n",
      "[2025-11-19 22:35:42,495] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3432.1992 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:35:44,162] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3473.3119 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:35:59,460] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3472.0786\n",
      "[2025-11-19 22:36:11,364] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3468.0947\n",
      "[2025-11-19 22:36:24,058] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3465.7620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 22:36:47,921] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3427.7068 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:36:49,006] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3475.6969 (beta=160.000, gamma=40.000)\n",
      "[2025-11-19 22:36:49,042] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3465.7620)\n",
      "[2025-11-19 22:36:49,954] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 22:36:49,955] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 22:36:49,956] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 22:36:49,956] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-19 22:36:49,957] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-19 22:36:49,957] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 22:36:49,958] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 22:36:49,959] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 22:36:49,959] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 22:36:49,960] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 22:36:49,960] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 22:36:49,961] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 22:36:49,961] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 11] Done in 14.6 min\n",
      "  best_val_loss              = 3465.762\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1621\n",
      "  Label transfer (ADT→RNA)  = 0.295\n",
      "\n",
      "================================================================================\n",
      "[Config 12] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 60.0,\n",
      "  \"gamma\": 120.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6e1f79f0384fe1ba5e9104f4a0c8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 22:37:03,219] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4400.7778 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:37:04,462] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3755.7924 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:37:04,613] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3755.7924\n",
      "[2025-11-19 22:37:19,193] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3632.9344\n",
      "[2025-11-19 22:37:33,017] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3584.7252\n",
      "[2025-11-19 22:37:46,769] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3561.0130\n",
      "[2025-11-19 22:37:59,329] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3555.1958 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:38:00,767] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3532.5126 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:38:00,891] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3532.5126\n",
      "[2025-11-19 22:38:12,278] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3453.2206\n",
      "[2025-11-19 22:38:23,102] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3392.8255\n",
      "[2025-11-19 22:38:34,594] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3355.5363\n",
      "[2025-11-19 22:38:41,155] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3333.1411\n",
      "[2025-11-19 22:38:46,747] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3302.1676 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:38:47,174] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3310.6995 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:38:47,386] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3310.6995\n",
      "[2025-11-19 22:38:53,047] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3293.8775\n",
      "[2025-11-19 22:38:57,917] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3271.2320\n",
      "[2025-11-19 22:39:07,611] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3270.0511\n",
      "[2025-11-19 22:39:15,990] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3264.0538\n",
      "[2025-11-19 22:39:27,798] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3234.7750 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:39:29,505] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3259.2960 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:39:29,721] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3259.2960\n",
      "[2025-11-19 22:39:57,248] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3247.8643\n",
      "[2025-11-19 22:40:28,538] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3241.5847\n",
      "[2025-11-19 22:40:41,770] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3209.9915 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:40:43,459] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3242.8807 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:40:56,103] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3231.4224\n",
      "[2025-11-19 22:41:26,545] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3228.6517\n",
      "[2025-11-19 22:41:42,513] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3225.5894\n",
      "[2025-11-19 22:41:55,972] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3182.6662 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:41:57,573] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3218.2107 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:41:57,660] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3218.2107\n",
      "[2025-11-19 22:42:28,467] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3212.4876\n",
      "[2025-11-19 22:42:44,186] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3202.6517\n",
      "[2025-11-19 22:42:58,054] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3199.4302\n",
      "[2025-11-19 22:43:11,399] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3162.1188 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:43:13,038] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3198.2884 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:43:13,078] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3198.2884\n",
      "[2025-11-19 22:43:42,915] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3191.4613\n",
      "[2025-11-19 22:44:26,276] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3132.2180 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:44:27,923] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3182.2016 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:44:28,043] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3182.2016\n",
      "[2025-11-19 22:44:58,438] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3177.3821\n",
      "[2025-11-19 22:45:28,410] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3175.1316\n",
      "[2025-11-19 22:45:42,184] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3119.2210 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:45:43,834] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3174.2038 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:45:43,877] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3174.2038\n",
      "[2025-11-19 22:45:58,996] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3168.9871\n",
      "[2025-11-19 22:46:14,744] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3168.5986\n",
      "[2025-11-19 22:46:30,252] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3166.8100\n",
      "[2025-11-19 22:46:58,545] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3102.5748 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:47:00,172] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3166.2967 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:47:00,300] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3166.2967\n",
      "[2025-11-19 22:47:44,496] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3162.3411\n",
      "[2025-11-19 22:47:55,501] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3162.2628\n",
      "[2025-11-19 22:48:03,310] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3093.4563 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:48:03,660] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3154.1315 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:48:03,673] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3154.1315\n",
      "[2025-11-19 22:48:39,584] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3146.6384\n",
      "[2025-11-19 22:48:48,266] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3079.4333 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:48:48,866] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3157.5460 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:49:33,287] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3073.8668 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:49:34,370] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3147.9452 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:50:22,827] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3064.5579 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:50:23,046] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3141.9667 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:50:23,171] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3141.9667\n",
      "[2025-11-19 22:50:51,115] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3138.8209\n",
      "[2025-11-19 22:50:58,480] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3135.2125\n",
      "[2025-11-19 22:51:05,793] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3055.7667 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:51:06,783] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3139.5014 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:51:40,662] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3133.2084\n",
      "[2025-11-19 22:51:49,306] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3047.4790 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:51:50,318] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3139.2560 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:52:26,069] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3129.2913\n",
      "[2025-11-19 22:52:34,541] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3038.8791 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:52:34,666] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3130.7734 (beta=60.000, gamma=120.000)\n",
      "[2025-11-19 22:52:34,677] [UniVITrainer] [INFO] Restored best model from epoch 79 (val loss = 3129.2913)\n",
      "[2025-11-19 22:52:35,510] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 22:52:35,511] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 22:52:35,512] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 22:52:35,513] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-19 22:52:35,514] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-19 22:52:35,515] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 22:52:35,515] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 22:52:35,516] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 22:52:35,517] [UniVITrainer] [INFO]   num_workers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 22:52:35,517] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 22:52:35,518] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 22:52:35,519] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 22:52:35,520] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 12] Done in 15.7 min\n",
      "  best_val_loss              = 3129.291\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0486\n",
      "  Label transfer (ADT→RNA)  = 0.627\n",
      "\n",
      "================================================================================\n",
      "[Config 13] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 40.0,\n",
      "  \"gamma\": 80.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329a43ae7e984aa7bf40b7c0322523b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 22:52:44,293] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3876.7316 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:52:45,279] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3646.6168 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:52:45,332] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3646.6168\n",
      "[2025-11-19 22:52:52,804] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3438.8608\n",
      "[2025-11-19 22:53:02,442] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3393.9136\n",
      "[2025-11-19 22:53:11,850] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3303.6275\n",
      "[2025-11-19 22:53:20,385] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3260.5778 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:53:20,982] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3277.8273 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:53:21,179] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3277.8273\n",
      "[2025-11-19 22:53:30,871] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3229.9361\n",
      "[2025-11-19 22:53:40,622] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3220.2256\n",
      "[2025-11-19 22:53:48,518] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3206.9505\n",
      "[2025-11-19 22:53:58,177] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3201.6002\n",
      "[2025-11-19 22:54:06,133] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3128.8351 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:54:06,271] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3157.6844 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:54:06,380] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3157.6844\n",
      "[2025-11-19 22:54:16,218] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3150.1071\n",
      "[2025-11-19 22:54:35,378] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3147.9695\n",
      "[2025-11-19 22:54:43,109] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3138.0924\n",
      "[2025-11-19 22:54:51,737] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3073.0936 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:54:52,735] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3131.8849 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:54:52,797] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3131.8849\n",
      "[2025-11-19 22:55:00,670] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3125.5033\n",
      "[2025-11-19 22:55:18,735] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3118.8509\n",
      "[2025-11-19 22:55:27,162] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3115.7739\n",
      "[2025-11-19 22:55:35,655] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3039.5277 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:55:36,730] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3100.8794 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:55:36,749] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3100.8794\n",
      "[2025-11-19 22:56:19,386] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3010.8315 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:56:19,859] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3109.4321 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:56:27,818] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3085.6604\n",
      "[2025-11-19 22:56:45,761] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3080.2541\n",
      "[2025-11-19 22:57:05,001] [UniVITrainer] [INFO] [Epoch 030] Train loss: 2984.6360 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:57:06,067] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3084.3887 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:57:16,034] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3079.2591\n",
      "[2025-11-19 22:57:26,502] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3077.3316\n",
      "[2025-11-19 22:57:36,888] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3073.6351\n",
      "[2025-11-19 22:57:47,136] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3069.4288\n",
      "[2025-11-19 22:57:54,597] [UniVITrainer] [INFO] [Epoch 035] Train loss: 2962.4613 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:57:55,663] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3067.9150 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:57:55,787] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3067.9150\n",
      "[2025-11-19 22:58:04,582] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3067.6753\n",
      "[2025-11-19 22:58:24,707] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3056.8279\n",
      "[2025-11-19 22:58:43,734] [UniVITrainer] [INFO] [Epoch 040] Train loss: 2947.5382 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:58:44,760] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3062.9620 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:58:53,174] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3054.4711\n",
      "[2025-11-19 22:59:28,774] [UniVITrainer] [INFO] [Epoch 045] Train loss: 2931.8403 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:59:29,834] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3064.9688 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 22:59:39,798] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3048.3656\n",
      "[2025-11-19 23:00:19,037] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3044.5622\n",
      "[2025-11-19 23:00:30,802] [UniVITrainer] [INFO] [Epoch 050] Train loss: 2918.1181 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:00:32,462] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3047.7841 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:01:01,798] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3041.3988\n",
      "[2025-11-19 23:01:45,389] [UniVITrainer] [INFO] [Epoch 055] Train loss: 2908.3739 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:01:47,043] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3036.6797 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:01:47,240] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3036.6797\n",
      "[2025-11-19 23:02:15,342] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3034.9404\n",
      "[2025-11-19 23:02:57,326] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2888.7432 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:02:58,984] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3044.2455 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:03:27,126] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3030.8652\n",
      "[2025-11-19 23:04:10,103] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2877.3275 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:04:11,440] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3033.3660 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:05:23,006] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2868.0743 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:05:24,660] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3033.4760 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:05:54,657] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3030.6820\n",
      "[2025-11-19 23:06:38,451] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2851.0789 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:06:40,081] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3027.5427 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:06:40,286] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 3027.5427\n",
      "[2025-11-19 23:06:54,647] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3026.6694\n",
      "[2025-11-19 23:07:09,681] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3025.6495\n",
      "[2025-11-19 23:07:52,709] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2844.1831 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:07:53,315] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3021.3748 (beta=40.000, gamma=80.000)\n",
      "[2025-11-19 23:07:53,518] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 3021.3748\n",
      "[2025-11-19 23:07:53,558] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 3021.3748)\n",
      "[2025-11-19 23:07:54,490] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 23:07:54,492] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 23:07:54,492] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 23:07:54,496] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-19 23:07:54,499] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-19 23:07:54,502] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 23:07:54,503] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 23:07:54,507] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 23:07:54,508] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 23:07:54,511] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 23:07:54,512] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 23:07:54,513] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 23:07:54,513] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 13] Done in 15.3 min\n",
      "  best_val_loss              = 3021.375\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0400\n",
      "  Label transfer (ADT→RNA)  = 0.669\n",
      "--> New best config (id=13) with score=1882.982\n",
      "\n",
      "================================================================================\n",
      "[Config 14] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 32,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 0.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e339e89eba084910be1fa4d13afc2540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 23:08:08,531] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3935.5798 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:08:10,218] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3811.8586 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:08:10,361] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3811.8586\n",
      "[2025-11-19 23:08:25,208] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3700.2476\n",
      "[2025-11-19 23:08:40,252] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3687.2250\n",
      "[2025-11-19 23:08:53,691] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3684.6528\n",
      "[2025-11-19 23:09:05,957] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3673.8861 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:09:07,655] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3672.7220 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:09:07,697] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3672.7220\n",
      "[2025-11-19 23:09:23,408] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3667.9605\n",
      "[2025-11-19 23:09:36,991] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3618.1040\n",
      "[2025-11-19 23:09:46,658] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3558.8388\n",
      "[2025-11-19 23:09:56,753] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3526.7726\n",
      "[2025-11-19 23:10:05,218] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3489.8927 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:10:06,281] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3495.8330 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:10:06,312] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3495.8330\n",
      "[2025-11-19 23:10:16,392] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3488.1503\n",
      "[2025-11-19 23:10:26,297] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3482.0741\n",
      "[2025-11-19 23:10:35,912] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3470.1780\n",
      "[2025-11-19 23:10:54,979] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3441.1357 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:10:56,011] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3457.2039 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:10:56,164] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3457.2039\n",
      "[2025-11-19 23:11:05,746] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3454.5447\n",
      "[2025-11-19 23:11:23,959] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3448.7943\n",
      "[2025-11-19 23:11:33,898] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3443.7755\n",
      "[2025-11-19 23:11:42,701] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3413.1769 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:11:43,736] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3445.2732 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:12:12,894] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3427.5268\n",
      "[2025-11-19 23:12:43,478] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3416.9688\n",
      "[2025-11-19 23:12:54,544] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3384.8289 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:12:56,232] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3422.8354 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:13:09,417] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3411.7775\n",
      "[2025-11-19 23:13:23,332] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3407.2053\n",
      "[2025-11-19 23:13:40,882] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3401.2638\n",
      "[2025-11-19 23:13:59,660] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3395.9962\n",
      "[2025-11-19 23:14:15,415] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3334.8008 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:14:17,108] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3377.8767 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:14:17,280] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3377.8767\n",
      "[2025-11-19 23:14:54,821] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3365.3266\n",
      "[2025-11-19 23:15:14,058] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3360.1631\n",
      "[2025-11-19 23:15:49,790] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3308.6302 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:15:52,021] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3368.5824 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:16:30,606] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3359.1587\n",
      "[2025-11-19 23:17:08,863] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3355.4720\n",
      "[2025-11-19 23:17:26,150] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3295.7251 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:17:27,770] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3346.2912 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:17:28,011] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3346.2912\n",
      "[2025-11-19 23:18:43,632] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3344.0140\n",
      "[2025-11-19 23:19:01,010] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3288.1051 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:19:03,224] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3346.1679 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:19:23,343] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3339.1014\n",
      "[2025-11-19 23:20:40,145] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3281.9050 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:20:42,356] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3340.0322 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:21:02,935] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3336.8947\n",
      "[2025-11-19 23:21:22,367] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3333.6243\n",
      "[2025-11-19 23:22:17,602] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3272.4905 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:22:19,598] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3332.7193 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:22:19,899] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3332.7193\n",
      "[2025-11-19 23:23:17,027] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3327.7043\n",
      "[2025-11-19 23:23:52,827] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3259.6116 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:23:55,052] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3332.3633 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:24:12,408] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3325.3875\n",
      "[2025-11-19 23:24:32,406] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3324.8363\n",
      "[2025-11-19 23:24:52,000] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3314.8320\n",
      "[2025-11-19 23:25:11,641] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3309.9794\n",
      "[2025-11-19 23:25:29,769] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3247.8100 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:25:31,493] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3312.5811 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:25:51,360] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3307.8756\n",
      "[2025-11-19 23:27:06,822] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3240.9730 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:27:08,969] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3314.4864 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:27:28,842] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3307.1772\n",
      "[2025-11-19 23:28:08,282] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3302.8057\n",
      "[2025-11-19 23:28:27,983] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3298.2079\n",
      "[2025-11-19 23:28:45,058] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3232.4326 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:28:47,273] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3305.8829 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:29:48,534] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3292.9021\n",
      "[2025-11-19 23:30:27,015] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3223.6806 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:30:29,254] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3295.0157 (beta=120.000, gamma=0.000)\n",
      "[2025-11-19 23:30:29,291] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3292.9021)\n",
      "[2025-11-19 23:30:30,430] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 23:30:30,431] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 23:30:30,431] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 23:30:30,432] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-19 23:30:30,432] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-19 23:30:30,433] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-19 23:30:30,434] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 23:30:30,434] [UniVITrainer] [INFO]   grad_clip: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 23:30:30,435] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 23:30:30,435] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 23:30:30,436] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 23:30:30,436] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 23:30:30,437] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 14] Done in 22.6 min\n",
      "  best_val_loss              = 3292.902\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.2914\n",
      "  Label transfer (ADT→RNA)  = 0.164\n",
      "\n",
      "================================================================================\n",
      "[Config 15] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 80.0,\n",
      "  \"gamma\": 160.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f6002e248946fbad01ebc5a4b53dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 23:30:48,075] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4364.8212 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:30:50,244] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3798.9471 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:30:50,506] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3798.9471\n",
      "[2025-11-19 23:31:10,647] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3737.2626\n",
      "[2025-11-19 23:31:30,527] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3647.4730\n",
      "[2025-11-19 23:31:50,436] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3584.1663\n",
      "[2025-11-19 23:32:08,039] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3580.1481 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:32:10,115] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3560.6458 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:32:10,393] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3560.6458\n",
      "[2025-11-19 23:32:30,133] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3459.1871\n",
      "[2025-11-19 23:32:50,351] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3438.0119\n",
      "[2025-11-19 23:33:10,218] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3420.8635\n",
      "[2025-11-19 23:33:30,104] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3410.4385\n",
      "[2025-11-19 23:33:47,712] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3394.2016 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:33:49,769] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3402.0444 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:33:50,012] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3402.0444\n",
      "[2025-11-19 23:34:10,220] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3371.6355\n",
      "[2025-11-19 23:34:30,417] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3343.3930\n",
      "[2025-11-19 23:34:50,649] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3335.8576\n",
      "[2025-11-19 23:35:10,688] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3321.4785\n",
      "[2025-11-19 23:35:28,406] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3295.5093 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:35:30,565] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3318.6258 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:35:30,837] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3318.6258\n",
      "[2025-11-19 23:35:50,996] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3304.1196\n",
      "[2025-11-19 23:36:11,327] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3293.9896\n",
      "[2025-11-19 23:37:08,881] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3250.8041 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:37:11,047] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3284.5642 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:37:11,312] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3284.5642\n",
      "[2025-11-19 23:37:51,520] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3283.0190\n",
      "[2025-11-19 23:38:30,856] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3275.8175\n",
      "[2025-11-19 23:38:48,227] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3233.9681 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:38:50,381] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3271.0392 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:38:50,634] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3271.0392\n",
      "[2025-11-19 23:39:29,909] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3267.0125\n",
      "[2025-11-19 23:39:49,737] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3264.5165\n",
      "[2025-11-19 23:40:09,600] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3263.5050\n",
      "[2025-11-19 23:40:27,303] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3222.0174 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:40:29,481] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3262.5208 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:40:29,740] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3262.5208\n",
      "[2025-11-19 23:40:49,673] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3261.6787\n",
      "[2025-11-19 23:42:06,525] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3215.3557 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:42:08,541] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3260.2402 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:42:08,811] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3260.2402\n",
      "[2025-11-19 23:42:48,685] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3255.6022\n",
      "[2025-11-19 23:43:45,901] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3203.8430 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:43:48,072] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3247.7633 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:43:48,364] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3247.7633\n",
      "[2025-11-19 23:45:20,092] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3194.6742 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:45:22,270] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3247.6390 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:45:22,548] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3247.6390\n",
      "[2025-11-19 23:46:14,243] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3243.4729\n",
      "[2025-11-19 23:46:32,236] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3243.4507\n",
      "[2025-11-19 23:46:45,337] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3184.4845 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:46:47,549] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3240.8176 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:46:47,794] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3240.8176\n",
      "[2025-11-19 23:47:39,437] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3237.2218\n",
      "[2025-11-19 23:48:07,384] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3169.5085 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:48:07,764] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3236.0184 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:48:08,034] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3236.0184\n",
      "[2025-11-19 23:48:15,047] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3235.6083\n",
      "[2025-11-19 23:48:28,580] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3229.1551\n",
      "[2025-11-19 23:49:12,257] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3162.4966 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:49:13,950] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3228.7206 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:49:14,077] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3228.7206\n",
      "[2025-11-19 23:49:29,704] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3223.6004\n",
      "[2025-11-19 23:49:45,148] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3214.1898\n",
      "[2025-11-19 23:50:16,552] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3211.2836\n",
      "[2025-11-19 23:50:30,204] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3139.2530 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:50:31,879] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3216.6654 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:51:47,487] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3130.1880 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:51:48,454] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3211.0391 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:51:48,616] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3211.0391\n",
      "[2025-11-19 23:52:04,514] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3207.7903\n",
      "[2025-11-19 23:53:04,505] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3122.1193 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:53:06,196] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3212.5148 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:53:37,424] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3203.9379\n",
      "[2025-11-19 23:54:04,954] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3202.8650\n",
      "[2025-11-19 23:54:18,176] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3121.5920 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:54:19,822] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3211.1440 (beta=80.000, gamma=160.000)\n",
      "[2025-11-19 23:54:19,849] [UniVITrainer] [INFO] Restored best model from epoch 79 (val loss = 3202.8650)\n",
      "[2025-11-19 23:54:20,689] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-19 23:54:20,690] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-19 23:54:20,690] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-19 23:54:20,690] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-19 23:54:20,691] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-19 23:54:20,691] [UniVITrainer] [INFO]   device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 23:54:20,692] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-19 23:54:20,692] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-19 23:54:20,693] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-19 23:54:20,693] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-19 23:54:20,693] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-19 23:54:20,694] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-19 23:54:20,694] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 15] Done in 23.8 min\n",
      "  best_val_loss              = 3202.865\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0667\n",
      "  Label transfer (ADT→RNA)  = 0.607\n",
      "\n",
      "================================================================================\n",
      "[Config 16] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 120,\n",
      "  \"beta\": 0.0,\n",
      "  \"gamma\": 40.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8773b3adc8df479f83376c1275d50719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 23:54:33,546] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3879.4029 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:54:35,188] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3318.9841 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:54:35,413] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3318.9841\n",
      "[2025-11-19 23:54:50,899] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3111.7012\n",
      "[2025-11-19 23:55:05,958] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3027.6545\n",
      "[2025-11-19 23:55:20,400] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 2976.3771\n",
      "[2025-11-19 23:55:34,361] [UniVITrainer] [INFO] [Epoch 005] Train loss: 2936.5931 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:55:35,452] [UniVITrainer] [INFO] [Epoch 005] Val loss: 2933.5079 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:55:35,674] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 2933.5079\n",
      "[2025-11-19 23:55:51,524] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 2913.5345\n",
      "[2025-11-19 23:56:07,024] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 2888.7931\n",
      "[2025-11-19 23:56:20,633] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 2871.5774\n",
      "[2025-11-19 23:56:36,272] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 2862.8585\n",
      "[2025-11-19 23:56:50,120] [UniVITrainer] [INFO] [Epoch 010] Train loss: 2809.8965 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:56:51,766] [UniVITrainer] [INFO] [Epoch 010] Val loss: 2851.1323 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:56:51,795] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 2851.1323\n",
      "[2025-11-19 23:57:07,774] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 2841.6587\n",
      "[2025-11-19 23:57:22,216] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 2826.0276\n",
      "[2025-11-19 23:57:37,860] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 2814.3481\n",
      "[2025-11-19 23:57:53,309] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 2800.2349\n",
      "[2025-11-19 23:58:06,595] [UniVITrainer] [INFO] [Epoch 015] Train loss: 2720.0479 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:58:08,246] [UniVITrainer] [INFO] [Epoch 015] Val loss: 2794.7500 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:58:08,463] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 2794.7500\n",
      "[2025-11-19 23:58:24,229] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 2772.3873\n",
      "[2025-11-19 23:58:39,977] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 2767.2573\n",
      "[2025-11-19 23:58:55,762] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 2754.0334\n",
      "[2025-11-19 23:59:11,032] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 2745.8272\n",
      "[2025-11-19 23:59:24,349] [UniVITrainer] [INFO] [Epoch 020] Train loss: 2647.6439 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:59:25,976] [UniVITrainer] [INFO] [Epoch 020] Val loss: 2736.8021 (beta=0.000, gamma=40.000)\n",
      "[2025-11-19 23:59:26,017] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 2736.8021\n",
      "[2025-11-19 23:59:41,197] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 2729.8182\n",
      "[2025-11-19 23:59:56,408] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 2719.5656\n",
      "[2025-11-20 00:00:12,106] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 2714.9883\n",
      "[2025-11-20 00:00:27,009] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 2703.6191\n",
      "[2025-11-20 00:00:40,017] [UniVITrainer] [INFO] [Epoch 025] Train loss: 2578.1582 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:00:40,980] [UniVITrainer] [INFO] [Epoch 025] Val loss: 2695.7744 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:00:41,215] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 2695.7744\n",
      "[2025-11-20 00:00:57,025] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 2689.2662\n",
      "[2025-11-20 00:01:12,811] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 2682.8561\n",
      "[2025-11-20 00:01:28,058] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 2674.3121\n",
      "[2025-11-20 00:01:42,318] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 2661.8604\n",
      "[2025-11-20 00:01:56,203] [UniVITrainer] [INFO] [Epoch 030] Train loss: 2517.8629 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:01:57,879] [UniVITrainer] [INFO] [Epoch 030] Val loss: 2665.2381 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:02:13,439] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 2658.1939\n",
      "[2025-11-20 00:02:28,998] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 2655.6469\n",
      "[2025-11-20 00:02:44,867] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 2648.4371\n",
      "[2025-11-20 00:03:00,393] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 2636.8846\n",
      "[2025-11-20 00:03:14,051] [UniVITrainer] [INFO] [Epoch 035] Train loss: 2462.1526 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:03:15,705] [UniVITrainer] [INFO] [Epoch 035] Val loss: 2634.2387 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:03:15,940] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 2634.2387\n",
      "[2025-11-20 00:03:31,665] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 2625.0317\n",
      "[2025-11-20 00:03:47,494] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 2622.2083\n",
      "[2025-11-20 00:04:01,852] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 2607.4532\n",
      "[2025-11-20 00:04:29,945] [UniVITrainer] [INFO] [Epoch 040] Train loss: 2400.4989 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:04:31,621] [UniVITrainer] [INFO] [Epoch 040] Val loss: 2602.1408 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:04:31,849] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 2602.1408\n",
      "[2025-11-20 00:05:00,721] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 2591.2587\n",
      "[2025-11-20 00:05:16,623] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 2589.8950\n",
      "[2025-11-20 00:05:31,566] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 2582.5438\n",
      "[2025-11-20 00:05:45,612] [UniVITrainer] [INFO] [Epoch 045] Train loss: 2341.0153 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:05:47,272] [UniVITrainer] [INFO] [Epoch 045] Val loss: 2579.2773 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:05:47,490] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 2579.2773\n",
      "[2025-11-20 00:06:03,502] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 2576.4834\n",
      "[2025-11-20 00:06:19,254] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 2563.7897\n",
      "[2025-11-20 00:06:35,079] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 2561.3419\n",
      "[2025-11-20 00:06:50,580] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 2557.6450\n",
      "[2025-11-20 00:07:04,434] [UniVITrainer] [INFO] [Epoch 050] Train loss: 2296.9099 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:07:06,083] [UniVITrainer] [INFO] [Epoch 050] Val loss: 2561.3410 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:07:19,845] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 2553.7339\n",
      "[2025-11-20 00:07:34,993] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 2542.9900\n",
      "[2025-11-20 00:08:17,983] [UniVITrainer] [INFO] [Epoch 055] Train loss: 2257.9665 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:08:19,534] [UniVITrainer] [INFO] [Epoch 055] Val loss: 2536.2080 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:08:19,780] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 2536.2080\n",
      "[2025-11-20 00:08:33,953] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 2532.9902\n",
      "[2025-11-20 00:08:48,739] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 2528.4894\n",
      "[2025-11-20 00:09:01,581] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 2524.1322\n",
      "[2025-11-20 00:09:15,264] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 2519.4738\n",
      "[2025-11-20 00:09:29,092] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2211.8941 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:09:30,731] [UniVITrainer] [INFO] [Epoch 060] Val loss: 2510.8724 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:09:30,965] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 2510.8724\n",
      "[2025-11-20 00:09:44,060] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 2509.4854\n",
      "[2025-11-20 00:10:28,122] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 2499.5407\n",
      "[2025-11-20 00:10:41,874] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2176.6782 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:10:43,324] [UniVITrainer] [INFO] [Epoch 065] Val loss: 2497.6278 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:10:43,552] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 2497.6278\n",
      "[2025-11-20 00:10:58,716] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 2495.4265\n",
      "[2025-11-20 00:11:14,420] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 2493.0397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 00:11:44,514] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 2482.0989\n",
      "[2025-11-20 00:11:58,130] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2142.8811 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:11:59,701] [UniVITrainer] [INFO] [Epoch 070] Val loss: 2484.5317 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:12:12,949] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 2480.0318\n",
      "[2025-11-20 00:12:25,902] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 2475.4073\n",
      "[2025-11-20 00:12:37,354] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 2468.8340\n",
      "[2025-11-20 00:12:51,828] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 2462.0649\n",
      "[2025-11-20 00:13:03,572] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2109.1492 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:13:04,984] [UniVITrainer] [INFO] [Epoch 075] Val loss: 2465.4590 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:13:51,540] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 2455.4641\n",
      "[2025-11-20 00:14:07,136] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 2448.1539\n",
      "[2025-11-20 00:14:19,269] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2086.7503 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:14:20,979] [UniVITrainer] [INFO] [Epoch 080] Val loss: 2456.3543 (beta=0.000, gamma=40.000)\n",
      "[2025-11-20 00:14:21,021] [UniVITrainer] [INFO] Restored best model from epoch 79 (val loss = 2448.1539)\n",
      "[2025-11-20 00:14:21,886] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 00:14:21,887] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 00:14:21,889] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 00:14:21,891] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 00:14:21,892] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 00:14:21,892] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 00:14:21,892] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 00:14:21,893] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 00:14:21,893] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 00:14:21,894] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 00:14:21,894] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 00:14:21,895] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 00:14:21,895] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 16] Done in 20.0 min\n",
      "  best_val_loss              = 2448.154\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0159\n",
      "  Label transfer (ADT→RNA)  = 0.841\n",
      "--> New best config (id=16) with score=1350.989\n",
      "\n",
      "================================================================================\n",
      "[Config 17] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 60.0,\n",
      "  \"gamma\": 60.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843c8f9893854e1286b3f65e4d4860b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 00:14:35,797] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3911.7049 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:14:37,502] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3695.4855 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:14:37,534] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3695.4855\n",
      "[2025-11-20 00:14:52,902] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3646.4067\n",
      "[2025-11-20 00:15:08,421] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3573.2219\n",
      "[2025-11-20 00:15:24,373] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3544.6431\n",
      "[2025-11-20 00:15:37,360] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3510.0147 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:15:39,058] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3517.0822 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:15:39,102] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3517.0822\n",
      "[2025-11-20 00:15:54,783] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3429.4845\n",
      "[2025-11-20 00:16:10,029] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3378.6526\n",
      "[2025-11-20 00:16:25,925] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3361.5902\n",
      "[2025-11-20 00:16:39,660] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3332.5653\n",
      "[2025-11-20 00:16:53,461] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3277.7297 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:16:55,131] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3315.2783 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:16:55,216] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3315.2783\n",
      "[2025-11-20 00:17:11,162] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3300.8412\n",
      "[2025-11-20 00:17:42,561] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3293.3952\n",
      "[2025-11-20 00:17:58,170] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3283.3163\n",
      "[2025-11-20 00:18:11,129] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3221.3514 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:18:12,809] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3268.2352 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:18:12,973] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3268.2352\n",
      "[2025-11-20 00:18:27,675] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3262.9197\n",
      "[2025-11-20 00:18:59,000] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3253.5877\n",
      "[2025-11-20 00:19:14,265] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3253.3195\n",
      "[2025-11-20 00:19:27,828] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3191.4830 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:19:29,335] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3240.1857 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:19:29,511] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3240.1857\n",
      "[2025-11-20 00:20:13,279] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3225.5157\n",
      "[2025-11-20 00:20:40,838] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3167.2737 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:20:42,478] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3223.0119 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:20:42,657] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3223.0119\n",
      "[2025-11-20 00:20:58,108] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3216.4629\n",
      "[2025-11-20 00:21:27,484] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3213.4689\n",
      "[2025-11-20 00:21:54,076] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3152.0026 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:21:55,719] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3215.6794 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:22:10,674] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3207.9991\n",
      "[2025-11-20 00:22:24,634] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3200.3854\n",
      "[2025-11-20 00:22:54,846] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3199.9530\n",
      "[2025-11-20 00:23:08,133] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3132.6201 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:23:09,769] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3199.6541 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:23:09,941] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3199.6541\n",
      "[2025-11-20 00:23:40,632] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3197.0601\n",
      "[2025-11-20 00:23:55,511] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3194.9887\n",
      "[2025-11-20 00:24:23,413] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3122.0506 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:24:24,967] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3195.5310 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:24:39,962] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3190.7369\n",
      "[2025-11-20 00:25:10,782] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3184.8501\n",
      "[2025-11-20 00:25:38,108] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3105.7517 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:25:39,807] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3186.8389 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:26:49,439] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3093.2095 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:26:51,144] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3181.4989 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:26:51,323] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3181.4989\n",
      "[2025-11-20 00:27:07,285] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3180.8689\n",
      "[2025-11-20 00:27:22,728] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3180.4106\n",
      "[2025-11-20 00:28:07,366] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3084.7132 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:28:08,931] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3172.9277 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:28:09,179] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3172.9277\n",
      "[2025-11-20 00:29:24,568] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3079.2159 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:29:26,260] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3177.2653 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:29:41,967] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3172.6387\n",
      "[2025-11-20 00:29:57,918] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3170.4169\n",
      "[2025-11-20 00:30:29,508] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3165.1893\n",
      "[2025-11-20 00:30:43,820] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3067.9850 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:30:45,497] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3165.5640 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:31:00,489] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3164.7279\n",
      "[2025-11-20 00:31:16,453] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3164.3060\n",
      "[2025-11-20 00:31:47,860] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3164.0623\n",
      "[2025-11-20 00:32:01,921] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3056.5247 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:32:03,615] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3158.4580 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:32:03,783] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3158.4580\n",
      "[2025-11-20 00:32:49,642] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3156.2022\n",
      "[2025-11-20 00:33:18,348] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3047.4354 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:33:20,012] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3158.6256 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:33:35,092] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3153.2887\n",
      "[2025-11-20 00:34:05,045] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3152.3288\n",
      "[2025-11-20 00:34:27,634] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3037.6369 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:34:29,295] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3150.4835 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:34:29,330] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 3150.4835\n",
      "[2025-11-20 00:34:29,357] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 3150.4835)\n",
      "[2025-11-20 00:34:30,193] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 00:34:30,194] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 00:34:30,195] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 00:34:30,200] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 00:34:30,201] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 00:34:30,201] [UniVITrainer] [INFO]   device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 00:34:30,211] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 00:34:30,212] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 00:34:30,212] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 00:34:30,213] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 00:34:30,213] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 00:34:30,214] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 00:34:30,215] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 17] Done in 20.1 min\n",
      "  best_val_loss              = 3150.483\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0577\n",
      "  Label transfer (ADT→RNA)  = 0.595\n",
      "\n",
      "================================================================================\n",
      "[Config 18] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 40.0,\n",
      "  \"gamma\": 160.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53330ed023c407f83b9e0aec0c9c56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 00:34:44,141] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4179.1456 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:34:45,785] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3712.0231 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:34:46,026] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3712.0231\n",
      "[2025-11-20 00:35:01,721] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3606.9835\n",
      "[2025-11-20 00:35:16,737] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3586.1167\n",
      "[2025-11-20 00:35:31,520] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3541.9632\n",
      "[2025-11-20 00:35:45,252] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3520.9068 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:35:46,901] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3524.3697 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:35:47,103] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3524.3697\n",
      "[2025-11-20 00:36:02,585] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3504.8464\n",
      "[2025-11-20 00:36:18,351] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3498.1989\n",
      "[2025-11-20 00:36:33,951] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3478.4338\n",
      "[2025-11-20 00:36:47,621] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3454.6651\n",
      "[2025-11-20 00:37:01,354] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3429.5704 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:37:03,021] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3436.0865 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:37:03,210] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3436.0865\n",
      "[2025-11-20 00:37:18,914] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3402.5793\n",
      "[2025-11-20 00:37:34,439] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3307.7197\n",
      "[2025-11-20 00:37:50,057] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3280.7768\n",
      "[2025-11-20 00:38:04,508] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3266.6832\n",
      "[2025-11-20 00:38:17,612] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3217.2840 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:38:19,305] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3262.6390 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:38:19,519] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3262.6390\n",
      "[2025-11-20 00:38:33,887] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3244.4783\n",
      "[2025-11-20 00:38:48,533] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3240.3698\n",
      "[2025-11-20 00:39:02,393] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3239.4686\n",
      "[2025-11-20 00:39:18,051] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3219.7311\n",
      "[2025-11-20 00:39:29,798] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3166.6413 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:39:31,486] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3222.4783 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:39:45,378] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3213.1197\n",
      "[2025-11-20 00:40:00,749] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3201.9029\n",
      "[2025-11-20 00:40:15,816] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3198.2313\n",
      "[2025-11-20 00:40:30,538] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3195.7485\n",
      "[2025-11-20 00:40:44,100] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3126.5501 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:40:45,783] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3189.7686 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:40:45,960] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3189.7686\n",
      "[2025-11-20 00:41:01,847] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3176.0291\n",
      "[2025-11-20 00:41:48,490] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3160.4842\n",
      "[2025-11-20 00:42:02,323] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3087.6576 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:42:03,765] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3164.0576 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:42:19,820] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3151.8189\n",
      "[2025-11-20 00:42:35,604] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3150.8265\n",
      "[2025-11-20 00:42:51,605] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3147.6814\n",
      "[2025-11-20 00:43:07,369] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3147.0384\n",
      "[2025-11-20 00:43:21,028] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3065.3478 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:43:22,708] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3142.1856 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:43:22,899] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3142.1856\n",
      "[2025-11-20 00:43:54,645] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3133.5159\n",
      "[2025-11-20 00:44:10,594] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3128.1557\n",
      "[2025-11-20 00:44:38,230] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3040.0962 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:44:39,915] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3132.1770 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:44:53,805] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3121.4310\n",
      "[2025-11-20 00:45:09,056] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3121.0113\n",
      "[2025-11-20 00:45:37,900] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3119.6950\n",
      "[2025-11-20 00:45:51,583] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3020.9565 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:45:52,773] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3115.7488 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:45:52,931] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3115.7488\n",
      "[2025-11-20 00:46:36,172] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3115.4885\n",
      "[2025-11-20 00:46:50,925] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3114.7914\n",
      "[2025-11-20 00:47:04,817] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3003.6156 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:47:06,475] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3121.2090 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:47:36,850] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3107.5458\n",
      "[2025-11-20 00:48:20,021] [UniVITrainer] [INFO] [Epoch 055] Train loss: 2992.7575 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:48:21,681] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3099.6039 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:48:21,723] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3099.6039\n",
      "[2025-11-20 00:49:07,998] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3099.0661\n",
      "[2025-11-20 00:49:37,071] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2977.7974 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:49:38,740] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3097.8704 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:49:38,958] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3097.8704\n",
      "[2025-11-20 00:50:09,804] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3094.1466\n",
      "[2025-11-20 00:50:54,747] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2964.3089 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:50:56,404] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3096.5561 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:51:12,213] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3091.4272\n",
      "[2025-11-20 00:51:55,736] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3091.0284\n",
      "[2025-11-20 00:52:07,962] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2951.1223 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:52:09,603] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3093.7149 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:52:37,199] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3086.7902\n",
      "[2025-11-20 00:53:06,756] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3083.9533\n",
      "[2025-11-20 00:53:19,771] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2938.4497 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:53:21,475] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3085.5480 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:53:36,838] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3081.6364\n",
      "[2025-11-20 00:53:50,647] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3081.5023\n",
      "[2025-11-20 00:54:33,306] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2933.3463 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:54:34,603] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3079.1254 (beta=40.000, gamma=160.000)\n",
      "[2025-11-20 00:54:34,805] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 3079.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 00:54:34,837] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 3079.1254)\n",
      "[2025-11-20 00:54:35,693] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 00:54:35,694] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 00:54:35,695] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 00:54:35,700] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 00:54:35,704] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 00:54:35,705] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 00:54:35,711] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 00:54:35,712] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 00:54:35,713] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 00:54:35,713] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 00:54:35,714] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 00:54:35,715] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 00:54:35,715] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 18] Done in 20.1 min\n",
      "  best_val_loss              = 3079.125\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0688\n",
      "  Label transfer (ADT→RNA)  = 0.567\n",
      "\n",
      "================================================================================\n",
      "[Config 19] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 60.0,\n",
      "  \"gamma\": 60.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3a872006d248fdb84342bc17c573a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 00:54:48,816] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4021.0698 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:54:50,303] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3692.5388 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:54:50,340] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3692.5388\n",
      "[2025-11-20 00:55:05,142] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3606.2569\n",
      "[2025-11-20 00:55:19,585] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3596.4190\n",
      "[2025-11-20 00:55:34,410] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3550.7077\n",
      "[2025-11-20 00:55:48,266] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3542.8745 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:55:49,919] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3525.0538 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:55:50,041] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3525.0538\n",
      "[2025-11-20 00:56:05,426] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3429.4229\n",
      "[2025-11-20 00:56:20,623] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3397.0464\n",
      "[2025-11-20 00:56:35,570] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3375.8658\n",
      "[2025-11-20 00:57:03,902] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3358.0013 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:57:05,541] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3354.7853 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:57:05,664] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3354.7853\n",
      "[2025-11-20 00:57:21,210] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3354.3738\n",
      "[2025-11-20 00:57:35,717] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3322.8189\n",
      "[2025-11-20 00:57:50,732] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3304.4112\n",
      "[2025-11-20 00:58:05,962] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3299.8338\n",
      "[2025-11-20 00:58:18,925] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3260.9165 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:58:19,710] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3296.9132 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:58:19,733] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3296.9132\n",
      "[2025-11-20 00:58:34,435] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3284.9204\n",
      "[2025-11-20 00:58:47,688] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3280.4469\n",
      "[2025-11-20 00:59:01,305] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3274.0337\n",
      "[2025-11-20 00:59:16,297] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3273.0867\n",
      "[2025-11-20 00:59:28,818] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3235.0372 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:59:30,384] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3277.3753 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 00:59:59,368] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3271.1831\n",
      "[2025-11-20 01:00:15,100] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3259.4126\n",
      "[2025-11-20 01:00:29,327] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3259.0729\n",
      "[2025-11-20 01:00:43,095] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3211.0121 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:00:44,791] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3246.1102 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:00:44,914] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3246.1102\n",
      "[2025-11-20 01:01:27,706] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3234.2142\n",
      "[2025-11-20 01:01:43,314] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3222.6768\n",
      "[2025-11-20 01:01:56,599] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3170.9490 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:01:58,297] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3218.6924 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:01:58,347] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3218.6924\n",
      "[2025-11-20 01:02:13,720] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3217.9844\n",
      "[2025-11-20 01:02:29,258] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3208.2909\n",
      "[2025-11-20 01:02:45,256] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3207.3890\n",
      "[2025-11-20 01:03:01,170] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3205.2586\n",
      "[2025-11-20 01:03:14,995] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3141.4619 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:03:16,690] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3200.5232 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:03:16,833] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3200.5232\n",
      "[2025-11-20 01:03:32,379] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3198.2542\n",
      "[2025-11-20 01:04:03,733] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3196.8681\n",
      "[2025-11-20 01:04:19,404] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3194.2008\n",
      "[2025-11-20 01:04:33,497] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3127.9734 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:04:35,169] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3192.7284 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:04:35,281] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3192.7284\n",
      "[2025-11-20 01:04:50,453] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3192.6095\n",
      "[2025-11-20 01:05:06,110] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3183.6385\n",
      "[2025-11-20 01:05:35,527] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3183.4435\n",
      "[2025-11-20 01:05:48,617] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3115.1739 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:05:50,273] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3183.5604 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:06:03,763] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3177.4840\n",
      "[2025-11-20 01:06:30,807] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3168.8299\n",
      "[2025-11-20 01:06:59,486] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3087.5815 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:07:00,934] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3171.3093 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:07:29,761] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3166.3303\n",
      "[2025-11-20 01:07:43,065] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3163.3919\n",
      "[2025-11-20 01:08:11,597] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3075.4923 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:08:13,251] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3156.1890 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:08:13,285] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3156.1890\n",
      "[2025-11-20 01:08:40,046] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3156.1556\n",
      "[2025-11-20 01:09:10,155] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3156.1436\n",
      "[2025-11-20 01:09:22,998] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3068.5380 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:09:24,631] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3157.6084 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:09:55,306] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3150.1791\n",
      "[2025-11-20 01:10:39,221] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3056.5243 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:10:40,887] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3149.1035 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:10:40,906] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3149.1035\n",
      "[2025-11-20 01:11:24,046] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3148.6023\n",
      "[2025-11-20 01:11:52,517] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3048.9738 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:11:54,171] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3149.7818 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:12:09,476] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3147.2450\n",
      "[2025-11-20 01:12:23,999] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3146.1480\n",
      "[2025-11-20 01:12:38,575] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3143.7994\n",
      "[2025-11-20 01:12:52,546] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3142.7386\n",
      "[2025-11-20 01:13:05,539] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3040.7766 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:13:07,188] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3147.6783 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:14:05,589] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3140.5534\n",
      "[2025-11-20 01:14:19,475] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3034.8335 (beta=60.000, gamma=60.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 01:14:21,140] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3144.2168 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 01:14:21,169] [UniVITrainer] [INFO] Restored best model from epoch 79 (val loss = 3140.5534)\n",
      "[2025-11-20 01:14:21,926] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 01:14:21,927] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 01:14:21,928] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 01:14:21,932] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 01:14:21,936] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 01:14:21,937] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 01:14:21,944] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 01:14:21,944] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 01:14:21,945] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 01:14:21,945] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 01:14:21,946] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 01:14:21,947] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 01:14:21,947] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 19] Done in 19.8 min\n",
      "  best_val_loss              = 3140.553\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0753\n",
      "  Label transfer (ADT→RNA)  = 0.555\n",
      "\n",
      "================================================================================\n",
      "[Config 20] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 100.0,\n",
      "  \"gamma\": 100.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d596e975c3fc4218b24f45e996782a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 01:14:34,561] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4332.9389 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:14:36,222] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3834.7931 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:14:36,407] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3834.7931\n",
      "[2025-11-20 01:14:52,052] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3721.5037\n",
      "[2025-11-20 01:15:07,863] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3698.0123\n",
      "[2025-11-20 01:15:36,692] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3686.2654 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:15:38,336] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3709.9182 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:15:51,858] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3691.3636\n",
      "[2025-11-20 01:16:04,716] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3680.1025\n",
      "[2025-11-20 01:16:33,661] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3676.7665\n",
      "[2025-11-20 01:16:44,858] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3620.0650 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:16:46,525] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3628.2872 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:16:46,706] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3628.2872\n",
      "[2025-11-20 01:17:01,582] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3619.0447\n",
      "[2025-11-20 01:17:44,251] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3611.5091\n",
      "[2025-11-20 01:17:57,443] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3584.4000 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:17:57,974] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3601.0190 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:17:58,155] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3601.0190\n",
      "[2025-11-20 01:18:11,089] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3596.9963\n",
      "[2025-11-20 01:18:26,417] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3580.8432\n",
      "[2025-11-20 01:18:55,955] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3576.7130\n",
      "[2025-11-20 01:19:08,791] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3551.1827 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:19:10,402] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3578.2626 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:19:25,800] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3576.0895\n",
      "[2025-11-20 01:19:39,371] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3571.6041\n",
      "[2025-11-20 01:19:53,749] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3566.7835\n",
      "[2025-11-20 01:20:08,879] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3565.3570\n",
      "[2025-11-20 01:20:21,165] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3517.2041 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:20:22,809] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3522.7266 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:20:23,022] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3522.7266\n",
      "[2025-11-20 01:20:35,614] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3468.0987\n",
      "[2025-11-20 01:20:51,157] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3457.0940\n",
      "[2025-11-20 01:21:06,427] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3438.7597\n",
      "[2025-11-20 01:21:21,869] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3434.7685\n",
      "[2025-11-20 01:21:35,201] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3388.5203 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:21:36,856] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3427.5897 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:21:37,009] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3427.5897\n",
      "[2025-11-20 01:21:52,296] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3416.8702\n",
      "[2025-11-20 01:22:07,632] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3412.3501\n",
      "[2025-11-20 01:22:36,659] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3406.4560\n",
      "[2025-11-20 01:22:49,804] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3363.6627 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:22:51,453] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3410.7235 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:23:05,400] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3400.4756\n",
      "[2025-11-20 01:23:18,830] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3395.3386\n",
      "[2025-11-20 01:24:01,449] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3344.8747 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:24:03,092] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3396.5793 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:24:31,599] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3389.4419\n",
      "[2025-11-20 01:24:47,188] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3382.6968\n",
      "[2025-11-20 01:25:01,221] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3375.4322\n",
      "[2025-11-20 01:25:12,666] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3314.7075 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:25:14,375] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3366.3794 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:25:14,569] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3366.3794\n",
      "[2025-11-20 01:25:27,873] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3354.9926\n",
      "[2025-11-20 01:25:44,069] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3354.4301\n",
      "[2025-11-20 01:25:57,330] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3349.6677\n",
      "[2025-11-20 01:26:13,166] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3345.8518\n",
      "[2025-11-20 01:26:26,211] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3280.1150 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:26:27,914] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3339.2171 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:26:28,102] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3339.2171\n",
      "[2025-11-20 01:26:42,194] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3333.0573\n",
      "[2025-11-20 01:26:58,170] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3330.0777\n",
      "[2025-11-20 01:27:27,367] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3325.4668\n",
      "[2025-11-20 01:27:41,153] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3268.0018 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:27:42,472] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3329.3861 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:27:58,073] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3325.1679\n",
      "[2025-11-20 01:28:13,368] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3321.1548\n",
      "[2025-11-20 01:28:27,950] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3320.3797\n",
      "[2025-11-20 01:28:42,961] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3308.7099\n",
      "[2025-11-20 01:28:57,057] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3250.2096 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:28:58,634] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3314.6038 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:29:45,184] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3304.9866\n",
      "[2025-11-20 01:30:15,096] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3240.6199 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:30:16,335] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3304.7427 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:30:16,509] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3304.7427\n",
      "[2025-11-20 01:30:43,848] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3299.1456\n",
      "[2025-11-20 01:31:13,502] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3296.6341\n",
      "[2025-11-20 01:31:25,345] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3228.2953 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:31:26,651] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3296.8696 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:31:57,442] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3296.0199\n",
      "[2025-11-20 01:32:39,436] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3223.3388 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:32:41,084] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3292.5049 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:32:41,258] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 3292.5049\n",
      "[2025-11-20 01:33:08,933] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3291.6763\n",
      "[2025-11-20 01:33:39,804] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3288.9967\n",
      "[2025-11-20 01:33:52,296] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3218.9683 (beta=100.000, gamma=100.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 01:33:53,943] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3298.5557 (beta=100.000, gamma=100.000)\n",
      "[2025-11-20 01:33:53,963] [UniVITrainer] [INFO] Restored best model from epoch 79 (val loss = 3288.9967)\n",
      "[2025-11-20 01:33:54,795] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 01:33:54,796] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 01:33:54,796] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 01:33:54,802] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 01:33:54,803] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 01:33:54,804] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 01:33:54,814] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 01:33:54,815] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 01:33:54,815] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 01:33:54,816] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 01:33:54,817] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 01:33:54,817] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 01:33:54,818] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 20] Done in 19.5 min\n",
      "  best_val_loss              = 3288.997\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0708\n",
      "  Label transfer (ADT→RNA)  = 0.505\n",
      "\n",
      "================================================================================\n",
      "[Config 21] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 120,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 100.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7e8703f77f4434bcde3671d3e43ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 01:34:08,724] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4917.4677 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:34:10,382] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3866.9527 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:34:10,570] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3866.9527\n",
      "[2025-11-20 01:34:25,660] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3743.3838\n",
      "[2025-11-20 01:34:41,353] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3738.8969\n",
      "[2025-11-20 01:34:56,914] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3716.1978\n",
      "[2025-11-20 01:35:10,399] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3707.7568 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:35:12,051] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3726.2610 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:35:27,407] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3715.9931\n",
      "[2025-11-20 01:35:42,062] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3711.7471\n",
      "[2025-11-20 01:36:12,922] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3710.1577\n",
      "[2025-11-20 01:36:26,437] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3702.3100 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:36:28,073] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3717.0484 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:36:42,546] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3708.9251\n",
      "[2025-11-20 01:36:57,745] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3706.1874\n",
      "[2025-11-20 01:37:39,964] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3687.1676 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:37:41,393] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3691.9607 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:37:41,575] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3691.9607\n",
      "[2025-11-20 01:38:54,850] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3676.6246 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:38:56,516] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3704.7898 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:39:11,806] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3690.4928\n",
      "[2025-11-20 01:40:10,063] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3662.3283 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:40:11,734] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3681.1379 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:40:11,928] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3681.1379\n",
      "[2025-11-20 01:40:24,239] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3648.1889\n",
      "[2025-11-20 01:40:39,757] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3641.0953\n",
      "[2025-11-20 01:40:55,508] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3637.9983\n",
      "[2025-11-20 01:41:11,225] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3629.3339\n",
      "[2025-11-20 01:41:24,566] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3605.3483 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:41:26,271] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3644.7617 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:41:41,842] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3627.5150\n",
      "[2025-11-20 01:42:38,713] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3596.4076 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:42:40,388] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3637.3657 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:42:55,678] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3624.5259\n",
      "[2025-11-20 01:43:26,574] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3624.4483\n",
      "[2025-11-20 01:43:54,960] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3592.0249 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:43:56,126] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3635.8733 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:44:11,677] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3614.5581\n",
      "[2025-11-20 01:45:10,411] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3578.4679 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:45:11,533] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3614.6816 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:45:26,297] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3602.4936\n",
      "[2025-11-20 01:45:41,999] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3567.5122\n",
      "[2025-11-20 01:45:57,383] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3524.6167\n",
      "[2025-11-20 01:46:11,404] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3510.1127\n",
      "[2025-11-20 01:46:25,555] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3471.3941 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:46:27,245] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3517.1452 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:46:41,508] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3503.6432\n",
      "[2025-11-20 01:47:28,234] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3495.0426\n",
      "[2025-11-20 01:47:40,568] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3456.3582 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:47:42,278] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3505.2380 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:48:56,694] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3451.5816 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:48:58,376] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3503.0579 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:49:44,833] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3489.6608\n",
      "[2025-11-20 01:50:14,055] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3445.0623 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:50:15,662] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3489.4211 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:50:15,864] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3489.4211\n",
      "[2025-11-20 01:50:46,292] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3489.2112\n",
      "[2025-11-20 01:51:29,562] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3442.8590 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:51:30,790] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3496.4502 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:52:15,123] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3488.9298\n",
      "[2025-11-20 01:52:30,346] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3484.2232\n",
      "[2025-11-20 01:52:43,778] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3435.8227 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:52:45,428] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3497.0592 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:53:29,641] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3477.5789\n",
      "[2025-11-20 01:53:58,496] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3430.1582 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:54:00,164] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3479.3933 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 01:54:00,197] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3477.5789)\n",
      "[2025-11-20 01:54:01,037] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 01:54:01,038] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 01:54:01,040] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 01:54:01,042] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 01:54:01,043] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 01:54:01,044] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 01:54:01,046] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 01:54:01,047] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 01:54:01,049] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 01:54:01,051] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 01:54:01,053] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 01:54:01,053] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 01:54:01,055] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 21] Done in 20.1 min\n",
      "  best_val_loss              = 3477.579\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1295\n",
      "  Label transfer (ADT→RNA)  = 0.347\n",
      "\n",
      "================================================================================\n",
      "[Config 22] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 120,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 160.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f501da72b434003a71033f700a2eb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 01:54:14,967] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4920.2266 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:54:16,607] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3858.8021 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:54:16,776] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3858.8021\n",
      "[2025-11-20 01:54:32,399] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3759.0693\n",
      "[2025-11-20 01:54:47,894] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3715.4210\n",
      "[2025-11-20 01:55:03,523] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3686.1860\n",
      "[2025-11-20 01:55:17,084] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3679.5055 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:55:18,744] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3663.2979 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:55:18,957] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3663.2979\n",
      "[2025-11-20 01:55:34,537] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3641.4870\n",
      "[2025-11-20 01:55:49,208] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3623.2729\n",
      "[2025-11-20 01:56:04,627] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3535.4389\n",
      "[2025-11-20 01:56:19,901] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3506.1666\n",
      "[2025-11-20 01:56:33,789] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3483.6562 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:56:34,786] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3491.5577 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:56:34,820] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3491.5577\n",
      "[2025-11-20 01:56:49,902] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3479.3807\n",
      "[2025-11-20 01:57:05,478] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3470.6820\n",
      "[2025-11-20 01:57:48,047] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3441.5723 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:57:49,704] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3462.3934 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:57:49,921] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3462.3934\n",
      "[2025-11-20 01:58:03,561] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3445.0438\n",
      "[2025-11-20 01:58:45,765] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3444.0330\n",
      "[2025-11-20 01:58:58,411] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3418.4805 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:59:00,122] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3438.9573 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 01:59:00,319] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3438.9573\n",
      "[2025-11-20 01:59:15,537] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3432.0650\n",
      "[2025-11-20 01:59:45,231] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3415.0796\n",
      "[2025-11-20 02:00:01,312] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3406.2429\n",
      "[2025-11-20 02:00:15,073] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3372.3435 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:00:16,616] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3402.7110 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:00:16,815] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3402.7110\n",
      "[2025-11-20 02:00:32,442] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3390.9042\n",
      "[2025-11-20 02:00:47,105] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3386.5274\n",
      "[2025-11-20 02:01:16,911] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3380.4197\n",
      "[2025-11-20 02:01:31,002] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3342.8700 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:01:32,713] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3384.8652 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:01:48,192] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3374.0012\n",
      "[2025-11-20 02:02:48,467] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3321.4899 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:02:50,181] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3373.4569 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:02:50,383] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3373.4569\n",
      "[2025-11-20 02:03:21,357] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3372.8497\n",
      "[2025-11-20 02:03:37,080] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3371.6185\n",
      "[2025-11-20 02:04:05,260] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3317.1976 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:04:06,960] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3368.6874 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:04:07,054] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3368.6874\n",
      "[2025-11-20 02:04:48,425] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3365.7504\n",
      "[2025-11-20 02:05:13,777] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3304.4084 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:05:14,545] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3366.4231 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:05:29,266] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3361.2843\n",
      "[2025-11-20 02:05:56,872] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3360.7179\n",
      "[2025-11-20 02:06:24,296] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3295.8111 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:06:25,996] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3361.1266 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:06:55,178] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3359.0798\n",
      "[2025-11-20 02:07:25,534] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3356.6520\n",
      "[2025-11-20 02:07:39,093] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3289.2194 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:07:40,767] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3360.6158 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:07:53,252] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3356.5564\n",
      "[2025-11-20 02:08:52,361] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3286.2107 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:08:54,031] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3356.9354 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:09:22,410] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3351.4614\n",
      "[2025-11-20 02:10:06,364] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3277.4671 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:10:08,061] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3354.3403 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:10:53,519] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3344.3044\n",
      "[2025-11-20 02:11:20,418] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3271.9159 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:11:22,086] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3343.4412 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:11:22,295] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3343.4412\n",
      "[2025-11-20 02:11:36,399] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3340.6214\n",
      "[2025-11-20 02:12:33,100] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3264.3116 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:12:34,776] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3333.1710 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:12:34,965] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 3333.1710\n",
      "[2025-11-20 02:13:17,530] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3331.2059\n",
      "[2025-11-20 02:13:45,997] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3254.3096 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:13:47,660] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3338.7781 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 02:13:47,694] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3331.2059)\n",
      "[2025-11-20 02:13:48,558] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 02:13:48,560] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 02:13:48,561] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 02:13:48,565] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 02:13:48,570] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 02:13:48,571] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 02:13:48,577] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 02:13:48,577] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 02:13:48,578] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 02:13:48,579] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 02:13:48,580] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 02:13:48,580] [UniVITrainer] [INFO]   patience: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 02:13:48,581] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 22] Done in 19.8 min\n",
      "  best_val_loss              = 3331.206\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1051\n",
      "  Label transfer (ADT→RNA)  = 0.459\n",
      "\n",
      "================================================================================\n",
      "[Config 23] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 20.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6588f929ded946fe89849373facaadcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 02:14:02,261] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3915.0276 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:14:03,917] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3703.5634 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:14:04,060] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3703.5634\n",
      "[2025-11-20 02:14:18,493] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3663.4425\n",
      "[2025-11-20 02:14:33,987] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3535.4625\n",
      "[2025-11-20 02:14:47,973] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3506.6064\n",
      "[2025-11-20 02:15:00,359] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3471.5296 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:15:02,033] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3487.1939 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:15:02,173] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3487.1939\n",
      "[2025-11-20 02:15:33,757] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3442.9561\n",
      "[2025-11-20 02:15:48,571] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3410.0811\n",
      "[2025-11-20 02:16:03,484] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3388.8028\n",
      "[2025-11-20 02:16:17,465] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3350.3992 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:16:19,136] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3390.0981 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:16:34,683] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3388.3292\n",
      "[2025-11-20 02:16:50,201] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3376.1922\n",
      "[2025-11-20 02:17:20,477] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3361.1870\n",
      "[2025-11-20 02:17:33,122] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3320.0558 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:17:34,794] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3359.4037 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:17:34,862] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3359.4037\n",
      "[2025-11-20 02:18:19,040] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3350.0351\n",
      "[2025-11-20 02:18:47,100] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3294.7456 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:18:48,815] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3338.3996 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:18:48,966] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3338.3996\n",
      "[2025-11-20 02:19:36,193] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3319.1278\n",
      "[2025-11-20 02:20:03,772] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3273.7428 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:20:04,595] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3325.2046 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:20:33,774] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3318.0396\n",
      "[2025-11-20 02:20:49,562] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3310.9773\n",
      "[2025-11-20 02:21:05,738] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3303.3138\n",
      "[2025-11-20 02:21:19,693] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3242.4058 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:21:21,614] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3307.6075 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:21:53,409] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3294.0531\n",
      "[2025-11-20 02:22:39,548] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3233.3596 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:22:41,269] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3304.6943 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:23:26,545] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3291.1179\n",
      "[2025-11-20 02:23:55,284] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3219.4186 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:23:56,869] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3296.8601 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:24:58,975] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3287.8597\n",
      "[2025-11-20 02:25:12,425] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3207.0831 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:25:14,108] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3289.2636 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:26:25,993] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3200.6116 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:26:27,697] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3286.9028 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:26:27,849] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3286.9028\n",
      "[2025-11-20 02:26:40,043] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3284.9162\n",
      "[2025-11-20 02:27:22,953] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3269.4061\n",
      "[2025-11-20 02:27:35,271] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3192.0767 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:27:36,949] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3289.2616 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:28:50,448] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3187.9021 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:28:52,151] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3275.6752 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:29:05,917] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3269.0157\n",
      "[2025-11-20 02:30:05,284] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3175.3424 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:30:06,989] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3266.1308 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:30:07,169] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3266.1308\n",
      "[2025-11-20 02:30:53,330] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3263.5086\n",
      "[2025-11-20 02:31:22,459] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3166.6566 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:31:24,156] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3268.0034 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:32:22,775] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3263.1716\n",
      "[2025-11-20 02:32:36,854] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3160.1618 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:32:38,761] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3263.9686 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:33:07,076] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3255.0249\n",
      "[2025-11-20 02:33:21,917] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3249.0751\n",
      "[2025-11-20 02:33:51,876] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3157.5457 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:33:53,550] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3263.5272 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 02:33:53,590] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3249.0751)\n",
      "[2025-11-20 02:33:54,358] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 02:33:54,359] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 02:33:54,360] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 02:33:54,362] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 02:33:54,363] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 02:33:54,365] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 02:33:54,367] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 02:33:54,368] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 02:33:54,370] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 02:33:54,371] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 02:33:54,372] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 02:33:54,372] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 02:33:54,373] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 23] Done in 20.1 min\n",
      "  best_val_loss              = 3249.075\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0785\n",
      "  Label transfer (ADT→RNA)  = 0.506\n",
      "\n",
      "================================================================================\n",
      "[Config 24] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 120,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 40.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbda99e66f7436386a2c44359f110b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 02:34:07,956] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4300.7182 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:34:09,637] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3828.9088 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:34:09,805] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3828.9088\n",
      "[2025-11-20 02:34:24,537] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3810.0304\n",
      "[2025-11-20 02:34:55,812] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3803.5395\n",
      "[2025-11-20 02:35:08,902] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3765.0738 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:35:10,558] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3711.1353 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:35:10,698] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3711.1353\n",
      "[2025-11-20 02:35:26,426] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3700.1300\n",
      "[2025-11-20 02:36:22,686] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3694.4610 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:36:24,379] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3704.8473 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:37:25,832] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3699.0709\n",
      "[2025-11-20 02:37:39,521] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3695.0922 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:37:40,993] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3699.2602 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:37:56,862] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3697.8899\n",
      "[2025-11-20 02:38:28,001] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3696.0675\n",
      "[2025-11-20 02:38:56,665] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3682.4203 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:38:58,329] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3702.6651 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:39:26,651] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3694.7111\n",
      "[2025-11-20 02:39:56,740] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3684.6725\n",
      "[2025-11-20 02:40:09,798] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3682.5239 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:40:11,365] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3694.8156 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:40:25,528] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3683.7162\n",
      "[2025-11-20 02:40:56,386] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3678.6504\n",
      "[2025-11-20 02:41:11,763] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3667.4801\n",
      "[2025-11-20 02:41:25,421] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3633.5049 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:41:27,069] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3625.0666 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:41:27,240] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3625.0666\n",
      "[2025-11-20 02:41:41,939] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3614.4231\n",
      "[2025-11-20 02:41:57,475] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3613.4833\n",
      "[2025-11-20 02:42:12,145] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3611.3578\n",
      "[2025-11-20 02:42:27,444] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3598.9239\n",
      "[2025-11-20 02:42:41,092] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3581.4507 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:42:42,078] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3597.1105 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:42:42,271] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3597.1105\n",
      "[2025-11-20 02:42:57,216] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3596.4423\n",
      "[2025-11-20 02:43:27,359] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3581.8477\n",
      "[2025-11-20 02:43:55,765] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3565.1926 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:43:57,418] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3590.8372 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:44:12,797] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3575.8395\n",
      "[2025-11-20 02:44:28,362] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3571.3906\n",
      "[2025-11-20 02:45:12,726] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3547.1046 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:45:14,390] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3571.5813 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:45:29,994] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3570.0805\n",
      "[2025-11-20 02:45:43,524] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3567.8786\n",
      "[2025-11-20 02:45:56,838] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3565.9235\n",
      "[2025-11-20 02:46:11,241] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3555.8155\n",
      "[2025-11-20 02:46:24,472] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3531.3684 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:46:26,120] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3554.8067 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:46:26,295] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3554.8067\n",
      "[2025-11-20 02:46:41,325] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3538.1018\n",
      "[2025-11-20 02:46:55,864] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3513.5778\n",
      "[2025-11-20 02:47:23,827] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3499.7281\n",
      "[2025-11-20 02:47:37,275] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3479.1873 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:47:38,903] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3501.9391 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:47:53,355] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3498.2170\n",
      "[2025-11-20 02:48:51,251] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3465.0995 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:48:52,890] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3496.1267 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:48:53,088] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3496.1267\n",
      "[2025-11-20 02:49:36,786] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3489.3092\n",
      "[2025-11-20 02:50:04,752] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3464.8678 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:50:05,795] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3491.0838 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:51:17,847] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3457.4634 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:51:19,490] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3484.1280 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:51:19,677] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3484.1280\n",
      "[2025-11-20 02:51:34,881] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3483.8928\n",
      "[2025-11-20 02:52:32,230] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3447.6190 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:52:33,748] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3487.2616 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:53:03,286] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3480.4812\n",
      "[2025-11-20 02:53:45,106] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3442.5617 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:53:46,751] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3482.4220 (beta=160.000, gamma=40.000)\n",
      "[2025-11-20 02:53:46,776] [UniVITrainer] [INFO] Restored best model from epoch 77 (val loss = 3480.4812)\n",
      "[2025-11-20 02:53:47,593] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 02:53:47,594] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 02:53:47,595] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 02:53:47,600] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 02:53:47,604] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 02:53:47,605] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 02:53:47,606] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 02:53:47,615] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 02:53:47,616] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 02:53:47,618] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 02:53:47,619] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 02:53:47,620] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 02:53:47,620] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 24] Done in 19.9 min\n",
      "  best_val_loss              = 3480.481\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1619\n",
      "  Label transfer (ADT→RNA)  = 0.303\n",
      "\n",
      "================================================================================\n",
      "[Config 25] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 0.0,\n",
      "  \"gamma\": 120.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f9711e95f44873a01f0be02ad401a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 02:53:59,451] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4198.9359 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:54:01,100] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3402.8033 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:54:01,213] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3402.8033\n",
      "[2025-11-20 02:54:16,479] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3147.3715\n",
      "[2025-11-20 02:54:32,072] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3064.0745\n",
      "[2025-11-20 02:54:47,147] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 2987.7842\n",
      "[2025-11-20 02:55:00,865] [UniVITrainer] [INFO] [Epoch 005] Train loss: 2957.5150 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:55:02,488] [UniVITrainer] [INFO] [Epoch 005] Val loss: 2947.0312 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:55:02,636] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 2947.0312\n",
      "[2025-11-20 02:55:17,753] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 2922.5600\n",
      "[2025-11-20 02:55:33,435] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 2898.1148\n",
      "[2025-11-20 02:55:48,644] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 2882.2129\n",
      "[2025-11-20 02:56:04,111] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 2866.8304\n",
      "[2025-11-20 02:56:17,888] [UniVITrainer] [INFO] [Epoch 010] Train loss: 2806.1174 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:56:19,538] [UniVITrainer] [INFO] [Epoch 010] Val loss: 2850.6122 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:56:19,599] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 2850.6122\n",
      "[2025-11-20 02:56:34,728] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 2833.5613\n",
      "[2025-11-20 02:57:05,461] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 2812.0850\n",
      "[2025-11-20 02:57:20,675] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 2800.8857\n",
      "[2025-11-20 02:57:34,453] [UniVITrainer] [INFO] [Epoch 015] Train loss: 2717.6759 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:57:35,932] [UniVITrainer] [INFO] [Epoch 015] Val loss: 2790.1799 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:57:36,020] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 2790.1799\n",
      "[2025-11-20 02:57:50,092] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 2780.2257\n",
      "[2025-11-20 02:58:04,865] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 2766.5040\n",
      "[2025-11-20 02:58:20,323] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 2758.4587\n",
      "[2025-11-20 02:58:35,513] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 2741.2529\n",
      "[2025-11-20 02:58:48,022] [UniVITrainer] [INFO] [Epoch 020] Train loss: 2639.2522 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:58:48,260] [UniVITrainer] [INFO] [Epoch 020] Val loss: 2732.6937 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 02:58:48,405] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 2732.6937\n",
      "[2025-11-20 02:59:03,007] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 2720.2865\n",
      "[2025-11-20 02:59:30,607] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 2700.1305\n",
      "[2025-11-20 02:59:58,819] [UniVITrainer] [INFO] [Epoch 025] Train loss: 2567.6988 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:00:00,530] [UniVITrainer] [INFO] [Epoch 025] Val loss: 2689.7734 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:00:00,684] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 2689.7734\n",
      "[2025-11-20 03:00:15,949] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 2687.5963\n",
      "[2025-11-20 03:00:31,771] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 2677.5080\n",
      "[2025-11-20 03:00:46,682] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 2673.6184\n",
      "[2025-11-20 03:01:01,296] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 2660.1353\n",
      "[2025-11-20 03:01:14,609] [UniVITrainer] [INFO] [Epoch 030] Train loss: 2508.0033 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:01:16,301] [UniVITrainer] [INFO] [Epoch 030] Val loss: 2661.5670 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:01:30,361] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 2650.7976\n",
      "[2025-11-20 03:01:43,076] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 2644.6826\n",
      "[2025-11-20 03:02:11,966] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 2638.0851\n",
      "[2025-11-20 03:02:25,526] [UniVITrainer] [INFO] [Epoch 035] Train loss: 2455.3473 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:02:26,798] [UniVITrainer] [INFO] [Epoch 035] Val loss: 2628.9991 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:02:26,832] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 2628.9991\n",
      "[2025-11-20 03:02:42,153] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 2624.3104\n",
      "[2025-11-20 03:02:56,857] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 2623.0002\n",
      "[2025-11-20 03:03:12,823] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 2614.9476\n",
      "[2025-11-20 03:03:28,589] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 2607.1589\n",
      "[2025-11-20 03:03:42,086] [UniVITrainer] [INFO] [Epoch 040] Train loss: 2405.5187 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:03:43,351] [UniVITrainer] [INFO] [Epoch 040] Val loss: 2599.0008 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:03:43,514] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 2599.0008\n",
      "[2025-11-20 03:03:59,545] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 2598.9697\n",
      "[2025-11-20 03:04:13,166] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 2589.3691\n",
      "[2025-11-20 03:04:28,072] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 2585.4556\n",
      "[2025-11-20 03:04:43,840] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 2579.3133\n",
      "[2025-11-20 03:04:57,525] [UniVITrainer] [INFO] [Epoch 045] Train loss: 2365.5606 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:04:59,040] [UniVITrainer] [INFO] [Epoch 045] Val loss: 2581.7657 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:05:14,592] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 2568.3799\n",
      "[2025-11-20 03:05:45,938] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 2566.3764\n",
      "[2025-11-20 03:06:00,177] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 2559.7579\n",
      "[2025-11-20 03:06:12,584] [UniVITrainer] [INFO] [Epoch 050] Train loss: 2321.7795 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:06:13,714] [UniVITrainer] [INFO] [Epoch 050] Val loss: 2548.7310 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:06:13,863] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 2548.7310\n",
      "[2025-11-20 03:06:29,241] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 2544.6075\n",
      "[2025-11-20 03:06:41,660] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 2539.6006\n",
      "[2025-11-20 03:06:56,626] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 2536.5425\n",
      "[2025-11-20 03:07:11,783] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 2531.8137\n",
      "[2025-11-20 03:07:23,636] [UniVITrainer] [INFO] [Epoch 055] Train loss: 2292.7218 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:07:25,343] [UniVITrainer] [INFO] [Epoch 055] Val loss: 2533.5291 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:07:55,153] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 2520.7390\n",
      "[2025-11-20 03:08:09,315] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 2518.3809\n",
      "[2025-11-20 03:08:24,384] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 2511.8310\n",
      "[2025-11-20 03:08:38,478] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2259.0569 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:08:40,100] [UniVITrainer] [INFO] [Epoch 060] Val loss: 2516.3471 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:08:55,674] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 2504.9780\n",
      "[2025-11-20 03:09:11,337] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 2502.8554\n",
      "[2025-11-20 03:09:26,883] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 2497.9572\n",
      "[2025-11-20 03:09:56,707] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2229.5469 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:09:57,608] [UniVITrainer] [INFO] [Epoch 065] Val loss: 2490.6587 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:09:57,765] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 2490.6587\n",
      "[2025-11-20 03:10:13,787] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 2489.2688\n",
      "[2025-11-20 03:10:29,787] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 2488.3065\n",
      "[2025-11-20 03:10:43,408] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 2484.0150\n",
      "[2025-11-20 03:10:59,225] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 2480.5198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 03:11:13,428] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2201.2825 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:11:15,133] [UniVITrainer] [INFO] [Epoch 070] Val loss: 2474.0406 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:11:15,173] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 2474.0406\n",
      "[2025-11-20 03:11:30,762] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 2469.7023\n",
      "[2025-11-20 03:12:01,295] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 2469.2364\n",
      "[2025-11-20 03:12:15,114] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 2466.9626\n",
      "[2025-11-20 03:12:28,567] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2175.7913 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:12:30,084] [UniVITrainer] [INFO] [Epoch 075] Val loss: 2469.0780 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:12:44,804] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 2460.4045\n",
      "[2025-11-20 03:13:14,791] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 2453.0206\n",
      "[2025-11-20 03:13:42,308] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2158.5950 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:13:43,948] [UniVITrainer] [INFO] [Epoch 080] Val loss: 2448.9595 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 03:13:43,988] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 2448.9595\n",
      "[2025-11-20 03:13:43,994] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 2448.9595)\n",
      "[2025-11-20 03:13:44,838] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 03:13:44,839] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 03:13:44,840] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 03:13:44,845] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 03:13:44,848] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 03:13:44,849] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 03:13:44,851] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 03:13:44,852] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 03:13:44,854] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 03:13:44,855] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 03:13:44,857] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 03:13:44,858] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 03:13:44,859] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 25] Done in 19.9 min\n",
      "  best_val_loss              = 2448.959\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0147\n",
      "  Label transfer (ADT→RNA)  = 0.856\n",
      "--> New best config (id=25) with score=1338.827\n",
      "\n",
      "================================================================================\n",
      "[Config 26] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 40.0,\n",
      "  \"gamma\": 0.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2939674c2d143d0adf2fb216b0ea9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 03:13:57,328] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3630.3044 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:13:58,964] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3447.4482 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:13:59,194] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3447.4482\n",
      "[2025-11-20 03:14:14,851] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3369.4484\n",
      "[2025-11-20 03:14:28,787] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3324.8688\n",
      "[2025-11-20 03:14:42,042] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3287.9226\n",
      "[2025-11-20 03:14:55,296] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3232.0663 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:14:56,928] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3252.2576 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:14:57,147] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3252.2576\n",
      "[2025-11-20 03:15:11,895] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3211.6496\n",
      "[2025-11-20 03:15:27,264] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3206.9683\n",
      "[2025-11-20 03:15:41,700] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3180.0420\n",
      "[2025-11-20 03:15:57,039] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3159.3712\n",
      "[2025-11-20 03:16:10,840] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3113.8691 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:16:12,478] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3158.6340 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:16:12,695] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3158.6340\n",
      "[2025-11-20 03:16:42,541] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3138.2489\n",
      "[2025-11-20 03:16:56,790] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3124.0534\n",
      "[2025-11-20 03:17:12,605] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3118.9881\n",
      "[2025-11-20 03:17:26,352] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3051.3686 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:17:27,988] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3110.9198 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:17:28,198] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3110.9198\n",
      "[2025-11-20 03:17:43,703] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3102.2842\n",
      "[2025-11-20 03:18:14,330] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3098.5120\n",
      "[2025-11-20 03:18:29,876] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3097.9069\n",
      "[2025-11-20 03:18:42,026] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3013.8416 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:18:43,667] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3088.6214 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:18:43,895] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3088.6214\n",
      "[2025-11-20 03:18:59,076] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3079.9849\n",
      "[2025-11-20 03:19:14,435] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3074.3705\n",
      "[2025-11-20 03:19:43,253] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3070.9375\n",
      "[2025-11-20 03:19:56,346] [UniVITrainer] [INFO] [Epoch 025] Train loss: 2985.5190 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:19:57,989] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3065.6493 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:19:58,206] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3065.6493\n",
      "[2025-11-20 03:20:12,113] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3064.7134\n",
      "[2025-11-20 03:20:27,045] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3060.8354\n",
      "[2025-11-20 03:20:57,789] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3047.6455\n",
      "[2025-11-20 03:21:11,198] [UniVITrainer] [INFO] [Epoch 030] Train loss: 2953.7914 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:21:11,821] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3055.6861 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:21:27,184] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3040.2090\n",
      "[2025-11-20 03:22:22,686] [UniVITrainer] [INFO] [Epoch 035] Train loss: 2927.7722 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:22:24,338] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3039.6616 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:22:24,540] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3039.6616\n",
      "[2025-11-20 03:23:09,973] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3038.6644\n",
      "[2025-11-20 03:23:25,468] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3028.8059\n",
      "[2025-11-20 03:23:38,674] [UniVITrainer] [INFO] [Epoch 040] Train loss: 2902.5762 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:23:40,332] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3029.0324 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:24:25,916] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3027.0049\n",
      "[2025-11-20 03:24:40,736] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3022.4352\n",
      "[2025-11-20 03:24:54,150] [UniVITrainer] [INFO] [Epoch 045] Train loss: 2878.5037 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:24:55,848] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3021.5189 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:24:56,112] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3021.5189\n",
      "[2025-11-20 03:25:11,172] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3015.1973\n",
      "[2025-11-20 03:26:07,404] [UniVITrainer] [INFO] [Epoch 050] Train loss: 2860.0341 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:26:09,093] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3008.7441 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:26:09,270] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3008.7441\n",
      "[2025-11-20 03:26:24,743] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3008.0681\n",
      "[2025-11-20 03:26:55,238] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3003.5074\n",
      "[2025-11-20 03:27:21,294] [UniVITrainer] [INFO] [Epoch 055] Train loss: 2830.7122 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:27:22,980] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3017.8772 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:28:22,838] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3000.6666\n",
      "[2025-11-20 03:28:35,670] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2811.3282 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:28:37,343] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3010.4653 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:29:23,304] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 2996.8058\n",
      "[2025-11-20 03:29:38,544] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 2993.2189\n",
      "[2025-11-20 03:29:51,300] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2792.4884 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:29:52,866] [UniVITrainer] [INFO] [Epoch 065] Val loss: 2993.1092 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:29:52,894] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 2993.1092\n",
      "[2025-11-20 03:30:24,201] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 2991.3625\n",
      "[2025-11-20 03:31:07,570] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2774.9918 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:31:09,276] [UniVITrainer] [INFO] [Epoch 070] Val loss: 2990.3778 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:31:09,507] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 2990.3778\n",
      "[2025-11-20 03:31:25,411] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 2988.2844\n",
      "[2025-11-20 03:32:23,259] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2749.3986 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:32:24,917] [UniVITrainer] [INFO] [Epoch 075] Val loss: 2992.2568 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:33:06,575] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 2984.7611\n",
      "[2025-11-20 03:33:35,838] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2733.6063 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:33:37,488] [UniVITrainer] [INFO] [Epoch 080] Val loss: 2990.5617 (beta=40.000, gamma=0.000)\n",
      "[2025-11-20 03:33:37,529] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 2984.7611)\n",
      "[2025-11-20 03:33:38,375] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 03:33:38,377] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 03:33:38,377] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 03:33:38,382] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 03:33:38,383] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 03:33:38,389] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 03:33:38,390] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 03:33:38,390] [UniVITrainer] [INFO]   grad_clip: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 03:33:38,391] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 03:33:38,400] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 03:33:38,401] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 03:33:38,402] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 03:33:38,402] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 26] Done in 19.9 min\n",
      "  best_val_loss              = 2984.761\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.2274\n",
      "  Label transfer (ADT→RNA)  = 0.227\n",
      "\n",
      "================================================================================\n",
      "[Config 27] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 32,\n",
      "  \"beta\": 0.0,\n",
      "  \"gamma\": 60.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51db01192d348ccaa07cb8f013ba053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 03:33:52,134] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3509.9936 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:33:53,786] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3117.7281 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:33:53,897] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3117.7281\n",
      "[2025-11-20 03:34:08,641] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 2962.8211\n",
      "[2025-11-20 03:34:24,156] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 2900.4257\n",
      "[2025-11-20 03:34:39,260] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 2857.4351\n",
      "[2025-11-20 03:34:52,765] [UniVITrainer] [INFO] [Epoch 005] Train loss: 2738.4167 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:34:54,416] [UniVITrainer] [INFO] [Epoch 005] Val loss: 2810.3612 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:34:54,515] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 2810.3612\n",
      "[2025-11-20 03:35:09,905] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 2770.6817\n",
      "[2025-11-20 03:35:25,332] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 2745.1516\n",
      "[2025-11-20 03:35:40,658] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 2721.9629\n",
      "[2025-11-20 03:35:55,746] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 2694.3720\n",
      "[2025-11-20 03:36:08,277] [UniVITrainer] [INFO] [Epoch 010] Train loss: 2505.1027 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:36:09,929] [UniVITrainer] [INFO] [Epoch 010] Val loss: 2671.5286 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:36:10,018] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 2671.5286\n",
      "[2025-11-20 03:36:25,400] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 2652.0855\n",
      "[2025-11-20 03:36:40,906] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 2629.4629\n",
      "[2025-11-20 03:36:54,568] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 2618.7247\n",
      "[2025-11-20 03:37:10,021] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 2605.4601\n",
      "[2025-11-20 03:37:23,544] [UniVITrainer] [INFO] [Epoch 015] Train loss: 2338.6181 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:37:25,194] [UniVITrainer] [INFO] [Epoch 015] Val loss: 2581.4160 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:37:25,307] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 2581.4160\n",
      "[2025-11-20 03:37:55,367] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 2561.2759\n",
      "[2025-11-20 03:38:10,886] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 2549.0257\n",
      "[2025-11-20 03:38:26,433] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 2546.0423\n",
      "[2025-11-20 03:38:40,167] [UniVITrainer] [INFO] [Epoch 020] Train loss: 2215.4058 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:38:41,854] [UniVITrainer] [INFO] [Epoch 020] Val loss: 2525.5764 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:38:41,922] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 2525.5764\n",
      "[2025-11-20 03:38:57,018] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 2512.2523\n",
      "[2025-11-20 03:39:11,290] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 2501.3053\n",
      "[2025-11-20 03:39:26,589] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 2493.1916\n",
      "[2025-11-20 03:39:42,119] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 2474.5028\n",
      "[2025-11-20 03:39:54,468] [UniVITrainer] [INFO] [Epoch 025] Train loss: 2114.9848 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:39:56,049] [UniVITrainer] [INFO] [Epoch 025] Val loss: 2482.4344 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:40:23,650] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 2459.6488\n",
      "[2025-11-20 03:40:37,369] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 2450.9312\n",
      "[2025-11-20 03:41:05,402] [UniVITrainer] [INFO] [Epoch 030] Train loss: 2031.4581 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:41:07,094] [UniVITrainer] [INFO] [Epoch 030] Val loss: 2439.6278 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:41:07,177] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 2439.6278\n",
      "[2025-11-20 03:41:22,863] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 2439.2453\n",
      "[2025-11-20 03:41:38,370] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 2428.1780\n",
      "[2025-11-20 03:41:53,261] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 2422.1890\n",
      "[2025-11-20 03:42:09,149] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 2420.3703\n",
      "[2025-11-20 03:42:23,323] [UniVITrainer] [INFO] [Epoch 035] Train loss: 1959.0376 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:42:25,013] [UniVITrainer] [INFO] [Epoch 035] Val loss: 2421.5833 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:42:40,906] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 2408.5964\n",
      "[2025-11-20 03:43:12,141] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 2405.3754\n",
      "[2025-11-20 03:43:27,629] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 2398.7861\n",
      "[2025-11-20 03:43:41,838] [UniVITrainer] [INFO] [Epoch 040] Train loss: 1907.7686 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:43:43,510] [UniVITrainer] [INFO] [Epoch 040] Val loss: 2397.3544 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:43:43,577] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 2397.3544\n",
      "[2025-11-20 03:43:59,124] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 2388.5967\n",
      "[2025-11-20 03:44:14,999] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 2384.3321\n",
      "[2025-11-20 03:44:26,418] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 2384.1942\n",
      "[2025-11-20 03:44:40,768] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 2381.7926\n",
      "[2025-11-20 03:44:54,532] [UniVITrainer] [INFO] [Epoch 045] Train loss: 1859.6177 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:44:56,221] [UniVITrainer] [INFO] [Epoch 045] Val loss: 2370.5136 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:44:56,298] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 2370.5136\n",
      "[2025-11-20 03:45:40,424] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 2367.9906\n",
      "[2025-11-20 03:46:07,983] [UniVITrainer] [INFO] [Epoch 050] Train loss: 1818.7784 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:46:09,636] [UniVITrainer] [INFO] [Epoch 050] Val loss: 2368.5911 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:46:51,586] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 2362.8149\n",
      "[2025-11-20 03:47:19,729] [UniVITrainer] [INFO] [Epoch 055] Train loss: 1777.8023 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:47:21,388] [UniVITrainer] [INFO] [Epoch 055] Val loss: 2364.0630 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:48:21,209] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 2359.0370\n",
      "[2025-11-20 03:48:34,601] [UniVITrainer] [INFO] [Epoch 060] Train loss: 1749.9732 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:48:36,263] [UniVITrainer] [INFO] [Epoch 060] Val loss: 2364.8894 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:48:50,948] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 2357.5668\n",
      "[2025-11-20 03:49:06,403] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 2350.8714\n",
      "[2025-11-20 03:49:50,956] [UniVITrainer] [INFO] [Epoch 065] Train loss: 1718.0058 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:49:52,594] [UniVITrainer] [INFO] [Epoch 065] Val loss: 2360.2728 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:51:04,207] [UniVITrainer] [INFO] [Epoch 070] Train loss: 1694.0518 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:51:05,858] [UniVITrainer] [INFO] [Epoch 070] Val loss: 2347.1788 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:51:05,947] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 2347.1788\n",
      "[2025-11-20 03:52:16,479] [UniVITrainer] [INFO] [Epoch 075] Train loss: 1671.8876 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:52:18,121] [UniVITrainer] [INFO] [Epoch 075] Val loss: 2355.4339 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:53:28,645] [UniVITrainer] [INFO] [Epoch 080] Train loss: 1658.3117 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:53:30,298] [UniVITrainer] [INFO] [Epoch 080] Val loss: 2350.0060 (beta=0.000, gamma=60.000)\n",
      "[2025-11-20 03:53:30,342] [UniVITrainer] [INFO] Restored best model from epoch 70 (val loss = 2347.1788)\n",
      "[2025-11-20 03:53:31,204] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 03:53:31,205] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 03:53:31,206] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 03:53:31,215] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 03:53:31,215] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 03:53:31,216] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 03:53:31,217] [UniVITrainer] [INFO]   log_every: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 03:53:31,218] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 03:53:31,228] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 03:53:31,228] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 03:53:31,229] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 03:53:31,229] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 03:53:31,230] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 27] Done in 19.9 min\n",
      "  best_val_loss              = 2347.179\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0210\n",
      "  Label transfer (ADT→RNA)  = 0.832\n",
      "--> New best config (id=27) with score=1308.299\n",
      "\n",
      "================================================================================\n",
      "[Config 28] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 50,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 160.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8501a9d0f03a490b8b1e2d897d0774e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 03:53:44,579] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4298.7642 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:53:46,226] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3823.3883 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:53:46,258] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3823.3883\n",
      "[2025-11-20 03:54:01,584] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3810.6552\n",
      "[2025-11-20 03:54:16,590] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3807.1080\n",
      "[2025-11-20 03:54:31,815] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3708.4268\n",
      "[2025-11-20 03:54:44,289] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3705.8231 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:54:45,941] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3698.5240 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:54:46,030] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3698.5240\n",
      "[2025-11-20 03:55:01,184] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3696.7346\n",
      "[2025-11-20 03:55:30,192] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3693.0304\n",
      "[2025-11-20 03:55:45,595] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3688.4544\n",
      "[2025-11-20 03:55:59,265] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3680.7933 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:56:00,625] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3682.2545 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:56:00,657] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3682.2545\n",
      "[2025-11-20 03:56:15,081] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3666.7838\n",
      "[2025-11-20 03:56:30,139] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3658.1105\n",
      "[2025-11-20 03:56:45,409] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3631.2496\n",
      "[2025-11-20 03:57:00,478] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3604.9523\n",
      "[2025-11-20 03:57:12,647] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3582.8115 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:57:13,728] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3585.6768 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:57:13,760] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3585.6768\n",
      "[2025-11-20 03:57:43,983] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3576.2511\n",
      "[2025-11-20 03:57:59,230] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3574.6135\n",
      "[2025-11-20 03:58:14,339] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3561.2544\n",
      "[2025-11-20 03:58:26,565] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3543.7540 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:58:28,058] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3557.3920 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:58:28,154] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3557.3920\n",
      "[2025-11-20 03:58:43,358] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3549.9203\n",
      "[2025-11-20 03:58:58,930] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3532.6982\n",
      "[2025-11-20 03:59:13,764] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3496.1123\n",
      "[2025-11-20 03:59:42,697] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3458.0257 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:59:44,361] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3467.5959 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 03:59:44,434] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3467.5959\n",
      "[2025-11-20 04:00:00,018] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3464.8128\n",
      "[2025-11-20 04:00:13,169] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3458.7465\n",
      "[2025-11-20 04:00:28,734] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3455.4229\n",
      "[2025-11-20 04:00:44,149] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3443.5445\n",
      "[2025-11-20 04:00:58,039] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3416.9669 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:00:59,689] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3439.9484 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:00:59,799] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3439.9484\n",
      "[2025-11-20 04:01:30,635] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3437.8233\n",
      "[2025-11-20 04:01:46,473] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3430.1412\n",
      "[2025-11-20 04:02:01,775] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3426.5269\n",
      "[2025-11-20 04:02:14,833] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3395.6041 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:02:16,498] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3431.9311 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:02:31,391] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3420.8609\n",
      "[2025-11-20 04:03:16,804] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3414.7926\n",
      "[2025-11-20 04:03:29,946] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3381.0527 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:03:31,536] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3410.2985 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:03:31,631] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3410.2985\n",
      "[2025-11-20 04:04:16,377] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3406.6504\n",
      "[2025-11-20 04:04:44,795] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3373.7095 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:04:46,484] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3404.0242 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:04:46,518] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3404.0242\n",
      "[2025-11-20 04:05:44,774] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3402.3539\n",
      "[2025-11-20 04:05:58,529] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3366.8412 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:06:00,196] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3410.7801 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:06:29,862] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3393.1859\n",
      "[2025-11-20 04:07:13,607] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3353.6756 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:07:15,543] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3400.9588 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:08:28,695] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3348.4915 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:08:30,394] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3396.2229 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:08:59,904] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3391.3778\n",
      "[2025-11-20 04:09:31,063] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3388.3611\n",
      "[2025-11-20 04:09:44,791] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3339.1517 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:09:46,491] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3398.5827 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:10:17,467] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3387.8507\n",
      "[2025-11-20 04:10:30,894] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3387.5171\n",
      "[2025-11-20 04:10:42,992] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3385.6033\n",
      "[2025-11-20 04:10:56,268] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3333.3265 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:10:57,283] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3390.0507 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:11:10,952] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3383.5692\n",
      "[2025-11-20 04:12:05,868] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3329.1939 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:12:06,930] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3378.2681 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:12:07,042] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 3378.2681\n",
      "[2025-11-20 04:12:33,789] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3374.4444\n",
      "[2025-11-20 04:12:49,641] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3373.0801\n",
      "[2025-11-20 04:13:04,813] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3366.9327\n",
      "[2025-11-20 04:13:16,502] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3300.6505 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:13:18,223] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3360.8021 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 04:13:18,320] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 3360.8021\n",
      "[2025-11-20 04:13:18,357] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 3360.8021)\n",
      "[2025-11-20 04:13:19,302] [UniVITrainer] [INFO] TrainingConfig:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 04:13:19,303] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 04:13:19,304] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 04:13:19,309] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 04:13:19,314] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 04:13:19,315] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 04:13:19,322] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 04:13:19,323] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 04:13:19,323] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 04:13:19,324] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 04:13:19,325] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 04:13:19,325] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 04:13:19,326] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 28] Done in 19.8 min\n",
      "  best_val_loss              = 3360.802\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0906\n",
      "  Label transfer (ADT→RNA)  = 0.460\n",
      "\n",
      "================================================================================\n",
      "[Config 29] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 32,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 20.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb49de3b6da4f76bc2c1e3eaed0be90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 04:13:32,973] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3942.8280 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:13:34,669] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3777.9753 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:13:34,817] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3777.9753\n",
      "[2025-11-20 04:13:50,217] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3636.1920\n",
      "[2025-11-20 04:14:04,844] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3611.6266\n",
      "[2025-11-20 04:14:20,543] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3582.1133\n",
      "[2025-11-20 04:14:34,304] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3545.2143 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:14:36,012] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3568.8988 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:14:36,147] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3568.8988\n",
      "[2025-11-20 04:14:51,074] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3527.3122\n",
      "[2025-11-20 04:15:06,527] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3512.9912\n",
      "[2025-11-20 04:15:21,929] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3490.8186\n",
      "[2025-11-20 04:15:36,393] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3484.0149\n",
      "[2025-11-20 04:15:47,905] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3441.8792 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:15:49,605] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3481.8161 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:15:49,782] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3481.8161\n",
      "[2025-11-20 04:16:05,667] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3468.8503\n",
      "[2025-11-20 04:16:21,673] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3462.0580\n",
      "[2025-11-20 04:16:37,579] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3454.0626\n",
      "[2025-11-20 04:17:07,035] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3416.0774 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:17:08,732] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3457.0855 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:17:23,964] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3449.3174\n",
      "[2025-11-20 04:17:52,163] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3446.4312\n",
      "[2025-11-20 04:18:07,227] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3440.7770\n",
      "[2025-11-20 04:18:20,161] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3395.6062 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:18:21,823] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3444.0777 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:18:51,615] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3440.6921\n",
      "[2025-11-20 04:19:07,030] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3428.9805\n",
      "[2025-11-20 04:19:19,608] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3428.3869\n",
      "[2025-11-20 04:19:32,403] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3380.4111 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:19:34,055] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3426.5122 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:19:34,192] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3426.5122\n",
      "[2025-11-20 04:19:48,863] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3425.4945\n",
      "[2025-11-20 04:20:04,354] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3425.2412\n",
      "[2025-11-20 04:20:48,200] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3362.8823 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:20:49,834] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3421.4887 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:20:49,985] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3421.4887\n",
      "[2025-11-20 04:21:36,064] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3417.6240\n",
      "[2025-11-20 04:22:04,246] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3351.3604 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:22:05,911] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3419.1760 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:22:21,664] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3413.1179\n",
      "[2025-11-20 04:23:20,862] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3341.7163 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:23:22,527] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3413.4887 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:23:53,704] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3407.1989\n",
      "[2025-11-20 04:24:36,741] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3334.7766 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:24:38,439] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3406.8523 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:24:38,587] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3406.8523\n",
      "[2025-11-20 04:25:06,291] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3397.5451\n",
      "[2025-11-20 04:25:21,337] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3395.4363\n",
      "[2025-11-20 04:25:47,699] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3330.2904 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:25:49,194] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3395.4638 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:27:00,360] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3325.6271 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:27:02,048] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3392.4021 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:27:02,069] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3392.4021\n",
      "[2025-11-20 04:27:47,505] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3390.8316\n",
      "[2025-11-20 04:28:16,206] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3312.4672 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:28:17,911] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3388.6475 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:28:18,073] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3388.6475\n",
      "[2025-11-20 04:28:33,664] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3384.6339\n",
      "[2025-11-20 04:28:47,791] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3379.1315\n",
      "[2025-11-20 04:29:03,431] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3377.3679\n",
      "[2025-11-20 04:29:33,545] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3307.2496 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:29:35,234] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3381.3178 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:30:19,086] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3376.3563\n",
      "[2025-11-20 04:30:48,323] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3298.3728 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:30:50,010] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3385.3277 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:31:16,215] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3370.9346\n",
      "[2025-11-20 04:31:58,301] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3293.3148 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:31:59,948] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3378.3254 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:33:10,904] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3293.7753 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:33:12,545] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3380.7519 (beta=160.000, gamma=20.000)\n",
      "[2025-11-20 04:33:12,582] [UniVITrainer] [INFO] Restored best model from epoch 72 (val loss = 3370.9346)\n",
      "[2025-11-20 04:33:13,409] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 04:33:13,410] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 04:33:13,411] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 04:33:13,416] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 04:33:13,417] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 04:33:13,418] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 04:33:13,430] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 04:33:13,431] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 04:33:13,431] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 04:33:13,432] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 04:33:13,433] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 04:33:13,434] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 04:33:13,435] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 29] Done in 19.9 min\n",
      "  best_val_loss              = 3370.935\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1472\n",
      "  Label transfer (ADT→RNA)  = 0.369\n",
      "\n",
      "================================================================================\n",
      "[Config 30] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 80.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5b5d0b615b4949bb936e9d51edb4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 04:33:26,938] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4290.4068 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:33:28,589] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3889.6701 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:33:28,677] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3889.6701\n",
      "[2025-11-20 04:33:43,703] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3748.1847\n",
      "[2025-11-20 04:33:58,870] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3707.1843\n",
      "[2025-11-20 04:34:11,868] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3680.7827\n",
      "[2025-11-20 04:34:24,024] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3658.9421 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:34:25,671] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3664.3429 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:34:25,800] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3664.3429\n",
      "[2025-11-20 04:34:39,979] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3661.5500\n",
      "[2025-11-20 04:34:53,697] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3647.0886\n",
      "[2025-11-20 04:35:08,835] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3637.2243\n",
      "[2025-11-20 04:35:37,262] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3598.2476 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:35:38,917] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3598.2086 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:35:39,042] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3598.2086\n",
      "[2025-11-20 04:35:54,054] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3535.3270\n",
      "[2025-11-20 04:36:08,874] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3499.3318\n",
      "[2025-11-20 04:36:23,840] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3493.1984\n",
      "[2025-11-20 04:36:38,973] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3464.6643\n",
      "[2025-11-20 04:36:52,484] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3428.8287 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:36:54,138] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3461.7567 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:36:54,283] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3461.7567\n",
      "[2025-11-20 04:37:09,385] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3450.6853\n",
      "[2025-11-20 04:37:24,148] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3436.3199\n",
      "[2025-11-20 04:38:05,197] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3397.3240 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:38:05,963] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3429.9782 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:38:06,100] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3429.9782\n",
      "[2025-11-20 04:38:19,740] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3422.1321\n",
      "[2025-11-20 04:38:49,490] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3415.0527\n",
      "[2025-11-20 04:39:18,111] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3370.5329 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:39:19,756] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3410.7284 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:39:19,883] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3410.7284\n",
      "[2025-11-20 04:39:35,144] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3399.7447\n",
      "[2025-11-20 04:40:05,388] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3398.9802\n",
      "[2025-11-20 04:40:31,125] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3345.6197 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:40:32,764] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3392.2372 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:40:32,903] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3392.2372\n",
      "[2025-11-20 04:40:48,282] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3387.9864\n",
      "[2025-11-20 04:41:17,368] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3385.4226\n",
      "[2025-11-20 04:41:32,924] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3379.9170\n",
      "[2025-11-20 04:41:46,608] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3324.0522 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:41:48,245] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3372.7981 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:41:48,367] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3372.7981\n",
      "[2025-11-20 04:42:18,536] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3361.6288\n",
      "[2025-11-20 04:43:02,352] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3314.4966 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:43:04,019] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3364.5290 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:44:04,673] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3358.6596\n",
      "[2025-11-20 04:44:17,971] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3300.4918 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:44:19,623] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3355.8955 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:44:19,667] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3355.8955\n",
      "[2025-11-20 04:44:33,737] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3348.5042\n",
      "[2025-11-20 04:45:30,232] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3285.8828 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:45:31,886] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3350.8208 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:46:17,115] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3347.2858\n",
      "[2025-11-20 04:46:45,558] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3281.7215 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:46:45,994] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3347.3510 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:47:16,418] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3343.8071\n",
      "[2025-11-20 04:47:31,267] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3343.5180\n",
      "[2025-11-20 04:47:59,107] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3269.6454 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:48:00,763] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3350.3097 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:48:31,706] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3334.5830\n",
      "[2025-11-20 04:49:15,044] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3265.3434 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:49:16,691] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3339.6709 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:49:47,675] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3330.6570\n",
      "[2025-11-20 04:50:30,670] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3258.6291 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:50:32,153] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3326.9733 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:50:32,280] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3326.9733\n",
      "[2025-11-20 04:51:32,955] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3321.8102\n",
      "[2025-11-20 04:51:44,773] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3250.6513 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:51:46,428] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3322.4798 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:52:30,701] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3321.5129\n",
      "[2025-11-20 04:52:45,963] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3320.6564\n",
      "[2025-11-20 04:52:59,157] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3243.2790 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:53:00,719] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3320.5940 (beta=120.000, gamma=80.000)\n",
      "[2025-11-20 04:53:00,833] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 3320.5940\n",
      "[2025-11-20 04:53:00,859] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 3320.5940)\n",
      "[2025-11-20 04:53:01,809] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 04:53:01,810] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 04:53:01,810] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 04:53:01,815] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 04:53:01,816] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 04:53:01,823] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 04:53:01,824] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 04:53:01,824] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 04:53:01,825] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 04:53:01,826] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 04:53:01,835] [UniVITrainer] [INFO]   early_stopping: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 04:53:01,836] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 04:53:01,837] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 30] Done in 19.8 min\n",
      "  best_val_loss              = 3320.594\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0865\n",
      "  Label transfer (ADT→RNA)  = 0.507\n",
      "\n",
      "================================================================================\n",
      "[Config 31] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 50,\n",
      "  \"beta\": 60.0,\n",
      "  \"gamma\": 60.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e9963bfe3d444faa939b2437ee50df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 04:53:15,557] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4103.3201 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:53:17,212] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3766.8253 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:53:17,284] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3766.8253\n",
      "[2025-11-20 04:53:32,593] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3595.5180\n",
      "[2025-11-20 04:53:47,736] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3536.7834\n",
      "[2025-11-20 04:54:03,286] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3457.4062\n",
      "[2025-11-20 04:54:16,789] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3444.6383 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:54:18,425] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3416.7844 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:54:18,631] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3416.7844\n",
      "[2025-11-20 04:54:32,380] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3385.4828\n",
      "[2025-11-20 04:54:47,965] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3357.0909\n",
      "[2025-11-20 04:55:03,024] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3318.5839\n",
      "[2025-11-20 04:55:16,996] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3306.9456\n",
      "[2025-11-20 04:55:29,645] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3279.9201 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:55:31,300] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3293.6889 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:55:31,475] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3293.6889\n",
      "[2025-11-20 04:55:44,679] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3290.7223\n",
      "[2025-11-20 04:55:59,607] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3266.8687\n",
      "[2025-11-20 04:56:13,516] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3256.9592\n",
      "[2025-11-20 04:56:28,668] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3250.3027\n",
      "[2025-11-20 04:56:40,962] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3217.4427 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:56:42,623] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3243.3190 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:56:42,817] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3243.3190\n",
      "[2025-11-20 04:56:56,986] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3236.7611\n",
      "[2025-11-20 04:57:11,863] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3221.1976\n",
      "[2025-11-20 04:57:27,109] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3220.2684\n",
      "[2025-11-20 04:57:42,091] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3209.9763\n",
      "[2025-11-20 04:57:55,408] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3166.4074 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:57:57,091] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3193.1217 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:57:57,293] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3193.1217\n",
      "[2025-11-20 04:58:26,183] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3191.4170\n",
      "[2025-11-20 04:58:41,066] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3190.6532\n",
      "[2025-11-20 04:58:56,881] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3182.6649\n",
      "[2025-11-20 04:59:10,460] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3141.3280 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:59:12,162] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3186.1129 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 04:59:27,147] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3181.2534\n",
      "[2025-11-20 04:59:58,441] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3172.1730\n",
      "[2025-11-20 05:00:25,961] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3117.4277 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:00:27,656] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3174.6886 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:00:43,434] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3165.1689\n",
      "[2025-11-20 05:01:14,005] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3162.9276\n",
      "[2025-11-20 05:01:28,942] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3158.7904\n",
      "[2025-11-20 05:01:43,168] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3097.8813 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:01:44,868] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3162.2042 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:02:45,045] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3149.4609\n",
      "[2025-11-20 05:02:59,065] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3087.2425 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:03:00,701] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3163.3436 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:04:13,245] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3069.4577 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:04:14,930] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3142.6686 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:04:15,137] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3142.6686\n",
      "[2025-11-20 05:04:44,806] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3141.0683\n",
      "[2025-11-20 05:05:27,437] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3052.4189 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:05:28,836] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3138.8883 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:05:29,028] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3138.8883\n",
      "[2025-11-20 05:05:43,309] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3137.9637\n",
      "[2025-11-20 05:06:12,635] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3136.3166\n",
      "[2025-11-20 05:06:28,045] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3132.5462\n",
      "[2025-11-20 05:06:41,528] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3043.7946 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:06:43,188] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3129.5901 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:06:43,382] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3129.5901\n",
      "[2025-11-20 05:07:56,166] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3035.9998 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:07:57,826] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3131.5286 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:08:11,618] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3128.6680\n",
      "[2025-11-20 05:08:27,456] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3123.6923\n",
      "[2025-11-20 05:08:43,133] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3119.4425\n",
      "[2025-11-20 05:09:12,308] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3023.0588 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:09:13,962] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3121.7661 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:10:00,632] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3113.3944\n",
      "[2025-11-20 05:10:28,893] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3018.8499 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:10:30,544] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3115.4941 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:11:29,320] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3110.8627\n",
      "[2025-11-20 05:11:42,974] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3016.5125 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:11:44,688] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3113.3430 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:12:55,144] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3006.6847 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:12:56,820] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3117.9259 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 05:12:56,871] [UniVITrainer] [INFO] Restored best model from epoch 74 (val loss = 3110.8627)\n",
      "[2025-11-20 05:12:57,705] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 05:12:57,707] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 05:12:57,709] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 05:12:57,711] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 05:12:57,712] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 05:12:57,714] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 05:12:57,715] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 05:12:57,717] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 05:12:57,719] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 05:12:57,720] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 05:12:57,722] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 05:12:57,723] [UniVITrainer] [INFO]   patience: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 05:12:57,723] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 31] Done in 19.9 min\n",
      "  best_val_loss              = 3110.863\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0423\n",
      "  Label transfer (ADT→RNA)  = 0.657\n",
      "\n",
      "================================================================================\n",
      "[Config 32] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 0.0,\n",
      "  \"gamma\": 80.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c964c316ee4e998d344fe55dcbcfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 05:13:10,668] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3495.4581 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:13:12,054] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3143.5301 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:13:12,188] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3143.5301\n",
      "[2025-11-20 05:13:27,943] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3025.5696\n",
      "[2025-11-20 05:13:43,265] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 2978.5288\n",
      "[2025-11-20 05:13:58,833] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 2945.5538\n",
      "[2025-11-20 05:14:12,527] [UniVITrainer] [INFO] [Epoch 005] Train loss: 2838.1769 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:14:14,103] [UniVITrainer] [INFO] [Epoch 005] Val loss: 2909.6758 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:14:14,233] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 2909.6758\n",
      "[2025-11-20 05:14:29,144] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 2870.8644\n",
      "[2025-11-20 05:14:44,745] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 2870.3834\n",
      "[2025-11-20 05:15:00,031] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 2835.2190\n",
      "[2025-11-20 05:15:15,702] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 2833.6454\n",
      "[2025-11-20 05:15:28,381] [UniVITrainer] [INFO] [Epoch 010] Train loss: 2694.0407 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:15:30,090] [UniVITrainer] [INFO] [Epoch 010] Val loss: 2813.1346 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:15:30,222] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 2813.1346\n",
      "[2025-11-20 05:15:44,798] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 2770.4435\n",
      "[2025-11-20 05:15:58,732] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 2752.6919\n",
      "[2025-11-20 05:16:30,507] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 2746.7564\n",
      "[2025-11-20 05:16:43,709] [UniVITrainer] [INFO] [Epoch 015] Train loss: 2554.8234 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:16:45,418] [UniVITrainer] [INFO] [Epoch 015] Val loss: 2712.2075 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:16:45,562] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 2712.2075\n",
      "[2025-11-20 05:17:00,716] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 2704.6746\n",
      "[2025-11-20 05:17:15,746] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 2704.5019\n",
      "[2025-11-20 05:17:31,647] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 2691.0929\n",
      "[2025-11-20 05:17:47,666] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 2675.1276\n",
      "[2025-11-20 05:18:01,646] [UniVITrainer] [INFO] [Epoch 020] Train loss: 2441.7414 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:18:03,352] [UniVITrainer] [INFO] [Epoch 020] Val loss: 2651.5942 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:18:03,519] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 2651.5942\n",
      "[2025-11-20 05:18:47,589] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 2646.2812\n",
      "[2025-11-20 05:19:02,646] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 2639.2085\n",
      "[2025-11-20 05:19:15,183] [UniVITrainer] [INFO] [Epoch 025] Train loss: 2320.3199 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:19:16,833] [UniVITrainer] [INFO] [Epoch 025] Val loss: 2603.4167 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:19:16,981] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 2603.4167\n",
      "[2025-11-20 05:19:45,707] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 2601.6788\n",
      "[2025-11-20 05:20:01,269] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 2578.1454\n",
      "[2025-11-20 05:20:29,632] [UniVITrainer] [INFO] [Epoch 030] Train loss: 2225.1881 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:20:31,269] [UniVITrainer] [INFO] [Epoch 030] Val loss: 2558.4288 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:20:31,422] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 2558.4288\n",
      "[2025-11-20 05:20:45,141] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 2550.0912\n",
      "[2025-11-20 05:20:59,884] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 2539.2655\n",
      "[2025-11-20 05:21:44,209] [UniVITrainer] [INFO] [Epoch 035] Train loss: 2133.1982 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:21:45,844] [UniVITrainer] [INFO] [Epoch 035] Val loss: 2536.0585 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:21:45,995] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 2536.0585\n",
      "[2025-11-20 05:22:16,247] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 2535.5733\n",
      "[2025-11-20 05:22:46,559] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 2521.0221\n",
      "[2025-11-20 05:23:00,208] [UniVITrainer] [INFO] [Epoch 040] Train loss: 2059.1612 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:23:01,837] [UniVITrainer] [INFO] [Epoch 040] Val loss: 2509.3591 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:23:01,922] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 2509.3591\n",
      "[2025-11-20 05:23:17,476] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 2499.2026\n",
      "[2025-11-20 05:23:32,776] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 2492.5664\n",
      "[2025-11-20 05:23:48,370] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 2478.0502\n",
      "[2025-11-20 05:24:16,382] [UniVITrainer] [INFO] [Epoch 045] Train loss: 1998.0161 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:24:18,021] [UniVITrainer] [INFO] [Epoch 045] Val loss: 2515.8497 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:25:02,794] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 2474.7042\n",
      "[2025-11-20 05:25:18,447] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 2471.6092\n",
      "[2025-11-20 05:25:32,028] [UniVITrainer] [INFO] [Epoch 050] Train loss: 1937.0473 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:25:33,668] [UniVITrainer] [INFO] [Epoch 050] Val loss: 2485.1656 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:25:48,423] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 2469.3294\n",
      "[2025-11-20 05:26:02,069] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 2465.9501\n",
      "[2025-11-20 05:26:31,938] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 2456.1319\n",
      "[2025-11-20 05:26:45,035] [UniVITrainer] [INFO] [Epoch 055] Train loss: 1889.5214 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:26:46,648] [UniVITrainer] [INFO] [Epoch 055] Val loss: 2465.6548 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:27:46,367] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 2454.8015\n",
      "[2025-11-20 05:27:59,684] [UniVITrainer] [INFO] [Epoch 060] Train loss: 1838.6209 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:28:01,316] [UniVITrainer] [INFO] [Epoch 060] Val loss: 2456.4647 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:28:16,318] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 2445.3576\n",
      "[2025-11-20 05:29:01,793] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 2436.4226\n",
      "[2025-11-20 05:29:15,548] [UniVITrainer] [INFO] [Epoch 065] Train loss: 1809.0802 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:29:17,178] [UniVITrainer] [INFO] [Epoch 065] Val loss: 2434.2879 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:29:17,225] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 2434.2879\n",
      "[2025-11-20 05:30:31,946] [UniVITrainer] [INFO] [Epoch 070] Train loss: 1766.3984 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:30:33,583] [UniVITrainer] [INFO] [Epoch 070] Val loss: 2451.7875 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:31:35,488] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 2430.9840\n",
      "[2025-11-20 05:31:47,496] [UniVITrainer] [INFO] [Epoch 075] Train loss: 1737.3690 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:31:49,135] [UniVITrainer] [INFO] [Epoch 075] Val loss: 2443.3723 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:32:03,415] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 2425.4755\n",
      "[2025-11-20 05:32:32,118] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 2418.0095\n",
      "[2025-11-20 05:33:00,494] [UniVITrainer] [INFO] [Epoch 080] Train loss: 1710.3211 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:33:00,999] [UniVITrainer] [INFO] [Epoch 080] Val loss: 2430.5001 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 05:33:01,017] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 2418.0095)\n",
      "[2025-11-20 05:33:01,877] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 05:33:01,878] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 05:33:01,879] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 05:33:01,879] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 05:33:01,879] [UniVITrainer] [INFO]   weight_decay: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 05:33:01,880] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 05:33:01,880] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 05:33:01,880] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 05:33:01,881] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 05:33:01,881] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 05:33:01,882] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 05:33:01,883] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 05:33:01,883] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 32] Done in 20.1 min\n",
      "  best_val_loss              = 2418.010\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0189\n",
      "  Label transfer (ADT→RNA)  = 0.818\n",
      "\n",
      "================================================================================\n",
      "[Config 33] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 60.0,\n",
      "  \"gamma\": 0.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a983219431f04c09a607861ab2f658da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 05:33:15,465] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3848.8327 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:33:17,098] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3609.8987 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:33:17,326] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3609.8987\n",
      "[2025-11-20 05:33:32,450] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3493.9450\n",
      "[2025-11-20 05:33:47,798] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3429.1916\n",
      "[2025-11-20 05:34:02,455] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3418.7786\n",
      "[2025-11-20 05:34:15,920] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3374.7800 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:34:17,372] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3391.4961 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:34:17,581] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3391.4961\n",
      "[2025-11-20 05:34:32,217] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3382.1105\n",
      "[2025-11-20 05:34:47,902] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3357.3458\n",
      "[2025-11-20 05:35:02,739] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3350.0283\n",
      "[2025-11-20 05:35:18,218] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3329.4076\n",
      "[2025-11-20 05:35:31,059] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3284.0403 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:35:32,699] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3327.4520 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:35:32,910] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3327.4520\n",
      "[2025-11-20 05:35:47,750] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3299.2766\n",
      "[2025-11-20 05:36:03,078] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3283.1630\n",
      "[2025-11-20 05:36:17,462] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3274.0649\n",
      "[2025-11-20 05:36:31,322] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3254.3195\n",
      "[2025-11-20 05:36:44,049] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3212.8979 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:36:45,696] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3244.5964 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:36:45,906] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3244.5964\n",
      "[2025-11-20 05:36:57,611] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3242.5532\n",
      "[2025-11-20 05:37:10,624] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3242.1930\n",
      "[2025-11-20 05:37:24,934] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3218.9720\n",
      "[2025-11-20 05:37:39,785] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3205.2127\n",
      "[2025-11-20 05:37:52,174] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3146.6948 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:37:53,807] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3198.9982 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:37:53,844] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3198.9982\n",
      "[2025-11-20 05:38:09,055] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3186.4794\n",
      "[2025-11-20 05:38:23,726] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3186.1883\n",
      "[2025-11-20 05:38:38,505] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3179.4414\n",
      "[2025-11-20 05:39:03,960] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3106.1896 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:39:05,510] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3172.6313 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:39:05,726] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3172.6313\n",
      "[2025-11-20 05:39:19,961] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3167.4736\n",
      "[2025-11-20 05:39:35,021] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3165.4580\n",
      "[2025-11-20 05:39:49,226] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3162.6232\n",
      "[2025-11-20 05:40:04,354] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3162.3217\n",
      "[2025-11-20 05:40:16,088] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3082.9533 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:40:17,615] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3158.2359 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:40:17,903] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3158.2359\n",
      "[2025-11-20 05:40:32,699] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3155.7728\n",
      "[2025-11-20 05:40:46,065] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3145.1198\n",
      "[2025-11-20 05:41:29,874] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3058.8181 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:41:31,576] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3141.7356 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:41:31,815] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3141.7356\n",
      "[2025-11-20 05:41:46,454] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3140.4210\n",
      "[2025-11-20 05:42:17,982] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3139.5810\n",
      "[2025-11-20 05:42:33,870] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3137.1616\n",
      "[2025-11-20 05:42:48,019] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3039.2513 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:42:49,713] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3128.5057 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:42:49,954] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3128.5057\n",
      "[2025-11-20 05:43:21,805] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3126.8939\n",
      "[2025-11-20 05:43:52,940] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3125.3216\n",
      "[2025-11-20 05:44:07,184] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3023.1953 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:44:08,564] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3123.8401 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:44:08,791] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3123.8401\n",
      "[2025-11-20 05:44:24,833] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3121.6152\n",
      "[2025-11-20 05:45:10,141] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3115.0999\n",
      "[2025-11-20 05:45:23,321] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3007.5987 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:45:24,981] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3113.0938 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:45:25,018] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3113.0938\n",
      "[2025-11-20 05:45:53,877] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3111.0970\n",
      "[2025-11-20 05:46:23,863] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3109.7087\n",
      "[2025-11-20 05:46:37,145] [UniVITrainer] [INFO] [Epoch 055] Train loss: 2997.7429 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:46:38,803] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3112.8712 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:47:24,425] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3109.6756\n",
      "[2025-11-20 05:47:39,448] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3108.3217\n",
      "[2025-11-20 05:47:50,277] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2981.9094 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:47:51,280] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3107.4165 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:47:51,419] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3107.4165\n",
      "[2025-11-20 05:48:22,571] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3099.2747\n",
      "[2025-11-20 05:49:05,978] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2969.7223 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:49:07,641] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3106.5791 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:49:22,742] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3098.0214\n",
      "[2025-11-20 05:50:20,447] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2957.6764 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:50:22,113] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3097.6963 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:50:22,331] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3097.6963\n",
      "[2025-11-20 05:51:18,958] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3089.7091\n",
      "[2025-11-20 05:51:32,758] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2944.9814 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:51:34,393] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3097.5599 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:51:49,166] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3088.5986\n",
      "[2025-11-20 05:52:41,892] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2935.6106 (beta=60.000, gamma=0.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 05:52:43,554] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3089.6490 (beta=60.000, gamma=0.000)\n",
      "[2025-11-20 05:52:43,596] [UniVITrainer] [INFO] Restored best model from epoch 76 (val loss = 3088.5986)\n",
      "[2025-11-20 05:52:44,371] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 05:52:44,372] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 05:52:44,373] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 05:52:44,374] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 05:52:44,374] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 05:52:44,375] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 05:52:44,375] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 05:52:44,376] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 05:52:44,376] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 05:52:44,377] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 05:52:44,377] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 05:52:44,378] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 05:52:44,378] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 33] Done in 19.7 min\n",
      "  best_val_loss              = 3088.599\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1543\n",
      "  Label transfer (ADT→RNA)  = 0.204\n",
      "\n",
      "================================================================================\n",
      "[Config 34] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 120.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693b9a2358004d3891c4c94f6db68785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 05:52:57,073] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4023.5833 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:52:58,727] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3799.3374 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:52:58,783] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3799.3374\n",
      "[2025-11-20 05:53:14,078] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3707.9258\n",
      "[2025-11-20 05:53:27,698] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3690.6165\n",
      "[2025-11-20 05:53:52,249] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3668.8602 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:53:53,902] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3673.9105 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:53:53,999] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3673.9105\n",
      "[2025-11-20 05:54:06,534] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3631.3778\n",
      "[2025-11-20 05:54:21,801] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3597.1435\n",
      "[2025-11-20 05:54:36,442] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3595.6171\n",
      "[2025-11-20 05:54:51,738] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3576.7429\n",
      "[2025-11-20 05:55:04,357] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3544.2960 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:55:05,137] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3550.1333 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:55:05,246] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3550.1333\n",
      "[2025-11-20 05:55:20,730] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3509.5366\n",
      "[2025-11-20 05:55:36,180] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3483.2048\n",
      "[2025-11-20 05:55:51,735] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3471.7441\n",
      "[2025-11-20 05:56:05,972] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3469.2103\n",
      "[2025-11-20 05:56:18,666] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3419.2465 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:56:20,231] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3462.2103 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:56:20,328] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3462.2103\n",
      "[2025-11-20 05:56:35,440] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3451.3161\n",
      "[2025-11-20 05:56:50,099] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3447.0945\n",
      "[2025-11-20 05:57:19,657] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3436.5731\n",
      "[2025-11-20 05:57:31,544] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3393.0550 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:57:33,205] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3435.0192 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:57:33,301] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3435.0192\n",
      "[2025-11-20 05:57:46,709] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3431.0655\n",
      "[2025-11-20 05:57:59,212] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3428.4515\n",
      "[2025-11-20 05:58:40,620] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3377.9731 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:58:42,117] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3420.8191 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:58:42,228] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3420.8191\n",
      "[2025-11-20 05:59:51,498] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3367.6525 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:59:53,164] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3417.5317 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 05:59:53,286] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3417.5317\n",
      "[2025-11-20 06:00:08,349] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3412.0201\n",
      "[2025-11-20 06:01:02,808] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3361.2216 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:01:04,393] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3419.8616 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:02:14,115] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3354.3714 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:02:15,778] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3409.8121 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:02:15,879] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3409.8121\n",
      "[2025-11-20 06:02:29,468] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3407.8599\n",
      "[2025-11-20 06:02:42,990] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3406.3504\n",
      "[2025-11-20 06:02:58,360] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3404.8507\n",
      "[2025-11-20 06:03:25,478] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3346.0770 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:03:27,125] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3397.8970 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:03:27,172] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3397.8970\n",
      "[2025-11-20 06:03:57,994] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3386.8027\n",
      "[2025-11-20 06:04:11,809] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3377.7766\n",
      "[2025-11-20 06:04:39,850] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3300.6180 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:04:40,654] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3376.0652 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:04:40,784] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3376.0652\n",
      "[2025-11-20 06:04:56,041] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3373.4795\n",
      "[2025-11-20 06:05:11,396] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3372.1299\n",
      "[2025-11-20 06:05:26,265] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3371.6657\n",
      "[2025-11-20 06:05:37,178] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3368.6291\n",
      "[2025-11-20 06:05:50,015] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3294.4582 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:05:51,584] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3372.6946 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:06:20,336] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3368.0813\n",
      "[2025-11-20 06:06:35,344] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3366.4709\n",
      "[2025-11-20 06:07:03,312] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3286.5149 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:07:04,959] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3357.3759 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:07:05,052] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3357.3759\n",
      "[2025-11-20 06:07:50,743] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3352.8000\n",
      "[2025-11-20 06:08:18,810] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3276.9544 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:08:20,464] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3350.9065 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:08:20,568] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3350.9065\n",
      "[2025-11-20 06:08:47,925] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3348.8212\n",
      "[2025-11-20 06:09:02,287] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3344.5591\n",
      "[2025-11-20 06:09:31,100] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3270.0891 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:09:32,764] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3353.1926 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:10:02,837] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3344.4331\n",
      "[2025-11-20 06:10:45,969] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3262.5077 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:10:47,534] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3348.3781 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:11:02,408] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3342.3072\n",
      "[2025-11-20 06:11:29,847] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3334.0589\n",
      "[2025-11-20 06:11:55,998] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3262.0835 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:11:57,674] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3345.0863 (beta=120.000, gamma=120.000)\n",
      "[2025-11-20 06:11:57,696] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3334.0589)\n",
      "[2025-11-20 06:11:58,534] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 06:11:58,535] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 06:11:58,536] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 06:11:58,536] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 06:11:58,537] [UniVITrainer] [INFO]   weight_decay: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 06:11:58,537] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 06:11:58,538] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 06:11:58,538] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 06:11:58,539] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 06:11:58,539] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 06:11:58,540] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 06:11:58,541] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 06:11:58,541] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 34] Done in 19.2 min\n",
      "  best_val_loss              = 3334.059\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0869\n",
      "  Label transfer (ADT→RNA)  = 0.460\n",
      "\n",
      "================================================================================\n",
      "[Config 35] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 32,\n",
      "  \"beta\": 100.0,\n",
      "  \"gamma\": 80.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826196b0c19045ccb3b9702f4b780e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 06:12:11,713] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3979.8606 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:12:13,019] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3731.4120 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:12:13,236] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3731.4120\n",
      "[2025-11-20 06:12:27,828] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3638.1011\n",
      "[2025-11-20 06:12:43,653] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3535.7507\n",
      "[2025-11-20 06:12:59,594] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3497.5890\n",
      "[2025-11-20 06:13:13,654] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3472.5861 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:13:15,302] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3460.4544 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:13:15,497] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3460.4544\n",
      "[2025-11-20 06:13:31,259] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3409.6472\n",
      "[2025-11-20 06:13:46,103] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3392.7393\n",
      "[2025-11-20 06:14:02,033] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3378.1879\n",
      "[2025-11-20 06:14:29,844] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3353.2891 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:14:31,554] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3379.9357 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:14:47,674] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3360.2831\n",
      "[2025-11-20 06:15:03,885] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3358.3241\n",
      "[2025-11-20 06:15:19,804] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3346.5522\n",
      "[2025-11-20 06:15:35,547] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3338.1812\n",
      "[2025-11-20 06:15:49,904] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3314.0986 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:15:51,183] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3344.4134 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:16:07,414] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3335.5578\n",
      "[2025-11-20 06:16:37,578] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3322.5167\n",
      "[2025-11-20 06:16:53,130] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3313.0354\n",
      "[2025-11-20 06:17:07,443] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3290.3143 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:17:09,134] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3313.2848 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:17:25,138] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3300.4776\n",
      "[2025-11-20 06:18:20,229] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3275.1766 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:18:21,873] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3304.1741 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:18:52,279] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3296.3167\n",
      "[2025-11-20 06:19:33,545] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3263.7086 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:19:35,182] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3298.4825 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:20:35,139] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3288.4664\n",
      "[2025-11-20 06:20:48,792] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3256.0771 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:20:50,432] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3291.6935 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:21:37,007] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3285.4287\n",
      "[2025-11-20 06:21:52,673] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3281.6989\n",
      "[2025-11-20 06:22:06,538] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3248.8743 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:22:08,184] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3288.8797 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:22:35,399] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3280.7647\n",
      "[2025-11-20 06:22:50,790] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3280.6046\n",
      "[2025-11-20 06:23:19,381] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3230.2886 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:23:21,020] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3283.2083 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:23:50,629] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3260.9946\n",
      "[2025-11-20 06:24:32,194] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3224.5953 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:24:33,741] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3289.0410 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:25:05,088] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3258.3966\n",
      "[2025-11-20 06:25:49,518] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3204.5306 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:25:51,206] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3266.5977 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:26:07,007] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3257.4655\n",
      "[2025-11-20 06:27:07,748] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3205.8294 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:27:09,445] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3260.5100 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:28:26,065] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3193.7206 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:28:27,770] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3256.3934 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:28:27,808] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3256.3934\n",
      "[2025-11-20 06:29:41,099] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3185.7755 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:29:42,688] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3263.1171 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:29:58,465] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3252.9113\n",
      "[2025-11-20 06:30:43,374] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 3248.0348\n",
      "[2025-11-20 06:30:56,071] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3187.7109 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:30:57,708] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3256.6961 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:31:09,091] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3245.4709\n",
      "[2025-11-20 06:31:53,469] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3245.2338\n",
      "[2025-11-20 06:32:07,297] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3173.8286 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:32:08,943] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3250.3402 (beta=100.000, gamma=80.000)\n",
      "[2025-11-20 06:32:08,983] [UniVITrainer] [INFO] Restored best model from epoch 79 (val loss = 3245.2338)\n",
      "[2025-11-20 06:32:10,008] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 06:32:10,009] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 06:32:10,009] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 06:32:10,009] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 06:32:10,010] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 06:32:10,010] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 06:32:10,010] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 06:32:10,010] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 06:32:10,011] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 06:32:10,011] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 06:32:10,011] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 06:32:10,012] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 06:32:10,012] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 35] Done in 20.2 min\n",
      "  best_val_loss              = 3245.234\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0553\n",
      "  Label transfer (ADT→RNA)  = 0.583\n",
      "\n",
      "================================================================================\n",
      "[Config 36] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 20,\n",
      "  \"beta\": 0.0,\n",
      "  \"gamma\": 120.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bf35fbe4644661962fdb1b6f6d78f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 06:32:23,333] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3562.8566 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:32:24,719] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3240.3987 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:32:24,836] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3240.3987\n",
      "[2025-11-20 06:32:38,441] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3115.5578\n",
      "[2025-11-20 06:32:52,289] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3079.8553\n",
      "[2025-11-20 06:33:06,780] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3013.6764\n",
      "[2025-11-20 06:33:20,511] [UniVITrainer] [INFO] [Epoch 005] Train loss: 2932.4618 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:33:22,156] [UniVITrainer] [INFO] [Epoch 005] Val loss: 2962.6184 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:33:22,274] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 2962.6184\n",
      "[2025-11-20 06:33:37,581] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 2955.7588\n",
      "[2025-11-20 06:33:52,966] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 2922.7509\n",
      "[2025-11-20 06:34:22,889] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 2916.4314\n",
      "[2025-11-20 06:34:34,262] [UniVITrainer] [INFO] [Epoch 010] Train loss: 2822.2773 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:34:35,900] [UniVITrainer] [INFO] [Epoch 010] Val loss: 2901.6361 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:34:35,996] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 2901.6361\n",
      "[2025-11-20 06:34:51,400] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 2889.7629\n",
      "[2025-11-20 06:35:05,571] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 2870.3640\n",
      "[2025-11-20 06:35:19,868] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 2866.3185\n",
      "[2025-11-20 06:35:33,780] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 2852.5930\n",
      "[2025-11-20 06:35:47,550] [UniVITrainer] [INFO] [Epoch 015] Train loss: 2752.3114 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:35:49,187] [UniVITrainer] [INFO] [Epoch 015] Val loss: 2842.2520 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:35:49,302] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 2842.2520\n",
      "[2025-11-20 06:36:19,812] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 2830.0366\n",
      "[2025-11-20 06:36:34,978] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 2824.1187\n",
      "[2025-11-20 06:36:49,322] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 2803.5225\n",
      "[2025-11-20 06:37:03,006] [UniVITrainer] [INFO] [Epoch 020] Train loss: 2680.0397 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:37:04,646] [UniVITrainer] [INFO] [Epoch 020] Val loss: 2807.1809 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:37:34,704] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 2783.5065\n",
      "[2025-11-20 06:37:49,573] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 2774.6861\n",
      "[2025-11-20 06:38:02,196] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 2751.1142\n",
      "[2025-11-20 06:38:14,544] [UniVITrainer] [INFO] [Epoch 025] Train loss: 2606.7598 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:38:15,957] [UniVITrainer] [INFO] [Epoch 025] Val loss: 2747.9427 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:38:16,056] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 2747.9427\n",
      "[2025-11-20 06:39:15,563] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 2727.2750\n",
      "[2025-11-20 06:39:29,343] [UniVITrainer] [INFO] [Epoch 030] Train loss: 2544.0256 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:39:31,006] [UniVITrainer] [INFO] [Epoch 030] Val loss: 2722.6563 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:39:31,101] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 2722.6563\n",
      "[2025-11-20 06:39:46,543] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 2711.1394\n",
      "[2025-11-20 06:40:17,129] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 2697.1268\n",
      "[2025-11-20 06:40:44,930] [UniVITrainer] [INFO] [Epoch 035] Train loss: 2468.6352 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:40:46,612] [UniVITrainer] [INFO] [Epoch 035] Val loss: 2692.7576 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:40:46,633] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 2692.7576\n",
      "[2025-11-20 06:41:02,106] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 2681.1272\n",
      "[2025-11-20 06:41:17,934] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 2677.3594\n",
      "[2025-11-20 06:41:33,433] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 2675.2330\n",
      "[2025-11-20 06:41:49,314] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 2665.6817\n",
      "[2025-11-20 06:42:02,477] [UniVITrainer] [INFO] [Epoch 040] Train loss: 2396.4337 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:42:04,144] [UniVITrainer] [INFO] [Epoch 040] Val loss: 2657.3490 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:42:04,249] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 2657.3490\n",
      "[2025-11-20 06:42:19,697] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 2645.8621\n",
      "[2025-11-20 06:42:51,055] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 2641.6976\n",
      "[2025-11-20 06:43:06,473] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 2629.3646\n",
      "[2025-11-20 06:43:20,204] [UniVITrainer] [INFO] [Epoch 045] Train loss: 2335.9393 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:43:20,517] [UniVITrainer] [INFO] [Epoch 045] Val loss: 2628.3902 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:43:20,554] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 2628.3902\n",
      "[2025-11-20 06:43:52,064] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 2622.5800\n",
      "[2025-11-20 06:44:22,279] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 2618.5953\n",
      "[2025-11-20 06:44:35,471] [UniVITrainer] [INFO] [Epoch 050] Train loss: 2275.2486 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:44:37,123] [UniVITrainer] [INFO] [Epoch 050] Val loss: 2604.2105 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:44:37,174] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 2604.2105\n",
      "[2025-11-20 06:45:04,452] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 2603.2440\n",
      "[2025-11-20 06:45:19,797] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 2594.1928\n",
      "[2025-11-20 06:45:48,045] [UniVITrainer] [INFO] [Epoch 055] Train loss: 2218.5682 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:45:49,706] [UniVITrainer] [INFO] [Epoch 055] Val loss: 2583.5283 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:45:49,747] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 2583.5283\n",
      "[2025-11-20 06:46:20,314] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 2582.5586\n",
      "[2025-11-20 06:47:01,480] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2179.1885 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:47:02,040] [UniVITrainer] [INFO] [Epoch 060] Val loss: 2575.3850 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:47:02,154] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 2575.3850\n",
      "[2025-11-20 06:47:46,750] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 2569.2397\n",
      "[2025-11-20 06:48:01,888] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 2567.4230\n",
      "[2025-11-20 06:48:15,404] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2137.1035 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:48:17,045] [UniVITrainer] [INFO] [Epoch 065] Val loss: 2563.8165 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:48:17,167] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 2563.8165\n",
      "[2025-11-20 06:48:32,884] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 2562.0993\n",
      "[2025-11-20 06:48:48,356] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 2560.9033\n",
      "[2025-11-20 06:49:31,490] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2099.7823 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:49:33,143] [UniVITrainer] [INFO] [Epoch 070] Val loss: 2572.7263 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:50:03,833] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 2551.7764\n",
      "[2025-11-20 06:50:47,303] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2064.3138 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:50:48,956] [UniVITrainer] [INFO] [Epoch 075] Val loss: 2553.4428 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:51:03,477] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 2543.2391\n",
      "[2025-11-20 06:51:59,888] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2037.8016 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:52:00,980] [UniVITrainer] [INFO] [Epoch 080] Val loss: 2541.5914 (beta=0.000, gamma=120.000)\n",
      "[2025-11-20 06:52:01,084] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 2541.5914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 06:52:01,098] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 2541.5914)\n",
      "[2025-11-20 06:52:01,878] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 06:52:01,879] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 06:52:01,879] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 06:52:01,880] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 06:52:01,883] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 06:52:01,884] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 06:52:01,886] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 06:52:01,886] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 06:52:01,889] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 06:52:01,889] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 06:52:01,889] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 06:52:01,890] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 06:52:01,890] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 36] Done in 19.9 min\n",
      "  best_val_loss              = 2541.591\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0204\n",
      "  Label transfer (ADT→RNA)  = 0.838\n",
      "\n",
      "================================================================================\n",
      "[Config 37] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 60.0,\n",
      "  \"gamma\": 60.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6c29d587594dacb67f645a0dbd90a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 06:52:15,350] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4049.0467 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:52:17,008] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3793.9551 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:52:17,070] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3793.9551\n",
      "[2025-11-20 06:52:31,252] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3664.5727\n",
      "[2025-11-20 06:52:43,425] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3650.1753\n",
      "[2025-11-20 06:52:56,724] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3601.0283\n",
      "[2025-11-20 06:53:09,540] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3583.8134 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:53:11,190] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3568.2469 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:53:11,359] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3568.2469\n",
      "[2025-11-20 06:53:25,528] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3548.8606\n",
      "[2025-11-20 06:53:40,395] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3534.0130\n",
      "[2025-11-20 06:53:55,989] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3513.0532\n",
      "[2025-11-20 06:54:08,869] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3463.2365\n",
      "[2025-11-20 06:54:22,128] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3409.6197 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:54:23,792] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3411.4429 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:54:23,926] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3411.4429\n",
      "[2025-11-20 06:54:39,202] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3379.5862\n",
      "[2025-11-20 06:54:54,294] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3375.3103\n",
      "[2025-11-20 06:55:09,054] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3362.1100\n",
      "[2025-11-20 06:55:23,822] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3326.4969\n",
      "[2025-11-20 06:55:35,141] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3284.8368 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:55:36,796] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3310.1672 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:55:36,903] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3310.1672\n",
      "[2025-11-20 06:55:52,050] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3294.7186\n",
      "[2025-11-20 06:56:06,251] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3286.2483\n",
      "[2025-11-20 06:56:21,550] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3278.1027\n",
      "[2025-11-20 06:56:36,294] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3271.9379\n",
      "[2025-11-20 06:56:49,768] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3226.7378 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:56:51,435] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3262.8468 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:56:51,453] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3262.8468\n",
      "[2025-11-20 06:57:04,866] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3255.5419\n",
      "[2025-11-20 06:57:18,654] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3252.1938\n",
      "[2025-11-20 06:57:44,975] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3249.0154\n",
      "[2025-11-20 06:57:57,867] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3199.9636 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:57:59,533] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3243.5979 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:57:59,680] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3243.5979\n",
      "[2025-11-20 06:58:27,472] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3241.9609\n",
      "[2025-11-20 06:58:42,096] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3235.1683\n",
      "[2025-11-20 06:58:56,043] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3228.3948\n",
      "[2025-11-20 06:59:09,382] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3181.7610 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:59:11,076] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3232.7371 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 06:59:25,745] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3228.3216\n",
      "[2025-11-20 06:59:54,541] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3222.8541\n",
      "[2025-11-20 07:00:09,560] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3219.5465\n",
      "[2025-11-20 07:00:23,613] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3165.2979 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:00:25,276] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3218.2277 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:00:25,395] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3218.2277\n",
      "[2025-11-20 07:00:39,350] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3214.7534\n",
      "[2025-11-20 07:00:54,736] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3210.7149\n",
      "[2025-11-20 07:01:09,020] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3199.3386\n",
      "[2025-11-20 07:01:37,984] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3137.0478 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:01:39,674] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3195.7850 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:01:39,686] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3195.7850\n",
      "[2025-11-20 07:01:55,056] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3190.5735\n",
      "[2025-11-20 07:02:41,927] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3185.7654\n",
      "[2025-11-20 07:02:55,106] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3121.5779 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:02:56,801] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3187.8283 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:03:12,567] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3185.4772\n",
      "[2025-11-20 07:03:27,706] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3184.4874\n",
      "[2025-11-20 07:03:43,437] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3178.3778\n",
      "[2025-11-20 07:03:59,356] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3170.9469\n",
      "[2025-11-20 07:04:13,131] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3098.5479 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:04:14,797] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3174.5648 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:04:30,133] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3169.2597\n",
      "[2025-11-20 07:04:59,214] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3168.3867\n",
      "[2025-11-20 07:05:13,692] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3167.3567\n",
      "[2025-11-20 07:05:24,942] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3085.9375 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:05:26,589] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3164.2048 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:05:26,601] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3164.2048\n",
      "[2025-11-20 07:05:56,841] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3161.2824\n",
      "[2025-11-20 07:06:12,284] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3159.8118\n",
      "[2025-11-20 07:06:27,109] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3157.5332\n",
      "[2025-11-20 07:06:38,753] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3076.3727 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:06:40,403] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3158.5636 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:07:09,378] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3155.5952\n",
      "[2025-11-20 07:07:38,888] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3154.1812\n",
      "[2025-11-20 07:07:51,907] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3069.7869 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:07:53,558] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3156.4743 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:08:08,619] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3152.1702\n",
      "[2025-11-20 07:08:23,495] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3150.0198\n",
      "[2025-11-20 07:08:52,437] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3146.2885\n",
      "[2025-11-20 07:09:06,190] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3057.5036 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:09:07,833] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3146.4020 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:09:51,212] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3140.3183\n",
      "[2025-11-20 07:10:19,406] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3050.3903 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:10:20,542] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3150.1612 (beta=60.000, gamma=60.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 07:10:49,999] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3137.4210\n",
      "[2025-11-20 07:11:31,042] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3041.0228 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:11:32,708] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3138.8872 (beta=60.000, gamma=60.000)\n",
      "[2025-11-20 07:11:32,728] [UniVITrainer] [INFO] Restored best model from epoch 77 (val loss = 3137.4210)\n",
      "[2025-11-20 07:11:33,560] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 07:11:33,562] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 07:11:33,563] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 07:11:33,565] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 07:11:33,567] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 07:11:33,569] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 07:11:33,569] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 07:11:33,569] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 07:11:33,571] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 07:11:33,573] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 07:11:33,573] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 07:11:33,574] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 07:11:33,575] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 37] Done in 19.5 min\n",
      "  best_val_loss              = 3137.421\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0525\n",
      "  Label transfer (ADT→RNA)  = 0.626\n",
      "\n",
      "================================================================================\n",
      "[Config 38] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 32,\n",
      "  \"beta\": 80.0,\n",
      "  \"gamma\": 120.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab26ba58ec524d56bd3e06d331bcc982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 07:11:44,794] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4074.4498 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:11:46,464] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3779.1518 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:11:46,499] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3779.1518\n",
      "[2025-11-20 07:12:01,575] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3665.7632\n",
      "[2025-11-20 07:12:15,196] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3632.2140\n",
      "[2025-11-20 07:12:29,463] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3592.9293\n",
      "[2025-11-20 07:12:42,867] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3550.0696 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:12:44,534] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3532.8425 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:12:44,648] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3532.8425\n",
      "[2025-11-20 07:12:59,711] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3501.9810\n",
      "[2025-11-20 07:13:14,932] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3471.8548\n",
      "[2025-11-20 07:13:29,242] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3441.4497\n",
      "[2025-11-20 07:13:43,694] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3419.6460\n",
      "[2025-11-20 07:13:56,643] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3372.9874 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:13:58,303] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3394.8192 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:13:58,402] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3394.8192\n",
      "[2025-11-20 07:14:41,903] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3376.9171\n",
      "[2025-11-20 07:15:07,826] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3321.0170 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:15:09,496] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3360.7233 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:15:09,559] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3360.7233\n",
      "[2025-11-20 07:15:24,214] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3341.8969\n",
      "[2025-11-20 07:15:54,785] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3341.1773\n",
      "[2025-11-20 07:16:10,192] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3333.0816\n",
      "[2025-11-20 07:16:21,944] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3285.2017 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:16:23,609] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3330.1643 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:16:23,707] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3330.1643\n",
      "[2025-11-20 07:16:39,085] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3321.9314\n",
      "[2025-11-20 07:16:54,641] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3316.9241\n",
      "[2025-11-20 07:17:39,034] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3258.6154 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:17:40,686] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3321.7219 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:17:55,820] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3310.8853\n",
      "[2025-11-20 07:18:24,922] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3302.7799\n",
      "[2025-11-20 07:18:38,871] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3292.8451\n",
      "[2025-11-20 07:18:51,938] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3235.2845 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:18:53,585] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3277.5117 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:18:53,620] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3277.5117\n",
      "[2025-11-20 07:19:08,726] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3277.2022\n",
      "[2025-11-20 07:19:24,264] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3272.6356\n",
      "[2025-11-20 07:19:39,175] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3264.1345\n",
      "[2025-11-20 07:20:06,486] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3195.8044 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:20:08,150] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3259.3711 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:20:08,264] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3259.3711\n",
      "[2025-11-20 07:20:21,736] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3253.1037\n",
      "[2025-11-20 07:21:02,649] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3246.5568\n",
      "[2025-11-20 07:21:16,000] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3177.3888 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:21:16,760] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3241.2806 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:21:16,863] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3241.2806\n",
      "[2025-11-20 07:21:46,054] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3235.2555\n",
      "[2025-11-20 07:22:30,402] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3164.0574 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:22:32,065] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3251.8050 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:23:33,083] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3233.9049\n",
      "[2025-11-20 07:23:46,531] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3146.7036 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:23:48,181] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3234.0043 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:24:34,340] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3227.1884\n",
      "[2025-11-20 07:24:47,014] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3226.9306\n",
      "[2025-11-20 07:24:59,730] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3139.4836 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:25:00,489] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3232.5674 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:25:15,324] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3225.5343\n",
      "[2025-11-20 07:25:29,996] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3225.1163\n",
      "[2025-11-20 07:25:45,555] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3224.1751\n",
      "[2025-11-20 07:26:00,696] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3216.2556\n",
      "[2025-11-20 07:26:14,602] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3125.2830 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:26:16,270] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3213.0985 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:26:16,368] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3213.0985\n",
      "[2025-11-20 07:27:37,198] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3117.9871 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:27:39,420] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3224.7553 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:28:00,129] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3210.9375\n",
      "[2025-11-20 07:28:41,333] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3210.1201\n",
      "[2025-11-20 07:29:20,038] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3105.5760 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:29:22,276] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3211.7305 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:29:42,952] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3206.2314\n",
      "[2025-11-20 07:31:02,373] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3098.1860 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:31:04,600] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3206.8767 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:31:24,715] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3202.2524\n",
      "[2025-11-20 07:32:42,914] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3089.4005 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:32:45,093] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3205.7742 (beta=80.000, gamma=120.000)\n",
      "[2025-11-20 07:32:45,139] [UniVITrainer] [INFO] Restored best model from epoch 76 (val loss = 3202.2524)\n",
      "[2025-11-20 07:32:45,960] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 07:32:45,961] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 07:32:45,962] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 07:32:45,962] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 07:32:45,963] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 07:32:45,963] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 07:32:45,964] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 07:32:45,964] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 07:32:45,965] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 07:32:45,966] [UniVITrainer] [INFO]   seed: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 07:32:45,966] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 07:32:45,967] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 07:32:45,967] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 38] Done in 21.2 min\n",
      "  best_val_loss              = 3202.252\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0744\n",
      "  Label transfer (ADT→RNA)  = 0.562\n",
      "\n",
      "================================================================================\n",
      "[Config 39] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 100.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1632b0b33d48e88f13ec3051260060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 07:33:03,924] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4154.6143 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:33:06,109] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3849.1444 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:33:06,322] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3849.1444\n",
      "[2025-11-20 07:33:26,641] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3733.4798\n",
      "[2025-11-20 07:33:46,978] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3716.0802\n",
      "[2025-11-20 07:34:24,091] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3711.0792 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:34:25,870] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3712.4902 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:34:26,091] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3712.4902\n",
      "[2025-11-20 07:35:24,976] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3710.2363\n",
      "[2025-11-20 07:35:45,212] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3705.7184\n",
      "[2025-11-20 07:36:02,282] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3692.1602 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:36:04,463] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3690.2600 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:36:04,691] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3690.2600\n",
      "[2025-11-20 07:36:42,310] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3689.7078\n",
      "[2025-11-20 07:37:02,567] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3686.2585\n",
      "[2025-11-20 07:37:22,727] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3673.8817\n",
      "[2025-11-20 07:37:40,516] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3650.9405 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:37:42,170] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3663.8424 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:37:42,380] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3663.8424\n",
      "[2025-11-20 07:38:00,537] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3631.0010\n",
      "[2025-11-20 07:38:20,633] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3615.1523\n",
      "[2025-11-20 07:38:40,972] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3610.5605\n",
      "[2025-11-20 07:39:01,258] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3605.6896\n",
      "[2025-11-20 07:39:19,083] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3577.3023 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:39:21,359] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3605.2621 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:39:21,578] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3605.2621\n",
      "[2025-11-20 07:39:41,860] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3597.1281\n",
      "[2025-11-20 07:40:01,964] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3594.0725\n",
      "[2025-11-20 07:40:59,571] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3561.6026 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:41:01,742] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3584.7623 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:41:01,960] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3584.7623\n",
      "[2025-11-20 07:41:22,402] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3581.6523\n",
      "[2025-11-20 07:42:22,662] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3568.0532\n",
      "[2025-11-20 07:42:40,554] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3530.0875 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:42:42,598] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3546.0472 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:42:42,826] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3546.0472\n",
      "[2025-11-20 07:43:03,095] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3492.6434\n",
      "[2025-11-20 07:43:43,405] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3482.3530\n",
      "[2025-11-20 07:44:21,096] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3442.3170 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:44:23,268] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3492.9377 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:44:43,524] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3476.1833\n",
      "[2025-11-20 07:45:03,493] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3469.8521\n",
      "[2025-11-20 07:45:23,730] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3466.8161\n",
      "[2025-11-20 07:46:01,550] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3426.1157 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:46:03,752] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3471.5540 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:47:03,713] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3462.6262\n",
      "[2025-11-20 07:47:41,375] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3420.8246 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:47:43,543] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3464.0638 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:48:57,509] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3420.0206 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:48:59,108] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3476.8140 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:49:58,793] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3462.0967\n",
      "[2025-11-20 07:50:12,285] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3411.5726 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:50:13,894] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3466.4871 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:51:25,893] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3405.9984 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:51:26,669] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3465.6777 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:52:01,371] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3459.9228\n",
      "[2025-11-20 07:52:10,180] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3403.8187 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:52:11,217] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3468.0999 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:52:59,007] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3403.5983 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:53:00,040] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3455.2605 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:53:00,078] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3455.2605\n",
      "[2025-11-20 07:53:46,866] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3404.6319 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:53:47,907] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3465.1088 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:54:36,008] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3395.8626 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:54:37,000] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3470.4212 (beta=160.000, gamma=100.000)\n",
      "[2025-11-20 07:54:37,024] [UniVITrainer] [INFO] Restored best model from epoch 70 (val loss = 3455.2605)\n",
      "[2025-11-20 07:54:37,831] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 07:54:37,832] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 07:54:37,833] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 07:54:37,834] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 07:54:37,835] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 07:54:37,836] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 07:54:37,836] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 07:54:37,837] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 07:54:37,838] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 07:54:37,838] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 07:54:37,839] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 07:54:37,840] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 07:54:37,840] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 39] Done in 21.9 min\n",
      "  best_val_loss              = 3455.260\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1457\n",
      "  Label transfer (ADT→RNA)  = 0.342\n",
      "\n",
      "================================================================================\n",
      "[Config 40] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 40.0,\n",
      "  \"gamma\": 100.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24990942573c44b589df9a243b479dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 07:54:46,838] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4406.6582 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:54:47,859] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3774.9172 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:54:47,888] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3774.9172\n",
      "[2025-11-20 07:54:53,279] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3582.7003\n",
      "[2025-11-20 07:54:59,004] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3489.3794\n",
      "[2025-11-20 07:55:08,759] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3445.7853\n",
      "[2025-11-20 07:55:17,701] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3448.5519 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:55:18,733] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3415.7206 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:55:18,783] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3415.7206\n",
      "[2025-11-20 07:55:28,683] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3390.0427\n",
      "[2025-11-20 07:55:38,527] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3369.2452\n",
      "[2025-11-20 07:55:48,205] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3349.4786\n",
      "[2025-11-20 07:55:57,566] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3325.5607\n",
      "[2025-11-20 07:56:05,519] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3305.9359 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:56:06,527] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3308.4304 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:56:06,564] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3308.4304\n",
      "[2025-11-20 07:56:15,903] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3294.2614\n",
      "[2025-11-20 07:56:25,394] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3273.7337\n",
      "[2025-11-20 07:56:35,177] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3257.9603\n",
      "[2025-11-20 07:56:44,982] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3255.9784\n",
      "[2025-11-20 07:56:53,267] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3224.0391 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:56:54,274] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3241.3325 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:56:54,494] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3241.3325\n",
      "[2025-11-20 07:57:04,625] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3223.3178\n",
      "[2025-11-20 07:57:14,483] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3211.5404\n",
      "[2025-11-20 07:57:24,370] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3206.3337\n",
      "[2025-11-20 07:57:33,862] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3191.8302\n",
      "[2025-11-20 07:57:42,737] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3165.7231 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:57:43,763] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3199.1613 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:57:53,738] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3187.6679\n",
      "[2025-11-20 07:58:03,708] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3186.9512\n",
      "[2025-11-20 07:58:13,655] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3174.0931\n",
      "[2025-11-20 07:58:32,216] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3140.1380 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:58:33,641] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3166.2043 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:58:33,867] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3166.2043\n",
      "[2025-11-20 07:58:48,240] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3158.4488\n",
      "[2025-11-20 07:59:32,223] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3148.9732\n",
      "[2025-11-20 07:59:45,083] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3106.2153 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:59:45,997] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3146.1321 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 07:59:46,240] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3146.1321\n",
      "[2025-11-20 08:00:01,600] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3137.4692\n",
      "[2025-11-20 08:00:15,040] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3133.6909\n",
      "[2025-11-20 08:00:25,174] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3133.3398\n",
      "[2025-11-20 08:00:38,096] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3127.2232\n",
      "[2025-11-20 08:00:51,219] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3069.6683 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:00:52,822] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3129.1184 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:01:08,052] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3117.7507\n",
      "[2025-11-20 08:01:45,818] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3114.2226\n",
      "[2025-11-20 08:01:57,584] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3051.5375 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:01:59,191] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3111.0903 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:01:59,420] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3111.0903\n",
      "[2025-11-20 08:02:14,206] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3109.1646\n",
      "[2025-11-20 08:02:26,677] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3102.9712\n",
      "[2025-11-20 08:03:01,451] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3035.3981 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:03:02,994] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3102.5096 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:03:03,221] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3102.5096\n",
      "[2025-11-20 08:03:18,229] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3100.6203\n",
      "[2025-11-20 08:03:29,040] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3094.0512\n",
      "[2025-11-20 08:04:10,686] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3013.2193 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:04:12,297] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3095.8473 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:04:26,690] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3085.9275\n",
      "[2025-11-20 08:04:41,174] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3084.3824\n",
      "[2025-11-20 08:04:56,119] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3082.7390\n",
      "[2025-11-20 08:05:25,241] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3004.1040 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:05:26,602] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3079.2629 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:05:26,857] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3079.2629\n",
      "[2025-11-20 08:06:23,156] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3073.2744\n",
      "[2025-11-20 08:06:41,626] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3072.7990\n",
      "[2025-11-20 08:06:57,375] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2988.0453 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:06:59,474] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3090.4242 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:07:17,925] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3068.6267\n",
      "[2025-11-20 08:08:28,585] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2975.3503 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:08:30,766] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3062.8228 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:08:31,005] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3062.8228\n",
      "[2025-11-20 08:08:50,090] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3061.1059\n",
      "[2025-11-20 08:09:26,129] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3058.4282\n",
      "[2025-11-20 08:09:44,025] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3056.3758\n",
      "[2025-11-20 08:09:59,777] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2960.5049 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:10:01,959] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3063.0100 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:10:38,316] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3054.7852\n",
      "[2025-11-20 08:11:32,225] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2951.7412 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:11:33,643] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3055.5691 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:11:52,519] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3052.9141\n",
      "[2025-11-20 08:12:12,424] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3050.5903\n",
      "[2025-11-20 08:12:51,781] [UniVITrainer] [INFO] [Epoch 079] New best val loss: 3047.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 08:13:08,480] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2942.0595 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:13:10,652] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3050.7066 (beta=40.000, gamma=100.000)\n",
      "[2025-11-20 08:13:10,698] [UniVITrainer] [INFO] Restored best model from epoch 79 (val loss = 3047.6399)\n",
      "[2025-11-20 08:13:11,677] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 08:13:11,678] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 08:13:11,679] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 08:13:11,679] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 08:13:11,680] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 08:13:11,680] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 08:13:11,681] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 08:13:11,681] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 08:13:11,682] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 08:13:11,682] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 08:13:11,683] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 08:13:11,683] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 08:13:11,684] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 40] Done in 18.5 min\n",
      "  best_val_loss              = 3047.640\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0346\n",
      "  Label transfer (ADT→RNA)  = 0.695\n",
      "\n",
      "================================================================================\n",
      "[Config 41] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 160.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1082a300b93f42b48ebb6a019bbff0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 08:13:28,369] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4189.1788 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:13:30,551] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3804.1705 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:13:30,730] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3804.1705\n",
      "[2025-11-20 08:13:50,643] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3710.3768\n",
      "[2025-11-20 08:14:09,700] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3705.3715\n",
      "[2025-11-20 08:14:47,288] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3688.7230 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:14:49,282] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3684.3351 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:14:49,464] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3684.3351\n",
      "[2025-11-20 08:15:28,756] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3626.2982\n",
      "[2025-11-20 08:15:48,481] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3588.7909\n",
      "[2025-11-20 08:16:25,387] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3577.2077 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:16:27,564] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3585.3240 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:16:27,709] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3585.3240\n",
      "[2025-11-20 08:16:47,590] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3572.2499\n",
      "[2025-11-20 08:17:07,658] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3558.4529\n",
      "[2025-11-20 08:18:04,417] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3529.5811 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:18:06,286] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3553.8750 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:18:06,452] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3553.8750\n",
      "[2025-11-20 08:18:25,676] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3540.2219\n",
      "[2025-11-20 08:19:03,407] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3535.0595\n",
      "[2025-11-20 08:19:17,521] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3528.1617\n",
      "[2025-11-20 08:19:28,827] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3485.3649 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:19:30,420] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3476.5341 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:19:30,580] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3476.5341\n",
      "[2025-11-20 08:19:45,338] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3446.6117\n",
      "[2025-11-20 08:20:00,405] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3442.6088\n",
      "[2025-11-20 08:20:30,564] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3433.6730\n",
      "[2025-11-20 08:20:43,202] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3396.4229 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:20:44,816] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3420.5261 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:20:44,850] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3420.5261\n",
      "[2025-11-20 08:21:30,434] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3416.8257\n",
      "[2025-11-20 08:21:58,766] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3370.4609 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:22:00,366] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3414.4133 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:22:00,517] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3414.4133\n",
      "[2025-11-20 08:22:15,661] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3413.6552\n",
      "[2025-11-20 08:22:30,750] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3401.7701\n",
      "[2025-11-20 08:23:14,400] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3353.9388 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:23:16,013] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3395.4592 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:23:16,152] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3395.4592\n",
      "[2025-11-20 08:23:30,755] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3391.3132\n",
      "[2025-11-20 08:23:45,960] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3381.4504\n",
      "[2025-11-20 08:24:15,932] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3377.6554\n",
      "[2025-11-20 08:24:29,406] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3324.6884 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:24:31,017] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3380.2776 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:25:44,187] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3314.6130 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:25:45,786] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3375.7773 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:25:45,924] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3375.7773\n",
      "[2025-11-20 08:26:31,218] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3363.5860\n",
      "[2025-11-20 08:26:59,440] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3308.2061 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:27:01,049] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3377.5051 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:28:14,543] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3297.0559 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:28:16,152] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3363.3414 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:28:16,290] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 3363.3414\n",
      "[2025-11-20 08:28:46,612] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 3362.1372\n",
      "[2025-11-20 08:29:16,200] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3354.5862\n",
      "[2025-11-20 08:29:29,692] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3281.6701 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:29:31,302] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3360.0334 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:30:11,798] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3352.1380\n",
      "[2025-11-20 08:30:26,370] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3345.9093\n",
      "[2025-11-20 08:30:39,226] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3273.0760 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:30:40,830] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3352.1927 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:30:55,670] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3341.5182\n",
      "[2025-11-20 08:31:39,693] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3340.2591\n",
      "[2025-11-20 08:31:53,181] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3262.8102 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:31:54,791] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3347.3001 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:32:10,018] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3339.5920\n",
      "[2025-11-20 08:32:38,617] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3336.6067\n",
      "[2025-11-20 08:33:06,949] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3254.9628 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:33:08,562] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3341.0412 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:33:53,755] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3332.3021\n",
      "[2025-11-20 08:34:21,049] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3252.4202 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:34:22,663] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3347.2099 (beta=120.000, gamma=160.000)\n",
      "[2025-11-20 08:34:22,694] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3332.3021)\n",
      "[2025-11-20 08:34:23,523] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 08:34:23,524] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 08:34:23,525] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 08:34:23,526] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 08:34:23,526] [UniVITrainer] [INFO]   weight_decay: 0.0001\n",
      "[2025-11-20 08:34:23,527] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 08:34:23,527] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 08:34:23,528] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 08:34:23,528] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 08:34:23,528] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 08:34:23,529] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 08:34:23,529] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 08:34:23,530] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 41] Done in 21.2 min\n",
      "  best_val_loss              = 3332.302\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0948\n",
      "  Label transfer (ADT→RNA)  = 0.463\n",
      "\n",
      "================================================================================\n",
      "[Config 42] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 20.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30c79d84ee64b428937aa436cce0fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 08:34:36,996] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4232.5142 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:34:38,601] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3873.7020 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:34:38,803] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3873.7020\n",
      "[2025-11-20 08:34:53,259] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3724.1721\n",
      "[2025-11-20 08:35:08,412] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3645.8405\n",
      "[2025-11-20 08:35:23,701] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3624.8479\n",
      "[2025-11-20 08:35:37,083] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3597.8732 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:35:38,693] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3597.2312 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:35:38,911] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3597.2312\n",
      "[2025-11-20 08:35:53,959] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3570.6641\n",
      "[2025-11-20 08:36:09,214] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3547.3506\n",
      "[2025-11-20 08:36:24,191] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3521.6256\n",
      "[2025-11-20 08:36:38,511] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3515.3125\n",
      "[2025-11-20 08:36:51,647] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3481.9905 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:36:52,410] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3505.9717 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:36:52,431] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3505.9717\n",
      "[2025-11-20 08:36:58,477] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3488.2844\n",
      "[2025-11-20 08:37:13,433] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3481.6623\n",
      "[2025-11-20 08:37:28,706] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3481.1200\n",
      "[2025-11-20 08:37:43,731] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3478.3872\n",
      "[2025-11-20 08:37:56,850] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3435.8277 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:37:58,444] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3456.0958 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:37:58,751] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3456.0958\n",
      "[2025-11-20 08:38:13,741] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3448.5323\n",
      "[2025-11-20 08:38:28,703] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3433.7665\n",
      "[2025-11-20 08:38:58,062] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3423.5546\n",
      "[2025-11-20 08:39:11,276] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3397.9842 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:39:12,887] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3423.1292 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:39:13,088] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3423.1292\n",
      "[2025-11-20 08:39:28,070] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3419.3887\n",
      "[2025-11-20 08:39:58,076] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3406.3134\n",
      "[2025-11-20 08:40:08,118] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3368.0482 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:40:08,447] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3405.7925 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:40:08,643] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3405.7925\n",
      "[2025-11-20 08:40:13,429] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3399.2523\n",
      "[2025-11-20 08:40:20,222] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3390.2845\n",
      "[2025-11-20 08:40:31,128] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3386.2930\n",
      "[2025-11-20 08:40:35,183] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3342.7155 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:40:36,068] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3385.2214 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:40:36,420] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3385.2214\n",
      "[2025-11-20 08:40:42,989] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3377.3206\n",
      "[2025-11-20 08:41:12,137] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3372.6310\n",
      "[2025-11-20 08:41:20,336] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3324.4958 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:41:21,341] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3364.7918 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:41:21,384] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3364.7918\n",
      "[2025-11-20 08:41:41,038] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3359.6726\n",
      "[2025-11-20 08:42:09,204] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3304.4594 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:42:10,220] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3359.8931 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:42:20,045] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3350.2010\n",
      "[2025-11-20 08:42:38,934] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3346.3934\n",
      "[2025-11-20 08:42:48,313] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 3346.0805\n",
      "[2025-11-20 08:42:56,822] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3292.9818 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:42:57,854] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3343.7803 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:42:57,881] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3343.7803\n",
      "[2025-11-20 08:43:17,451] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3342.5939\n",
      "[2025-11-20 08:43:36,990] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3340.0506\n",
      "[2025-11-20 08:43:45,566] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3279.6461 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:43:46,573] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3339.9953 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:43:46,616] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3339.9953\n",
      "[2025-11-20 08:43:56,135] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3334.9702\n",
      "[2025-11-20 08:44:05,908] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3333.8664\n",
      "[2025-11-20 08:44:25,123] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3327.8267\n",
      "[2025-11-20 08:44:33,844] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3267.7636 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:44:34,848] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3332.3004 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:44:44,517] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3325.3092\n",
      "[2025-11-20 08:45:03,712] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3321.7236\n",
      "[2025-11-20 08:45:13,572] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3319.2150\n",
      "[2025-11-20 08:45:22,218] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3256.2617 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:45:23,248] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3316.5744 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:45:23,269] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3316.5744\n",
      "[2025-11-20 08:45:33,048] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3309.7690\n",
      "[2025-11-20 08:45:42,677] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3309.5558\n",
      "[2025-11-20 08:45:52,390] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3309.3592\n",
      "[2025-11-20 08:46:01,630] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3306.7153\n",
      "[2025-11-20 08:46:09,202] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3248.6323 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:46:10,183] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3301.9376 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:46:10,231] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3301.9376\n",
      "[2025-11-20 08:46:57,343] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3235.0725 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:46:58,350] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3305.4111 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:47:18,031] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3300.2564\n",
      "[2025-11-20 08:47:27,753] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3298.4289\n",
      "[2025-11-20 08:47:46,342] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3234.9267 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:47:47,358] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3301.0518 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:48:06,664] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3293.4174\n",
      "[2025-11-20 08:48:43,536] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3225.4640 (beta=120.000, gamma=20.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 08:48:43,927] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3292.2677 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 08:48:44,167] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 3292.2677\n",
      "[2025-11-20 08:48:44,200] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 3292.2677)\n",
      "[2025-11-20 08:48:45,138] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 08:48:45,139] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 08:48:45,140] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 08:48:45,146] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 08:48:45,150] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 08:48:45,151] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 08:48:45,158] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 08:48:45,159] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 08:48:45,159] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 08:48:45,160] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 08:48:45,161] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 08:48:45,161] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 08:48:45,162] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 42] Done in 14.3 min\n",
      "  best_val_loss              = 3292.268\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0915\n",
      "  Label transfer (ADT→RNA)  = 0.534\n",
      "\n",
      "================================================================================\n",
      "[Config 43] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 80.0,\n",
      "  \"gamma\": 100.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1173e17221402687aedff0187a073b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 08:48:58,928] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4258.8784 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:49:00,559] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3791.3635 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:49:00,631] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3791.3635\n",
      "[2025-11-20 08:49:14,795] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3673.5250\n",
      "[2025-11-20 08:49:25,658] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3569.4756\n",
      "[2025-11-20 08:49:39,431] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3527.0475\n",
      "[2025-11-20 08:49:52,401] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3486.6292 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:49:54,030] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3465.2187 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:49:54,158] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3465.2187\n",
      "[2025-11-20 08:50:08,266] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3433.9570\n",
      "[2025-11-20 08:50:22,251] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3382.2348\n",
      "[2025-11-20 08:50:35,434] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3363.3090\n",
      "[2025-11-20 08:50:49,808] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3352.3648\n",
      "[2025-11-20 08:51:03,482] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3321.6532 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:51:04,722] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3344.9373 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:51:04,867] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3344.9373\n",
      "[2025-11-20 08:51:19,119] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3337.3403\n",
      "[2025-11-20 08:51:34,295] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3320.4238\n",
      "[2025-11-20 08:51:47,978] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3309.3845\n",
      "[2025-11-20 08:52:15,709] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3268.5511 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:52:17,345] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3306.5449 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:52:17,454] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3306.5449\n",
      "[2025-11-20 08:52:30,593] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3295.5639\n",
      "[2025-11-20 08:52:44,749] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3293.1371\n",
      "[2025-11-20 08:52:59,954] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3290.8852\n",
      "[2025-11-20 08:53:27,546] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3241.2772 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:53:29,178] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3285.0423 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:53:29,294] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3285.0423\n",
      "[2025-11-20 08:53:44,632] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3280.5764\n",
      "[2025-11-20 08:53:58,999] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3279.9072\n",
      "[2025-11-20 08:54:13,989] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3279.3339\n",
      "[2025-11-20 08:54:42,297] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3229.7552 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:54:43,877] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3272.1465 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:54:44,021] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3272.1465\n",
      "[2025-11-20 08:54:57,547] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3268.5944\n",
      "[2025-11-20 08:55:31,799] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3262.4807\n",
      "[2025-11-20 08:55:43,778] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3213.3783 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:55:45,407] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3269.3617 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:55:59,382] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3262.4044\n",
      "[2025-11-20 08:56:28,174] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3257.1779\n",
      "[2025-11-20 08:56:40,557] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3254.4381\n",
      "[2025-11-20 08:56:53,048] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3200.2737 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:56:54,181] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3255.1223 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:57:08,871] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3253.2815\n",
      "[2025-11-20 08:57:22,287] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3242.4224\n",
      "[2025-11-20 08:57:50,248] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3241.9527\n",
      "[2025-11-20 08:58:02,552] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3178.0873 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:58:04,188] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3236.1594 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:58:04,284] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3236.1594\n",
      "[2025-11-20 08:58:19,185] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3233.4313\n",
      "[2025-11-20 08:58:33,593] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3227.7357\n",
      "[2025-11-20 08:58:46,616] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3221.9452\n",
      "[2025-11-20 08:59:08,025] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3162.2747 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:59:09,392] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3231.0146 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 08:59:23,133] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3221.3163\n",
      "[2025-11-20 08:59:37,955] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3218.3516\n",
      "[2025-11-20 09:00:20,461] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3158.7930 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:00:22,099] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3223.5653 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:01:15,894] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3147.7716 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:01:17,529] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3219.8043 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:01:25,898] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3214.3843\n",
      "[2025-11-20 09:02:08,656] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3214.3128\n",
      "[2025-11-20 09:02:21,695] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3136.3982 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:02:22,642] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3211.0941 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:02:22,764] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3211.0941\n",
      "[2025-11-20 09:02:56,411] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 3211.0322\n",
      "[2025-11-20 09:03:20,553] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3137.1169 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:03:21,897] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3217.1865 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:04:02,669] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3208.7982\n",
      "[2025-11-20 09:04:23,436] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3126.7142 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:04:24,848] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3212.1086 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:04:40,734] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 3207.3000\n",
      "[2025-11-20 09:05:33,319] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3127.4254 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:05:34,323] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3206.1211 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:05:34,457] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 3206.1211\n",
      "[2025-11-20 09:05:59,537] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3204.0150\n",
      "[2025-11-20 09:06:15,961] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3121.7985 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:06:16,377] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3207.9863 (beta=80.000, gamma=100.000)\n",
      "[2025-11-20 09:06:16,403] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3204.0150)\n",
      "[2025-11-20 09:06:17,232] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 09:06:17,233] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 09:06:17,233] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 09:06:17,234] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 09:06:17,234] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 09:06:17,235] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 09:06:17,235] [UniVITrainer] [INFO]   log_every: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 09:06:17,236] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 09:06:17,236] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 09:06:17,237] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 09:06:17,238] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 09:06:17,238] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 09:06:17,239] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 43] Done in 17.5 min\n",
      "  best_val_loss              = 3204.015\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0646\n",
      "  Label transfer (ADT→RNA)  = 0.597\n",
      "\n",
      "================================================================================\n",
      "[Config 44] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 20,\n",
      "  \"beta\": 80.0,\n",
      "  \"gamma\": 160.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b542a64d508a4a28852ae0ce3a8e285c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 09:06:25,051] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3886.4463 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:06:26,070] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3699.5335 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:06:26,123] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3699.5335\n",
      "[2025-11-20 09:06:35,778] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3628.5558\n",
      "[2025-11-20 09:06:54,528] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3547.0303\n",
      "[2025-11-20 09:07:03,183] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3534.5425 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:07:04,178] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3524.8291 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:07:04,218] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3524.8291\n",
      "[2025-11-20 09:07:13,840] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3443.8655\n",
      "[2025-11-20 09:07:23,407] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3440.6350\n",
      "[2025-11-20 09:07:33,006] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3421.8369\n",
      "[2025-11-20 09:07:51,254] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3389.4021 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:07:52,263] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3490.9609 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:08:02,053] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3396.4248\n",
      "[2025-11-20 09:08:31,007] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3382.5229\n",
      "[2025-11-20 09:08:39,485] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3343.3208 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:08:40,503] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3386.6811 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:08:50,125] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3375.5082\n",
      "[2025-11-20 09:09:09,364] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3357.9742\n",
      "[2025-11-20 09:09:18,918] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3356.4434\n",
      "[2025-11-20 09:09:27,489] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3282.0823 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:09:28,469] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3344.6150 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:09:28,635] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3344.6150\n",
      "[2025-11-20 09:09:30,261] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3329.9068\n",
      "[2025-11-20 09:09:48,614] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3319.6771\n",
      "[2025-11-20 09:10:06,236] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3253.8320 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:10:07,260] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3320.9438 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:10:16,800] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3306.9915\n",
      "[2025-11-20 09:10:35,637] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3306.0876\n",
      "[2025-11-20 09:10:53,989] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3244.4626 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:10:54,985] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3313.3870 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:11:04,555] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 3302.0829\n",
      "[2025-11-20 09:11:13,937] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3294.8756\n",
      "[2025-11-20 09:11:22,590] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3288.3031\n",
      "[2025-11-20 09:11:32,364] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3281.7103\n",
      "[2025-11-20 09:11:40,837] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3212.8758 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:11:41,860] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3279.6591 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:11:41,907] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3279.6591\n",
      "[2025-11-20 09:12:08,404] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3271.2922\n",
      "[2025-11-20 09:12:25,668] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3202.6694 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:12:26,664] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3271.2199 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:12:26,691] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3271.2199\n",
      "[2025-11-20 09:12:35,617] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3266.4093\n",
      "[2025-11-20 09:12:44,704] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3259.4981\n",
      "[2025-11-20 09:13:10,293] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3184.2583 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:13:11,306] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3257.4159 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:13:11,347] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3257.4159\n",
      "[2025-11-20 09:13:30,362] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 3250.5061\n",
      "[2025-11-20 09:13:57,998] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3167.1940 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:13:58,998] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3254.3966 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:14:08,577] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3248.6060\n",
      "[2025-11-20 09:14:18,121] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3244.2534\n",
      "[2025-11-20 09:14:27,881] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3243.9836\n",
      "[2025-11-20 09:14:37,561] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3242.6583\n",
      "[2025-11-20 09:14:46,128] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3148.4333 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:14:47,153] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3244.0878 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:15:14,837] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3237.4353\n",
      "[2025-11-20 09:15:33,061] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3137.6252 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:15:34,082] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3242.3510 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:15:43,764] [UniVITrainer] [INFO] [Epoch 061] New best val loss: 3231.3570\n",
      "[2025-11-20 09:15:53,509] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3229.5865\n",
      "[2025-11-20 09:16:21,516] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3131.3815 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:16:22,525] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3252.5566 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:16:41,580] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 3227.8076\n",
      "[2025-11-20 09:17:00,846] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3227.1456\n",
      "[2025-11-20 09:17:09,461] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3117.2654 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:17:10,433] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3221.4019 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:17:10,458] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3221.4019\n",
      "[2025-11-20 09:17:29,775] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 3221.0940\n",
      "[2025-11-20 09:18:04,295] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3110.5346 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:18:05,742] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3225.7927 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:19:16,943] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3103.7277 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:19:18,332] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3223.1692 (beta=80.000, gamma=160.000)\n",
      "[2025-11-20 09:19:18,391] [UniVITrainer] [INFO] Restored best model from epoch 72 (val loss = 3221.0940)\n",
      "[2025-11-20 09:19:19,418] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 09:19:19,419] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 09:19:19,421] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 09:19:19,423] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 09:19:19,424] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 09:19:19,425] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 09:19:19,427] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 09:19:19,428] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 09:19:19,431] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 09:19:19,431] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 09:19:19,434] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 09:19:19,435] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 09:19:19,437] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 44] Done in 13.0 min\n",
      "  best_val_loss              = 3221.094\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0731\n",
      "  Label transfer (ADT→RNA)  = 0.550\n",
      "\n",
      "================================================================================\n",
      "[Config 45] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 82,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 80.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396fa792d2ec4aec8a27949c55f76fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 09:19:32,091] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4684.1685 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:19:33,725] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3909.9529 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:19:33,852] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3909.9529\n",
      "[2025-11-20 09:19:48,071] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3787.1340\n",
      "[2025-11-20 09:20:02,895] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3748.6241\n",
      "[2025-11-20 09:20:16,655] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3714.4092\n",
      "[2025-11-20 09:20:28,677] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3687.2670 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:20:30,316] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3703.6651 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:20:30,434] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3703.6651\n",
      "[2025-11-20 09:20:45,232] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3685.0486\n",
      "[2025-11-20 09:21:00,400] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3678.8542\n",
      "[2025-11-20 09:21:15,464] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3674.5726\n",
      "[2025-11-20 09:21:28,745] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3666.4684\n",
      "[2025-11-20 09:21:41,909] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3648.6119 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:21:43,549] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3664.7192 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:21:43,581] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3664.7192\n",
      "[2025-11-20 09:21:56,532] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3647.0218\n",
      "[2025-11-20 09:22:11,580] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3646.6863\n",
      "[2025-11-20 09:22:25,720] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3639.5346\n",
      "[2025-11-20 09:22:40,886] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3592.1538\n",
      "[2025-11-20 09:22:54,487] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3546.3668 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:22:56,112] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3550.9242 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:22:56,252] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3550.9242\n",
      "[2025-11-20 09:23:11,102] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3540.5450\n",
      "[2025-11-20 09:23:26,097] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3539.2425\n",
      "[2025-11-20 09:23:40,959] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3529.9184\n",
      "[2025-11-20 09:23:56,481] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3524.4875\n",
      "[2025-11-20 09:24:13,741] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3497.6228 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:24:15,858] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3516.2087 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:24:15,996] [UniVITrainer] [INFO] [Epoch 020] New best val loss: 3516.2087\n",
      "[2025-11-20 09:24:33,287] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3514.7429\n",
      "[2025-11-20 09:25:12,204] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3513.0700\n",
      "[2025-11-20 09:25:32,311] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3507.7998\n",
      "[2025-11-20 09:25:49,806] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3474.0353 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:25:51,982] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3498.6680 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:25:52,088] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3498.6680\n",
      "[2025-11-20 09:26:29,461] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3495.9496\n",
      "[2025-11-20 09:26:49,228] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3495.1507\n",
      "[2025-11-20 09:27:08,833] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3483.3430\n",
      "[2025-11-20 09:27:24,470] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3455.0291 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:27:26,066] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3490.1634 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:28:23,135] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3478.7090\n",
      "[2025-11-20 09:28:35,711] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3441.6359 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:28:37,298] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3484.2025 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:29:06,479] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3473.3146\n",
      "[2025-11-20 09:29:21,541] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 3470.4705\n",
      "[2025-11-20 09:29:49,403] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3432.6393 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:29:50,999] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3473.0176 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:30:06,066] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3465.8642\n",
      "[2025-11-20 09:31:03,703] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3429.6114 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:31:05,300] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3465.6717 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:31:05,435] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3465.6717\n",
      "[2025-11-20 09:31:48,814] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3463.5538\n",
      "[2025-11-20 09:32:16,648] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3422.0778 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:32:18,234] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3466.0292 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:32:48,036] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 3463.0367\n",
      "[2025-11-20 09:33:17,316] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3457.8260\n",
      "[2025-11-20 09:33:30,526] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3419.2193 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:33:31,959] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3465.1844 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:34:44,446] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3409.8127 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:34:46,041] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3459.8873 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:35:14,849] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3454.6733\n",
      "[2025-11-20 09:35:57,083] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3404.2900 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:35:58,677] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3460.6100 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:36:43,089] [UniVITrainer] [INFO] [Epoch 068] New best val loss: 3451.4586\n",
      "[2025-11-20 09:37:19,749] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3397.6099 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:37:21,980] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3450.3813 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:37:22,102] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3450.3813\n",
      "[2025-11-20 09:38:58,655] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3393.7522 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:39:00,818] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3449.6727 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:39:00,941] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 3449.6727\n",
      "[2025-11-20 09:39:40,766] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 3447.3483\n",
      "[2025-11-20 09:40:00,729] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3440.3522\n",
      "[2025-11-20 09:40:34,654] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3367.6272 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:40:36,834] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3443.6548 (beta=160.000, gamma=80.000)\n",
      "[2025-11-20 09:40:36,878] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3440.3522)\n",
      "[2025-11-20 09:40:37,850] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 09:40:37,851] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 09:40:37,852] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 09:40:37,853] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 09:40:37,853] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 09:40:37,854] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 09:40:37,854] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 09:40:37,855] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 09:40:37,855] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 09:40:37,856] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 09:40:37,856] [UniVITrainer] [INFO]   early_stopping: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 09:40:37,857] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 09:40:37,858] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 45] Done in 21.3 min\n",
      "  best_val_loss              = 3440.352\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.1432\n",
      "  Label transfer (ADT→RNA)  = 0.339\n",
      "\n",
      "================================================================================\n",
      "[Config 46] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 64,\n",
      "  \"beta\": 80.0,\n",
      "  \"gamma\": 20.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": true,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff13da821894e488f6b331d77d6c288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 09:40:53,423] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4048.3431 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:40:55,598] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3702.0320 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:40:55,799] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3702.0320\n",
      "[2025-11-20 09:41:14,271] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3574.2351\n",
      "[2025-11-20 09:41:31,729] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3530.9464\n",
      "[2025-11-20 09:41:51,866] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3516.9134\n",
      "[2025-11-20 09:42:09,599] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3496.8286 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:42:11,771] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3474.3869 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:42:11,985] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3474.3869\n",
      "[2025-11-20 09:42:32,147] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3456.2692\n",
      "[2025-11-20 09:42:52,408] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3427.6290\n",
      "[2025-11-20 09:43:12,731] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3415.6269\n",
      "[2025-11-20 09:43:33,311] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 3387.5081\n",
      "[2025-11-20 09:43:51,243] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3350.1097 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:43:53,412] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3360.1854 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:43:53,706] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3360.1854\n",
      "[2025-11-20 09:44:14,131] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3357.8585\n",
      "[2025-11-20 09:44:34,489] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 3338.7228\n",
      "[2025-11-20 09:44:52,892] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3319.6913\n",
      "[2025-11-20 09:45:12,997] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3304.1195\n",
      "[2025-11-20 09:45:30,440] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3272.0652 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:45:32,593] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3293.6867 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:45:32,884] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3293.6867\n",
      "[2025-11-20 09:45:52,672] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3286.3136\n",
      "[2025-11-20 09:46:12,608] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3272.6322\n",
      "[2025-11-20 09:46:32,024] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3265.5979\n",
      "[2025-11-20 09:46:52,168] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3260.0074\n",
      "[2025-11-20 09:47:09,971] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3228.0526 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:47:12,135] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3262.8991 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:47:32,515] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 3250.9940\n",
      "[2025-11-20 09:48:13,050] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 3246.6579\n",
      "[2025-11-20 09:48:32,879] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3242.5631\n",
      "[2025-11-20 09:48:50,875] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3200.0886 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:48:53,241] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3237.8895 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:48:53,520] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 3237.8895\n",
      "[2025-11-20 09:49:14,056] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 3235.8341\n",
      "[2025-11-20 09:50:14,331] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 3227.3394\n",
      "[2025-11-20 09:50:32,051] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3180.1698 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:50:34,291] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3225.5585 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:50:34,651] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 3225.5585\n",
      "[2025-11-20 09:51:15,120] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3219.5078\n",
      "[2025-11-20 09:51:35,057] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3218.9535\n",
      "[2025-11-20 09:51:54,447] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3213.8292\n",
      "[2025-11-20 09:52:06,043] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3155.4701 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:52:07,598] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3207.9441 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:52:07,931] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3207.9441\n",
      "[2025-11-20 09:52:22,625] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3205.7690\n",
      "[2025-11-20 09:53:15,327] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3139.8365 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:53:17,019] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3200.8301 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:53:17,235] [UniVITrainer] [INFO] [Epoch 040] New best val loss: 3200.8301\n",
      "[2025-11-20 09:53:31,755] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3200.3332\n",
      "[2025-11-20 09:54:00,773] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 3194.2747\n",
      "[2025-11-20 09:54:28,985] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3121.5099 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:54:30,482] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3191.1704 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:54:30,687] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 3191.1704\n",
      "[2025-11-20 09:55:14,670] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3188.7457\n",
      "[2025-11-20 09:55:29,962] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 3187.8503\n",
      "[2025-11-20 09:55:41,472] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3109.2750 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:55:43,038] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3183.6321 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:55:43,237] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3183.6321\n",
      "[2025-11-20 09:55:58,052] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 3179.4958\n",
      "[2025-11-20 09:56:29,012] [UniVITrainer] [INFO] [Epoch 053] New best val loss: 3178.8698\n",
      "[2025-11-20 09:56:58,153] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3101.4848 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:56:59,856] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3185.2208 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:57:14,185] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 3178.8312\n",
      "[2025-11-20 09:57:43,044] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 3172.0539\n",
      "[2025-11-20 09:58:10,292] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3088.5918 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:58:10,698] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3187.3289 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:58:40,887] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 3170.1636\n",
      "[2025-11-20 09:59:08,133] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 3168.4498\n",
      "[2025-11-20 09:59:22,021] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3080.0624 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:59:23,198] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3163.5985 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 09:59:23,387] [UniVITrainer] [INFO] [Epoch 065] New best val loss: 3163.5985\n",
      "[2025-11-20 10:00:20,668] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 3163.2431\n",
      "[2025-11-20 10:00:34,476] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3069.3386 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 10:00:36,120] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3162.5423 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 10:00:36,324] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3162.5423\n",
      "[2025-11-20 10:01:22,353] [UniVITrainer] [INFO] [Epoch 073] New best val loss: 3159.5239\n",
      "[2025-11-20 10:01:50,076] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3062.1693 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 10:01:50,657] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3175.9326 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 10:02:05,642] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3157.4106\n",
      "[2025-11-20 10:02:36,729] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 3156.9513\n",
      "[2025-11-20 10:03:05,522] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3051.0041 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 10:03:07,174] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3168.1423 (beta=80.000, gamma=20.000)\n",
      "[2025-11-20 10:03:07,212] [UniVITrainer] [INFO] Restored best model from epoch 78 (val loss = 3156.9513)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 10:03:08,023] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 10:03:08,024] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 10:03:08,026] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 10:03:08,028] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 10:03:08,030] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 10:03:08,031] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 10:03:08,033] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 10:03:08,034] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 10:03:08,036] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 10:03:08,045] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 10:03:08,045] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 10:03:08,047] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 10:03:08,048] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 46] Done in 22.5 min\n",
      "  best_val_loss              = 3156.951\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0659\n",
      "  Label transfer (ADT→RNA)  = 0.606\n",
      "\n",
      "================================================================================\n",
      "[Config 47] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 20,\n",
      "  \"beta\": 0.0,\n",
      "  \"gamma\": 80.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5044fa5573849608c4d18c6baecabdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 10:03:19,920] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3649.4001 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:03:21,575] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3298.6466 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:03:21,725] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3298.6466\n",
      "[2025-11-20 10:03:36,751] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3149.0400\n",
      "[2025-11-20 10:03:52,151] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3079.4166\n",
      "[2025-11-20 10:04:07,134] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3057.9966\n",
      "[2025-11-20 10:04:20,483] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3005.0698 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:04:22,137] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3011.5502 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:04:22,307] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3011.5502\n",
      "[2025-11-20 10:04:38,597] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 2989.6965\n",
      "[2025-11-20 10:04:57,511] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 2949.6382\n",
      "[2025-11-20 10:05:16,123] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 2923.9141\n",
      "[2025-11-20 10:05:33,944] [UniVITrainer] [INFO] [Epoch 009] New best val loss: 2916.6599\n",
      "[2025-11-20 10:05:47,948] [UniVITrainer] [INFO] [Epoch 010] Train loss: 2857.7005 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:05:49,830] [UniVITrainer] [INFO] [Epoch 010] Val loss: 2899.4535 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:05:50,101] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 2899.4535\n",
      "[2025-11-20 10:06:09,203] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 2886.2437\n",
      "[2025-11-20 10:06:27,728] [UniVITrainer] [INFO] [Epoch 012] New best val loss: 2872.8323\n",
      "[2025-11-20 10:07:03,061] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 2862.9864\n",
      "[2025-11-20 10:07:20,829] [UniVITrainer] [INFO] [Epoch 015] Train loss: 2791.5347 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:07:23,076] [UniVITrainer] [INFO] [Epoch 015] Val loss: 2857.1938 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:07:23,332] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 2857.1938\n",
      "[2025-11-20 10:07:43,599] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 2854.1720\n",
      "[2025-11-20 10:08:02,459] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 2853.2546\n",
      "[2025-11-20 10:08:21,277] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 2836.7204\n",
      "[2025-11-20 10:08:40,485] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 2826.7095\n",
      "[2025-11-20 10:08:58,443] [UniVITrainer] [INFO] [Epoch 020] Train loss: 2736.7135 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:09:00,669] [UniVITrainer] [INFO] [Epoch 020] Val loss: 2827.6498 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:09:19,199] [UniVITrainer] [INFO] [Epoch 021] New best val loss: 2819.4422\n",
      "[2025-11-20 10:09:37,767] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 2810.1643\n",
      "[2025-11-20 10:09:58,203] [UniVITrainer] [INFO] [Epoch 023] New best val loss: 2802.6533\n",
      "[2025-11-20 10:10:18,971] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 2793.3869\n",
      "[2025-11-20 10:10:36,706] [UniVITrainer] [INFO] [Epoch 025] Train loss: 2689.5750 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:10:39,060] [UniVITrainer] [INFO] [Epoch 025] Val loss: 2788.3490 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:10:39,342] [UniVITrainer] [INFO] [Epoch 025] New best val loss: 2788.3490\n",
      "[2025-11-20 10:10:59,909] [UniVITrainer] [INFO] [Epoch 026] New best val loss: 2776.6558\n",
      "[2025-11-20 10:11:38,463] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 2767.0445\n",
      "[2025-11-20 10:11:58,896] [UniVITrainer] [INFO] [Epoch 029] New best val loss: 2760.2812\n",
      "[2025-11-20 10:12:16,882] [UniVITrainer] [INFO] [Epoch 030] Train loss: 2645.3144 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:12:19,112] [UniVITrainer] [INFO] [Epoch 030] Val loss: 2755.7473 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:12:19,360] [UniVITrainer] [INFO] [Epoch 030] New best val loss: 2755.7473\n",
      "[2025-11-20 10:12:39,373] [UniVITrainer] [INFO] [Epoch 031] New best val loss: 2750.0543\n",
      "[2025-11-20 10:12:57,556] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 2747.3998\n",
      "[2025-11-20 10:13:17,064] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 2739.5754\n",
      "[2025-11-20 10:13:36,726] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 2733.9388\n",
      "[2025-11-20 10:13:54,313] [UniVITrainer] [INFO] [Epoch 035] Train loss: 2603.7703 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:13:56,562] [UniVITrainer] [INFO] [Epoch 035] Val loss: 2730.6964 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:13:56,809] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 2730.6964\n",
      "[2025-11-20 10:14:16,969] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 2724.2683\n",
      "[2025-11-20 10:14:34,579] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 2721.6305\n",
      "[2025-11-20 10:14:55,934] [UniVITrainer] [INFO] [Epoch 038] New best val loss: 2716.3124\n",
      "[2025-11-20 10:15:14,724] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 2701.9559\n",
      "[2025-11-20 10:15:29,474] [UniVITrainer] [INFO] [Epoch 040] Train loss: 2560.6234 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:15:30,892] [UniVITrainer] [INFO] [Epoch 040] Val loss: 2704.0235 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:15:49,946] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 2697.9573\n",
      "[2025-11-20 10:16:27,257] [UniVITrainer] [INFO] [Epoch 043] New best val loss: 2693.2018\n",
      "[2025-11-20 10:16:45,298] [UniVITrainer] [INFO] [Epoch 044] New best val loss: 2691.8255\n",
      "[2025-11-20 10:17:00,107] [UniVITrainer] [INFO] [Epoch 045] Train loss: 2514.1883 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:17:02,375] [UniVITrainer] [INFO] [Epoch 045] Val loss: 2682.0584 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:17:02,626] [UniVITrainer] [INFO] [Epoch 045] New best val loss: 2682.0584\n",
      "[2025-11-20 10:17:20,700] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 2677.7457\n",
      "[2025-11-20 10:17:38,452] [UniVITrainer] [INFO] [Epoch 047] New best val loss: 2675.7833\n",
      "[2025-11-20 10:17:57,554] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 2672.8142\n",
      "[2025-11-20 10:18:17,384] [UniVITrainer] [INFO] [Epoch 049] New best val loss: 2666.5075\n",
      "[2025-11-20 10:18:35,017] [UniVITrainer] [INFO] [Epoch 050] Train loss: 2474.0652 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:18:37,016] [UniVITrainer] [INFO] [Epoch 050] Val loss: 2669.9699 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:18:56,340] [UniVITrainer] [INFO] [Epoch 051] New best val loss: 2666.0917\n",
      "[2025-11-20 10:19:14,779] [UniVITrainer] [INFO] [Epoch 052] New best val loss: 2661.5742\n",
      "[2025-11-20 10:19:53,149] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 2656.5879\n",
      "[2025-11-20 10:20:07,234] [UniVITrainer] [INFO] [Epoch 055] Train loss: 2437.5765 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:20:09,441] [UniVITrainer] [INFO] [Epoch 055] Val loss: 2654.1436 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:20:09,714] [UniVITrainer] [INFO] [Epoch 055] New best val loss: 2654.1436\n",
      "[2025-11-20 10:20:28,967] [UniVITrainer] [INFO] [Epoch 056] New best val loss: 2652.2500\n",
      "[2025-11-20 10:20:49,436] [UniVITrainer] [INFO] [Epoch 057] New best val loss: 2645.8679\n",
      "[2025-11-20 10:21:14,421] [UniVITrainer] [INFO] [Epoch 058] New best val loss: 2642.4970\n",
      "[2025-11-20 10:21:44,145] [UniVITrainer] [INFO] [Epoch 060] Train loss: 2408.9838 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:21:46,080] [UniVITrainer] [INFO] [Epoch 060] Val loss: 2643.2907 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:22:26,877] [UniVITrainer] [INFO] [Epoch 062] New best val loss: 2636.8033\n",
      "[2025-11-20 10:22:47,098] [UniVITrainer] [INFO] [Epoch 063] New best val loss: 2635.9292\n",
      "[2025-11-20 10:23:08,183] [UniVITrainer] [INFO] [Epoch 064] New best val loss: 2627.9232\n",
      "[2025-11-20 10:23:26,643] [UniVITrainer] [INFO] [Epoch 065] Train loss: 2376.1564 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:23:28,984] [UniVITrainer] [INFO] [Epoch 065] Val loss: 2628.8025 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:24:10,672] [UniVITrainer] [INFO] [Epoch 067] New best val loss: 2624.9522\n",
      "[2025-11-20 10:24:52,013] [UniVITrainer] [INFO] [Epoch 069] New best val loss: 2619.1189\n",
      "[2025-11-20 10:25:10,654] [UniVITrainer] [INFO] [Epoch 070] Train loss: 2344.8814 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:25:12,840] [UniVITrainer] [INFO] [Epoch 070] Val loss: 2623.2208 (beta=0.000, gamma=80.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 10:25:33,847] [UniVITrainer] [INFO] [Epoch 071] New best val loss: 2615.4743\n",
      "[2025-11-20 10:25:54,938] [UniVITrainer] [INFO] [Epoch 072] New best val loss: 2614.2635\n",
      "[2025-11-20 10:26:36,703] [UniVITrainer] [INFO] [Epoch 074] New best val loss: 2607.9428\n",
      "[2025-11-20 10:26:55,183] [UniVITrainer] [INFO] [Epoch 075] Train loss: 2321.9909 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:26:57,674] [UniVITrainer] [INFO] [Epoch 075] Val loss: 2606.7596 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:26:57,841] [UniVITrainer] [INFO] [Epoch 075] New best val loss: 2606.7596\n",
      "[2025-11-20 10:27:18,677] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 2605.5999\n",
      "[2025-11-20 10:27:39,758] [UniVITrainer] [INFO] [Epoch 077] New best val loss: 2603.2244\n",
      "[2025-11-20 10:28:00,878] [UniVITrainer] [INFO] [Epoch 078] New best val loss: 2599.6877\n",
      "[2025-11-20 10:28:39,301] [UniVITrainer] [INFO] [Epoch 080] Train loss: 2296.5203 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:28:41,536] [UniVITrainer] [INFO] [Epoch 080] Val loss: 2597.8627 (beta=0.000, gamma=80.000)\n",
      "[2025-11-20 10:28:41,702] [UniVITrainer] [INFO] [Epoch 080] New best val loss: 2597.8627\n",
      "[2025-11-20 10:28:41,734] [UniVITrainer] [INFO] Restored best model from epoch 80 (val loss = 2597.8627)\n",
      "[2025-11-20 10:28:42,676] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 10:28:42,677] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 10:28:42,677] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 10:28:42,678] [UniVITrainer] [INFO]   lr: 0.0005\n",
      "[2025-11-20 10:28:42,678] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 10:28:42,679] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 10:28:42,679] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 10:28:42,680] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 10:28:42,681] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 10:28:42,681] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 10:28:42,682] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 10:28:42,682] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 10:28:42,683] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 47] Done in 25.6 min\n",
      "  best_val_loss              = 2597.863\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.0256\n",
      "  Label transfer (ADT→RNA)  = 0.859\n",
      "\n",
      "================================================================================\n",
      "[Config 48] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 40,\n",
      "  \"beta\": 160.0,\n",
      "  \"gamma\": 0.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.0,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_med2\",\n",
      "  \"adt_arch\": \"adt_small2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cf378ff611445bb41461665f215584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 10:29:01,096] [UniVITrainer] [INFO] [Epoch 001] Train loss: 3987.0893 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:29:03,603] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3818.1179 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:29:03,701] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3818.1179\n",
      "[2025-11-20 10:29:24,121] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3780.1546\n",
      "[2025-11-20 10:29:44,623] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3710.5401\n",
      "[2025-11-20 10:30:05,683] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3696.2768\n",
      "[2025-11-20 10:30:23,662] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3681.1483 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:30:25,728] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3694.8083 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:30:25,828] [UniVITrainer] [INFO] [Epoch 005] New best val loss: 3694.8083\n",
      "[2025-11-20 10:30:45,972] [UniVITrainer] [INFO] [Epoch 006] New best val loss: 3688.2156\n",
      "[2025-11-20 10:31:27,251] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3684.5791\n",
      "[2025-11-20 10:32:05,758] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3667.0623 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:32:08,005] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3680.1213 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:32:08,103] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3680.1213\n",
      "[2025-11-20 10:32:28,678] [UniVITrainer] [INFO] [Epoch 011] New best val loss: 3668.0688\n",
      "[2025-11-20 10:33:08,081] [UniVITrainer] [INFO] [Epoch 013] New best val loss: 3648.1668\n",
      "[2025-11-20 10:33:26,852] [UniVITrainer] [INFO] [Epoch 014] New best val loss: 3611.6986\n",
      "[2025-11-20 10:33:44,006] [UniVITrainer] [INFO] [Epoch 015] Train loss: 3572.8463 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:33:46,244] [UniVITrainer] [INFO] [Epoch 015] Val loss: 3587.5918 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:33:46,346] [UniVITrainer] [INFO] [Epoch 015] New best val loss: 3587.5918\n",
      "[2025-11-20 10:34:06,617] [UniVITrainer] [INFO] [Epoch 016] New best val loss: 3584.9491\n",
      "[2025-11-20 10:34:27,148] [UniVITrainer] [INFO] [Epoch 017] New best val loss: 3578.5125\n",
      "[2025-11-20 10:34:47,919] [UniVITrainer] [INFO] [Epoch 018] New best val loss: 3571.2670\n",
      "[2025-11-20 10:35:08,497] [UniVITrainer] [INFO] [Epoch 019] New best val loss: 3564.8942\n",
      "[2025-11-20 10:35:26,946] [UniVITrainer] [INFO] [Epoch 020] Train loss: 3540.8318 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:35:28,990] [UniVITrainer] [INFO] [Epoch 020] Val loss: 3572.2574 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:36:07,815] [UniVITrainer] [INFO] [Epoch 022] New best val loss: 3559.1405\n",
      "[2025-11-20 10:36:47,753] [UniVITrainer] [INFO] [Epoch 024] New best val loss: 3554.4832\n",
      "[2025-11-20 10:37:05,502] [UniVITrainer] [INFO] [Epoch 025] Train loss: 3528.6783 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:37:07,677] [UniVITrainer] [INFO] [Epoch 025] Val loss: 3560.6110 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:37:47,438] [UniVITrainer] [INFO] [Epoch 027] New best val loss: 3551.7312\n",
      "[2025-11-20 10:38:07,449] [UniVITrainer] [INFO] [Epoch 028] New best val loss: 3545.2413\n",
      "[2025-11-20 10:38:45,374] [UniVITrainer] [INFO] [Epoch 030] Train loss: 3515.9501 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:38:47,557] [UniVITrainer] [INFO] [Epoch 030] Val loss: 3549.7836 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:39:27,426] [UniVITrainer] [INFO] [Epoch 032] New best val loss: 3535.6221\n",
      "[2025-11-20 10:39:47,241] [UniVITrainer] [INFO] [Epoch 033] New best val loss: 3526.6645\n",
      "[2025-11-20 10:40:07,206] [UniVITrainer] [INFO] [Epoch 034] New best val loss: 3503.6456\n",
      "[2025-11-20 10:40:24,848] [UniVITrainer] [INFO] [Epoch 035] Train loss: 3459.9408 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:40:27,032] [UniVITrainer] [INFO] [Epoch 035] Val loss: 3494.2842 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:40:27,138] [UniVITrainer] [INFO] [Epoch 035] New best val loss: 3494.2842\n",
      "[2025-11-20 10:40:43,689] [UniVITrainer] [INFO] [Epoch 036] New best val loss: 3491.8270\n",
      "[2025-11-20 10:40:58,184] [UniVITrainer] [INFO] [Epoch 037] New best val loss: 3482.6309\n",
      "[2025-11-20 10:41:20,717] [UniVITrainer] [INFO] [Epoch 039] New best val loss: 3481.6270\n",
      "[2025-11-20 10:41:32,594] [UniVITrainer] [INFO] [Epoch 040] Train loss: 3436.7458 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:41:34,340] [UniVITrainer] [INFO] [Epoch 040] Val loss: 3482.5877 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:41:47,682] [UniVITrainer] [INFO] [Epoch 041] New best val loss: 3474.7906\n",
      "[2025-11-20 10:42:02,660] [UniVITrainer] [INFO] [Epoch 042] New best val loss: 3473.5689\n",
      "[2025-11-20 10:42:45,289] [UniVITrainer] [INFO] [Epoch 045] Train loss: 3430.6648 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:42:46,938] [UniVITrainer] [INFO] [Epoch 045] Val loss: 3475.4050 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:43:01,772] [UniVITrainer] [INFO] [Epoch 046] New best val loss: 3471.1178\n",
      "[2025-11-20 10:43:30,845] [UniVITrainer] [INFO] [Epoch 048] New best val loss: 3468.0441\n",
      "[2025-11-20 10:43:57,831] [UniVITrainer] [INFO] [Epoch 050] Train loss: 3422.7499 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:43:59,482] [UniVITrainer] [INFO] [Epoch 050] Val loss: 3464.6964 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:43:59,578] [UniVITrainer] [INFO] [Epoch 050] New best val loss: 3464.6964\n",
      "[2025-11-20 10:45:08,225] [UniVITrainer] [INFO] [Epoch 054] New best val loss: 3461.8593\n",
      "[2025-11-20 10:45:26,055] [UniVITrainer] [INFO] [Epoch 055] Train loss: 3418.7765 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:45:28,128] [UniVITrainer] [INFO] [Epoch 055] Val loss: 3465.7077 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:46:47,455] [UniVITrainer] [INFO] [Epoch 059] New best val loss: 3460.1635\n",
      "[2025-11-20 10:47:05,364] [UniVITrainer] [INFO] [Epoch 060] Train loss: 3412.7398 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:47:07,540] [UniVITrainer] [INFO] [Epoch 060] Val loss: 3459.6750 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:47:07,682] [UniVITrainer] [INFO] [Epoch 060] New best val loss: 3459.6750\n",
      "[2025-11-20 10:48:43,703] [UniVITrainer] [INFO] [Epoch 065] Train loss: 3407.4734 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:48:46,037] [UniVITrainer] [INFO] [Epoch 065] Val loss: 3460.9336 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:49:05,598] [UniVITrainer] [INFO] [Epoch 066] New best val loss: 3454.0091\n",
      "[2025-11-20 10:50:22,060] [UniVITrainer] [INFO] [Epoch 070] Train loss: 3401.9623 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:50:23,947] [UniVITrainer] [INFO] [Epoch 070] Val loss: 3453.0919 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:50:24,030] [UniVITrainer] [INFO] [Epoch 070] New best val loss: 3453.0919\n",
      "[2025-11-20 10:52:04,169] [UniVITrainer] [INFO] [Epoch 075] Train loss: 3396.7310 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:52:06,395] [UniVITrainer] [INFO] [Epoch 075] Val loss: 3455.1867 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:52:27,114] [UniVITrainer] [INFO] [Epoch 076] New best val loss: 3448.6429\n",
      "[2025-11-20 10:53:47,878] [UniVITrainer] [INFO] [Epoch 080] Train loss: 3398.9367 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:53:50,104] [UniVITrainer] [INFO] [Epoch 080] Val loss: 3452.7292 (beta=160.000, gamma=0.000)\n",
      "[2025-11-20 10:53:50,130] [UniVITrainer] [INFO] Restored best model from epoch 76 (val loss = 3448.6429)\n",
      "[2025-11-20 10:53:50,938] [UniVITrainer] [INFO] TrainingConfig:\n",
      "[2025-11-20 10:53:50,939] [UniVITrainer] [INFO]   n_epochs: 80\n",
      "[2025-11-20 10:53:50,940] [UniVITrainer] [INFO]   batch_size: 256\n",
      "[2025-11-20 10:53:50,940] [UniVITrainer] [INFO]   lr: 0.001\n",
      "[2025-11-20 10:53:50,941] [UniVITrainer] [INFO]   weight_decay: 1e-05\n",
      "[2025-11-20 10:53:50,941] [UniVITrainer] [INFO]   device: cuda\n",
      "[2025-11-20 10:53:50,942] [UniVITrainer] [INFO]   log_every: 5\n",
      "[2025-11-20 10:53:50,942] [UniVITrainer] [INFO]   grad_clip: 5.0\n",
      "[2025-11-20 10:53:50,943] [UniVITrainer] [INFO]   num_workers: 0\n",
      "[2025-11-20 10:53:50,943] [UniVITrainer] [INFO]   seed: 42\n",
      "[2025-11-20 10:53:50,944] [UniVITrainer] [INFO]   early_stopping: True\n",
      "[2025-11-20 10:53:50,945] [UniVITrainer] [INFO]   patience: 15\n",
      "[2025-11-20 10:53:50,945] [UniVITrainer] [INFO]   min_delta: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config 48] Done in 25.1 min\n",
      "  best_val_loss              = 3448.643\n",
      "  FOSCTTM (RNA vs ADT, val) = 0.3191\n",
      "  Label transfer (ADT→RNA)  = 0.080\n",
      "\n",
      "================================================================================\n",
      "[Config 49] Hyperparameters:\n",
      "{\n",
      "  \"latent_dim\": 120,\n",
      "  \"beta\": 120.0,\n",
      "  \"gamma\": 20.0,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"encoder_dropout\": 0.1,\n",
      "  \"decoder_batchnorm\": false,\n",
      "  \"rna_arch\": \"rna_wide3\",\n",
      "  \"adt_arch\": \"adt_med2\"\n",
      "}\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739716d7de114488bf40caf3163d9288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training UniVI:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 10:54:08,274] [UniVITrainer] [INFO] [Epoch 001] Train loss: 4137.6525 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 10:54:10,473] [UniVITrainer] [INFO] [Epoch 001] Val loss: 3812.6934 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 10:54:10,800] [UniVITrainer] [INFO] [Epoch 001] New best val loss: 3812.6934\n",
      "[2025-11-20 10:54:30,670] [UniVITrainer] [INFO] [Epoch 002] New best val loss: 3700.8991\n",
      "[2025-11-20 10:54:50,553] [UniVITrainer] [INFO] [Epoch 003] New best val loss: 3689.4646\n",
      "[2025-11-20 10:55:10,521] [UniVITrainer] [INFO] [Epoch 004] New best val loss: 3684.8397\n",
      "[2025-11-20 10:55:28,031] [UniVITrainer] [INFO] [Epoch 005] Train loss: 3687.2206 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 10:55:30,264] [UniVITrainer] [INFO] [Epoch 005] Val loss: 3696.5149 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 10:56:10,440] [UniVITrainer] [INFO] [Epoch 007] New best val loss: 3640.6954\n",
      "[2025-11-20 10:56:30,400] [UniVITrainer] [INFO] [Epoch 008] New best val loss: 3624.5726\n",
      "[2025-11-20 10:57:07,446] [UniVITrainer] [INFO] [Epoch 010] Train loss: 3590.0699 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 10:57:09,682] [UniVITrainer] [INFO] [Epoch 010] Val loss: 3570.5482 (beta=120.000, gamma=20.000)\n",
      "[2025-11-20 10:57:10,018] [UniVITrainer] [INFO] [Epoch 010] New best val loss: 3570.5482\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 6. Run the search\n",
    "# ------------------------------------------------------\n",
    "\n",
    "all_results = []\n",
    "best_score = None\n",
    "best_result = None\n",
    "\n",
    "def score_fn(res):\n",
    "    \"\"\"\n",
    "    Composite objective:\n",
    "      - lower val_loss, lower FOSCTTM, higher label_transfer.\n",
    "    \"\"\"\n",
    "    return res[\"best_val_loss\"] * (1.0 + res[\"foscttm_val\"]) / (1.0 + res[\"label_transfer_val\"])\n",
    "\n",
    "for i, hp in enumerate(iter_hparam_configs(search_space), start=1):\n",
    "    res = evaluate_config(hp, config_id=i)\n",
    "    s = score_fn(res)\n",
    "    res[\"score\"] = float(s)\n",
    "    all_results.append(res)\n",
    "\n",
    "    if best_score is None or s < best_score:\n",
    "        best_score = s\n",
    "        best_result = res\n",
    "        print(f\"--> New best config (id={i}) with score={s:.3f}\")\n",
    "\n",
    "print(\"\\n========================================\")\n",
    "print(\"Hyperparameter search finished.\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# Sort configs by score\n",
    "all_results_sorted = sorted(all_results, key=lambda r: r[\"score\"])\n",
    "for r in all_results_sorted:\n",
    "    hp = r[\"hp\"]\n",
    "    print(\n",
    "        f\"Config {r['config_id']:02d} | \"\n",
    "        f\"latent={hp['latent_dim']:>2d}, \"\n",
    "        f\"beta={hp['beta']:>5.1f}, \"\n",
    "        f\"gamma={hp['gamma']:>5.1f}, \"\n",
    "        f\"lr={hp['lr']:.0e}, \"\n",
    "        f\"wd={hp['weight_decay']:.0e}, \"\n",
    "        f\"enc_drop={hp['encoder_dropout']:.2f}, \"\n",
    "        f\"dec_bn={hp['decoder_batchnorm']} | \"\n",
    "        f\"rna_arch={hp['rna_arch']['name']}, \"\n",
    "        f\"adt_arch={hp['adt_arch']['name']} | \"\n",
    "        f\"val_loss={r['best_val_loss']:.2f}, \"\n",
    "        f\"FOS={r['foscttm_val']:.4f}, \"\n",
    "        f\"acc={r['label_transfer_val']:.3f}, \"\n",
    "        f\"score={r['score']:.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nBest configuration hyperparameters:\")\n",
    "pretty_best = {\n",
    "    **{k: v for k, v in best_result[\"hp\"].items() if k not in (\"rna_arch\", \"adt_arch\")},\n",
    "    \"rna_arch\": best_result[\"hp\"][\"rna_arch\"][\"name\"],\n",
    "    \"adt_arch\": best_result[\"hp\"][\"adt_arch\"][\"name\"],\n",
    "}\n",
    "print(json.dumps(pretty_best, indent=2))\n",
    "print(\n",
    "    f\"Best val_loss={best_result['best_val_loss']:.3f}, \"\n",
    "    f\"FOS={best_result['foscttm_val']:.4f}, \"\n",
    "    f\"label_transfer={best_result['label_transfer_val']:.3f}, \"\n",
    "    f\"score={best_result['score']:.2f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdab44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Config 5] Hyperparameters:\n",
    "{\n",
    "  \"latent_dim\": 20,\n",
    "  \"beta\": 40.0,\n",
    "  \"gamma\": 60.0,\n",
    "  \"lr\": 0.001,\n",
    "  \"weight_decay\": 0.0001,\n",
    "  \"encoder_dropout\": 0.0,\n",
    "  \"decoder_batchnorm\": false,\n",
    "  \"rna_arch\": \"rna_med2\",\n",
    "  \"adt_arch\": \"adt_small2\"\n",
    "}\n",
    "\n",
    "[Config 5] Done in 4.7 min\n",
    "  best_val_loss              = 3034.068\n",
    "  FOSCTTM (RNA vs ADT, val) = 0.0453\n",
    "  Label transfer (ADT→RNA)  = 0.674\n",
    "--> New best config (id=5) with score=1894.050\n",
    "'''\n",
    "\n",
    "'''\n",
    "[Config 13] Hyperparameters:\n",
    "{\n",
    "  \"latent_dim\": 64,\n",
    "  \"beta\": 40.0,\n",
    "  \"gamma\": 80.0,\n",
    "  \"lr\": 0.001,\n",
    "  \"weight_decay\": 1e-05,\n",
    "  \"encoder_dropout\": 0.0,\n",
    "  \"decoder_batchnorm\": true,\n",
    "  \"rna_arch\": \"rna_wide3\",\n",
    "  \"adt_arch\": \"adt_med2\"\n",
    "}\n",
    "\n",
    "[Config 13] Done in 15.3 min\n",
    "  best_val_loss              = 3021.375\n",
    "  FOSCTTM (RNA vs ADT, val) = 0.0400\n",
    "  Label transfer (ADT→RNA)  = 0.669\n",
    "--> New best config (id=13) with score=1882.982\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 0. Collect results into a DataFrame\n",
    "# ------------------------------------------------------\n",
    "rows = []\n",
    "for r in all_results:\n",
    "    hp = r[\"hp\"]\n",
    "    rows.append(\n",
    "        {\n",
    "            \"config_id\": r[\"config_id\"],\n",
    "            \"latent_dim\": hp[\"latent_dim\"],\n",
    "            \"beta\": hp[\"beta\"],\n",
    "            \"gamma\": hp[\"gamma\"],\n",
    "            \"lr\": hp[\"lr\"],\n",
    "            \"weight_decay\": hp[\"weight_decay\"],\n",
    "            \"encoder_dropout\": hp[\"encoder_dropout\"],\n",
    "            \"decoder_batchnorm\": hp[\"decoder_batchnorm\"],\n",
    "            \"rna_arch\": hp[\"rna_arch\"][\"name\"],\n",
    "            \"adt_arch\": hp[\"adt_arch\"][\"name\"],\n",
    "            \"val_loss\": r[\"best_val_loss\"],\n",
    "            \"foscttm\": r[\"foscttm_val\"],\n",
    "            \"label_transfer\": r[\"label_transfer_val\"],\n",
    "            \"score\": r[\"score\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Hyperparameter search results (head):\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Metric relationships: pairplot\n",
    "# ------------------------------------------------------\n",
    "metrics = [\"val_loss\", \"foscttm\", \"label_transfer\", \"score\"]\n",
    "g = sns.pairplot(df[metrics], diag_kind=\"kde\")\n",
    "g.fig.suptitle(\"Metric relationships across CITE-seq configs\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. FOSCTTM vs label transfer, colored by composite score\n",
    "# ------------------------------------------------------\n",
    "plt.figure(figsize=(6, 5))\n",
    "sc = plt.scatter(\n",
    "    df[\"foscttm\"],\n",
    "    df[\"label_transfer\"],\n",
    "    c=df[\"score\"],\n",
    "    s=70,\n",
    "    cmap=\"viridis\",\n",
    "    edgecolor=\"k\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "plt.colorbar(sc, label=\"Composite score (lower = better)\")\n",
    "plt.xlabel(\"FOSCTTM (lower = better)\")\n",
    "plt.ylabel(\"Label transfer accuracy (higher = better)\")\n",
    "plt.title(\"CITE-seq: FOSCTTM vs label transfer, colored by score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. Score vs individual numeric hyperparameters\n",
    "# ------------------------------------------------------\n",
    "num_hps = [\"latent_dim\", \"beta\", \"gamma\", \"lr\", \"weight_decay\", \"encoder_dropout\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, hp_name in zip(axes, num_hps):\n",
    "    ax.scatter(df[hp_name], df[\"score\"], s=60, alpha=0.8)\n",
    "    ax.set_xlabel(hp_name)\n",
    "    ax.set_ylabel(\"score\")\n",
    "    ax.set_title(f\"Score vs {hp_name}\")\n",
    "    if hp_name in [\"lr\", \"weight_decay\"]:\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "plt.suptitle(\"CITE-seq: Score vs numeric hyperparameters\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. Categorical hyperparams vs score (boxplots)\n",
    "# ------------------------------------------------------\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(data=df, x=\"decoder_batchnorm\", y=\"score\")\n",
    "plt.xlabel(\"decoder_batchnorm\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title(\"CITE-seq: effect of decoder_batchnorm on score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(data=df, x=\"rna_arch\", y=\"score\")\n",
    "plt.xlabel(\"rna_arch\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title(\"CITE-seq: score distribution by RNA architecture\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(data=df, x=\"adt_arch\", y=\"score\")\n",
    "plt.xlabel(\"adt_arch\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title(\"CITE-seq: score distribution by ADT architecture\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5. Beta vs gamma with score as color (2D hyperparam landscape)\n",
    "# ------------------------------------------------------\n",
    "plt.figure(figsize=(6, 5))\n",
    "sc = plt.scatter(\n",
    "    df[\"beta\"],\n",
    "    df[\"gamma\"],\n",
    "    c=df[\"score\"],\n",
    "    s=80,\n",
    "    cmap=\"viridis\",\n",
    "    edgecolor=\"k\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "plt.colorbar(sc, label=\"Composite score (lower = better)\")\n",
    "plt.xlabel(\"beta\")\n",
    "plt.ylabel(\"gamma\")\n",
    "plt.title(\"CITE-seq: beta vs gamma hyperparameter landscape\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 6. Correlation heatmap: hyperparams & metrics\n",
    "# ------------------------------------------------------\n",
    "corr_cols = [\n",
    "    \"latent_dim\",\n",
    "    \"beta\",\n",
    "    \"gamma\",\n",
    "    \"lr\",\n",
    "    \"weight_decay\",\n",
    "    \"encoder_dropout\",\n",
    "    \"val_loss\",\n",
    "    \"foscttm\",\n",
    "    \"label_transfer\",\n",
    "    \"score\",\n",
    "]\n",
    "corr = df[corr_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"vlag\",\n",
    "    center=0.0,\n",
    "    square=True,\n",
    ")\n",
    "plt.title(\"CITE-seq: correlation between hyperparameters and metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 7. Training curves for top K configs (by score)\n",
    "# ------------------------------------------------------\n",
    "top_k = 3  # tweak as desired\n",
    "top_ids = df.nsmallest(top_k, \"score\")[\"config_id\"].tolist()\n",
    "print(f\"Top {top_k} CITE-seq configs by score:\", top_ids)\n",
    "\n",
    "for cid in top_ids:\n",
    "    res_c = next(r for r in all_results if r[\"config_id\"] == cid)\n",
    "    hist_c = res_c[\"history\"]\n",
    "    epochs_c = np.arange(1, len(hist_c[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs_c, hist_c[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(epochs_c, hist_c[\"val_loss\"], label=\"val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"CITE-seq training curves – Config {cid} (score={res_c['score']:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 8. Overlay val-loss curves for all configs\n",
    "# ------------------------------------------------------\n",
    "plt.figure(figsize=(7, 5))\n",
    "for r in all_results:\n",
    "    hist_r = r[\"history\"]\n",
    "    epochs_r = np.arange(1, len(hist_r[\"train_loss\"]) + 1)\n",
    "    plt.plot(\n",
    "        epochs_r,\n",
    "        hist_r[\"val_loss\"],\n",
    "        alpha=0.3,\n",
    "        linewidth=1.0,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Val loss\")\n",
    "plt.title(\"CITE-seq: val loss curves for all configs (overlay)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0964a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 9. Plot training curve for best config\n",
    "# ------------------------------------------------------\n",
    "hist = best_result[\"history\"]\n",
    "epochs = np.arange(1, len(hist[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epochs, hist[\"train_loss\"], label=\"train\")\n",
    "plt.plot(epochs, hist[\"val_loss\"], label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Best UniVI config training curves (search)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da5743",
   "metadata": {},
   "source": [
    "#### Initialize model and data via dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from univi.config import ModalityConfig, UniVIConfig, TrainingConfig\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load JSON\n",
    "# -------------------------\n",
    "with open(\"../parameter_files/defaults_cite_seq.json\") as f:\n",
    "    cfg_json = json.load(f)\n",
    "\n",
    "data_cfg  = cfg_json[\"data\"]\n",
    "model_cfg = cfg_json[\"model\"]\n",
    "train_cfg_json = cfg_json[\"training\"]\n",
    "\n",
    "# -------------------------\n",
    "# 2. Build ModalityConfig list\n",
    "# -------------------------\n",
    "adata_by_mod = {\n",
    "    \"rna\": rna_adata_hvg,   # make sure these exist\n",
    "    \"adt\": adt_adata,\n",
    "}\n",
    "\n",
    "modality_cfgs = []\n",
    "for m in data_cfg[\"modalities\"]:\n",
    "    name = m[\"name\"]\n",
    "    if name not in adata_by_mod:\n",
    "        raise ValueError(f\"Modality '{name}' not found in adata_by_mod\")\n",
    "\n",
    "    adata = adata_by_mod[name]\n",
    "    hidden = m.get(\"hidden_dims\", model_cfg[\"hidden_dims_default\"])\n",
    "\n",
    "    mc = ModalityConfig(\n",
    "        name=name,\n",
    "        input_dim=int(adata.n_vars),\n",
    "        encoder_hidden=hidden,\n",
    "        decoder_hidden=hidden,\n",
    "        likelihood=m[\"likelihood\"],   # \"nb\", \"gaussian\", \"zinb\", etc.\n",
    "    )\n",
    "    modality_cfgs.append(mc)\n",
    "\n",
    "print(\"Built ModalityConfig list:\")\n",
    "for mc in modality_cfgs:\n",
    "    print(\" \", mc)\n",
    "\n",
    "assert len(modality_cfgs) > 0, \"No modalities found for UniVIConfig!\"\n",
    "\n",
    "# -------------------------\n",
    "# 3. UniVIConfig\n",
    "# -------------------------\n",
    "univi_cfg = UniVIConfig(\n",
    "    latent_dim=model_cfg[\"latent_dim\"],\n",
    "    modalities=modality_cfgs,\n",
    "    beta=model_cfg[\"beta\"],\n",
    "    gamma=model_cfg[\"gamma\"],\n",
    "    encoder_dropout=model_cfg.get(\"dropout\", 0.0),\n",
    "    encoder_batchnorm=model_cfg.get(\"batchnorm\", True),\n",
    "    #kl_anneal_start=model_cfg.get(\"kl_anneal_start\", 0),\n",
    "    kl_anneal_start=0,\n",
    "    #kl_anneal_end=model_cfg.get(\"kl_anneal_end\", 0),\n",
    "    kl_anneal_end=0,\n",
    "    #align_anneal_start=model_cfg.get(\"align_anneal_start\", 0),\n",
    "    align_anneal_start=0,\n",
    "    #align_anneal_end=model_cfg.get(\"align_anneal_end\", 0),\n",
    "    align_anneal_end=0,\n",
    ")\n",
    "\n",
    "print(\"UniVIConfig:\", univi_cfg)\n",
    "\n",
    "# -------------------------\n",
    "# 4. TrainingConfig\n",
    "# -------------------------\n",
    "train_cfg = TrainingConfig(\n",
    "    n_epochs=train_cfg_json[\"n_epochs\"],\n",
    "    batch_size=train_cfg_json[\"batch_size\"],\n",
    "    lr=train_cfg_json[\"lr\"],\n",
    "    weight_decay=train_cfg_json.get(\"weight_decay\", 0.0),\n",
    "    #device=train_cfg_json.get(\"device\", \"cpu\"),  # use \"cpu\" if no CUDA\n",
    "    device=device,\n",
    "    log_every=train_cfg_json.get(\"log_every\", 10),\n",
    "    grad_clip=train_cfg_json.get(\"grad_clip\", None),\n",
    "    num_workers=train_cfg_json.get(\"num_workers\", 0),\n",
    "    seed=train_cfg_json.get(\"seed\", 0),\n",
    "    early_stopping=train_cfg_json.get(\"early_stopping\", True),\n",
    "    patience=train_cfg_json.get(\"patience\", 20),\n",
    "    min_delta=train_cfg_json.get(\"min_delta\", 0.0),\n",
    ")\n",
    "\n",
    "print(\"TrainingConfig:\", train_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabdf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from univi.data import MultiModalDataset\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 0. Sanity check: RNA / ADT are already aligned\n",
    "# --------------------------------------------------\n",
    "assert rna_adata_hvg.n_obs == adt_adata.n_obs, \"RNA and ADT have different #cells\"\n",
    "assert np.array_equal(rna_adata_hvg.obs_names, adt_adata.obs_names), (\n",
    "    \"RNA and ADT obs_names are not aligned – align them first.\"\n",
    ")\n",
    "\n",
    "print(f\"Total paired cells BEFORE subsampling: {rna_adata_hvg.n_obs}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Per-celltype subsampling for balance\n",
    "# --------------------------------------------------\n",
    "celltype_key = \"celltype.l1\"\n",
    "max_per_type = 2000\n",
    "\n",
    "labels = rna_adata_hvg.obs[celltype_key].astype(str).values\n",
    "unique_ct = np.unique(labels)\n",
    "\n",
    "rng = np.random.default_rng(train_cfg.seed)\n",
    "\n",
    "selected_indices_list = []\n",
    "for ct in unique_ct:\n",
    "    idx_ct = np.where(labels == ct)[0]\n",
    "    if len(idx_ct) == 0:\n",
    "        continue\n",
    "    if len(idx_ct) > max_per_type:\n",
    "        chosen = rng.choice(idx_ct, size=max_per_type, replace=False)\n",
    "    else:\n",
    "        chosen = idx_ct\n",
    "    selected_indices_list.append(chosen)\n",
    "\n",
    "selected_indices = np.concatenate(selected_indices_list)\n",
    "rng.shuffle(selected_indices)\n",
    "\n",
    "n_cells = len(selected_indices)\n",
    "print(f\"Total paired cells AFTER per-celltype cap: {n_cells}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Build MultiModalDataset (full, indices will subset)\n",
    "# --------------------------------------------------\n",
    "adata_by_mod = {\"rna\": rna_adata_hvg, \"adt\": adt_adata}\n",
    "\n",
    "full_dataset = MultiModalDataset(\n",
    "    adata_dict=adata_by_mod,\n",
    "    X_key=\"X\",                # or your desired layer/key\n",
    "    device=train_cfg.device,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Train / val / test splits on selected_indices\n",
    "# --------------------------------------------------\n",
    "frac_train = 0.8\n",
    "frac_val   = 0.1\n",
    "\n",
    "n_train = int(frac_train * n_cells)\n",
    "n_val   = int(frac_val   * n_cells)\n",
    "\n",
    "train_idx = selected_indices[:n_train]\n",
    "val_idx   = selected_indices[n_train:n_train + n_val]\n",
    "test_idx  = selected_indices[n_train + n_val:]\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset   = Subset(full_dataset, val_idx)\n",
    "test_dataset  = Subset(full_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=train_cfg.num_workers,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Mark splits + unused cells for later reference\n",
    "#    (CRITICAL: use rna_adata_hvg, not rna_adata)\n",
    "# --------------------------------------------------\n",
    "def init_split_column(adata, col=\"univi_split\"):\n",
    "    if col not in adata.obs.columns:\n",
    "        adata.obs[col] = \"unused\"\n",
    "    else:\n",
    "        adata.obs[col] = \"unused\"\n",
    "\n",
    "# mark on HVG RNA and on ADT\n",
    "init_split_column(rna_adata_hvg, \"univi_split\")\n",
    "init_split_column(adt_adata,     \"univi_split\")\n",
    "\n",
    "# start everything as unused\n",
    "rna_adata_hvg.obs[\"univi_split\"] = \"unused\"\n",
    "adt_adata.obs[\"univi_split\"]     = \"unused\"\n",
    "\n",
    "# mark train / val / test by positional indices\n",
    "rna_adata_hvg.obs.iloc[train_idx, rna_adata_hvg.obs.columns.get_loc(\"univi_split\")] = \"train\"\n",
    "rna_adata_hvg.obs.iloc[val_idx,   rna_adata_hvg.obs.columns.get_loc(\"univi_split\")] = \"val\"\n",
    "rna_adata_hvg.obs.iloc[test_idx,  rna_adata_hvg.obs.columns.get_loc(\"univi_split\")] = \"test\"\n",
    "\n",
    "adt_adata.obs.iloc[train_idx, adt_adata.obs.columns.get_loc(\"univi_split\")] = \"train\"\n",
    "adt_adata.obs.iloc[val_idx,   adt_adata.obs.columns.get_loc(\"univi_split\")] = \"val\"\n",
    "adt_adata.obs.iloc[test_idx,  adt_adata.obs.columns.get_loc(\"univi_split\")] = \"test\"\n",
    "\n",
    "# split AnnData *in the same feature space UniVI was trained on*\n",
    "rna_train_adata = rna_adata_hvg[rna_adata_hvg.obs[\"univi_split\"] == \"train\"].copy()\n",
    "rna_val_adata   = rna_adata_hvg[rna_adata_hvg.obs[\"univi_split\"] == \"val\"].copy()\n",
    "rna_test_adata  = rna_adata_hvg[rna_adata_hvg.obs[\"univi_split\"] == \"test\"].copy()\n",
    "rna_unused      = rna_adata_hvg[rna_adata_hvg.obs[\"univi_split\"] == \"unused\"].copy()\n",
    "\n",
    "adt_train_adata = adt_adata[adt_adata.obs[\"univi_split\"] == \"train\"].copy()\n",
    "adt_val_adata   = adt_adata[adt_adata.obs[\"univi_split\"] == \"val\"].copy()\n",
    "adt_test_adata  = adt_adata[adt_adata.obs[\"univi_split\"] == \"test\"].copy()\n",
    "adt_unused      = adt_adata[adt_adata.obs[\"univi_split\"] == \"unused\"].copy()\n",
    "\n",
    "print(\n",
    "    \"RNA (HVG) split sizes:\",\n",
    "    {k: v.n_obs for k, v in dict(\n",
    "        train=rna_train_adata,\n",
    "        val=rna_val_adata,\n",
    "        test=rna_test_adata,\n",
    "        unused=rna_unused,\n",
    "    ).items()},\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"ADT split sizes:\",\n",
    "    {k: v.n_obs for k, v in dict(\n",
    "        train=adt_train_adata,\n",
    "        val=adt_val_adata,\n",
    "        test=adt_test_adata,\n",
    "        unused=adt_unused,\n",
    "    ).items()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5283515b",
   "metadata": {},
   "source": [
    "#### Make quick UMAP plots of test sets to see before training clustering etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_rna_umap = rna_test_adata.copy()\n",
    "for_adt_umap = adt_test_adata.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(for_rna_umap)\n",
    "print(for_rna_umap.X.min())\n",
    "print(for_rna_umap.X.max())\n",
    "\n",
    "for_rna_umap.layers['scaled'] = for_rna_umap.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1bb104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) RNA: UMAP from rna_adata.hvg.layers['log1p']\n",
    "# =========================\n",
    "\n",
    "print(\"RNA shape:\", for_rna_umap.shape)\n",
    "\n",
    "# Shallow copy for PCA so we don't mess with the original X\n",
    "rna_pca = for_rna_umap.copy()\n",
    "\n",
    "# Use the scaled log1p layer as X for PCA\n",
    "rna_pca.X = for_rna_umap.layers[\"scaled\"].copy()\n",
    "\n",
    "# Run PCA\n",
    "sc.tl.pca(rna_pca, n_comps=50)\n",
    "\n",
    "# Store PCA back on the HVG object\n",
    "for_rna_umap.obsm[\"X_pca\"] = rna_pca.obsm[\"X_pca\"]\n",
    "for_rna_umap.varm[\"PCs\"]   = rna_pca.varm[\"PCs\"]\n",
    "\n",
    "# ---- clear neighbors from previous attempts ----\n",
    "for k in [\"neighbors\"]:\n",
    "    if k in for_rna_umap.uns:\n",
    "        del for_rna_umap.uns[k]\n",
    "\n",
    "for k in [\"distances\", \"connectivities\"]:\n",
    "    if k in for_rna_umap.obsp:\n",
    "        del for_rna_umap.obsp[k]\n",
    "\n",
    "# ---- recompute neighbors / UMAP ----\n",
    "sc.pp.neighbors(\n",
    "    for_rna_umap,\n",
    "    use_rep=\"X_pca\",    # use your PCA\n",
    "    n_neighbors=30,     # neighbors (NOT n_pcs)\n",
    "    metric=\"cosine\",    # common for scRNA\n",
    ")\n",
    "\n",
    "sc.tl.umap(for_rna_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a430fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    for_rna_umap,\n",
    "    color=\"celltype.l1\",  # or whatever key you want\n",
    "    size=10,\n",
    "    title=\"RNA UMAP (Z-scaled)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76907a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    for_rna_umap,\n",
    "    color=\"celltype.l2\",  # or whatever key you want\n",
    "    size=10,\n",
    "    title=\"RNA UMAP (Z-scaled)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9aa575",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    for_rna_umap,\n",
    "    color=\"celltype.l3\",  # or whatever key you want\n",
    "    size=10,\n",
    "    title=\"RNA UMAP (Z-scaled)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(for_adt_umap)\n",
    "print(for_adt_umap.X.min())\n",
    "print(for_adt_umap.X.max())\n",
    "\n",
    "for_adt_umap.layers['scaled'] = for_adt_umap.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8431f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) ADT: UMAP from adt_adata.layers['log1p']\n",
    "# =========================\n",
    "\n",
    "print(\"ADT shape:\", for_adt_umap.shape)\n",
    "\n",
    "# Make a shallow copy for PCA so we don't touch the original adt_adata.X\n",
    "adt_pca = for_adt_umap.copy()\n",
    "\n",
    "# Use the Z-scaled CLR-normed layer as X for PCA\n",
    "adt_pca.X = for_adt_umap.layers[\"scaled\"].copy()\n",
    "\n",
    "# Run PCA on the log1p data\n",
    "sc.tl.pca(adt_pca, n_comps=50)\n",
    "\n",
    "# Copy the PCA results back to the original ADT object\n",
    "for_adt_umap.obsm[\"X_pca\"] = adt_pca.obsm[\"X_pca\"]\n",
    "for_adt_umap.varm[\"PCs\"]   = adt_pca.varm[\"PCs\"]\n",
    "\n",
    "# ---- clear any old neighbors graph to avoid 'n_neighbors' KeyError ----\n",
    "if \"neighbors\" in for_adt_umap.uns:\n",
    "    del for_adt_umap.uns[\"neighbors\"]\n",
    "\n",
    "for k in [\"distances\", \"connectivities\"]:\n",
    "    if k in for_adt_umap.obsp:\n",
    "        del for_adt_umap.obsp[k]\n",
    "\n",
    "# ---- recompute neighbors / UMAP using the PCA embedding ----\n",
    "sc.pp.neighbors(\n",
    "    for_adt_umap,\n",
    "    use_rep=\"X_pca\",   # use the PCA we just computed\n",
    "    n_neighbors=30,    # neighbors (NOT n_pcs) – adjust if you like\n",
    "    metric=\"cosine\",\n",
    ")\n",
    "sc.tl.umap(for_adt_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    for_adt_umap,\n",
    "    color=\"celltype.l1\",  # or whatever key you want\n",
    "    size=10,\n",
    "    title=\"ADT UMAP (Z-scaled)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38240e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    for_adt_umap,\n",
    "    color=\"celltype.l2\",  # or whatever key you want\n",
    "    size=10,\n",
    "    title=\"ADT UMAP (Z-scaled)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    for_adt_umap,\n",
    "    color=\"celltype.l3\",  # or whatever key you want\n",
    "    size=10,\n",
    "    title=\"ADT UMAP (Z-scaled)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aaec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Modalities in univi_cfg:\")\n",
    "for m in univi_cfg.modalities:\n",
    "    print(\" \", m)\n",
    "\n",
    "print(\"Number of modalities:\", len(univi_cfg.modalities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f56ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7. Instantiate model + trainer\n",
    "# -------------------------\n",
    "model = UniVIMultiModalVAE(univi_cfg).to(train_cfg.device)\n",
    "\n",
    "trainer = UniVITrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    train_cfg=train_cfg,\n",
    "    device=train_cfg.device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c6ed2",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f789498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 8. Train!\n",
    "# -------------------------\n",
    "history = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16667c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history[\"train_loss\"], label=\"train\")\n",
    "ax.plot(history[\"val_loss\"], label=\"val\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"UniVI CITE-seq training curves\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history[\"beta\"], label=\"beta\")\n",
    "ax.plot(history[\"gamma\"], label=\"gamma\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.set_title(\"KL / alignment annealing\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "os.makedirs(\"../saved_models\", exist_ok=True)\n",
    "\n",
    "# after training\n",
    "#history = trainer.fit()\n",
    "\n",
    "# trainer.model already has the best weights (because we restored best_state_dict)\n",
    "ckpt_path = \"../saved_models/univi_hao_level1_celltype_2k_cap_beta_100_gamma_120_40_hidden_dims_gaussian_decoders_both.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": trainer.model.state_dict(),\n",
    "        \"univi_cfg\": asdict(univi_cfg),\n",
    "        \"best_epoch\": trainer.best_epoch,\n",
    "        \"best_val_loss\": trainer.best_val_loss,\n",
    "    },\n",
    "    ckpt_path,\n",
    ")\n",
    "print(\"Saved best model to:\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcace2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later to reload model\n",
    "'''\n",
    "import torch\n",
    "from univi.config import UniVIConfig, ModalityConfig\n",
    "from univi.models.univi import UniVIMultiModalVAE\n",
    "\n",
    "device = \"cpu\"  # or \"cuda\" if available\n",
    "\n",
    "ckpt = torch.load(\n",
    "    #\"../saved_models/univi_hao_level2_celltype_1k_cap_beta_20_gamma_80.pt\",\n",
    "    #\"../saved_models/univi_hao_level2_celltype_1k_cap_beta_80_gamma_120.pt\",\n",
    "    \"../saved_models/univi_hao_level1_celltype_2k_cap_beta_100_gamma_120_40_hidden_dims_gaussian_decoders_both.pt\",\n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "# ---- Rebuild UniVIConfig, making sure modalities are ModalityConfig objects ----\n",
    "cfg_dict = ckpt[\"univi_cfg\"]\n",
    "\n",
    "# If this is an OmegaConf object or similar, make sure it's a plain dict\n",
    "try:\n",
    "    from omegaconf import DictConfig, OmegaConf\n",
    "    if isinstance(cfg_dict, DictConfig):\n",
    "        cfg_dict = OmegaConf.to_container(cfg_dict, resolve=True)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Now rehydrate each modality\n",
    "modalities = [ModalityConfig(**m) for m in cfg_dict[\"modalities\"]]\n",
    "cfg_dict = {**cfg_dict, \"modalities\": modalities}\n",
    "\n",
    "univi_cfg_loaded = UniVIConfig(**cfg_dict)\n",
    "\n",
    "# ---- Rebuild model + load weights ----\n",
    "model_loaded = UniVIMultiModalVAE(univi_cfg_loaded).to(device)\n",
    "model_loaded.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "print(\"Best epoch was:\", ckpt.get(\"best_epoch\"), \"val loss =\", ckpt.get(\"best_val_loss\"))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bca0ca",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b50e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Previous code results! Beta: 20, Gamma: 80\n",
    "'''\n",
    "FOSCTTM (rna vs adt): 0.0995\n",
    "Modality mixing score (k=20): 0.056\n",
    "Label transfer accuracy (ADT → RNA, k=15): 0.543\n",
    "Mean ADT MSE (RNA→ADT): 4224.1214\n",
    "Mean ADT Pearson r (RNA→ADT): 0.502\n",
    "'''\n",
    "\n",
    "# Previous code results! Beta: 80, Gamma: 120\n",
    "'''\n",
    "FOSCTTM (rna vs adt): 0.0340\n",
    "Modality mixing score (k=20): 0.282\n",
    "Label transfer accuracy (ADT → RNA, k=15): 0.566\n",
    "Mean ADT MSE (RNA→ADT): 12781.6353\n",
    "Mean ADT Pearson r (RNA→ADT): 0.369\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "\n",
    "from univi import evaluation as univi_eval\n",
    "# from univi import plotting as univi_plot  # only needed if you still want their saver\n",
    "\n",
    "# -------------------------\n",
    "# 1. Encode latent embeddings\n",
    "# -------------------------\n",
    "z_rna = trainer.encode_modality(rna_test_adata, modality=\"rna\")\n",
    "z_adt = trainer.encode_modality(adt_test_adata, modality=\"adt\")\n",
    "\n",
    "rna_test_adata.obsm[\"X_univi\"] = z_rna\n",
    "adt_test_adata.obsm[\"X_univi\"] = z_adt\n",
    "\n",
    "# -------------------------\n",
    "# 2. FOSCTTM (global alignment)\n",
    "# -------------------------\n",
    "foscttm = univi_eval.compute_foscttm(z_rna, z_adt)\n",
    "print(f\"FOSCTTM (rna vs adt): {foscttm:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Modality mixing in joint embedding\n",
    "# -------------------------\n",
    "Z_joint = np.concatenate([z_rna, z_adt], axis=0)\n",
    "modality_labels = np.array(\n",
    "    [\"rna\"] * z_rna.shape[0] + [\"adt\"] * z_adt.shape[0]\n",
    ")\n",
    "\n",
    "mixing_score = univi_eval.compute_modality_mixing(\n",
    "    Z_joint,\n",
    "    modality_labels,\n",
    "    k=20,\n",
    ")\n",
    "print(f\"Modality mixing score (k=20): {mixing_score:.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. Label transfer (ADT → RNA)\n",
    "# -------------------------\n",
    "labels_rna = rna_test_adata.obs[\"celltype.l2\"].astype(str).values\n",
    "labels_adt = adt_test_adata.obs[\"celltype.l2\"].astype(str).values\n",
    "\n",
    "pred_rna_from_adt, acc_rna, cm_rna = univi_eval.label_transfer_knn(\n",
    "    Z_source=z_adt,\n",
    "    labels_source=labels_adt,\n",
    "    Z_target=z_rna,\n",
    "    labels_target=labels_rna,\n",
    "    k=15,\n",
    ")\n",
    "\n",
    "print(f\"Label transfer accuracy (ADT → RNA, k=15): {acc_rna:.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4a. Confusion matrix plot (show)\n",
    "# -------------------------\n",
    "uniq_labels = np.unique(labels_rna)\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "sns.heatmap(\n",
    "    cm_rna,\n",
    "    annot=False,\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=uniq_labels,\n",
    "    yticklabels=uniq_labels,\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    ")\n",
    "plt.xlabel(\"Predicted (ADT → RNA)\")\n",
    "plt.ylabel(\"True (RNA)\")\n",
    "plt.title(\"ADT → RNA label transfer confusion matrix\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: also save with univi_plot if you like\n",
    "# from univi import plotting as univi_plot\n",
    "# univi_plot.plot_confusion_matrix(\n",
    "#     cm_rna,\n",
    "#     labels=uniq_labels,\n",
    "#     title=\"ADT → RNA label transfer\",\n",
    "#     savepath=\"../figures/citeseq_univi_label_transfer_cm.png\",\n",
    "# )\n",
    "\n",
    "# -------------------------\n",
    "# 5. UMAP visualization (biological structure)\n",
    "#    on the UniVI latent space\n",
    "# -------------------------\n",
    "\n",
    "# Make copies and tag modality\n",
    "rna_tmp = rna_test_adata.copy()\n",
    "adt_tmp = adt_test_adata.copy()\n",
    "rna_tmp.obs[\"modality\"] = \"rna\"\n",
    "adt_tmp.obs[\"modality\"] = \"adt\"\n",
    "\n",
    "# Concatenate\n",
    "combined = rna_tmp.concatenate(\n",
    "    adt_tmp,\n",
    "    join=\"outer\",\n",
    "    batch_key=\"concat_batch\",\n",
    "    batch_categories=[\"rna\", \"adt\"],\n",
    "    index_unique=None,\n",
    ")\n",
    "\n",
    "# Ensure stacked X_univi matches the concat order\n",
    "combined.obsm[\"X_univi\"] = np.vstack([\n",
    "    rna_test_adata.obsm[\"X_univi\"],\n",
    "    adt_test_adata.obsm[\"X_univi\"],\n",
    "])\n",
    "\n",
    "# Neighbors/UMAP on UniVI embedding\n",
    "sc.pp.neighbors(combined, use_rep=\"X_univi\", n_neighbors=30)\n",
    "sc.tl.umap(combined)\n",
    "\n",
    "# Show UMAP with two panels: modality and celltype.l2\n",
    "sc.pl.umap(\n",
    "    combined,\n",
    "    color=[\"modality\", \"celltype.l2\"],\n",
    "    wspace=0.4,\n",
    "    size=10,\n",
    "    alpha=0.7,\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "# Optional: save figure via scanpy\n",
    "# sc.pl.umap(\n",
    "#     combined,\n",
    "#     color=[\"modality\", \"celltype.l2\"],\n",
    "#     wspace=0.4,\n",
    "#     size=10,\n",
    "#     alpha=0.7,\n",
    "#     save=\"_citeseq_univi_umap.png\",\n",
    "# )\n",
    "\n",
    "# -------------------------\n",
    "# 6. Optional: cross-modal reconstruction metrics (RNA → ADT)\n",
    "# -------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_rna = rna_test_adata.X\n",
    "    if sp.issparse(X_rna):\n",
    "        X_rna = X_rna.toarray()\n",
    "    X_rna_t = torch.as_tensor(X_rna, dtype=torch.float32, device=trainer.device)\n",
    "\n",
    "    xhat_adt_list = []\n",
    "    batch_size = 512\n",
    "    for start in range(0, X_rna_t.shape[0], batch_size):\n",
    "        xb = X_rna_t[start:start + batch_size]\n",
    "        mu_dict, logvar_dict = model.encode_modalities({\"rna\": xb})\n",
    "        mu_z, logvar_z = model.mixture_of_experts(mu_dict, logvar_dict)\n",
    "        xhat_dict = model.decode_modalities(mu_z)\n",
    "        xhat_adt_list.append(xhat_dict[\"adt\"].cpu().numpy())\n",
    "\n",
    "    xhat_adt = np.vstack(xhat_adt_list)\n",
    "\n",
    "# compare to observed ADT\n",
    "X_adt = adt_test_adata.X\n",
    "if sp.issparse(X_adt):\n",
    "    X_adt = X_adt.toarray()\n",
    "\n",
    "mse_feat = univi_eval.mse_per_feature(X_adt, xhat_adt)\n",
    "corr_feat = univi_eval.pearson_corr_per_feature(X_adt, xhat_adt)\n",
    "\n",
    "print(f\"Mean ADT MSE (RNA→ADT): {mse_feat.mean():.4f}\")\n",
    "print(f\"Mean ADT Pearson r (RNA→ADT): {corr_feat.mean():.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6a. Histogram of feature-wise Pearson r\n",
    "# -------------------------\n",
    "plt.figure(figsize=(18, 16))\n",
    "plt.hist(corr_feat, bins=30)\n",
    "plt.xlabel(\"Pearson r (per ADT feature)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"RNA→ADT reconstruction: feature-wise correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7879daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e24f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# New eval code!\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "\n",
    "from univi import evaluation as univi_eval\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    silhouette_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Which celltype resolution to use?\n",
    "celltype_res = 'celltype.l1'\n",
    "\n",
    "# ============================================================\n",
    "# 0. Sanity checks\n",
    "# ============================================================\n",
    "assert rna_test_adata.n_obs == adt_test_adata.n_obs, \"RNA and ADT TEST sets must have same #cells\"\n",
    "assert np.array_equal(rna_test_adata.obs_names, adt_test_adata.obs_names), (\n",
    "    \"RNA and ADT obs_names must match 1:1 for pairwise metrics.\"\n",
    ")\n",
    "\n",
    "print(f\"Test cells: {rna_test_adata.n_obs}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. Encode latent embeddings (UniVI)\n",
    "# ============================================================\n",
    "z_rna = trainer.encode_modality(rna_test_adata, modality=\"rna\")\n",
    "z_adt = trainer.encode_modality(adt_test_adata, modality=\"adt\")\n",
    "\n",
    "rna_test_adata.obsm[\"X_univi\"] = z_rna\n",
    "adt_test_adata.obsm[\"X_univi\"] = z_adt\n",
    "\n",
    "# joint embedding for convenience\n",
    "Z_joint = np.concatenate([z_rna, z_adt], axis=0)\n",
    "modality_labels = np.array(\n",
    "    [\"rna\"] * z_rna.shape[0] + [\"adt\"] * z_adt.shape[0]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 2. FOSCTTM (global alignment)\n",
    "# ============================================================\n",
    "foscttm = univi_eval.compute_foscttm(z_rna, z_adt)\n",
    "print(f\"FOSCTTM (rna vs adt): {foscttm:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. Modality mixing score (global)\n",
    "# ============================================================\n",
    "mixing_score = univi_eval.compute_modality_mixing(\n",
    "    Z_joint,\n",
    "    modality_labels,\n",
    "    k=20,\n",
    ")\n",
    "print(f\"Modality mixing score (k=20): {mixing_score:.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. Label transfer (ADT → RNA) + confusion matrix\n",
    "# ============================================================\n",
    "labels_rna_l1 = rna_test_adata.obs[celltype_res].astype(str).values\n",
    "labels_adt_l1 = adt_test_adata.obs[celltype_res].astype(str).values\n",
    "\n",
    "pred_rna_from_adt, acc_rna, cm_rna = univi_eval.label_transfer_knn(\n",
    "    Z_source=z_adt,\n",
    "    labels_source=labels_adt_l1,\n",
    "    Z_target=z_rna,\n",
    "    labels_target=labels_rna_l1,\n",
    "    k=15,\n",
    ")\n",
    "\n",
    "print(f\"Label transfer accuracy (ADT → RNA, k=15): {acc_rna:.3f}\")\n",
    "\n",
    "uniq_labels = np.unique(labels_rna_l1)\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "sns.heatmap(\n",
    "    cm_rna,\n",
    "    annot=False,\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=uniq_labels,\n",
    "    yticklabels=uniq_labels,\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    ")\n",
    "plt.xlabel(\"Predicted (ADT → RNA)\")\n",
    "plt.ylabel(\"True (RNA)\")\n",
    "plt.title(\"ADT → RNA label transfer confusion matrix\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-celltype accuracy for ADT→RNA\n",
    "ct_accs = []\n",
    "for ct in uniq_labels:\n",
    "    mask = labels_rna_l1 == ct\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    ct_acc = (pred_rna_from_adt[mask] == labels_rna_l1[mask]).mean()\n",
    "    ct_accs.append((ct, ct_acc))\n",
    "\n",
    "ct_names, ct_vals = zip(*ct_accs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=ct_vals, y=ct_names, orient=\"h\")\n",
    "plt.xlabel(\"Accuracy (ADT → RNA)\")\n",
    "plt.ylabel(celltype_res)\n",
    "plt.title(\"Label transfer accuracy per celltype (ADT → RNA)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 4b. Symmetric label transfer (RNA → ADT) + confusion matrix\n",
    "# ============================================================\n",
    "pred_adt_from_rna, acc_adt, cm_adt = univi_eval.label_transfer_knn(\n",
    "    Z_source=z_rna,\n",
    "    labels_source=labels_rna_l1,\n",
    "    Z_target=z_adt,\n",
    "    labels_target=labels_adt_l1,\n",
    "    k=15,\n",
    ")\n",
    "\n",
    "print(f\"Label transfer accuracy (RNA → ADT, k=15): {acc_adt:.3f}\")\n",
    "print(f\"Symmetric label transfer (mean ACC): {(acc_rna + acc_adt) / 2:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "sns.heatmap(\n",
    "    cm_adt,\n",
    "    annot=False,\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=uniq_labels,\n",
    "    yticklabels=uniq_labels,\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    ")\n",
    "plt.xlabel(\"Predicted (RNA → ADT)\")\n",
    "plt.ylabel(\"True (ADT)\")\n",
    "plt.title(\"RNA → ADT label transfer confusion matrix\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 5. UMAP visualization on UniVI latent space\n",
    "# ============================================================\n",
    "\n",
    "rna_tmp = rna_test_adata.copy()\n",
    "adt_tmp = adt_test_adata.copy()\n",
    "rna_tmp.obs[\"modality\"] = \"rna\"\n",
    "adt_tmp.obs[\"modality\"] = \"adt\"\n",
    "\n",
    "combined = rna_tmp.concatenate(\n",
    "    adt_tmp,\n",
    "    join=\"outer\",\n",
    "    batch_key=\"concat_batch\",\n",
    "    batch_categories=[\"rna\", \"adt\"],\n",
    "    index_unique=None,\n",
    ")\n",
    "\n",
    "combined.obsm[\"X_univi\"] = np.vstack([\n",
    "    rna_test_adata.obsm[\"X_univi\"],\n",
    "    adt_test_adata.obsm[\"X_univi\"],\n",
    "])\n",
    "\n",
    "sc.pp.neighbors(combined, use_rep=\"X_univi\", n_neighbors=30)\n",
    "sc.tl.umap(combined)\n",
    "\n",
    "# base UMAP panels: modality + celltype.l1 (+ celltype.l2 if present)\n",
    "umap_colors = [\"modality\", \"celltype.l1\"]\n",
    "if \"celltype.l2\" in combined.obs.columns:\n",
    "    umap_colors.append(\"celltype.l2\")\n",
    "if \"celltype.l3\" in combined.obs.columns:\n",
    "    umap_colors.append(\"celltype.l3\")\n",
    "\n",
    "sc.pl.umap(\n",
    "    combined,\n",
    "    color=umap_colors,\n",
    "    wspace=0.4,\n",
    "    size=8,\n",
    "    alpha=0.7,\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6. Clustering quality: Leiden on UniVI + ARI / NMI\n",
    "# ============================================================\n",
    "sc.tl.leiden(combined, key_added=\"leiden_univi\", resolution=1.0)\n",
    "\n",
    "labels_true = combined.obs[celltype_res].astype(str)\n",
    "clusters = combined.obs[celltype_res].astype(str)\n",
    "\n",
    "mask_valid = labels_true.notna()\n",
    "ari = adjusted_rand_score(labels_true[mask_valid], clusters[mask_valid])\n",
    "nmi = normalized_mutual_info_score(labels_true[mask_valid], clusters[mask_valid])\n",
    "\n",
    "print(f\"Leiden vs \" + celltype_Res + \" ARI: {ari:.3f}\")\n",
    "print(f\"Leiden vs \" + celltype_res + \" NMI: {nmi:.3f}\")\n",
    "\n",
    "# UMAP with Leiden clusters\n",
    "sc.pl.umap(\n",
    "    combined,\n",
    "    color=[\"leiden_univi\"],\n",
    "    size=8,\n",
    "    alpha=0.7,\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 7. Silhouette scores (celltype vs modality)\n",
    "# ============================================================\n",
    "Z_for_sil = combined.obsm[\"X_univi\"]\n",
    "n_cells = combined.n_obs\n",
    "max_cells_for_sil = 20000\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "if n_cells > max_cells_for_sil:\n",
    "    idx_sil = rng.choice(n_cells, size=max_cells_for_sil, replace=False)\n",
    "    Z_sil = Z_for_sil[idx_sil]\n",
    "    labels_ct_sil = combined.obs[celltype_res].astype(str).values[idx_sil]\n",
    "    modality_sil = combined.obs[\"modality\"].astype(str).values[idx_sil]\n",
    "else:\n",
    "    idx_sil = np.arange(n_cells)\n",
    "    Z_sil = Z_for_sil\n",
    "    labels_ct_sil = combined.obs[celltype_res].astype(str).values\n",
    "    modality_sil = combined.obs[\"modality\"].astype(str).values\n",
    "\n",
    "# sometimes silhouette fails if only 1 class; guard a bit\n",
    "unique_ct_sil = np.unique(labels_ct_sil)\n",
    "if len(unique_ct_sil) > 1:\n",
    "    sil_celltype = silhouette_score(Z_sil, labels_ct_sil)\n",
    "else:\n",
    "    sil_celltype = np.nan\n",
    "\n",
    "unique_mod_sil = np.unique(modality_sil)\n",
    "if len(unique_mod_sil) > 1:\n",
    "    sil_modality = silhouette_score(Z_sil, modality_sil)\n",
    "else:\n",
    "    sil_modality = np.nan\n",
    "\n",
    "print(f\"Silhouette (\" + celltype_res + \") on UniVI latent: {sil_celltype:.3f}\")\n",
    "print(f\"Silhouette (modality) on UniVI latent:   {sil_modality:.3f}  (lower is better for mixing)\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. Neighborhood label purity + modality entropy\n",
    "# ============================================================\n",
    "# kNN in UniVI space\n",
    "n_neighbors_local = 20\n",
    "nn_joint = NearestNeighbors(n_neighbors=n_neighbors_local, metric=\"euclidean\")\n",
    "nn_joint.fit(Z_for_sil)\n",
    "dist_joint, idx_joint = nn_joint.kneighbors(Z_for_sil)\n",
    "\n",
    "labels_ct = combined.obs[celltype_res].astype(str).values\n",
    "mods = combined.obs[\"modality\"].astype(str).values\n",
    "\n",
    "local_label_purity = []\n",
    "local_modality_entropy = []\n",
    "\n",
    "for i in range(Z_for_sil.shape[0]):\n",
    "    neigh = idx_joint[i, 1:]  # exclude self at [0]\n",
    "    neigh_ct = labels_ct[neigh]\n",
    "    neigh_mod = mods[neigh]\n",
    "\n",
    "    # label purity\n",
    "    purity = (neigh_ct == labels_ct[i]).mean()\n",
    "    local_label_purity.append(purity)\n",
    "\n",
    "    # modality entropy (2-modal max=1 bit)\n",
    "    p_rna = (neigh_mod == \"rna\").mean()\n",
    "    p_adt = 1.0 - p_rna\n",
    "    entropy = 0.0\n",
    "    for p in [p_rna, p_adt]:\n",
    "        if p > 0:\n",
    "            entropy -= p * np.log2(p)\n",
    "    local_modality_entropy.append(entropy)\n",
    "\n",
    "local_label_purity = np.asarray(local_label_purity)\n",
    "local_modality_entropy = np.asarray(local_modality_entropy)\n",
    "\n",
    "combined.obs[\"local_label_purity\"] = local_label_purity\n",
    "combined.obs[\"local_modality_entropy\"] = local_modality_entropy\n",
    "\n",
    "print(f\"Mean local label purity (k={n_neighbors_local}): {local_label_purity.mean():.3f}\")\n",
    "for m in [\"rna\", \"adt\"]:\n",
    "    mask_m = (mods == m)\n",
    "    print(f\"  {m} mean local label purity: {local_label_purity[mask_m].mean():.3f}\")\n",
    "\n",
    "print(f\"Mean local modality entropy (k={n_neighbors_local}): {local_modality_entropy.mean():.3f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(local_label_purity, bins=30)\n",
    "plt.xlabel(\"Local label purity\")\n",
    "plt.ylabel(\"Cells\")\n",
    "plt.title(\"kNN label purity\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(local_modality_entropy, bins=30)\n",
    "plt.xlabel(\"Local modality entropy (bits)\")\n",
    "plt.ylabel(\"Cells\")\n",
    "plt.title(\"kNN modality entropy\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 9. Pairwise matching metrics (top-1 / top-5 / top-10)\n",
    "#    Uses true RNA–ADT pairing via obs_names\n",
    "# ============================================================\n",
    "k_match = 10\n",
    "nn_adt_for_rna = NearestNeighbors(n_neighbors=k_match, metric=\"euclidean\")\n",
    "nn_adt_for_rna.fit(z_adt)\n",
    "dist_ra, idx_ra = nn_adt_for_rna.kneighbors(z_rna)\n",
    "\n",
    "true_idx = np.arange(z_rna.shape[0])\n",
    "top1_hits = (idx_ra[:, 0] == true_idx)\n",
    "top5_hits = (idx_ra[:, :5] == true_idx[:, None]).any(axis=1)\n",
    "top10_hits = (idx_ra[:, :10] == true_idx[:, None]).any(axis=1)\n",
    "\n",
    "print(f\"Pairwise matching (RNA→ADT):\")\n",
    "print(f\"  Top-1 accuracy:  {top1_hits.mean():.3f}\")\n",
    "print(f\"  Top-5 accuracy:  {top5_hits.mean():.3f}\")\n",
    "print(f\"  Top-10 accuracy: {top10_hits.mean():.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(\n",
    "    [\"Top-1\", \"Top-5\", \"Top-10\"],\n",
    "    [top1_hits.mean(), top5_hits.mean(), top10_hits.mean()],\n",
    ")\n",
    "plt.ylabel(\"Fraction of correctly matched pairs\")\n",
    "plt.title(\"Cross-modal matching accuracy (RNA→ADT)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 10. Cross-modal reconstruction metrics (RNA → ADT)\n",
    "# ============================================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_rna = rna_test_adata.X\n",
    "    if sp.issparse(X_rna):\n",
    "        X_rna = X_rna.toarray()\n",
    "    X_rna_t = torch.as_tensor(X_rna, dtype=torch.float32, device=trainer.device)\n",
    "\n",
    "    xhat_adt_list = []\n",
    "    batch_size = 512\n",
    "    for start in range(0, X_rna_t.shape[0], batch_size):\n",
    "        xb = X_rna_t[start:start + batch_size]\n",
    "        mu_dict, logvar_dict = model.encode_modalities({\"rna\": xb})\n",
    "        mu_z, logvar_z = model.mixture_of_experts(mu_dict, logvar_dict)\n",
    "        xhat_dict = model.decode_modalities(mu_z)\n",
    "        xhat_adt_list.append(xhat_dict[\"adt\"].cpu().numpy())\n",
    "\n",
    "    xhat_adt = np.vstack(xhat_adt_list)\n",
    "\n",
    "X_adt = adt_test_adata.X\n",
    "if sp.issparse(X_adt):\n",
    "    X_adt = X_adt.toarray()\n",
    "\n",
    "mse_feat = univi_eval.mse_per_feature(X_adt, xhat_adt)\n",
    "corr_feat = univi_eval.pearson_corr_per_feature(X_adt, xhat_adt)\n",
    "\n",
    "print(f\"Mean ADT MSE (RNA→ADT): {mse_feat.mean():.4f}\")\n",
    "print(f\"Mean ADT Pearson r (RNA→ADT): {corr_feat.mean():.3f}\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(corr_feat, bins=30)\n",
    "plt.xlabel(\"Pearson r (per ADT feature)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"RNA→ADT reconstruction: feature-wise correlation\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(mse_feat, bins=30)\n",
    "plt.xlabel(\"MSE (per ADT feature)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"RNA→ADT reconstruction: feature-wise MSE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10b. Feature-wise R²\n",
    "# ------------------------------------------------------------\n",
    "var_true = X_adt.var(axis=0)\n",
    "# avoid division by 0\n",
    "var_true[var_true == 0] = np.nan\n",
    "r2_feat = 1.0 - mse_feat / var_true\n",
    "\n",
    "print(f\"Mean ADT R² (RNA→ADT): {np.nanmean(r2_feat):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(r2_feat[~np.isnan(r2_feat)], bins=30)\n",
    "plt.xlabel(\"R² (per ADT feature)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"RNA→ADT reconstruction: feature-wise R²\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10c. Per-cell Pearson r (across ADT features)\n",
    "# ------------------------------------------------------------\n",
    "def rowwise_corr(x, y):\n",
    "    # x, y: (n_cells x n_features)\n",
    "    x_center = x - x.mean(axis=1, keepdims=True)\n",
    "    y_center = y - y.mean(axis=1, keepdims=True)\n",
    "    num = (x_center * y_center).sum(axis=1)\n",
    "    denom = np.sqrt((x_center**2).sum(axis=1) * (y_center**2).sum(axis=1))\n",
    "    denom[denom == 0] = np.nan\n",
    "    return num / denom\n",
    "\n",
    "cell_corr = rowwise_corr(X_adt, xhat_adt)\n",
    "print(f\"Mean per-cell ADT Pearson r (RNA→ADT): {np.nanmean(cell_corr):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(cell_corr[~np.isnan(cell_corr)], bins=30)\n",
    "plt.xlabel(\"Pearson r (per cell, ADT profile)\")\n",
    "plt.ylabel(\"Cells\")\n",
    "plt.title(\"RNA→ADT reconstruction: per-cell correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: attach to AnnData for later plotting\n",
    "adt_test_adata.obs[\"rna2adt_cell_corr\"] = cell_corr\n",
    "\n",
    "# Example: UMAP colored by reconstruction quality\n",
    "sc.pl.umap(\n",
    "    combined,\n",
    "    color=[\"local_label_purity\", \"local_modality_entropy\"],\n",
    "    wspace=0.4,\n",
    "    size=8,\n",
    "    alpha=0.7,\n",
    "    show=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b92f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc732e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode new cells to latent embeddings\n",
    "z_rna = univi_eval.encode_adata(model, rna_unused, modality=\"rna\", device=train_cfg.device)\n",
    "z_adt = univi_eval.encode_adata(model, adt_unused, modality=\"adt\", device=train_cfg.device)\n",
    "\n",
    "rna_unused.obsm[\"X_univi\"] = z_rna\n",
    "adt_unused.obsm[\"X_univi\"] = z_adt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross modal generation from one modality to another: RNA -> ADT example\n",
    "Xhat_adt_from_rna = univi_eval.cross_modal_predict(\n",
    "    model,\n",
    "    adata_src=rna_unused,\n",
    "    src_mod=\"rna\",\n",
    "    tgt_mod=\"adt\",\n",
    "    device=train_cfg.device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising using the decoders\n",
    "univi_eval.denoise_adata(model, rna_unused, modality=\"rna\", device=train_cfg.device)\n",
    "univi_eval.denoise_adata(model, adt_unused, modality=\"adt\", device=train_cfg.device)\n",
    "\n",
    "# you now have rna_test_adata.layers[\"univi_denoised\"] etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ec77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "celltype.l2\n",
    "CD14 Mono            42690\n",
    "CD4 Naive            17479\n",
    "NK                   17173\n",
    "CD4 TCM              14889\n",
    "CD8 TEM              11727\n",
    "CD8 Naive            10768\n",
    "B naive               7718\n",
    "CD16 Mono             6320\n",
    "CD4 TEM               4282\n",
    "gdT                   3649\n",
    "B memory              3285\n",
    "CD8 TCM               2883\n",
    "MAIT                  2784\n",
    "Treg                  2507\n",
    "cDC2                  2501\n",
    "B intermediate        2431\n",
    "Platelet              2293\n",
    "CD4 CTL               1736\n",
    "NK_CD56bright          943\n",
    "pDC                    861\n",
    "Doublet                605\n",
    "NK Proliferating       548\n",
    "Plasmablast            366\n",
    "dnT                    356\n",
    "HSPC                   329\n",
    "cDC1                   151\n",
    "ILC                    132\n",
    "CD4 Proliferating      108\n",
    "CD8 Proliferating       91\n",
    "Eryth                   83\n",
    "ASDC                    76\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check total l2 celltypes for the unused RNA adata\n",
    "print(rna_unused)\n",
    "print(set(rna_unused.obs['celltype.l2']))\n",
    "print(rna_unused.obs['celltype.l2'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check total l2 celltypes for the unused ADT adata - should be the same as RNA adata above..\n",
    "print(adt_unused)\n",
    "print(set(adt_unused.obs['celltype.l2']))\n",
    "print(adt_unused.obs['celltype.l2'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from the latent space per cell type\n",
    "\n",
    "# fit per-celltype Gaussians in latent space\n",
    "labels_rna = rna_unused.obs[\"celltype.l2\"].astype(str).values\n",
    "Z_rna = rna_unused.obsm[\"X_univi\"]  # from encode_adata earlier\n",
    "\n",
    "gauss_by_ct = univi_eval.fit_latent_gaussians_by_label(Z_rna, labels_rna)\n",
    "\n",
    "# define how many samples per cell type\n",
    "spec = {\n",
    "    'CD14 Mono': 1000,\n",
    "    'CD4 Naive': 1000,\n",
    "    'NK': 1000,\n",
    "    'CD4 TCM': 1000,\n",
    "    'CD8 TEM': 1000,\n",
    "    'CD8 Naive': 1000,\n",
    "    'B naive': 1000,\n",
    "    'CD16 Mono': 1000,\n",
    "    'CD4 TEM': 1000,\n",
    "    'gdT': 1000,\n",
    "    'B memory': 1000,\n",
    "    'CD8 TCM': 1000,\n",
    "    'MAIT': 1000,\n",
    "    'Treg': 1000,\n",
    "    'cDC2': 1000,\n",
    "    'B intermediate': 1000,\n",
    "    'Platelet': 1000,\n",
    "    'CD4 CTL': 1000,\n",
    "    'NK_CD56bright': 1000,\n",
    "    'pDC': 1000,\n",
    "    'Doublet': 1000,\n",
    "    'NK Proliferating': 1000,\n",
    "    'Plasmablast': 1000,\n",
    "    'dnT': 1000,\n",
    "    'HSPC': 1000,\n",
    "    'cDC1': 1000,\n",
    "    'ILC': 1000,\n",
    "    'CD4 Proliferating': 1000,\n",
    "    'CD8 Proliferating': 1000,\n",
    "    'Eryth': 1000,\n",
    "    'ASDC': 1000,\n",
    "}\n",
    "\n",
    "z_samp_by_ct = univi_eval.sample_from_latent_gaussians(gauss_by_ct, spec, random_state=42)\n",
    "\n",
    "# decode to desired modality\n",
    "def decode_latent_samples(model, z_samp_by_ct, modality: str, device: str = \"cpu\"):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    with torch.no_grad():\n",
    "        for lab, Z in z_samp_by_ct.items():\n",
    "            z_t = torch.as_tensor(Z, dtype=torch.float32, device=device)\n",
    "            xhat_dict = model.decode_modalities(z_t)\n",
    "            out[lab] = xhat_dict[modality].cpu().numpy()\n",
    "    return out\n",
    "\n",
    "synthetic_adt_by_ct = decode_latent_samples(model, z_samp_by_ct, modality=\"adt\", device=train_cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc607280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synthetic_adt_by_ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b39b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "# -----------------------------\n",
    "# Build matrix + labels from dict\n",
    "# -----------------------------\n",
    "# z_samp_by_ct: dict[celltype -> (n_samples, latent_dim)]\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for ct, Z in z_samp_by_ct.items():\n",
    "    X_list.append(Z)\n",
    "    y_list.extend([ct] * Z.shape[0])\n",
    "\n",
    "X = np.vstack(X_list)               # (N_total, latent_dim)\n",
    "y = np.array(y_list, dtype=str)     # (N_total,)\n",
    "\n",
    "print(\"Total synthetic samples:\", X.shape[0])\n",
    "print(\"Latent dim:\", X.shape[1])\n",
    "\n",
    "# -----------------------------\n",
    "# UMAP embedding\n",
    "# -----------------------------\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.3,\n",
    "    metric=\"euclidean\",\n",
    "    random_state=42,\n",
    ")\n",
    "X_umap = reducer.fit_transform(X)   # (N_total, 2)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot colored by cell type\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(18, 16))\n",
    "\n",
    "uniq_cts = np.unique(y)\n",
    "# use a categorical colormap with enough colors\n",
    "cmap = plt.cm.get_cmap(\"tab20\", len(uniq_cts))\n",
    "\n",
    "for i, ct in enumerate(uniq_cts):\n",
    "    idx = (y == ct)\n",
    "    plt.scatter(\n",
    "        X_umap[idx, 0],\n",
    "        X_umap[idx, 1],\n",
    "        s=5,\n",
    "        alpha=0.6,\n",
    "        color=cmap(i),\n",
    "        label=ct,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"UMAP1\")\n",
    "plt.ylabel(\"UMAP2\")\n",
    "plt.title(\"UMAP of synthetic latent samples by cell type\")\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1.0),\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0.0,\n",
    "    fontsize=8,\n",
    "    ncol=1,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "# -----------------------------\n",
    "# Build matrix + labels from dict\n",
    "# -----------------------------\n",
    "# z_samp_by_ct: dict[celltype -> (n_samples, latent_dim)]\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for ct, Z in synthetic_adt_by_ct.items():\n",
    "    X_list.append(Z)\n",
    "    y_list.extend([ct] * Z.shape[0])\n",
    "\n",
    "X = np.vstack(X_list)               # (N_total, latent_dim)\n",
    "y = np.array(y_list, dtype=str)     # (N_total,)\n",
    "\n",
    "print(\"Total synthetic samples:\", X.shape[0])\n",
    "print(\"Decoded dim:\", X.shape[1])\n",
    "\n",
    "# -----------------------------\n",
    "# UMAP embedding\n",
    "# -----------------------------\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.3,\n",
    "    metric=\"euclidean\",\n",
    "    random_state=42,\n",
    ")\n",
    "X_umap = reducer.fit_transform(X)   # (N_total, 2)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot colored by cell type\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(18, 16))\n",
    "\n",
    "uniq_cts = np.unique(y)\n",
    "# use a categorical colormap with enough colors\n",
    "cmap = plt.cm.get_cmap(\"tab20\", len(uniq_cts))\n",
    "\n",
    "for i, ct in enumerate(uniq_cts):\n",
    "    idx = (y == ct)\n",
    "    plt.scatter(\n",
    "        X_umap[idx, 0],\n",
    "        X_umap[idx, 1],\n",
    "        s=5,\n",
    "        alpha=0.6,\n",
    "        color=cmap(i),\n",
    "        label=ct,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"UMAP1\")\n",
    "plt.ylabel(\"UMAP2\")\n",
    "plt.title(\"UMAP of synthetic decoded samples by cell type\")\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1.0),\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0.0,\n",
    "    fontsize=8,\n",
    "    ncol=1,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f46ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ba014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee19ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c99b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7355b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9669a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bdf2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniVI_working_environment_v2",
   "language": "python",
   "name": "univi_working_environment_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
