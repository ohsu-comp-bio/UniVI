{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f458749c-59e2-496d-98cf-2598ae6cb29f",
   "metadata": {},
   "source": [
    "## UniVI manuscript - Figure 8 generation reproducible workflow\n",
    "### Commonly-used data integration tool benchmarking test for Genome Research manuscript revisions - UniVI benchmark (Multiome PBMC): UniVI vs Python baselines that use PyTorch\n",
    "\n",
    "Andrew Ashford, Pathways + Omics Group, Oregon Health & Science University, Portland, OR – 12/17/2025\n",
    "\n",
    "This notebook:\n",
    "- loads paired Multiome PBMC (RNA+ATAC)\n",
    "- runs integration baselines available in a PyTorch/Python environment:\n",
    "  - UniVI (our method)\n",
    "  - scvi-tools MultiVI (paired RNA+ATAC)\n",
    "  - MultiMAP (Teichlab)\n",
    "  - scGLUE (optional; requires a guidance graph)\n",
    "- evaluates with:\n",
    "  - FOSCTTM (paired alignment)\n",
    "  - modality mixing (kNN modality entropy-ish)\n",
    "  - kNN label transfer (RNA→ATAC, ATAC→RNA)\n",
    "- saves embeddings + a summary table\n",
    "\n",
    "You can see our manuscript, \"Unifying multimodal single-cell data with a mixture-of-experts β-variational autoencoder framework\" which is currently being revised for Genome Research and is available on bioRxiv at the following link: https://www.biorxiv.org/content/10.1101/2025.02.28.640429v1.full\n",
    "\n",
    "GitHub for the project - including a Quickstart guide - can be found at: https://github.com/Ashford-A/UniVI\n",
    "\n",
    "Package is pip installable via the command: \n",
    "```bash\n",
    "pip install univi\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24224c47-16b8-4ded-b1b8-b405717ec421",
   "metadata": {},
   "source": [
    "### Setting cache directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb51c89-d170-4fd9-a0f2-1c7e629e4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Put these on your larger-quota filesystem (RDS). Adjust if needed.\n",
    "RDS = Path(\"/home/groups/precepts/ashforda\")\n",
    "CACHE = RDS / \"cache\"\n",
    "WORK = RDS / \"runs_multiome_py_1-31-2026\"       # outputs go here\n",
    "# Should have used WORK directory:\n",
    "#WORK = RDS / \"univi_bench/runs_multiome_py_1-24-2026\"\n",
    "DATA = RDS / \"data\"                             # where your .h5mu / 10x files live\n",
    "\n",
    "for p in [CACHE, WORK, DATA]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"XDG_CACHE_HOME\"] = str(CACHE / \"xdg\")\n",
    "os.environ[\"MPLCONFIGDIR\"] = str(CACHE / \"mpl\")\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = str(CACHE / \"numba\")\n",
    "os.environ[\"HF_HOME\"] = str(CACHE / \"hf\")\n",
    "os.environ[\"TORCH_HOME\"] = str(CACHE / \"torch\")\n",
    "os.environ[\"JUPYTER_PLATFORM_DIRS\"] = \"1\"\n",
    "\n",
    "print(\"WORK:\", WORK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4ad96-8db5-412e-bca8-2dfa91e1df71",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76df30-3570-4699-89fc-e90bc61bbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, pairwise_distances, adjusted_rand_score, normalized_mutual_info_score, silhouette_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a164863-9b17-4912-ab2d-8862ab18129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "\n",
    "x = np.linspace(0, np.pi, 5)\n",
    "print(special.sph_legendre_p(2, 1, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a665c3-bd0a-468b-a578-ac4f504a2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.metadata as im\n",
    "for pkg in [\"jax\", \"jaxlib\"]:\n",
    "    try:\n",
    "        dist = im.distribution(pkg)\n",
    "        print(pkg, dist.version, \"installer:\", dist.read_text(\"INSTALLER\"))\n",
    "    except Exception as e:\n",
    "        print(pkg, \"not found:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c20d4dd-0b27-4f42-9024-9ae0a9feac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "env_bin = str(Path(sys.executable).resolve().parent)  # .../univi-bench-py/bin\n",
    "os.environ[\"PATH\"] = env_bin + \":\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "print(\"env_bin:\", env_bin)\n",
    "print(\"bedtools which:\", shutil.which(\"bedtools\"))\n",
    "!bedtools --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba75754-93cb-4291-ab53-7246c1f0054e",
   "metadata": {},
   "source": [
    "### Universal helper functions - timing, subsampling, saving, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130f841-fc85-43d1-a5b2-7f9c5f899783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return time.perf_counter()\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def subsample_idx(n, k, seed=0):\n",
    "    if k is None or k >= n:\n",
    "        return np.arange(n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.choice(n, size=int(k), replace=False)\n",
    "\n",
    "def save_npz(path, **arrays):\n",
    "    path = str(path)\n",
    "    np.savez_compressed(path, **arrays)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88633519-97f9-4ccb-b505-495726262735",
   "metadata": {},
   "source": [
    "### Load Multiome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef02e44-153f-4b0e-9ce5-5d1086ec8151",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"/home/groups/precepts/ashforda/UniVI_v2/UniVI_older-non_git/data/PBMC_10x_Multiome_data/10x_Genomics_Multiome_data\")\n",
    "\n",
    "RNA_PATH = DATA_ROOT / \"10x-Multiome-Pbmc10k-RNA.h5ad\"\n",
    "ATAC_PATH = DATA_ROOT / \"10x-Multiome-Pbmc10k-ATAC.h5ad\"\n",
    "\n",
    "print(\"RNA file:\", RNA_PATH)\n",
    "print(\"ATAC file:\", ATAC_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fec10-65c7-4c41-8c92-8915186be74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save plots/metrics (Path-safe, uses WORK)\n",
    "BENCH_DIR = WORK / \"benchmark_eval\"\n",
    "FIGDIR = BENCH_DIR / \"figures\"\n",
    "RUNDIR = WORK / \"runs_multiome\"   # keep consistent with your earlier variable name\n",
    "\n",
    "BENCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# neighbor params\n",
    "K_MIX = 30\n",
    "K_LT = 15\n",
    "#K_LT = 3\n",
    "\n",
    "# FOSCTTM\n",
    "FOSCTTM_SUBSAMPLE_N = 3000  # None = all\n",
    "#RNG_SEED = 0\n",
    "RNG_SEED = 67\n",
    "\n",
    "sc.settings.set_figure_params(dpi=200, figsize=(8, 6))\n",
    "\n",
    "print(\"BENCH_DIR:\", BENCH_DIR)\n",
    "print(\"FIGDIR:\", FIGDIR)\n",
    "print(\"RUNDIR:\", RUNDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fa672-642d-4f49-aefd-082be46e2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = sc.read_h5ad(RNA_PATH)\n",
    "atac = sc.read_h5ad(ATAC_PATH)\n",
    "\n",
    "print(rna)\n",
    "print(atac)\n",
    "\n",
    "print(\"RNA obs names head:\", rna.obs_names[:5].tolist())\n",
    "print(\"ATAC obs names head:\", atac.obs_names[:5].tolist())\n",
    "\n",
    "print(\"paired cells:\", rna.n_obs, \"shared:\", rna.n_obs == atac.n_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e4f77-02a4-4795-ab9f-6d9d3eee7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def _is_integerish(X, *, tol=1e-6, max_check=200_000, seed=0):\n",
    "    \"\"\"\n",
    "    True if values look like integers (within tol). Works for dense/sparse.\n",
    "    For big matrices, randomly samples up to max_check entries.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    if sp.issparse(X):\n",
    "        data = X.data\n",
    "        if data.size == 0:\n",
    "            return True  # all zeros\n",
    "        # sample nonzeros\n",
    "        if data.size > max_check:\n",
    "            idx = rng.choice(data.size, size=max_check, replace=False)\n",
    "            data = data[idx]\n",
    "        return np.all(np.abs(data - np.round(data)) <= tol)\n",
    "\n",
    "    arr = np.asarray(X)\n",
    "    if arr.size == 0:\n",
    "        return True\n",
    "    flat = arr.ravel()\n",
    "    if flat.size > max_check:\n",
    "        idx = rng.choice(flat.size, size=max_check, replace=False)\n",
    "        flat = flat[idx]\n",
    "    # ignore NaNs if any\n",
    "    flat = flat[np.isfinite(flat)]\n",
    "    if flat.size == 0:\n",
    "        return True\n",
    "    return np.all(np.abs(flat - np.round(flat)) <= tol)\n",
    "\n",
    "def _quick_matrix_info(X):\n",
    "    if sp.issparse(X):\n",
    "        return {\n",
    "            \"type\": type(X).__name__,\n",
    "            \"shape\": X.shape,\n",
    "            \"dtype\": str(X.dtype),\n",
    "            \"nnz\": int(X.nnz),\n",
    "            \"min_nonzero\": float(X.data.min()) if X.nnz else 0.0,\n",
    "            \"max_nonzero\": float(X.data.max()) if X.nnz else 0.0,\n",
    "        }\n",
    "    arr = np.asarray(X)\n",
    "    return {\n",
    "        \"type\": type(arr).__name__,\n",
    "        \"shape\": arr.shape,\n",
    "        \"dtype\": str(arr.dtype),\n",
    "        \"min\": float(np.nanmin(arr)),\n",
    "        \"max\": float(np.nanmax(arr)),\n",
    "    }\n",
    "\n",
    "def inspect_modality(ad, name):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"layers:\", list(ad.layers.keys()))\n",
    "    print(\"raw:\", ad.raw is not None)\n",
    "    print(\"X info:\", _quick_matrix_info(ad.X))\n",
    "    print(\"X integer-ish?\", _is_integerish(ad.X))\n",
    "    for k in ad.layers.keys():\n",
    "        print(f\"layer {k} info:\", _quick_matrix_info(ad.layers[k]))\n",
    "        print(f\"layer {k} integer-ish?\", _is_integerish(ad.layers[k]))\n",
    "\n",
    "inspect_modality(rna, \"RNA\")\n",
    "inspect_modality(atac, \"ATAC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c5d53-701c-485c-b6fb-996441a9f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RNA layers:\", list(rna.layers.keys()))\n",
    "print(\"ATAC layers:\", list(atac.layers.keys()))\n",
    "print(\"RNA raw:\", rna.raw is not None)\n",
    "print(\"RNA X integer-ish?\", _is_integerish(rna.X))\n",
    "for k in rna.layers.keys():\n",
    "    print(\"RNA layer\", k, \"integer-ish?\", _is_integerish(rna.layers[k]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3476a-70e2-426d-b249-d25131f9a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rna)\n",
    "print(atac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4ed31-08ae-485f-b824-8dd7306f5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_KEY = \"cell_type\"  # <-- change to your obs column\n",
    "assert LABEL_KEY in rna.obs.columns, f\"{LABEL_KEY} not in rna.obs\"\n",
    "assert (rna.obs[LABEL_KEY].values == atac.obs[LABEL_KEY].values).all(), \"Labels differ between paired modalities!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4189ee9-d33a-4d8f-9df2-8e8a2429ccc7",
   "metadata": {},
   "source": [
    "### Data preprocessing/splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae32c80-6050-4a86-ac6e-ffa5419739aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_shared_splits(n, labels, *, seed=0, train_frac=0.8, val_frac=0.1):\n",
    "    idx_all = np.arange(n)\n",
    "    y = np.asarray(labels).astype(str)\n",
    "\n",
    "    idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
    "        idx_all, y,\n",
    "        test_size=(1.0 - train_frac),\n",
    "        random_state=seed,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    test_frac = 1.0 - train_frac - val_frac\n",
    "    val_frac_of_tmp = val_frac / (val_frac + test_frac)\n",
    "\n",
    "    idx_val, idx_test = train_test_split(\n",
    "        idx_tmp,\n",
    "        test_size=(1.0 - val_frac_of_tmp),\n",
    "        random_state=seed,\n",
    "        stratify=y_tmp,\n",
    "    )\n",
    "\n",
    "    return {\"train\": np.sort(idx_train), \"val\": np.sort(idx_val), \"test\": np.sort(idx_test)}\n",
    "\n",
    "labels_all = rna.obs[LABEL_KEY].astype(str).to_numpy()\n",
    "splits = make_shared_splits(rna.n_obs, labels_all, seed=RNG_SEED, train_frac=0.8, val_frac=0.1)\n",
    "\n",
    "print({k: len(v) for k, v in splits.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691f021-3d96-441d-864b-99a6421e76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna[splits[\"train\"]].obs[LABEL_KEY].value_counts().to_frame(\"n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d9209-5b00-4a06-9058-f6cc0ec9403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna[splits[\"val\"]].obs[LABEL_KEY].value_counts().to_frame(\"n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279fe75f-37dc-49d8-b226-8d06a73c8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna[splits[\"test\"]].obs[LABEL_KEY].value_counts().to_frame(\"n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f92ef-177d-4694-86cb-b6f388477d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# small utilities\n",
    "# -------------------------\n",
    "def _is_integerish(X) -> bool:\n",
    "    \"\"\"Heuristic: treat sparse/dense as integer-ish if all stored values are near integers.\"\"\"\n",
    "    if sp.issparse(X):\n",
    "        d = X.data\n",
    "        if d.size == 0:\n",
    "            return True\n",
    "        return np.all(np.isfinite(d)) and np.all(np.abs(d - np.rint(d)) < 1e-6)\n",
    "    X = np.asarray(X)\n",
    "    return np.all(np.isfinite(X)) and np.all(np.abs(X - np.rint(X)) < 1e-6)\n",
    "\n",
    "\n",
    "def ensure_counts_layer(adata, layer=\"counts\"):\n",
    "    adata = adata.copy()\n",
    "    if layer in adata.layers and _is_integerish(adata.layers[layer]):\n",
    "        return adata\n",
    "    if adata.raw is not None and _is_integerish(adata.raw.X):\n",
    "        adata.layers[layer] = adata.raw.X.copy()\n",
    "        return adata\n",
    "    adata.layers[layer] = adata.X.copy()\n",
    "    return adata\n",
    "\n",
    "\n",
    "def _to_csr_float32(X):\n",
    "    if sp.issparse(X):\n",
    "        X = X.tocsr(copy=False)\n",
    "        return X.astype(np.float32, copy=False) if X.dtype != np.float32 else X\n",
    "    return sp.csr_matrix(np.asarray(X, dtype=np.float32))\n",
    "\n",
    "\n",
    "def binarize_csr(X):\n",
    "    X = X.tocsr(copy=True)\n",
    "    if X.data.size:\n",
    "        X.data[:] = 1.0\n",
    "    return X\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RNA: HVGs on TRAIN, then (counts subset) + (log1p subset)\n",
    "# -------------------------\n",
    "def fit_hvgs_on_train(rna, train_idx, *, counts_layer=\"counts\", n_hvg=2000, seed=0):\n",
    "    rna = ensure_counts_layer(rna, layer=counts_layer)\n",
    "    rna_tr = rna[train_idx].copy()\n",
    "    rna_tr.X = rna_tr.layers[counts_layer]\n",
    "    try:\n",
    "        sc.pp.highly_variable_genes(rna_tr, n_top_genes=int(n_hvg), flavor=\"seurat_v3\", layer=None)\n",
    "    except Exception:\n",
    "        sc.pp.highly_variable_genes(rna_tr, n_top_genes=int(n_hvg), flavor=\"seurat\", layer=None)\n",
    "\n",
    "    hvg = rna_tr.var_names[rna_tr.var[\"highly_variable\"].to_numpy()].astype(str).tolist()\n",
    "    if len(hvg) == 0:\n",
    "        raise RuntimeError(\"HVG selection returned 0 genes.\")\n",
    "    return hvg\n",
    "\n",
    "\n",
    "def transform_rna_log_hvg(rna, hvg, *, counts_layer=\"counts\", target_sum=1e4):\n",
    "    rna = ensure_counts_layer(rna, layer=counts_layer)\n",
    "    a = rna[:, hvg].copy()\n",
    "    a.layers[\"log1p\"] = a.layers[counts_layer].copy()\n",
    "    sc.pp.normalize_total(a, target_sum=float(target_sum), layer=\"log1p\")\n",
    "    sc.pp.log1p(a, layer=\"log1p\")\n",
    "    a.X = a.layers[\"log1p\"]\n",
    "    return a\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ATAC: TF-IDF + SVD fit on TRAIN, apply to ALL (LSI)\n",
    "# -------------------------\n",
    "def fit_atac_lsi_on_train(\n",
    "    atac, train_idx, *,\n",
    "    counts_layer=\"counts\",\n",
    "    n_lsi=50,\n",
    "    seed=0,\n",
    "    tfidf_kwargs=None,\n",
    "    svd_kwargs=None,\n",
    "    do_l2_norm=False,\n",
    "    do_scale=True,\n",
    "):\n",
    "    atac = ensure_counts_layer(atac, layer=counts_layer)\n",
    "    X = _to_csr_float32(atac.layers[counts_layer])\n",
    "\n",
    "    tfidf_kwargs = tfidf_kwargs or dict(norm=None, use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "    tfidf = TfidfTransformer(**tfidf_kwargs)\n",
    "\n",
    "    Xtr_t = tfidf.fit_transform(X[train_idx])\n",
    "\n",
    "    svd_kwargs = svd_kwargs or dict(algorithm=\"randomized\", n_iter=7, random_state=int(seed))\n",
    "    svd = TruncatedSVD(n_components=int(n_lsi), **svd_kwargs)\n",
    "    Ztr = svd.fit_transform(Xtr_t)\n",
    "\n",
    "    if do_l2_norm:\n",
    "        Ztr = normalize(Ztr, norm=\"l2\", axis=1)\n",
    "\n",
    "    scaler = None\n",
    "    if do_scale:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        Ztr = scaler.fit_transform(Ztr)\n",
    "\n",
    "    return tfidf, svd, scaler\n",
    "\n",
    "\n",
    "def transform_atac_lsi(atac, tfidf, svd, scaler, *, counts_layer=\"counts\", n_lsi=None, do_l2_norm=False):\n",
    "    atac = ensure_counts_layer(atac, layer=counts_layer)\n",
    "    X = _to_csr_float32(atac.layers[counts_layer])\n",
    "\n",
    "    Xt = tfidf.transform(X)\n",
    "    Z = svd.transform(Xt)\n",
    "\n",
    "    if do_l2_norm:\n",
    "        Z = normalize(Z, norm=\"l2\", axis=1)\n",
    "\n",
    "    if scaler is not None:\n",
    "        Z = scaler.transform(Z)\n",
    "\n",
    "    Z = Z.astype(np.float32, copy=False)\n",
    "    if n_lsi is not None:\n",
    "        Z = Z[:, : int(n_lsi)].copy()\n",
    "\n",
    "    atac_lsi = ad.AnnData(\n",
    "        X=Z,\n",
    "        obs=atac.obs.copy(),\n",
    "        var=pd.DataFrame(index=[f\"LSI_{i}\" for i in range(Z.shape[1])]),\n",
    "    )\n",
    "    atac_lsi.obsm[\"X_lsi\"] = Z\n",
    "    return atac_lsi\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ATAC peaks for MultiVI/PeakVI/scMoMaT/etc:\n",
    "# FIXED: select peaks using TRAIN-only detection-rate window + variability\n",
    "# -------------------------\n",
    "def fit_peaks_by_detection_window_on_train(\n",
    "    atac_counts, train_idx, *,\n",
    "    counts_layer=\"counts\",\n",
    "    dr_min=0.01,\n",
    "    dr_max=0.30,\n",
    "    n_peaks=4000,\n",
    "    prefer_var=\"bernoulli\",  # \"bernoulli\" (p*(1-p)) or \"sample_var\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Select peaks using TRAIN only:\n",
    "      1) binarize on the fly\n",
    "      2) filter by detection rate window (dr_min..dr_max)\n",
    "      3) rank by variability (default Bernoulli var p(1-p))\n",
    "    Returns: list of peak names (var_names)\n",
    "    \"\"\"\n",
    "    train_idx = np.asarray(train_idx, dtype=int)\n",
    "    atac_counts = ensure_counts_layer(atac_counts, layer=counts_layer)\n",
    "\n",
    "    X = _to_csr_float32(atac_counts.layers[counts_layer])\n",
    "    Xtr = X[train_idx]\n",
    "    Xtr_bin = (Xtr > 0).astype(np.float32)\n",
    "\n",
    "    dr = np.asarray(Xtr_bin.mean(axis=0)).ravel()  # fraction of train cells with peak open\n",
    "    keep = (dr >= float(dr_min)) & (dr <= float(dr_max))\n",
    "    keep_idx = np.where(keep)[0]\n",
    "    if keep_idx.size == 0:\n",
    "        qs = np.quantile(dr, [0, 0.01, 0.05, 0.1, 0.5, 0.9, 0.95, 0.99, 1.0])\n",
    "        raise RuntimeError(\n",
    "            \"No peaks pass detection-rate window on TRAIN.\\n\"\n",
    "            f\"Try loosening dr_min/dr_max. Current dr_min={dr_min}, dr_max={dr_max}\\n\"\n",
    "            f\"TRAIN detection-rate quantiles: {qs}\"\n",
    "        )\n",
    "\n",
    "    if str(prefer_var).lower() == \"sample_var\":\n",
    "        # variance of binary across train (equivalent-ish to p(1-p), but computed directly)\n",
    "        # NOTE: toarray is heavy; but for 4k selection, we only do it on kept peaks\n",
    "        Xk = Xtr_bin[:, keep_idx].toarray()\n",
    "        score = Xk.var(axis=0)\n",
    "    else:\n",
    "        p = dr[keep_idx]\n",
    "        score = p * (1.0 - p)  # Bernoulli variance\n",
    "\n",
    "    # take top n_peaks by score\n",
    "    k = int(min(n_peaks, keep_idx.size))\n",
    "    order = np.argsort(-score)[:k]\n",
    "    top = keep_idx[order]\n",
    "    top = np.sort(top)\n",
    "\n",
    "    return atac_counts.var_names[top].astype(str).tolist()\n",
    "\n",
    "\n",
    "def subset_to_chr_features(adata, *, chrom_col_candidates=(\"chrom\", \"interval\")):\n",
    "    for col in chrom_col_candidates:\n",
    "        if col in adata.var.columns:\n",
    "            s = adata.var[col].astype(str)\n",
    "            mask = s.str.startswith(\"chr\")\n",
    "            return adata[:, mask].copy()\n",
    "    return adata\n",
    "\n",
    "\n",
    "def summarize_peak_detection(atac_bin, train_idx, *, label=\"ATAC\", n_q=9):\n",
    "    train_idx = np.asarray(train_idx, dtype=int)\n",
    "    X = atac_bin.X\n",
    "    X = X.tocsr(copy=False) if sp.issparse(X) else sp.csr_matrix(np.asarray(X))\n",
    "    Xtr = X[train_idx]\n",
    "    dr = np.asarray((Xtr > 0).mean(axis=0)).ravel()\n",
    "    qs = np.quantile(dr, [0, .01, .05, .10, .50, .90, .95, .99, 1.0])\n",
    "    print(f\"{label} TRAIN detection-rate quantiles (min/1/5/10/50/90/95/99/max): {qs}\")\n",
    "    return dr\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# build everything\n",
    "# -------------------------\n",
    "def build_shared_inputs(\n",
    "    rna, atac, splits, *,\n",
    "    rna_counts_layer=\"counts\",\n",
    "    atac_counts_layer=\"counts\",\n",
    "    n_hvg=2000,\n",
    "    target_sum=1e4,\n",
    "    n_lsi=100,\n",
    "    # peak selection knobs (IMPORTANT)\n",
    "    n_peaks_multivi=4002,\n",
    "    dr_min=0.01,\n",
    "    dr_max=0.30,\n",
    "    seed=0,\n",
    "):\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "\n",
    "    # ---- RNA ----\n",
    "    hvg = fit_hvgs_on_train(rna, tr, counts_layer=rna_counts_layer, n_hvg=n_hvg, seed=seed)\n",
    "\n",
    "    rna = ensure_counts_layer(rna, layer=rna_counts_layer)\n",
    "    rna_counts_hvg = rna[:, hvg].copy()\n",
    "    rna_counts_hvg.X = rna_counts_hvg.layers[rna_counts_layer].copy()\n",
    "\n",
    "    rna_log_hvg = transform_rna_log_hvg(rna, hvg, counts_layer=rna_counts_layer, target_sum=target_sum)\n",
    "\n",
    "    # ---- ATAC (counts + LSI) ----\n",
    "    atac = ensure_counts_layer(atac, layer=atac_counts_layer)\n",
    "    tfidf, svd, scaler = fit_atac_lsi_on_train(\n",
    "        atac, tr, counts_layer=atac_counts_layer, n_lsi=n_lsi, seed=seed,\n",
    "        do_l2_norm=False, do_scale=True,\n",
    "    )\n",
    "    atac_lsi = transform_atac_lsi(atac, tfidf, svd, scaler, counts_layer=atac_counts_layer, n_lsi=n_lsi)\n",
    "\n",
    "    # ---- ATAC (binary counts for peak-based models) ----\n",
    "    atac_counts_bin = atac.copy()\n",
    "    X = _to_csr_float32(atac_counts_bin.layers[atac_counts_layer])\n",
    "    atac_counts_bin.X = binarize_csr(X)\n",
    "\n",
    "    # ---- FIXED peak selection: TRAIN-only DR window + variability ----\n",
    "    peaks = fit_peaks_by_detection_window_on_train(\n",
    "        atac, tr,\n",
    "        counts_layer=atac_counts_layer,\n",
    "        dr_min=dr_min,\n",
    "        dr_max=dr_max,\n",
    "        n_peaks=n_peaks_multivi,\n",
    "        prefer_var=\"bernoulli\",\n",
    "    )\n",
    "    atac_counts_bin_hv = atac_counts_bin[:, peaks].copy()\n",
    "    atac_counts_bin_hv = subset_to_chr_features(atac_counts_bin_hv)\n",
    "\n",
    "    # quick sanity print (so we never again accidentally pick ubiquitous peaks)\n",
    "    _ = summarize_peak_detection(atac_counts_bin_hv, tr, label=\"ATAC selected-peaks\")\n",
    "\n",
    "    return dict(\n",
    "        hvg=hvg,\n",
    "        peaks=peaks,\n",
    "        tfidf=tfidf,\n",
    "        svd=svd,\n",
    "        scaler=scaler,\n",
    "        rna_counts_hvg=rna_counts_hvg,\n",
    "        rna_log_hvg=rna_log_hvg,\n",
    "        atac_counts_bin=atac_counts_bin,\n",
    "        atac_counts_bin_hv=atac_counts_bin_hv,\n",
    "        atac_lsi=atac_lsi,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---- build everything ONCE ----\n",
    "shared = build_shared_inputs(\n",
    "    rna, atac, splits,\n",
    "    n_hvg=2000,\n",
    "    n_peaks_multivi=4002,\n",
    "    dr_min=0.01,   # try 0.005 if too strict\n",
    "    dr_max=0.30,   # try 0.20-0.40 depending on PBMC resolution\n",
    "    n_lsi=101,\n",
    "    seed=RNG_SEED,\n",
    ")\n",
    "\n",
    "rna_counts_hvg      = shared[\"rna_counts_hvg\"]\n",
    "rna_log_hvg         = shared[\"rna_log_hvg\"]\n",
    "atac_counts_bin     = shared[\"atac_counts_bin\"]\n",
    "atac_counts_bin_hv  = shared[\"atac_counts_bin_hv\"]\n",
    "atac_lsi            = shared[\"atac_lsi\"]\n",
    "\n",
    "print(\"ATAC full peaks (bin):\", atac_counts_bin.n_vars)\n",
    "print(\"ATAC selected peaks (bin):\", atac_counts_bin_hv.n_vars)\n",
    "\n",
    "# Canonical per-method inputs (as you were doing)\n",
    "rna_univi     = rna_log_hvg\n",
    "atac_univi    = atac_lsi\n",
    "atac_multivi  = atac_counts_bin_hv\n",
    "rna_multimap  = rna_log_hvg\n",
    "atac_multimap = atac_lsi\n",
    "atac_peakvi   = atac_counts_bin_hv\n",
    "\n",
    "print(\"rna_counts_hvg:\", rna_counts_hvg.shape, \"X dtype:\", rna_counts_hvg.X.dtype)\n",
    "print(\"rna_log_hvg:\", rna_log_hvg.shape, \"X dtype:\", rna_log_hvg.X.dtype)\n",
    "print(\"atac_counts_bin:\", atac_counts_bin.shape, \"X type:\", type(atac_counts_bin.X))\n",
    "print(\"atac_counts_bin_hv:\", atac_counts_bin_hv.shape, \"X type:\", type(atac_counts_bin_hv.X))\n",
    "print(\"atac_lsi:\", atac_lsi.shape, \"X dtype:\", atac_lsi.X.dtype, \"obsm['X_lsi']:\", atac_lsi.obsm[\"X_lsi\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae207874-4736-4e2a-9de7-11c85d221d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "X = atac_counts_bin.X\n",
    "assert sp.issparse(X)\n",
    "print(\"min/max data:\", (X.data.min() if X.data.size else 0), (X.data.max() if X.data.size else 0))\n",
    "print(\"unique values in data (up to 10):\", np.unique(X.data)[:10])\n",
    "# Expect max==1.0 and unique subset of {1.0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0829c9-9a71-4f94-a9e1-f3af04e2ef45",
   "metadata": {},
   "source": [
    "## Unified \"runner\" interface for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e9a8f-c31c-4406-8437-ce31b387abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for some of the outputs of all the functions\n",
    "def _standard_extra_json(*, transductive: bool, uses_labels: bool, **kw):\n",
    "    d = dict(transductive=bool(transductive), uses_labels=bool(uses_labels))\n",
    "    d.update(kw)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f99ff-3219-4d80-b18f-e1c6a0738f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_flags(*, transductive: bool, uses_labels: bool, **extra):\n",
    "    \"\"\"\n",
    "    Standard metadata for fairness/comparability.\n",
    "    transductive=True  -> model fit used non-train (e.g., all cells incl test/val)\n",
    "    uses_labels=True   -> ground-truth labels used during training (semi-supervised)\n",
    "    \"\"\"\n",
    "    d = {\"transductive\": bool(transductive), \"uses_labels\": bool(uses_labels)}\n",
    "    d.update(extra)\n",
    "    return d\n",
    "\n",
    "\n",
    "def ensure_flags(out: dict, *, default_transductive=True, default_uses_labels=False):\n",
    "    \"\"\"\n",
    "    Guarantees flags exist, even if a runner forgot.\n",
    "    \"\"\"\n",
    "    out = dict(out) if out is not None else {}\n",
    "    ej = dict(out.get(\"extra_json\", {}) or {})\n",
    "    ej.setdefault(\"transductive\", bool(default_transductive))\n",
    "    ej.setdefault(\"uses_labels\", bool(default_uses_labels))\n",
    "    out[\"extra_json\"] = ej\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32038b75-fca2-4391-acdc-63945b11421d",
   "metadata": {},
   "source": [
    "### 1) UniVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5ed84-497a-4871-ad43-ae44548c8a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_univi(rna_adata, atac_adata, *, out_dir, splits, seed=0, X_key=\"X\", loss_mode=\"v1\"):\n",
    "    set_seed(seed)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    import univi\n",
    "    from univi import UniVIConfig, ModalityConfig, TrainingConfig, UniVIMultiModalVAE\n",
    "    from univi.data import MultiModalDataset, align_paired_obs_names\n",
    "    from univi.trainer import UniVITrainer\n",
    "\n",
    "    adata_dict = {\"rna\": rna_adata, \"atac\": atac_adata}\n",
    "    adata_dict = align_paired_obs_names(adata_dict)\n",
    "    rna_adata = adata_dict[\"rna\"]\n",
    "    atac_adata = adata_dict[\"atac\"]\n",
    "\n",
    "    cfg = UniVIConfig(\n",
    "        latent_dim=30,\n",
    "        #beta=1.0, gamma=1.25,\n",
    "        #beta=1.25, gamma=4.25,\n",
    "        # Best so far:\n",
    "        beta=1.35, gamma=3.75,\n",
    "        #beta=1.45, gamma=3.25,\n",
    "        # Best so far:\n",
    "        #encoder_dropout=0.05, decoder_dropout=0.00,\n",
    "        encoder_dropout=0.10, decoder_dropout=0.05,\n",
    "        encoder_batchnorm=True, decoder_batchnorm=False,\n",
    "        #encoder_batchnorm=False, decoder_batchnorm=False,\n",
    "        #kl_anneal_start=0, kl_anneal_end=15,\n",
    "        #align_anneal_start=10, align_anneal_end=25,\n",
    "        # Best so far:\n",
    "        kl_anneal_start=25, kl_anneal_end=75,\n",
    "        align_anneal_start=45, align_anneal_end=95,\n",
    "        #kl_anneal_start=0, kl_anneal_end=30,\n",
    "        #align_anneal_start=15, align_anneal_end=45,\n",
    "        modalities=[\n",
    "            ModalityConfig(\"rna\",  rna_adata.n_vars,  [512, 256, 128], [128, 256, 512], likelihood=\"gaussian\"),\n",
    "            ModalityConfig(\"atac\", atac_adata.n_vars, [256, 128, 64],  [64, 128, 256],  likelihood=\"gaussian\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    train_cfg = TrainingConfig(\n",
    "        n_epochs=200, batch_size=256, lr=1e-3, weight_decay=1e-4,\n",
    "        device=device, early_stopping=True, patience=50, log_every=100\n",
    "    )\n",
    "\n",
    "    dataset = MultiModalDataset(adata_dict=adata_dict, X_key=X_key, device=None)\n",
    "\n",
    "    train_loader = DataLoader(Subset(dataset, splits[\"train\"]), batch_size=train_cfg.batch_size, shuffle=True,  num_workers=0)\n",
    "    val_loader   = DataLoader(Subset(dataset, splits[\"val\"]),   batch_size=train_cfg.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = UniVIMultiModalVAE(cfg, loss_mode=loss_mode, v1_recon=\"avg\", normalize_v1_terms=True).to(device)\n",
    "    #model = UniVIMultiModalVAE(cfg, loss_mode=loss_mode, v1_recon=\"avg\", recon_normalize_by_dim=True, recon_dim_power=0.40, normalize_v1_terms=True).to(device)\n",
    "    \n",
    "    trainer = UniVITrainer(model=model, train_loader=train_loader, val_loader=val_loader, train_cfg=train_cfg, device=device)\n",
    "\n",
    "    t0 = now()\n",
    "    _  = trainer.fit()\n",
    "    t1 = now()\n",
    "\n",
    "    # Encode ALL cells (evaluation will slice test)\n",
    "    model.eval()\n",
    "    full_loader = DataLoader(dataset, batch_size=train_cfg.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    Z_rna, Z_atac, Z_fused = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in full_loader:\n",
    "            x_dict = {k: v.to(device) for k, v in batch.items()}\n",
    "            mu_dict, _ = model.encode_modalities(x_dict)\n",
    "            Z_rna.append(mu_dict[\"rna\"].detach().cpu().numpy())\n",
    "            Z_atac.append(mu_dict[\"atac\"].detach().cpu().numpy())\n",
    "            mu_z, _, _ = model.encode_fused(x_dict, use_mean=True)\n",
    "            Z_fused.append(mu_z.detach().cpu().numpy())\n",
    "            \n",
    "    fit_seconds = float(t1 - t0)\n",
    "    \n",
    "    return ensure_flags({\n",
    "        \"Z_rna\": np.concatenate(Z_rna, axis=0),\n",
    "        \"Z_atac\": np.concatenate(Z_atac, axis=0),\n",
    "        \"Z_fused\": np.concatenate(Z_fused, axis=0),\n",
    "        \"fit_seconds\": fit_seconds,\n",
    "        \"extra_json\": standard_flags(transductive=False, uses_labels=False),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab8e03-a883-436d-93b8-65c890a374bc",
   "metadata": {},
   "source": [
    "### 2) MultiVI (scvi-tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc1377-3d1e-4c6f-bb9c-8738e246d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, importlib.util\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"sys.path[0]:\", sys.path[0])\n",
    "print(\"Any local jax* files?:\", glob.glob(\"jax*\") + glob.glob(\"**/jax*\", recursive=False))\n",
    "\n",
    "spec = importlib.util.find_spec(\"jax\")\n",
    "print(\"jax spec:\", spec)\n",
    "print(\"jax origin:\", None if spec is None else spec.origin)\n",
    "\n",
    "spec2 = importlib.util.find_spec(\"jaxlib\")\n",
    "print(\"jaxlib spec:\", spec2)\n",
    "print(\"jaxlib origin:\", None if spec2 is None else spec2.origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6e49c-0d22-42e5-b3aa-d475e6b84485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scvi_train_with_patience(\n",
    "    model,\n",
    "    *,\n",
    "    max_epochs=200,\n",
    "    patience=50,\n",
    "    monitor=\"elbo_validation\",\n",
    "    mode=\"min\",\n",
    "    **train_kwargs,\n",
    "):\n",
    "    import inspect\n",
    "    from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "    sig = inspect.signature(model.train)\n",
    "\n",
    "    # 1) Force scvi internal early stopping OFF (prevents TrainRunner double-passing)\n",
    "    if \"early_stopping\" in sig.parameters:\n",
    "        train_kwargs[\"early_stopping\"] = False\n",
    "\n",
    "    # 2) Remove early-stopping args from the top-level kwargs\n",
    "    for k in (\"early_stopping_patience\", \"early_stopping_monitor\", \"early_stopping_min_delta\"):\n",
    "        train_kwargs.pop(k, None)\n",
    "\n",
    "    # 3) ALSO remove them from scvi plan kwargs containers (where duplicates often come from)\n",
    "    for plan_key in (\"plan_kwargs\", \"training_plan_kwargs\"):\n",
    "        if plan_key in sig.parameters and plan_key in train_kwargs and train_kwargs[plan_key] is not None:\n",
    "            d = dict(train_kwargs[plan_key])\n",
    "            for k in (\"early_stopping_patience\", \"early_stopping_monitor\", \"early_stopping_min_delta\"):\n",
    "                d.pop(k, None)\n",
    "            train_kwargs[plan_key] = d\n",
    "\n",
    "    # 4) Add Lightning EarlyStopping callback via trainer_kwargs (or callbacks if supported)\n",
    "    cb = EarlyStopping(monitor=monitor, patience=int(patience), mode=mode)\n",
    "\n",
    "    if \"trainer_kwargs\" in sig.parameters:\n",
    "        tk = dict(train_kwargs.get(\"trainer_kwargs\") or {})\n",
    "        cbs = list(tk.get(\"callbacks\") or [])\n",
    "        cbs.append(cb)\n",
    "        tk[\"callbacks\"] = cbs\n",
    "        train_kwargs[\"trainer_kwargs\"] = tk\n",
    "    elif \"callbacks\" in sig.parameters:\n",
    "        cbs = list(train_kwargs.get(\"callbacks\") or [])\n",
    "        cbs.append(cb)\n",
    "        train_kwargs[\"callbacks\"] = cbs\n",
    "    else:\n",
    "        # last resort: just train without early stopping rather than crashing\n",
    "        # (you can still set max_epochs lower)\n",
    "        pass\n",
    "\n",
    "    return model.train(max_epochs=int(max_epochs), **train_kwargs)\n",
    "\n",
    "def multivi_latents(model, mdata):\n",
    "    try:\n",
    "        Z_joint = np.asarray(model.get_latent_representation(mdata, modality=\"joint\"))\n",
    "    except Exception:\n",
    "        Z_joint = np.asarray(model.get_latent_representation(mdata))\n",
    "    Z_expr = np.asarray(model.get_latent_representation(mdata, modality=\"rna\"))\n",
    "    Z_acc  = np.asarray(model.get_latent_representation(mdata, modality=\"atac\"))\n",
    "    return Z_expr, Z_acc, Z_joint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7f1da-2699-4fe7-8c5a-29dd1259af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_multivi_mudata(MULTIVI, mdata):\n",
    "    import inspect\n",
    "    sig = inspect.signature(MULTIVI.setup_mudata)\n",
    "\n",
    "    # try the most common patterns\n",
    "    if \"rna_layer\" in sig.parameters or \"atac_layer\" in sig.parameters:\n",
    "        return MULTIVI.setup_mudata(mdata, rna_layer=None, atac_layer=None)\n",
    "\n",
    "    if \"modalities\" in sig.parameters:\n",
    "        # often expects mapping from modality type -> mudata key\n",
    "        return MULTIVI.setup_mudata(mdata, modalities={\"expression\": \"rna\", \"accessibility\": \"atac\"})\n",
    "\n",
    "    # last resort\n",
    "    return MULTIVI.setup_mudata(mdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af4037-6cfc-4e5e-bc6f-5d3481da0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def _align_by_obs_names(a, b):\n",
    "    \"\"\"\n",
    "    Return (a2, b2) restricted to overlapping obs_names, **preserving `a`'s order**.\n",
    "    This avoids index drift (e.g., from np.intersect1d sorting) and keeps splits valid\n",
    "    as long as they were made on `a`'s current order.\n",
    "    \"\"\"\n",
    "    a_names = np.asarray(a.obs_names, dtype=str)\n",
    "    b_names = np.asarray(b.obs_names, dtype=str)\n",
    "\n",
    "    b_set = set(b_names.tolist())\n",
    "\n",
    "    # Preserve the order of `a` (critical!)\n",
    "    common_in_a_order = [x for x in a_names if x in b_set]\n",
    "    if len(common_in_a_order) == 0:\n",
    "        raise ValueError(\"RNA/ATAC have zero overlapping obs_names. Cannot run paired methods.\")\n",
    "\n",
    "    a2 = a[common_in_a_order].copy()\n",
    "    b2 = b[common_in_a_order].copy()\n",
    "\n",
    "    # Final sanity: exact same names + order\n",
    "    if not np.array_equal(np.asarray(a2.obs_names, dtype=str), np.asarray(b2.obs_names, dtype=str)):\n",
    "        raise RuntimeError(\"Post-align mismatch in obs_names order between RNA and ATAC.\")\n",
    "\n",
    "    return a2, b2\n",
    "    \n",
    "\n",
    "def run_multivi(\n",
    "    adata_rna,\n",
    "    adata_atac,\n",
    "    *,\n",
    "    out_dir,\n",
    "    splits,\n",
    "    seed=0,\n",
    "    n_latent=30,\n",
    "    max_epochs=200,\n",
    "    patience=50,\n",
    "    batch_key=None,\n",
    "    rna_layer=\"counts\",\n",
    "    atac_layer=\"counts\",\n",
    "    encode_batch_size=256,\n",
    "):\n",
    "    \"\"\"\n",
    "    scvi-tools MULTIVI runner (leakage-aware):\n",
    "      - trains ONLY on (train+val)\n",
    "      - reloads model onto FULL data to compute embeddings for ALL cells\n",
    "      - returns Z_fused and (if supported) Z_rna / Z_atac\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    import shutil\n",
    "    import muon as mu\n",
    "    import scvi\n",
    "    from scvi.model import MULTIVI\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    scvi.settings.seed = int(seed)\n",
    "\n",
    "    # -------------------------\n",
    "    # align cells by obs_names\n",
    "    # -------------------------\n",
    "    rna0, atac0 = _align_by_obs_names(adata_rna, adata_atac)\n",
    "\n",
    "    # -------------------------\n",
    "    # build FULL MuData\n",
    "    # -------------------------\n",
    "    rna = rna0.copy()\n",
    "    atac = atac0.copy()\n",
    "\n",
    "    # Only create layers if they don't already exist\n",
    "    if rna_layer is not None and rna_layer not in rna.layers:\n",
    "        rna.layers[rna_layer] = rna.X.copy()\n",
    "    if atac_layer is not None and atac_layer not in atac.layers:\n",
    "        atac.layers[atac_layer] = atac.X.copy()\n",
    "\n",
    "    m_full = mu.MuData({\"rna\": rna, \"atac\": atac})\n",
    "    m_full.update()\n",
    "\n",
    "    # -------------------------\n",
    "    # define FIT idx = train + val (your split)\n",
    "    # -------------------------\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "    va = np.asarray(splits.get(\"val\", []), dtype=int)\n",
    "    fit_idx = np.concatenate([tr, va]) if va.size else tr\n",
    "    fit_idx = np.asarray(fit_idx, dtype=int)\n",
    "\n",
    "    # Make FIT mudata (no test cells seen by training)\n",
    "    m_fit = m_full[fit_idx].copy()\n",
    "    m_fit.update()\n",
    "\n",
    "    # -------------------------\n",
    "    # setup_mudata (API drift tolerant)\n",
    "    # -------------------------\n",
    "    setup_err = None\n",
    "    try:\n",
    "        MULTIVI.setup_mudata(\n",
    "            m_fit,\n",
    "            rna_layer=rna_layer,\n",
    "            atac_layer=atac_layer,\n",
    "            batch_key=batch_key,\n",
    "        )\n",
    "    except Exception as e1:\n",
    "        setup_err = e1\n",
    "        # Some versions require modalities mapping\n",
    "        try:\n",
    "            modalities = {\"rna_layer\": \"rna\", \"atac_layer\": \"atac\"}\n",
    "            MULTIVI.setup_mudata(\n",
    "                m_fit,\n",
    "                rna_layer=rna_layer,\n",
    "                atac_layer=atac_layer,\n",
    "                batch_key=batch_key,\n",
    "                modalities=modalities,\n",
    "            )\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(\n",
    "                \"MultiVI setup_mudata failed.\\n\"\n",
    "                f\"m_fit.mod keys={list(m_fit.mod.keys())}\\n\"\n",
    "                f\"Original error: {repr(setup_err)}\\n\"\n",
    "                f\"Fallback error: {repr(e2)}\"\n",
    "            ) from e2\n",
    "\n",
    "    # -------------------------\n",
    "    # train on FIT only\n",
    "    # -------------------------\n",
    "    model = MULTIVI(m_fit, n_latent=int(n_latent))\n",
    "\n",
    "    # Prefer your helper if it exists; else fall back to model.train\n",
    "    if \"scvi_train_with_patience\" in globals():\n",
    "        scvi_train_with_patience(\n",
    "            model,\n",
    "            max_epochs=max_epochs,\n",
    "            patience=patience,\n",
    "            monitor=\"elbo_validation\",\n",
    "            mode=\"min\",\n",
    "            check_val_every_n_epoch=1,\n",
    "        )\n",
    "    else:\n",
    "        # pure scvi fallback\n",
    "        train_sig = inspect.signature(model.train)\n",
    "        train_kwargs = dict(\n",
    "            max_epochs=int(max_epochs),\n",
    "            early_stopping=True,\n",
    "            check_val_every_n_epoch=1,\n",
    "        )\n",
    "        # only pass patience if supported\n",
    "        for k in (\"early_stopping_patience\", \"patience\"):\n",
    "            if k in train_sig.parameters:\n",
    "                train_kwargs[k] = int(patience)\n",
    "                break\n",
    "        model.train(**train_kwargs)\n",
    "\n",
    "    # -------------------------\n",
    "    # save + reload onto FULL data to encode ALL cells\n",
    "    # (this is the key to leakage-aware training but full-data embedding)\n",
    "    # -------------------------\n",
    "    save_dir = out_dir / \"multivi_model\"\n",
    "    if save_dir.exists():\n",
    "        shutil.rmtree(save_dir)\n",
    "    model.save(str(save_dir), overwrite=True)\n",
    "\n",
    "    # Setup FULL mudata with same config *before* loading (important)\n",
    "    # If you don't do this, some versions won't attach the right AnnDataManager.\n",
    "    setup_err2 = None\n",
    "    try:\n",
    "        MULTIVI.setup_mudata(\n",
    "            m_full,\n",
    "            rna_layer=rna_layer,\n",
    "            atac_layer=atac_layer,\n",
    "            batch_key=batch_key,\n",
    "        )\n",
    "    except Exception as e1:\n",
    "        setup_err2 = e1\n",
    "        try:\n",
    "            modalities = {\"rna_layer\": \"rna\", \"atac_layer\": \"atac\"}\n",
    "            MULTIVI.setup_mudata(\n",
    "                m_full,\n",
    "                rna_layer=rna_layer,\n",
    "                atac_layer=atac_layer,\n",
    "                batch_key=batch_key,\n",
    "                modalities=modalities,\n",
    "            )\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(\n",
    "                \"MultiVI setup_mudata on FULL data failed (needed for reload).\\n\"\n",
    "                f\"Original error: {repr(setup_err2)}\\n\"\n",
    "                f\"Fallback error: {repr(e2)}\"\n",
    "            ) from e2\n",
    "\n",
    "    # load signature differs across scvi versions\n",
    "    load_fn = getattr(MULTIVI, \"load\", None)\n",
    "    if load_fn is None or not callable(load_fn):\n",
    "        raise RuntimeError(\"Your scvi-tools MULTIVI has no .load(...). Cannot reload onto full data.\")\n",
    "\n",
    "    load_sig = inspect.signature(load_fn)\n",
    "    load_kwargs = {}\n",
    "    # common: MULTIVI.load(path, adata=...) OR MULTIVI.load(path, mudata=...)\n",
    "    if \"adata\" in load_sig.parameters:\n",
    "        load_kwargs[\"adata\"] = m_full\n",
    "    elif \"mudata\" in load_sig.parameters:\n",
    "        load_kwargs[\"mudata\"] = m_full\n",
    "    else:\n",
    "        # worst case: load(path, ...) but no adata param exposed\n",
    "        # try calling without and hope it was saved with data\n",
    "        load_kwargs = {}\n",
    "\n",
    "    model_full = MULTIVI.load(str(save_dir), **load_kwargs)\n",
    "\n",
    "    # -------------------------\n",
    "    # latent extraction (API drift tolerant)\n",
    "    # -------------------------\n",
    "    def _get_latent(modality=None):\n",
    "        fn = model_full.get_latent_representation\n",
    "        sig = inspect.signature(fn)\n",
    "        kwargs = {}\n",
    "        if \"adata\" in sig.parameters:\n",
    "            kwargs[\"adata\"] = m_full\n",
    "        if \"batch_size\" in sig.parameters:\n",
    "            kwargs[\"batch_size\"] = int(encode_batch_size)\n",
    "        if \"modality\" in sig.parameters and modality is not None:\n",
    "            kwargs[\"modality\"] = modality\n",
    "        return np.asarray(fn(**kwargs), dtype=np.float32)\n",
    "\n",
    "    #Z_fused = _get_latent(modality=None)\n",
    "    # prefer true joint\n",
    "    try:\n",
    "        Z_fused = _get_latent(modality=\"joint\")\n",
    "    except Exception:\n",
    "        Z_fused = _get_latent(modality=None)\n",
    "    \n",
    "    Z_rna = None\n",
    "    Z_atac = None\n",
    "    try:\n",
    "        #Z_rna = _get_latent(modality=\"rna\")\n",
    "        Z_rna = _get_latent(modality=\"expression\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        #Z_atac = _get_latent(modality=\"atac\")\n",
    "        Z_atac = _get_latent(modality=\"accessibility\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    fit_seconds = float(time.perf_counter() - t0)\n",
    "\n",
    "    return ensure_flags({\n",
    "        \"Z_rna\": Z_rna,\n",
    "        \"Z_atac\": Z_atac,\n",
    "        \"Z_fused\": Z_fused,\n",
    "        \"fit_seconds\": fit_seconds,\n",
    "        \"extra_json\": standard_flags(\n",
    "            transductive=False,  # <-- IMPORTANT: trained on train+val only\n",
    "            uses_labels=False,\n",
    "            note=\"Trained on train+val only; model reloaded onto full data to encode all cells.\"\n",
    "        ),\n",
    "    }, default_transductive=False, default_uses_labels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61af51-1b6c-478d-9f3e-a34b1e8500e5",
   "metadata": {},
   "source": [
    "### 3) scGLUE (RNA-ATAC integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91afd0a-1d9e-4768-ae65-1ac355ee2b7e",
   "metadata": {},
   "source": [
    "#### a) Calculate guidance-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabf7ad-9aab-4625-ac28-45f7eaf093a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gzip\n",
    "\n",
    "def _open_textmaybe_gz(path):\n",
    "    path = str(path)\n",
    "    if path.endswith(\".gz\"):\n",
    "        return gzip.open(path, \"rt\")\n",
    "    return open(path, \"rt\")\n",
    "\n",
    "def parse_peaks_from_var(atac, *, chrom_col=None, start_col=None, end_col=None):\n",
    "    \"\"\"\n",
    "    Returns DataFrame with columns: peak, chrom, start, end, mid\n",
    "    Supports:\n",
    "      - atac.var has chrom/start/end columns\n",
    "      - atac.var_names formatted like 'chr1:123-456' or 'chr1_123_456'\n",
    "    \"\"\"\n",
    "    var = atac.var.copy()\n",
    "    if chrom_col and start_col and end_col and all(c in var.columns for c in [chrom_col, start_col, end_col]):\n",
    "        df = pd.DataFrame({\n",
    "            \"peak\": atac.var_names.astype(str),\n",
    "            \"chrom\": var[chrom_col].astype(str).values,\n",
    "            \"start\": var[start_col].astype(int).values,\n",
    "            \"end\":   var[end_col].astype(int).values,\n",
    "        })\n",
    "    elif all(c in var.columns for c in [\"chrom\", \"start\", \"end\"]):\n",
    "        df = pd.DataFrame({\n",
    "            \"peak\": atac.var_names.astype(str),\n",
    "            \"chrom\": var[\"chrom\"].astype(str).values,\n",
    "            \"start\": var[\"start\"].astype(int).values,\n",
    "            \"end\":   var[\"end\"].astype(int).values,\n",
    "        })\n",
    "    else:\n",
    "        peaks = atac.var_names.astype(str)\n",
    "        chrom = []\n",
    "        start = []\n",
    "        end = []\n",
    "        for p in peaks:\n",
    "            m = re.match(r\"^(chr[^:_]+)[:_](\\d+)[\\-_](\\d+)$\", p)\n",
    "            if m is None:\n",
    "                raise ValueError(f\"Can't parse peak name: {p!r}. Provide chrom/start/end columns in atac.var.\")\n",
    "            chrom.append(m.group(1))\n",
    "            start.append(int(m.group(2)))\n",
    "            end.append(int(m.group(3)))\n",
    "        df = pd.DataFrame({\"peak\": peaks, \"chrom\": chrom, \"start\": start, \"end\": end})\n",
    "\n",
    "    df[\"mid\"] = ((df[\"start\"].values + df[\"end\"].values) // 2).astype(int)\n",
    "    return df\n",
    "\n",
    "def parse_gtf_genes(gtf_path, *, gene_id_key=\"gene_id\", gene_name_key=\"gene_name\"):\n",
    "    \"\"\"\n",
    "    Minimal GTF parser: returns DataFrame for 'gene' features with:\n",
    "    gene_id, gene_name, chrom, start, end, strand, tss\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with _open_textmaybe_gz(gtf_path) as f:\n",
    "        for line in f:\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(parts) != 9:\n",
    "                continue\n",
    "            chrom, source, feature, start, end, score, strand, frame, attrs = parts\n",
    "            if feature != \"gene\":\n",
    "                continue\n",
    "\n",
    "            # attribute parsing: key \"value\";\n",
    "            def get_attr(key):\n",
    "                m = re.search(rf'{re.escape(key)}\\s+\"([^\"]+)\"', attrs)\n",
    "                return m.group(1) if m else None\n",
    "\n",
    "            gid = get_attr(gene_id_key)\n",
    "            gname = get_attr(gene_name_key)\n",
    "\n",
    "            if gid is None and gname is None:\n",
    "                continue\n",
    "\n",
    "            start_i = int(start)\n",
    "            end_i = int(end)\n",
    "            tss = start_i if strand == \"+\" else end_i\n",
    "\n",
    "            rows.append({\n",
    "                \"gene_id\": gid,\n",
    "                \"gene_name\": gname,\n",
    "                \"chrom\": chrom,\n",
    "                \"start\": start_i,\n",
    "                \"end\": end_i,\n",
    "                \"strand\": strand,\n",
    "                \"tss\": int(tss),\n",
    "            })\n",
    "\n",
    "    genes = pd.DataFrame(rows)\n",
    "    if genes.empty:\n",
    "        raise ValueError(f\"No gene records parsed from: {gtf_path}\")\n",
    "    return genes\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def restrict_features_to_fit_cells(rna_counts, atac_counts, fit_idx):\n",
    "    \"\"\"\n",
    "    Leakage-aware feature filtering:\n",
    "    keep only genes/peaks that are observed (nnz>0) in FIT cells,\n",
    "    then apply that feature subset to ALL cells.\n",
    "    \"\"\"\n",
    "    fit_idx = np.asarray(fit_idx)\n",
    "\n",
    "    rna_fit = rna_counts[fit_idx]\n",
    "    atac_fit = atac_counts[fit_idx]\n",
    "\n",
    "    Xr = rna_fit.X\n",
    "    Xa = atac_fit.X\n",
    "\n",
    "    if sp.issparse(Xr):\n",
    "        gene_nnz = np.asarray((Xr > 0).sum(axis=0)).ravel()\n",
    "    else:\n",
    "        gene_nnz = (np.asarray(Xr) > 0).sum(axis=0)\n",
    "\n",
    "    if sp.issparse(Xa):\n",
    "        peak_nnz = np.asarray((Xa > 0).sum(axis=0)).ravel()\n",
    "    else:\n",
    "        peak_nnz = (np.asarray(Xa) > 0).sum(axis=0)\n",
    "\n",
    "    keep_genes = gene_nnz > 0\n",
    "    keep_peaks = peak_nnz > 0\n",
    "\n",
    "    # subset ALL cells to the features discovered on FIT cells\n",
    "    rna2 = rna_counts[:, keep_genes].copy()\n",
    "    atac2 = atac_counts[:, keep_peaks].copy()\n",
    "\n",
    "    return rna2, atac2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191ce2f-fafa-4d97-8e9d-55d8887a01e5",
   "metadata": {},
   "source": [
    "You need the guidance graph node names to match rna.var_names and atac.var_names.\n",
    "- If rna.var_names are Ensembl IDs → use gene_id.\n",
    "- If rna.var_names are gene symbols → use gene_name.\n",
    "- If you have both in rna.var, you can map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c795dd-965c-4909-8d4b-55173584a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_gene_key(rna):\n",
    "    # Heuristic: Ensembl IDs often start with ENS\n",
    "    sample = rna.var_names.astype(str)[:100]\n",
    "    ens_like = np.mean([s.startswith(\"ENS\") for s in sample])\n",
    "    return \"gene_id\" if ens_like > 0.5 else \"gene_name\"\n",
    "\n",
    "GENE_KEY = pick_gene_key(rna)\n",
    "print(\"Using GENE_KEY:\", GENE_KEY, \"(must match rna.var_names)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa40d74-edc4-43ab-9da5-a2976322d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rna.var_names)\n",
    "print(GENE_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b4910-e05b-4001-b90f-234de0ecb664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Are var_names unique?\n",
    "print(\"rna var_names unique:\", rna.var_names.is_unique)\n",
    "\n",
    "# 2) After annotation, how many are missing?\n",
    "cols = [\"chrom\", \"chromStart\", \"chromEnd\"]\n",
    "print(\"missing coords:\", rna.var[cols].isna().any(axis=1).sum(), \"of\", rna.n_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855ac2f-a803-4c3c-9a80-3679d8f0a697",
   "metadata": {},
   "source": [
    "Build a bounded distance-based peak↔gene graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0192bf-4f41-4d1f-b068-b0a657024528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_peaks_from_var(atac, chrom_col=None, start_col=None, end_col=None):\n",
    "    # If you *already* have structured columns, prefer them\n",
    "    if chrom_col in (atac.var.columns if hasattr(atac, \"var\") else []) and \\\n",
    "       start_col in atac.var.columns and end_col in atac.var.columns:\n",
    "        df = atac.var[[chrom_col, start_col, end_col]].copy()\n",
    "        df.columns = [\"chrom\", \"start\", \"end\"]\n",
    "        df[\"start\"] = df[\"start\"].astype(int)\n",
    "        df[\"end\"] = df[\"end\"].astype(int)\n",
    "        df[\"peak\"] = atac.var_names.astype(str)\n",
    "    else:\n",
    "        peaks = atac.var_names.astype(str)\n",
    "\n",
    "        chrom, start, end = [], [], []\n",
    "        # accepts: chr1:1-2, 1:1-2, KI270727.1:52331-52752, chr1_1-2, etc.\n",
    "        pat = re.compile(r\"^([^:_]+)[:_](\\d+)[\\-_](\\d+)$\")\n",
    "\n",
    "        for p in peaks:\n",
    "            m = pat.match(p)\n",
    "            if m is None:\n",
    "                raise ValueError(\n",
    "                    f\"Can't parse peak name: {p!r}. \"\n",
    "                    f\"Expected something like 'chr1:100-200' or 'KI270727.1:100-200'.\"\n",
    "                )\n",
    "            chrom.append(m.group(1))\n",
    "            start.append(int(m.group(2)))\n",
    "            end.append(int(m.group(3)))\n",
    "\n",
    "        df = pd.DataFrame({\"chrom\": chrom, \"start\": start, \"end\": end, \"peak\": peaks})\n",
    "\n",
    "    df[\"mid\"] = ((df[\"start\"].values + df[\"end\"].values) // 2).astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccf574-2f69-4831-a329-c987e25b448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_chr_prefix(genes, peaks):\n",
    "    genes_has_chr = (genes[\"chrom\"].astype(str).str.startswith(\"chr\")).mean() > 0.5\n",
    "    peaks_has_chr = (peaks[\"chrom\"].astype(str).str.startswith(\"chr\")).mean() > 0.5\n",
    "\n",
    "    if genes_has_chr and not peaks_has_chr:\n",
    "        # add chr to canonical chroms only\n",
    "        canon = re.compile(r\"^(?:\\d+|X|Y|M|MT)$\")\n",
    "        peaks[\"chrom\"] = peaks[\"chrom\"].astype(str).map(lambda c: (\"chr\" + c) if canon.match(c) else c)\n",
    "\n",
    "    elif peaks_has_chr and not genes_has_chr:\n",
    "        genes[\"chrom\"] = genes[\"chrom\"].astype(str).str.replace(r\"^chr\", \"\", regex=True)\n",
    "\n",
    "    return genes, peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cd8f2-8f90-41d9-adc5-e9eb91ae6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_guidance_graph_distance(\n",
    "    rna, atac, gtf_path,\n",
    "    *,\n",
    "    window=200_000,\n",
    "    k_nearest=10,\n",
    "    promoter_width=3_000,\n",
    "    gene_id_key=\"gene_id\",\n",
    "    gene_name_key=\"gene_name\",\n",
    "    gene_key_for_nodes=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a guidance graph (networkx.Graph) where nodes are:\n",
    "      - genes (named exactly like rna.var_names)\n",
    "      - peaks (named exactly like atac.var_names)\n",
    "\n",
    "    Edges: peak -- gene with attributes:\n",
    "      weight: float\n",
    "      sign: +1\n",
    "      distance: int\n",
    "      in_promoter: bool\n",
    "    \"\"\"\n",
    "    gene_key_for_nodes = gene_key_for_nodes or pick_gene_key(rna)\n",
    "\n",
    "    # Parse and restrict genes to those present in RNA\n",
    "    genes = parse_gtf_genes(gtf_path, gene_id_key=gene_id_key, gene_name_key=gene_name_key)\n",
    "\n",
    "    # Which column will be the \"node name\" to match rna.var_names?\n",
    "    node_col = \"gene_id\" if gene_key_for_nodes == \"gene_id\" else \"gene_name\"\n",
    "    genes = genes.dropna(subset=[node_col]).copy()\n",
    "    genes[\"node\"] = genes[node_col].astype(str)\n",
    "\n",
    "    present_genes = pd.Index(rna.var_names.astype(str))\n",
    "    genes = genes[genes[\"node\"].isin(present_genes)].copy()\n",
    "    if genes.empty:\n",
    "        raise ValueError(\"No genes matched between GTF and rna.var_names. Check GENE_KEY / annotation.\")\n",
    "\n",
    "    # Parse peaks\n",
    "    peaks = parse_peaks_from_var(atac)\n",
    "    peaks[\"node\"] = peaks[\"peak\"].astype(str)\n",
    "\n",
    "    # >>> ADD THIS BLOCK RIGHT HERE <<<\n",
    "    genes, peaks = harmonize_chr_prefix(genes, peaks)\n",
    "\n",
    "    # Optional but helpful: keep only peaks on chroms present in the GTF genes\n",
    "    valid_chroms = set(genes[\"chrom\"].astype(str).unique())\n",
    "    peaks = peaks[peaks[\"chrom\"].astype(str).isin(valid_chroms)].copy()\n",
    "\n",
    "    # Build edges efficiently by chromosome with sorting + two-pointer\n",
    "    G = nx.Graph()\n",
    "    # add nodes\n",
    "    for g in genes[\"node\"].values:\n",
    "        G.add_node(g, kind=\"gene\")\n",
    "    for p in peaks[\"node\"].values:\n",
    "        G.add_node(p, kind=\"peak\")\n",
    "\n",
    "    genes_by_chr = {c: df.sort_values(\"tss\") for c, df in genes.groupby(\"chrom\", sort=False)}\n",
    "    peaks_by_chr = {c: df.sort_values(\"mid\") for c, df in peaks.groupby(\"chrom\", sort=False)}\n",
    "\n",
    "    def _edge_weight(dist):\n",
    "        # smooth decay; tweak if you like\n",
    "        return float(np.exp(-dist / 50_000))\n",
    "\n",
    "    total_edges = 0\n",
    "    for chrom, pdf in peaks_by_chr.items():\n",
    "        gdf = genes_by_chr.get(chrom, None)\n",
    "        if gdf is None or gdf.empty or pdf.empty:\n",
    "            continue\n",
    "\n",
    "        g_tss = gdf[\"tss\"].values\n",
    "        g_nodes = gdf[\"node\"].values\n",
    "\n",
    "        # for each peak, find genes within window using binary search range\n",
    "        for peak_node, mid in zip(pdf[\"node\"].values, pdf[\"mid\"].values):\n",
    "            lo = np.searchsorted(g_tss, mid - window, side=\"left\")\n",
    "            hi = np.searchsorted(g_tss, mid + window, side=\"right\")\n",
    "            if hi <= lo:\n",
    "                continue\n",
    "\n",
    "            # distances for candidates\n",
    "            cand_tss = g_tss[lo:hi]\n",
    "            cand_nodes = g_nodes[lo:hi]\n",
    "            dists = np.abs(cand_tss - mid)\n",
    "\n",
    "            # choose up to k nearest\n",
    "            if len(dists) > k_nearest:\n",
    "                idx = np.argpartition(dists, k_nearest)[:k_nearest]\n",
    "                dists = dists[idx]\n",
    "                cand_nodes = cand_nodes[idx]\n",
    "\n",
    "            # add edges\n",
    "            for gene_node, dist in zip(cand_nodes, dists):\n",
    "                in_prom = bool(dist <= promoter_width)\n",
    "                w = _edge_weight(int(dist))\n",
    "                if in_prom:\n",
    "                    w = float(min(1.0, w * 2.0))  # bump promoter edges\n",
    "\n",
    "                G.add_edge(\n",
    "                    peak_node, gene_node,\n",
    "                    weight=w,\n",
    "                    sign=1,\n",
    "                    distance=int(dist),\n",
    "                    in_promoter=in_prom,\n",
    "                )\n",
    "                total_edges += 1\n",
    "\n",
    "    if total_edges == 0:\n",
    "        raise ValueError(\"Built graph has 0 edges. Likely chr naming mismatch (e.g. '1' vs 'chr1').\")\n",
    "\n",
    "    return G\n",
    "\n",
    "# TODO: set your GTF path\n",
    "GTF = Path(\"/home/groups/precepts/ashforda/scOPE_github_stuff/data/reference/Homo_sapiens_GRCh38.p13.gencode.annotation.gtf\")\n",
    "\n",
    "guidance_graph = build_guidance_graph_distance(\n",
    "    rna_counts_hvg, atac_counts_bin, GTF,\n",
    "    window=200_000,\n",
    "    k_nearest=15,\n",
    "    promoter_width=3_000,\n",
    "    gene_key_for_nodes=pick_gene_key(rna_counts_hvg),\n",
    ")\n",
    "\n",
    "'''\n",
    "# build graph on HV peaks instead of full peaks\n",
    "guidance_graph = build_guidance_graph_distance(\n",
    "    rna_counts_hvg, atac_counts_bin_hv, GTF,\n",
    "    window=150_000, k_nearest=5, promoter_width=2_000,\n",
    "    gene_key_for_nodes=pick_gene_key(rna_counts_hvg),\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Graph nodes prior to adding self-loops:\", guidance_graph.number_of_nodes())\n",
    "print(\"Graph edges prior to adding self-loops:\", guidance_graph.number_of_edges())\n",
    "\n",
    "def add_self_loops(G):\n",
    "    G = G.copy()\n",
    "    for n in G.nodes():\n",
    "        if not G.has_edge(n, n):\n",
    "            G.add_edge(n, n, weight=1.0, sign=1)\n",
    "    return G\n",
    "\n",
    "guidance_graph = add_self_loops(guidance_graph)\n",
    "\n",
    "print(\"Graph nodes:\", guidance_graph.number_of_nodes())\n",
    "print(\"Graph edges:\", guidance_graph.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29151959-e462-4b19-9c7e-f5867bab10aa",
   "metadata": {},
   "source": [
    "(optional) save/reload the guidance graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4693f2e-21e4-4dd6-8ca4-51d52e59e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "GG_PATH = WORK / \"scglue_guidance_graph.pkl\"\n",
    "\n",
    "with open(GG_PATH, \"wb\") as f:\n",
    "    pickle.dump(guidance_graph, f)\n",
    "\n",
    "print(\"Saved:\", GG_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4060ff-7c01-4b72-a8e3-8719ba995780",
   "metadata": {},
   "source": [
    "#### b) Run scGLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9089e86-bf0e-4fb7-8ee5-92a6cf405aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_to_graph(rna_adata, atac_adata, G):\n",
    "    gene_nodes = [n for n, a in G.nodes(data=True) if a.get(\"kind\") == \"gene\"]\n",
    "    peak_nodes = [n for n, a in G.nodes(data=True) if a.get(\"kind\") == \"peak\"]\n",
    "\n",
    "    rna2  = rna_adata[:, rna_adata.var_names.isin(gene_nodes)].copy()\n",
    "    atac2 = atac_adata[:, atac_adata.var_names.isin(peak_nodes)].copy()\n",
    "\n",
    "    if rna2.n_vars == 0 or atac2.n_vars == 0:\n",
    "        raise ValueError(f\"After subsetting to graph nodes: rna vars={rna2.n_vars}, atac vars={atac2.n_vars}\")\n",
    "\n",
    "    return rna2, atac2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eace46-46a5-4288-bd97-d52a61da2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def compute_hvg_on_fit_cells(rna, fit_idx, *, n_top_genes=2000, flavor=\"seurat_v3\", batch_key=None):\n",
    "    \"\"\"\n",
    "    Compute HVGs using FIT cells only, then write rna.var['highly_variable'] for ALL cells.\n",
    "    \"\"\"\n",
    "    import scanpy as sc\n",
    "    fit_idx = np.asarray(fit_idx)\n",
    "\n",
    "    rna_fit = rna[fit_idx].copy()\n",
    "    sc.pp.highly_variable_genes(\n",
    "        rna_fit,\n",
    "        n_top_genes=int(min(n_top_genes, rna_fit.n_vars)),\n",
    "        flavor=flavor,\n",
    "        batch_key=batch_key,\n",
    "        subset=False,\n",
    "        inplace=True,\n",
    "    )\n",
    "    hv = rna_fit.var[\"highly_variable\"].reindex(rna.var_names).fillna(False).astype(bool).values\n",
    "    rna.var[\"highly_variable\"] = hv\n",
    "    return hv\n",
    "\n",
    "\n",
    "def fit_pca_train_apply_all(rna, fit_idx, *, n_pca=100, target_sum=1e4, seed=0, layer=\"counts\"):\n",
    "    \"\"\"\n",
    "    Fit scaler+PCA on FIT cells only, then apply to ALL cells, storing rna.obsm['X_pca'].\n",
    "    Uses rna.layers[layer] if present, else rna.X.\n",
    "    \"\"\"\n",
    "    import scanpy as sc\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    fit_idx = np.asarray(fit_idx)\n",
    "\n",
    "    tmp = rna.copy()\n",
    "    if layer is not None and layer in tmp.layers:\n",
    "        tmp.X = tmp.layers[layer].copy()\n",
    "    else:\n",
    "        tmp.X = tmp.X.copy()\n",
    "\n",
    "    sc.pp.normalize_total(tmp, target_sum=target_sum)\n",
    "    sc.pp.log1p(tmp)\n",
    "\n",
    "    X = tmp.X.toarray() if sp.issparse(tmp.X) else np.asarray(tmp.X)\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_fit = scaler.fit_transform(X[fit_idx])\n",
    "\n",
    "    ncomp = int(min(n_pca, max(2, X_fit.shape[1] - 1)))\n",
    "    pca = PCA(n_components=ncomp, random_state=int(seed))\n",
    "    pca.fit(X_fit)\n",
    "\n",
    "    rna.obsm[\"X_pca\"] = pca.transform(scaler.transform(X)).astype(np.float32, copy=False)\n",
    "    return scaler, pca\n",
    "\n",
    "\n",
    "def _tfidf_fit_transform(X_fit_csr: sp.csr_matrix):\n",
    "    from sklearn.preprocessing import normalize\n",
    "    tf = normalize(X_fit_csr, norm=\"l1\", axis=1)\n",
    "    df = np.asarray((X_fit_csr > 0).sum(axis=0)).ravel().astype(np.float64)\n",
    "    N = X_fit_csr.shape[0]\n",
    "    idf = np.log1p(N / (1.0 + df))\n",
    "    return tf.multiply(idf).tocsr(), idf\n",
    "\n",
    "\n",
    "def _tfidf_transform(X_all_csr: sp.csr_matrix, idf: np.ndarray):\n",
    "    from sklearn.preprocessing import normalize\n",
    "    tf = normalize(X_all_csr, norm=\"l1\", axis=1)\n",
    "    return tf.multiply(idf).tocsr()\n",
    "\n",
    "\n",
    "def compute_hv_peaks_on_fit_cells(atac, fit_idx, *, n_top_peaks=20000):\n",
    "    \"\"\"\n",
    "    Compute HV peaks using FIT cells only (variance in TF-IDF space),\n",
    "    then write atac.var['highly_variable'] for ALL cells.\n",
    "    \"\"\"\n",
    "    fit_idx = np.asarray(fit_idx)\n",
    "\n",
    "    X_all = atac.X\n",
    "    if not sp.issparse(X_all):\n",
    "        X_all = sp.csr_matrix(np.asarray(X_all))\n",
    "    else:\n",
    "        X_all = X_all.tocsr(copy=False)\n",
    "    X_all = X_all.astype(np.float32, copy=False)\n",
    "\n",
    "    X_fit = X_all[fit_idx]\n",
    "    X_tfidf_fit, _idf = _tfidf_fit_transform(X_fit)\n",
    "\n",
    "    mean = np.asarray(X_tfidf_fit.mean(axis=0)).ravel()\n",
    "    mean2 = np.asarray(X_tfidf_fit.power(2).mean(axis=0)).ravel()\n",
    "    var = np.maximum(mean2 - mean**2, 0.0)\n",
    "\n",
    "    n_top = int(min(n_top_peaks, atac.n_vars))\n",
    "    top_idx = np.argpartition(var, -n_top)[-n_top:]\n",
    "    hv = np.zeros(atac.n_vars, dtype=bool)\n",
    "    hv[top_idx] = True\n",
    "\n",
    "    atac.var[\"highly_variable\"] = hv\n",
    "    return hv\n",
    "\n",
    "\n",
    "def fit_lsi_train_apply_all(atac, fit_idx, *, n_lsi=100, seed=0, l2norm=True, drop_first=True):\n",
    "    \"\"\"\n",
    "    Fit TF-IDF + TruncatedSVD on FIT cells only, apply to ALL cells, store atac.obsm['X_lsi'].\n",
    "\n",
    "    If drop_first=True, we fit n_lsi+1 components and drop component 0 (LSI1),\n",
    "    returning exactly n_lsi dims in atac.obsm['X_lsi'].\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    from sklearn.preprocessing import normalize\n",
    "    import numpy as np\n",
    "    import scipy.sparse as sp\n",
    "\n",
    "    fit_idx = np.asarray(fit_idx, dtype=int)\n",
    "\n",
    "    X_all = atac.X\n",
    "    if not sp.issparse(X_all):\n",
    "        X_all = sp.csr_matrix(np.asarray(X_all))\n",
    "    else:\n",
    "        X_all = X_all.tocsr(copy=False)\n",
    "    X_all = X_all.astype(np.float32, copy=False)\n",
    "\n",
    "    X_fit = X_all[fit_idx]\n",
    "    X_tfidf_fit, idf = _tfidf_fit_transform(X_fit)\n",
    "\n",
    "    n_components = int(n_lsi) + (1 if drop_first else 0)\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=int(seed))\n",
    "    svd.fit(X_tfidf_fit)\n",
    "\n",
    "    Z_all = svd.transform(_tfidf_transform(X_all, idf))\n",
    "\n",
    "    if drop_first:\n",
    "        Z_all = Z_all[:, 1:]  # drop LSI1\n",
    "\n",
    "    if l2norm:\n",
    "        Z_all = normalize(Z_all, norm=\"l2\", axis=1)\n",
    "\n",
    "    atac.obsm[\"X_lsi\"] = Z_all.astype(np.float32, copy=False)\n",
    "    return idf, svd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a344359-1840-443f-a867-db237442e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scglue, ignite, torch\n",
    "print(\"scglue:\", getattr(scglue, \"__version__\", \"unknown\"))\n",
    "print(\"ignite:\", getattr(ignite, \"__version__\", \"unknown\"))\n",
    "print(\"torch :\", torch.__version__)\n",
    "print(\"python:\", __import__(\"sys\").version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba3966-34de-464c-bb1c-1fc8955aa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SCGLUE SIMPLE (robust + debug)\n",
    "# - fixes indentation + undefined vars\n",
    "# - passes init_kws/compile_kws/fit_kws/balance_kws safely\n",
    "# - directory is always a real path\n",
    "# ============================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def _annotate_peaks_from_varnames(atac):\n",
    "    \"\"\"\n",
    "    Robust peak parser:\n",
    "      - supports 'chr1:123-456'\n",
    "      - supports 'chr1_123_456'\n",
    "    Drops peaks that cannot be parsed (prevents IntCastingNaNError).\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    peaks = pd.Index(atac.var_names.astype(str))\n",
    "\n",
    "    # pattern A: chr1:123-456\n",
    "    m = peaks.to_series().str.extract(\n",
    "        r\"^(?P<chrom>[^:]+):(?P<start>\\d+)-(?P<end>\\d+)$\"\n",
    "    )\n",
    "\n",
    "    # pattern B: chr1_123_456\n",
    "    bad = m[\"start\"].isna()\n",
    "    if bad.any():\n",
    "        m2 = peaks[bad].to_series().str.extract(\n",
    "            r\"^(?P<chrom>[^_]+)_(?P<start>\\d+)_(?P<end>\\d+)$\"\n",
    "        )\n",
    "        m.loc[bad, [\"chrom\", \"start\", \"end\"]] = m2[[\"chrom\", \"start\", \"end\"]].values\n",
    "\n",
    "    ok = m[\"start\"].notna() & m[\"end\"].notna() & m[\"chrom\"].notna()\n",
    "    n_bad = int((~ok).sum())\n",
    "    if n_bad:\n",
    "        print(f\"[scglue] peak parsing: dropping {n_bad}/{len(peaks)} peaks with unparseable var_names\")\n",
    "        atac = atac[:, ok.values].copy()\n",
    "        m = m.loc[ok].copy()\n",
    "\n",
    "    atac.var[\"chrom\"] = m[\"chrom\"].astype(str).values\n",
    "    atac.var[\"chromStart\"] = m[\"start\"].astype(np.int64).values\n",
    "    atac.var[\"chromEnd\"] = m[\"end\"].astype(np.int64).values\n",
    "    return atac\n",
    "\n",
    "\n",
    "def _fit_scglue_robust(scglue, adatas, graph, *, model_class=None,\n",
    "                      skip_balance=False, init_kws=None, compile_kws=None,\n",
    "                      fit_kws=None, balance_kws=None):\n",
    "    \"\"\"\n",
    "    Call scglue.models.fit_SCGLUE across signature drift safely.\n",
    "    Your install: (adatas, graph, model=..., skip_balance=..., init_kws=..., compile_kws=..., fit_kws=..., balance_kws=...)\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "\n",
    "    init_kws = {} if init_kws is None else init_kws\n",
    "    compile_kws = {} if compile_kws is None else compile_kws\n",
    "    fit_kws = {} if fit_kws is None else fit_kws\n",
    "    balance_kws = {} if balance_kws is None else balance_kws\n",
    "\n",
    "    if not isinstance(init_kws, dict):\n",
    "        raise TypeError(f\"init_kws must be dict, got {type(init_kws)}: {init_kws!r}\")\n",
    "    if not isinstance(compile_kws, dict):\n",
    "        raise TypeError(f\"compile_kws must be dict, got {type(compile_kws)}: {compile_kws!r}\")\n",
    "    if not isinstance(fit_kws, dict):\n",
    "        raise TypeError(f\"fit_kws must be dict, got {type(fit_kws)}: {fit_kws!r}\")\n",
    "    if not isinstance(balance_kws, dict):\n",
    "        raise TypeError(f\"balance_kws must be dict, got {type(balance_kws)}: {balance_kws!r}\")\n",
    "\n",
    "    sig = inspect.signature(scglue.models.fit_SCGLUE)\n",
    "    params = sig.parameters\n",
    "\n",
    "    kwargs = {}\n",
    "    # model / skip_balance are top-level in your signature\n",
    "    if \"model\" in params and model_class is not None:\n",
    "        kwargs[\"model\"] = model_class\n",
    "    if \"skip_balance\" in params:\n",
    "        kwargs[\"skip_balance\"] = bool(skip_balance)\n",
    "\n",
    "    # nested kw dicts in your signature\n",
    "    if \"init_kws\" in params:\n",
    "        kwargs[\"init_kws\"] = init_kws\n",
    "    if \"compile_kws\" in params:\n",
    "        kwargs[\"compile_kws\"] = compile_kws\n",
    "    if \"fit_kws\" in params:\n",
    "        kwargs[\"fit_kws\"] = fit_kws\n",
    "    if \"balance_kws\" in params:\n",
    "        kwargs[\"balance_kws\"] = balance_kws\n",
    "\n",
    "    # fallback for older forks that *don't* nest\n",
    "    if \"init_kws\" not in params:\n",
    "        kwargs.update(init_kws)\n",
    "    if \"compile_kws\" not in params:\n",
    "        kwargs.update(compile_kws)\n",
    "    if \"fit_kws\" not in params:\n",
    "        kwargs.update(fit_kws)\n",
    "    if \"balance_kws\" not in params:\n",
    "        kwargs.update(balance_kws)\n",
    "\n",
    "    return scglue.models.fit_SCGLUE(adatas, graph, **kwargs)\n",
    "\n",
    "\n",
    "def run_scglue_simple(\n",
    "    rna_raw,\n",
    "    atac_raw,\n",
    "    *,\n",
    "    gtf_path,\n",
    "    splits,\n",
    "    out_dir,\n",
    "    seed=0,\n",
    "    latent_dim=30,\n",
    "    max_epochs=200,\n",
    "    val_split=0.1,\n",
    "    n_hvg=2000,\n",
    "    n_pca=100,\n",
    "    n_lsi=101,\n",
    "    fuse=\"mean\",                 # \"mean\" | \"rna\" | \"atac\" | \"concat\" | None\n",
    "    model_class=None,            # e.g. scglue.models.PairedSCGLUEModel (optional)\n",
    "    skip_balance=False,          # set True to skip balancing weights\n",
    "    patience=15,                 # set 0 to avoid restore logic entirely\n",
    "    reduce_lr_patience=8,\n",
    "    debug_print_kws=True,\n",
    "):\n",
    "    import random\n",
    "    import scanpy as sc\n",
    "    import scglue\n",
    "    import torch\n",
    "\n",
    "    # -------------------------\n",
    "    # seeds\n",
    "    # -------------------------\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # -------------------------\n",
    "    # FIT cells (train+val only; test held out)\n",
    "    # -------------------------\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "    va = np.asarray(splits.get(\"val\", []), dtype=int)\n",
    "    fit_idx = np.concatenate([tr, va]) if va.size else tr\n",
    "\n",
    "    # -------------------------\n",
    "    # Restrict features to those observed in FIT cells\n",
    "    # -------------------------\n",
    "    def _nnz_mask(X):\n",
    "        if sp.issparse(X):\n",
    "            return np.asarray((X > 0).sum(axis=0)).ravel() > 0\n",
    "        X = np.asarray(X)\n",
    "        return (X > 0).sum(axis=0) > 0\n",
    "\n",
    "    keep_genes = _nnz_mask(rna_raw[fit_idx].X)\n",
    "    keep_peaks = _nnz_mask(atac_raw[fit_idx].X)\n",
    "\n",
    "    rna = rna_raw[:, keep_genes].copy()\n",
    "    atac = atac_raw[:, keep_peaks].copy()\n",
    "\n",
    "    # -------------------------\n",
    "    # Genomic coords\n",
    "    # -------------------------\n",
    "    gtf_path = str(Path(gtf_path))\n",
    "\n",
    "    v = rna.var_names.astype(str)\n",
    "    frac_ens = np.mean([s.startswith(\"ENS\") for s in v[: min(200, len(v))]])\n",
    "    gtf_by = \"gene_id\" if frac_ens > 0.5 else \"gene_name\"\n",
    "\n",
    "    scglue.data.get_gene_annotation(rna, gtf=gtf_path, gtf_by=gtf_by)\n",
    "\n",
    "    gene_ok = rna.var[[\"chrom\", \"chromStart\", \"chromEnd\"]].notna().all(axis=1).to_numpy()\n",
    "    if gene_ok.sum() < rna.n_vars:\n",
    "        print(f\"[scglue] gene annotation: dropping {rna.n_vars - gene_ok.sum()}/{rna.n_vars} genes without coords\")\n",
    "        rna = rna[:, gene_ok].copy()\n",
    "\n",
    "    atac = _annotate_peaks_from_varnames(atac)\n",
    "\n",
    "    # -------------------------\n",
    "    # RNA preprocessing (HVGs fit on FIT only)\n",
    "    # -------------------------\n",
    "    rna.layers[\"counts\"] = rna.X.copy()\n",
    "\n",
    "    rna_fit2 = rna[fit_idx].copy()\n",
    "    sc.pp.highly_variable_genes(\n",
    "        rna_fit2,\n",
    "        n_top_genes=int(min(n_hvg, rna_fit2.n_vars)),\n",
    "        flavor=\"seurat_v3\",\n",
    "    )\n",
    "    hv = rna_fit2.var[\"highly_variable\"].reindex(rna.var_names).fillna(False).to_numpy(dtype=bool)\n",
    "    rna.var[\"highly_variable\"] = hv\n",
    "\n",
    "    sc.pp.normalize_total(rna, target_sum=1e4)\n",
    "    sc.pp.log1p(rna)\n",
    "    sc.pp.scale(rna, max_value=10, zero_center=False)\n",
    "\n",
    "    try:\n",
    "        sc.tl.pca(rna, n_comps=int(n_pca), svd_solver=\"auto\", mask_var=\"highly_variable\")\n",
    "    except TypeError:\n",
    "        sc.tl.pca(rna, n_comps=int(n_pca), svd_solver=\"auto\", use_highly_variable=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # ATAC preprocessing\n",
    "    # -------------------------\n",
    "    scglue.data.lsi(atac, n_components=int(n_lsi), n_iter=15)\n",
    "\n",
    "    # -------------------------\n",
    "    # Guidance graph + check\n",
    "    # -------------------------\n",
    "    guidance = scglue.genomics.rna_anchored_guidance_graph(rna, atac)\n",
    "    scglue.graph.check_graph(guidance, [rna, atac])\n",
    "\n",
    "    # -------------------------\n",
    "    # Configure dataset for GLUE\n",
    "    # -------------------------\n",
    "    scglue.models.configure_dataset(\n",
    "        rna, \"NB\",\n",
    "        use_highly_variable=True,\n",
    "        use_layer=\"counts\",\n",
    "        use_rep=\"X_pca\",\n",
    "        use_obs_names=True,\n",
    "    )\n",
    "    scglue.models.configure_dataset(\n",
    "        atac, \"NB\",\n",
    "        use_highly_variable=True,\n",
    "        use_rep=\"X_lsi\",\n",
    "        use_obs_names=True,\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # run directory (must exist)\n",
    "    # -------------------------\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    run_dir = out_dir / \"scglue_train\" / time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "    fit_kws = dict(\n",
    "        max_epochs=int(max_epochs),\n",
    "        val_split=float(val_split),\n",
    "        directory=str(run_dir),\n",
    "        patience=int(patience),\n",
    "        reduce_lr_patience=int(reduce_lr_patience),\n",
    "    )\n",
    "    init_kws = dict(latent_dim=int(latent_dim), random_seed=int(seed))\n",
    "    compile_kws = {}   # keep explicit; some forks accept optimizer/lr here\n",
    "    balance_kws = {}   # keep explicit; some forks accept params here\n",
    "\n",
    "    def _ck(name, obj):\n",
    "        if isinstance(obj, str):\n",
    "            s = obj if len(obj) <= 120 else (obj[:120] + \"…\")\n",
    "            print(f\"[scglue][debug] {name}: {type(obj)} {s!r}\")\n",
    "        else:\n",
    "            print(f\"[scglue][debug] {name}: {type(obj)} {obj}\")\n",
    "\n",
    "    if debug_print_kws:\n",
    "        _ck(\"init_kws\", init_kws)\n",
    "        _ck(\"compile_kws\", compile_kws)\n",
    "        _ck(\"fit_kws\", fit_kws)\n",
    "        _ck(\"balance_kws\", balance_kws)\n",
    "        _ck(\"model_class\", model_class)\n",
    "\n",
    "    # -------------------------\n",
    "    # Fit (only on FIT cells)\n",
    "    # -------------------------\n",
    "    glue = _fit_scglue_robust(\n",
    "        scglue,\n",
    "        {\"rna\": rna[fit_idx].copy(), \"atac\": atac[fit_idx].copy()},\n",
    "        guidance,\n",
    "        model_class=model_class,\n",
    "        skip_balance=skip_balance,\n",
    "        init_kws=init_kws,\n",
    "        compile_kws=compile_kws,\n",
    "        fit_kws=fit_kws,\n",
    "        balance_kws=balance_kws,\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Encode ALL cells (evaluation will slice test)\n",
    "    # -------------------------\n",
    "    Z_rna = np.asarray(glue.encode_data(\"rna\", rna), dtype=np.float32)\n",
    "    Z_atac = np.asarray(glue.encode_data(\"atac\", atac), dtype=np.float32)\n",
    "\n",
    "    if fuse == \"mean\":\n",
    "        Z_fused = 0.5 * (Z_rna + Z_atac)\n",
    "    elif fuse == \"rna\":\n",
    "        Z_fused = Z_rna\n",
    "    elif fuse == \"atac\":\n",
    "        Z_fused = Z_atac\n",
    "    elif fuse == \"concat\":\n",
    "        Z_fused = np.concatenate([Z_rna, Z_atac], axis=1)\n",
    "    elif fuse is None:\n",
    "        Z_fused = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown fuse={fuse!r}\")\n",
    "\n",
    "    return dict(\n",
    "        Z_fused=Z_fused,\n",
    "        Z_rna=Z_rna,\n",
    "        Z_atac=Z_atac,\n",
    "        fit_idx=fit_idx,\n",
    "        gtf_by=gtf_by,\n",
    "        n_genes=rna.n_vars,\n",
    "        n_peaks=atac.n_vars,\n",
    "        run_dir=str(run_dir),\n",
    "        fit_kws=fit_kws,\n",
    "        init_kws=init_kws,\n",
    "        extra_json={\"transductive\": False, \"uses_labels\": False},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4572f1c-3109-4a25-a16a-fd0c87f44cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def _annotate_peaks_from_varnames(atac):\n",
    "    import pandas as pd\n",
    "\n",
    "    peaks = pd.Index(atac.var_names.astype(str))\n",
    "\n",
    "    m = peaks.to_series().str.extract(r\"^(?P<chrom>[^:]+):(?P<start>\\d+)-(?P<end>\\d+)$\")\n",
    "    bad = m[\"start\"].isna()\n",
    "    if bad.any():\n",
    "        m2 = peaks[bad].to_series().str.extract(r\"^(?P<chrom>[^_]+)_(?P<start>\\d+)_(?P<end>\\d+)$\")\n",
    "        m.loc[bad, [\"chrom\", \"start\", \"end\"]] = m2[[\"chrom\", \"start\", \"end\"]].values\n",
    "\n",
    "    ok = m[\"start\"].notna() & m[\"end\"].notna() & m[\"chrom\"].notna()\n",
    "    n_bad = int((~ok).sum())\n",
    "    if n_bad:\n",
    "        print(f\"[scglue] peak parsing: dropping {n_bad}/{len(peaks)} peaks with unparseable var_names\")\n",
    "        atac = atac[:, ok.values].copy()\n",
    "        m = m.loc[ok].copy()\n",
    "\n",
    "    atac.var[\"chrom\"] = m[\"chrom\"].astype(str).values\n",
    "    atac.var[\"chromStart\"] = m[\"start\"].astype(np.int64).values\n",
    "    atac.var[\"chromEnd\"] = m[\"end\"].astype(np.int64).values\n",
    "    return atac\n",
    "\n",
    "\n",
    "def _nnz_mask_cellslice(X):\n",
    "    \"\"\"Return boolean mask over features: feature has at least 1 nonzero in this cell slice.\"\"\"\n",
    "    if sp.issparse(X):\n",
    "        return np.asarray((X > 0).sum(axis=0)).ravel() > 0\n",
    "    X = np.asarray(X)\n",
    "    return (X > 0).sum(axis=0) > 0\n",
    "\n",
    "\n",
    "def _topk_by_counts(X, k: int):\n",
    "    \"\"\"Pick top-k features by total counts (sparse-safe).\"\"\"\n",
    "    if sp.issparse(X):\n",
    "        s = np.asarray(X.sum(axis=0)).ravel()\n",
    "    else:\n",
    "        s = np.asarray(X).sum(axis=0)\n",
    "    k = int(min(k, s.size))\n",
    "    if k <= 0:\n",
    "        return np.zeros(s.size, dtype=bool)\n",
    "    idx = np.argpartition(-s, kth=k - 1)[:k]\n",
    "    m = np.zeros(s.size, dtype=bool)\n",
    "    m[idx] = True\n",
    "    return m\n",
    "\n",
    "\n",
    "def _fit_scglue_robust(scglue, adatas, graph, *, model_class=None,\n",
    "                      skip_balance=False, init_kws=None, compile_kws=None,\n",
    "                      fit_kws=None, balance_kws=None):\n",
    "    import inspect\n",
    "\n",
    "    init_kws = {} if init_kws is None else init_kws\n",
    "    compile_kws = {} if compile_kws is None else compile_kws\n",
    "    fit_kws = {} if fit_kws is None else fit_kws\n",
    "    balance_kws = {} if balance_kws is None else balance_kws\n",
    "\n",
    "    for nm, obj in [(\"init_kws\", init_kws), (\"compile_kws\", compile_kws), (\"fit_kws\", fit_kws), (\"balance_kws\", balance_kws)]:\n",
    "        if not isinstance(obj, dict):\n",
    "            raise TypeError(f\"{nm} must be dict, got {type(obj)}: {obj!r}\")\n",
    "\n",
    "    sig = inspect.signature(scglue.models.fit_SCGLUE)\n",
    "    params = sig.parameters\n",
    "\n",
    "    kwargs = {}\n",
    "    if \"model\" in params and model_class is not None:\n",
    "        kwargs[\"model\"] = model_class\n",
    "    if \"skip_balance\" in params:\n",
    "        kwargs[\"skip_balance\"] = bool(skip_balance)\n",
    "\n",
    "    if \"init_kws\" in params:\n",
    "        kwargs[\"init_kws\"] = init_kws\n",
    "    else:\n",
    "        kwargs.update(init_kws)\n",
    "\n",
    "    if \"compile_kws\" in params:\n",
    "        kwargs[\"compile_kws\"] = compile_kws\n",
    "    else:\n",
    "        kwargs.update(compile_kws)\n",
    "\n",
    "    if \"fit_kws\" in params:\n",
    "        kwargs[\"fit_kws\"] = fit_kws\n",
    "    else:\n",
    "        kwargs.update(fit_kws)\n",
    "\n",
    "    if \"balance_kws\" in params:\n",
    "        kwargs[\"balance_kws\"] = balance_kws\n",
    "    else:\n",
    "        kwargs.update(balance_kws)\n",
    "\n",
    "    return scglue.models.fit_SCGLUE(adatas, graph, **kwargs)\n",
    "\n",
    "\n",
    "def run_scglue_simple_v2(\n",
    "    rna_raw,\n",
    "    atac_raw,\n",
    "    *,\n",
    "    gtf_path,\n",
    "    splits,\n",
    "    out_dir,\n",
    "    seed=0,\n",
    "    latent_dim=30,\n",
    "    max_epochs=200,\n",
    "\n",
    "    # HV / PCA / LSI\n",
    "    n_hvg=2000,\n",
    "    n_pca=100,\n",
    "    n_lsi=101,\n",
    "    n_hvpeaks=50000,          # IMPORTANT: define HV peaks for ATAC if using use_highly_variable=True\n",
    "\n",
    "    # training/early stopping behavior\n",
    "    val_split=0.1,            # internal val fraction (must be >0 for many scglue versions)\n",
    "    patience=30,\n",
    "    reduce_lr_patience=15,\n",
    "\n",
    "    # fusion\n",
    "    fuse=\"mean\",              # \"mean\" | \"rna\" | \"atac\" | \"concat\" | None\n",
    "\n",
    "    # model options\n",
    "    model_class=None,\n",
    "    skip_balance=False,\n",
    "\n",
    "    # debug\n",
    "    debug=True,\n",
    "):\n",
    "    import random\n",
    "    import scanpy as sc\n",
    "    import scglue\n",
    "    import torch\n",
    "\n",
    "    # -------------------------\n",
    "    # seeds\n",
    "    # -------------------------\n",
    "    random.seed(int(seed))\n",
    "    np.random.seed(int(seed))\n",
    "    torch.manual_seed(int(seed))\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(int(seed))\n",
    "\n",
    "    # -------------------------\n",
    "    # FIT cells: train (+ optional val) but test held out\n",
    "    # -------------------------\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "    va = np.asarray(splits.get(\"val\", []), dtype=int)\n",
    "    fit_idx = np.concatenate([tr, va]) if va.size else tr\n",
    "\n",
    "    if fit_idx.size < 100:\n",
    "        raise ValueError(f\"[scglue] Too few fit cells ({fit_idx.size}). Something is off in splits.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # sanity: ensure X is counts-like\n",
    "    # -------------------------\n",
    "    def _assert_counts_like(adata, name):\n",
    "        X = adata.X\n",
    "        if not sp.issparse(X):\n",
    "            X = np.asarray(X)\n",
    "        # allow float, but must be nonnegative\n",
    "        mn = X.min() if not sp.issparse(X) else X.min()\n",
    "        if mn < 0:\n",
    "            raise ValueError(f\"[scglue] {name}.X has negative values (min={mn}). \"\n",
    "                             f\"Feed raw counts in .X; put TFIDF/LSI in obsm, not .X.\")\n",
    "    _assert_counts_like(rna_raw, \"rna_raw\")\n",
    "    _assert_counts_like(atac_raw, \"atac_raw\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Restrict to features observed in FIT cells\n",
    "    # -------------------------\n",
    "    keep_genes = _nnz_mask_cellslice(rna_raw[fit_idx].X)\n",
    "    keep_peaks = _nnz_mask_cellslice(atac_raw[fit_idx].X)\n",
    "\n",
    "    rna = rna_raw[:, keep_genes].copy()\n",
    "    atac = atac_raw[:, keep_peaks].copy()\n",
    "\n",
    "    if rna.n_vars < 500:\n",
    "        raise ValueError(f\"[scglue] After FIT filtering, only {rna.n_vars} genes remain.\")\n",
    "    if atac.n_vars < 1000:\n",
    "        raise ValueError(f\"[scglue] After FIT filtering, only {atac.n_vars} peaks remain.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Gene + peak genomic coords\n",
    "    # -------------------------\n",
    "    gtf_path = str(Path(gtf_path))\n",
    "\n",
    "    v = rna.var_names.astype(str)\n",
    "    frac_ens = np.mean([s.startswith(\"ENS\") for s in v[: min(200, len(v))]])\n",
    "    gtf_by = \"gene_id\" if frac_ens > 0.5 else \"gene_name\"\n",
    "\n",
    "    scglue.data.get_gene_annotation(rna, gtf=gtf_path, gtf_by=gtf_by)\n",
    "\n",
    "    gene_ok = rna.var[[\"chrom\", \"chromStart\", \"chromEnd\"]].notna().all(axis=1).to_numpy()\n",
    "    if gene_ok.sum() < rna.n_vars:\n",
    "        drop = int(rna.n_vars - gene_ok.sum())\n",
    "        print(f\"[scglue] gene annotation: dropping {drop}/{rna.n_vars} genes without coords\")\n",
    "        rna = rna[:, gene_ok].copy()\n",
    "\n",
    "    atac = _annotate_peaks_from_varnames(atac)\n",
    "\n",
    "    # -------------------------\n",
    "    # RNA preprocessing (HVGs fit on FIT only)\n",
    "    # -------------------------\n",
    "    rna.layers[\"counts\"] = rna.X.copy()\n",
    "\n",
    "    rna_fit = rna[fit_idx].copy()\n",
    "    sc.pp.highly_variable_genes(\n",
    "        rna_fit,\n",
    "        n_top_genes=int(min(n_hvg, rna_fit.n_vars)),\n",
    "        flavor=\"seurat_v3\",\n",
    "    )\n",
    "    hv = rna_fit.var[\"highly_variable\"].reindex(rna.var_names).fillna(False).to_numpy(dtype=bool)\n",
    "    rna.var[\"highly_variable\"] = hv\n",
    "\n",
    "    sc.pp.normalize_total(rna, target_sum=1e4)\n",
    "    sc.pp.log1p(rna)\n",
    "    sc.pp.scale(rna, max_value=10, zero_center=False)\n",
    "\n",
    "    try:\n",
    "        sc.tl.pca(rna, n_comps=int(n_pca), svd_solver=\"auto\", mask_var=\"highly_variable\")\n",
    "    except TypeError:\n",
    "        sc.tl.pca(rna, n_comps=int(n_pca), svd_solver=\"auto\", use_highly_variable=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # ATAC HV peaks + LSI\n",
    "    # -------------------------\n",
    "    atac.layers[\"counts\"] = atac.X.copy()\n",
    "\n",
    "    # define HV peaks ON FIT cells (counts-based; robust + fast)\n",
    "    hvp = _topk_by_counts(atac[fit_idx].X, k=int(min(n_hvpeaks, atac.n_vars)))\n",
    "    atac.var[\"highly_variable\"] = hvp\n",
    "\n",
    "    # LSI uses counts in .X (or counts layer depending on scglue version)\n",
    "    scglue.data.lsi(atac, n_components=int(n_lsi), n_iter=15)\n",
    "\n",
    "    # -------------------------\n",
    "    # Guidance graph + check\n",
    "    # -------------------------\n",
    "    guidance = scglue.genomics.rna_anchored_guidance_graph(rna, atac)\n",
    "    scglue.graph.check_graph(guidance, [rna, atac])\n",
    "\n",
    "    # -------------------------\n",
    "    # Configure datasets\n",
    "    # -------------------------\n",
    "    scglue.models.configure_dataset(\n",
    "        rna, \"NB\",\n",
    "        use_highly_variable=True,\n",
    "        use_layer=\"counts\",\n",
    "        use_rep=\"X_pca\",\n",
    "        use_obs_names=True,\n",
    "    )\n",
    "    scglue.models.configure_dataset(\n",
    "        atac, \"NB\",\n",
    "        use_highly_variable=True,      # NOW SAFE because we created atac.var[\"highly_variable\"]\n",
    "        use_layer=\"counts\",\n",
    "        use_rep=\"X_lsi\",\n",
    "        use_obs_names=True,\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # run directory\n",
    "    # -------------------------\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    run_dir = out_dir / \"scglue_train\" / time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "    # enforce strictly-positive val_split (some scglue versions require this)\n",
    "    val_split_eff = float(val_split)\n",
    "    if not (0.0 < val_split_eff < 1.0):\n",
    "        # choose something sane + safe\n",
    "        val_split_eff = min(0.1, max(0.01, 1.0 / float(fit_idx.size)))\n",
    "        if debug:\n",
    "            print(f\"[scglue] val_split was {val_split!r}; using safe val_split_eff={val_split_eff:.4g}\")\n",
    "\n",
    "    fit_kws = dict(\n",
    "        max_epochs=int(max_epochs),\n",
    "        val_split=float(val_split_eff),\n",
    "        directory=str(run_dir),\n",
    "        patience=int(patience),\n",
    "        reduce_lr_patience=int(reduce_lr_patience),\n",
    "    )\n",
    "    init_kws = dict(latent_dim=int(latent_dim), random_seed=int(seed))\n",
    "    compile_kws = {}\n",
    "    balance_kws = {}\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[scglue][dbg] fit cells={fit_idx.size} genes={rna.n_vars} peaks={atac.n_vars}\")\n",
    "        print(f\"[scglue][dbg] RNA HVGs={int(rna.var['highly_variable'].sum())} \"\n",
    "              f\"ATAC HVpeaks={int(atac.var['highly_variable'].sum())}\")\n",
    "        print(f\"[scglue][dbg] run_dir={run_dir}\")\n",
    "        print(f\"[scglue][dbg] init_kws={init_kws}\")\n",
    "        print(f\"[scglue][dbg] fit_kws={fit_kws}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Fit (on FIT cells)\n",
    "    # -------------------------\n",
    "    glue = _fit_scglue_robust(\n",
    "        scglue,\n",
    "        {\"rna\": rna[fit_idx].copy(), \"atac\": atac[fit_idx].copy()},\n",
    "        guidance,\n",
    "        model_class=model_class,\n",
    "        skip_balance=skip_balance,\n",
    "        init_kws=init_kws,\n",
    "        compile_kws=compile_kws,\n",
    "        fit_kws=fit_kws,\n",
    "        balance_kws=balance_kws,\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Encode ALL cells\n",
    "    # -------------------------\n",
    "    Z_rna = np.asarray(glue.encode_data(\"rna\", rna), dtype=np.float32)\n",
    "    Z_atac = np.asarray(glue.encode_data(\"atac\", atac), dtype=np.float32)\n",
    "\n",
    "    if fuse == \"mean\":\n",
    "        Z_fused = 0.5 * (Z_rna + Z_atac)\n",
    "    elif fuse == \"rna\":\n",
    "        Z_fused = Z_rna\n",
    "    elif fuse == \"atac\":\n",
    "        Z_fused = Z_atac\n",
    "    elif fuse == \"concat\":\n",
    "        Z_fused = np.concatenate([Z_rna, Z_atac], axis=1)\n",
    "    elif fuse is None:\n",
    "        Z_fused = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown fuse={fuse!r}\")\n",
    "\n",
    "    return dict(\n",
    "        Z_fused=Z_fused,\n",
    "        Z_rna=Z_rna,\n",
    "        Z_atac=Z_atac,\n",
    "        fit_idx=fit_idx,\n",
    "        gtf_by=gtf_by,\n",
    "        n_genes=int(rna.n_vars),\n",
    "        n_peaks=int(atac.n_vars),\n",
    "        run_dir=str(run_dir),\n",
    "        fit_kws=fit_kws,\n",
    "        init_kws=init_kws,\n",
    "        extra_json={\"transductive\": False, \"uses_labels\": False},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5abb21d-c8d2-48b0-a796-bb763ae490b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# NEW: scGLUE silencing utils\n",
    "# =========================\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def silence_scglue(\n",
    "    *,\n",
    "    verbose: bool = False,\n",
    "    keep_user_prints: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Quiet scGLUE/scanpy/lightning + common torch pin_memory deprecation spam.\n",
    "    - verbose=True => do nothing (full logs)\n",
    "    - keep_user_prints kept for clarity (we don't redirect stdout here)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        yield\n",
    "        return\n",
    "\n",
    "    # Progress bars: many libs respect these\n",
    "    os.environ.setdefault(\"TQDM_DISABLE\", \"1\")\n",
    "    os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
    "\n",
    "    # Clamp noisy loggers\n",
    "    noisy = [\n",
    "        \"scglue\",\n",
    "        \"scanpy\",\n",
    "        \"anndata\",\n",
    "        \"muon\",\n",
    "        \"pytorch_lightning\",\n",
    "        \"lightning\",\n",
    "        \"torch\",\n",
    "    ]\n",
    "    old_levels = {}\n",
    "    for name in noisy:\n",
    "        lg = logging.getLogger(name)\n",
    "        old_levels[name] = lg.level\n",
    "        lg.setLevel(logging.ERROR)\n",
    "\n",
    "    # Warnings: keep it surgical\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=ResourceWarning)\n",
    "\n",
    "        # This is the ultra-spammy one in your log:\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            category=DeprecationWarning,\n",
    "            module=r\"torch\\.utils\\.data\\._utils\\.pin_memory\",\n",
    "        )\n",
    "\n",
    "        # Often seen with lightning/torch compile churn\n",
    "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "        # Scanpy verbosity if available\n",
    "        try:\n",
    "            import scanpy as sc\n",
    "            sc.settings.verbosity = 0\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            for name, lvl in old_levels.items():\n",
    "                logging.getLogger(name).setLevel(lvl)\n",
    "\n",
    "\n",
    "def _filter_kwargs_to_callable(fn, kwargs: dict) -> dict:\n",
    "    \"\"\"Return subset of kwargs accepted by fn's signature (API-drift tolerant).\"\"\"\n",
    "    import inspect\n",
    "    try:\n",
    "        sig = inspect.signature(fn)\n",
    "    except Exception:\n",
    "        return dict(kwargs)\n",
    "    return {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
    "\n",
    "\n",
    "def _set_scglue_trainer_quiet(fit_kws: dict, *, verbose: bool) -> dict:\n",
    "    \"\"\"\n",
    "    scGLUE fit_SCGLUE doesn't necessarily accept Lightning Trainer args.\n",
    "    So we DO NOT add enable_progress_bar/log_every_n_steps here.\n",
    "    Quieting is handled via `silence_scglue(...)` (warnings/loggers/tqdm env).\n",
    "    \"\"\"\n",
    "    # If you want to be extra safe, strip keys that might sneak in from elsewhere:\n",
    "    banned = {\"enable_progress_bar\", \"log_every_n_steps\", \"enable_model_summary\", \"trainer_kwargs\", \"callbacks\"}\n",
    "    out = {k: v for k, v in fit_kws.items() if k not in banned}\n",
    "    return out\n",
    "\n",
    "\n",
    "def _ensure_counts_layers_for_scglue(rna, atac, *, rna_layer=\"counts\", atac_layer=\"counts\"):\n",
    "    \"\"\"\n",
    "    Ensure layers exist for scglue.configure_dataset(use_layer=...).\n",
    "    Tries to avoid guessing counts, but will fall back to copying .X.\n",
    "    \"\"\"\n",
    "    if rna_layer not in rna.layers:\n",
    "        if getattr(rna, \"raw\", None) is not None and getattr(rna.raw, \"X\", None) is not None:\n",
    "            rna.layers[rna_layer] = rna.raw.X.copy()\n",
    "        else:\n",
    "            rna.layers[rna_layer] = rna.X.copy()\n",
    "\n",
    "    if atac_layer not in atac.layers:\n",
    "        if getattr(atac, \"raw\", None) is not None and getattr(atac.raw, \"X\", None) is not None:\n",
    "            atac.layers[atac_layer] = atac.raw.X.copy()\n",
    "        else:\n",
    "            atac.layers[atac_layer] = atac.X.copy()\n",
    "\n",
    "    return rna, atac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08ec58-f675-41f9-b5de-b3fbbde8bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# REWRITE: scGLUE runner (quiet-by-default)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def run_scglue_fair(\n",
    "    rna_raw,\n",
    "    atac_raw,\n",
    "    *,\n",
    "    gtf_path,\n",
    "    splits,\n",
    "    out_dir,\n",
    "    seed=0,\n",
    "    latent_dim=30,\n",
    "    max_epochs=200,\n",
    "    # feature dims\n",
    "    n_hvg=2000,\n",
    "    n_pca=100,\n",
    "    n_lsi=101,\n",
    "    n_hvpeaks=50000,\n",
    "    # scglue training behavior\n",
    "    val_split=0.1,\n",
    "    patience=30,\n",
    "    reduce_lr_patience=15,\n",
    "    fuse=\"mean\",     # mean|rna|atac|concat|None\n",
    "    model_class=None,\n",
    "    skip_balance=False,\n",
    "    # layers (explicit)\n",
    "    rna_counts_layer=\"counts\",\n",
    "    atac_counts_layer=\"counts\",\n",
    "    # verbosity\n",
    "    verbose=False,\n",
    "    debug=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Leakage-aware scGLUE runner (quiet by default):\n",
    "      - Restricts features (genes/peaks) using FIT cells only (train+val)\n",
    "      - Fits HVG+PCA on FIT only -> applies to ALL\n",
    "      - Fits HV-peaks+LSI on FIT only -> applies to ALL\n",
    "      - Trains GLUE on FIT only -> encodes ALL (test inference-only)\n",
    "\n",
    "    Requires your existing helpers:\n",
    "      - _annotate_peaks_from_varnames(atac)\n",
    "      - compute_hvg_on_fit_cells(...)\n",
    "      - fit_pca_train_apply_all(...)\n",
    "      - compute_hv_peaks_on_fit_cells(...)\n",
    "      - fit_lsi_train_apply_all(...)\n",
    "      - _fit_scglue_robust(...)\n",
    "      - ensure_flags(...)\n",
    "      - standard_flags(...)\n",
    "    \"\"\"\n",
    "    import scanpy as sc\n",
    "    import scglue\n",
    "    import torch\n",
    "\n",
    "    # -------------------------\n",
    "    # seeds\n",
    "    # -------------------------\n",
    "    random.seed(int(seed))\n",
    "    np.random.seed(int(seed))\n",
    "    torch.manual_seed(int(seed))\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(int(seed))\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "    va = np.asarray(splits.get(\"val\", []), dtype=int)\n",
    "    fit_idx = np.concatenate([tr, va]) if va.size else tr\n",
    "    fit_idx = np.asarray(fit_idx, dtype=int)\n",
    "\n",
    "    # keep val strictly inside FIT; no test leakage\n",
    "    val_split_eff = float(val_split)\n",
    "    if not (0.0 < val_split_eff < 1.0):\n",
    "        val_split_eff = min(0.1, max(0.01, 1.0 / float(max(1, fit_idx.size))))\n",
    "\n",
    "    # -------------------------\n",
    "    # small utilities\n",
    "    # -------------------------\n",
    "    def _as_csr(X):\n",
    "        if sp.issparse(X):\n",
    "            return X.tocsr()\n",
    "        return sp.csr_matrix(np.asarray(X))\n",
    "\n",
    "    def _nnz_mask(X):\n",
    "        X = _as_csr(X)\n",
    "        return np.asarray((X > 0).sum(axis=0)).ravel() > 0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    with silence_scglue(verbose=bool(verbose)):\n",
    "        # -------------------------\n",
    "        # feature restriction based on FIT cells only\n",
    "        # -------------------------\n",
    "        keep_genes = _nnz_mask(rna_raw[fit_idx].X)\n",
    "        keep_peaks = _nnz_mask(atac_raw[fit_idx].X)\n",
    "\n",
    "        rna = rna_raw[:, keep_genes].copy()\n",
    "        atac = atac_raw[:, keep_peaks].copy()\n",
    "\n",
    "        # -------------------------\n",
    "        # annotate genes + peaks\n",
    "        # -------------------------\n",
    "        gtf_path = str(Path(gtf_path))\n",
    "        frac_ens = np.mean([str(s).startswith(\"ENS\") for s in rna.var_names[: min(200, rna.n_vars)]])\n",
    "        gtf_by = \"gene_id\" if frac_ens > 0.5 else \"gene_name\"\n",
    "\n",
    "        scglue.data.get_gene_annotation(rna, gtf=gtf_path, gtf_by=gtf_by)\n",
    "        gene_ok = rna.var[[\"chrom\", \"chromStart\", \"chromEnd\"]].notna().all(axis=1).to_numpy()\n",
    "        if gene_ok.sum() < rna.n_vars:\n",
    "            if debug:\n",
    "                print(f\"[scglue] dropping {rna.n_vars - gene_ok.sum()}/{rna.n_vars} genes without coords\")\n",
    "            rna = rna[:, gene_ok].copy()\n",
    "\n",
    "        atac = _annotate_peaks_from_varnames(atac)  # your robust parser\n",
    "\n",
    "        # -------------------------\n",
    "        # ensure counts layers\n",
    "        # -------------------------\n",
    "        rna, atac = _ensure_counts_layers_for_scglue(\n",
    "            rna, atac,\n",
    "            rna_layer=rna_counts_layer,\n",
    "            atac_layer=atac_counts_layer,\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # HVGs / PCA fit on FIT only -> apply to ALL\n",
    "        # -------------------------\n",
    "        compute_hvg_on_fit_cells(\n",
    "            rna, fit_idx,\n",
    "            n_top_genes=int(n_hvg),\n",
    "            flavor=\"seurat_v3\",\n",
    "            batch_key=None,\n",
    "        )\n",
    "        fit_pca_train_apply_all(\n",
    "            rna, fit_idx,\n",
    "            n_pca=int(n_pca),\n",
    "            target_sum=1e4,\n",
    "            seed=int(seed),\n",
    "            layer=rna_counts_layer,\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # HV-peaks / LSI fit on FIT only -> apply to ALL\n",
    "        # -------------------------\n",
    "        compute_hv_peaks_on_fit_cells(atac, fit_idx, n_top_peaks=int(n_hvpeaks))\n",
    "        fit_lsi_train_apply_all(atac, fit_idx, n_lsi=int(n_lsi), seed=int(seed), l2norm=True)\n",
    "\n",
    "        # -------------------------\n",
    "        # build guidance graph + check\n",
    "        # -------------------------\n",
    "        guidance = scglue.genomics.rna_anchored_guidance_graph(rna, atac)\n",
    "        scglue.graph.check_graph(guidance, [rna, atac])\n",
    "\n",
    "        # -------------------------\n",
    "        # configure datasets (use reps we computed leakage-safe)\n",
    "        # -------------------------\n",
    "        scglue.models.configure_dataset(\n",
    "            rna, \"NB\",\n",
    "            use_highly_variable=True,\n",
    "            use_layer=rna_counts_layer,\n",
    "            use_rep=\"X_pca\",\n",
    "            use_obs_names=True,\n",
    "        )\n",
    "        scglue.models.configure_dataset(\n",
    "            atac, \"NB\",\n",
    "            use_highly_variable=True,\n",
    "            use_layer=atac_counts_layer,\n",
    "            use_rep=\"X_lsi\",\n",
    "            use_obs_names=True,\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # train on FIT only\n",
    "        # -------------------------\n",
    "        run_dir = out_dir / \"scglue_train\" / time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        run_dir.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "        init_kws = dict(latent_dim=int(latent_dim), random_seed=int(seed))\n",
    "        fit_kws = dict(\n",
    "            max_epochs=int(max_epochs),\n",
    "            val_split=float(val_split_eff),\n",
    "            directory=str(run_dir),\n",
    "            patience=int(patience),\n",
    "            reduce_lr_patience=int(reduce_lr_patience),\n",
    "        )\n",
    "        fit_kws = _set_scglue_trainer_quiet(fit_kws, verbose=bool(verbose))\n",
    "\n",
    "        glue = _fit_scglue_robust(\n",
    "            scglue,\n",
    "            {\"rna\": rna[fit_idx].copy(), \"atac\": atac[fit_idx].copy()},\n",
    "            guidance,\n",
    "            model_class=model_class,\n",
    "            skip_balance=skip_balance,\n",
    "            init_kws=init_kws,\n",
    "            compile_kws={},\n",
    "            fit_kws=fit_kws,\n",
    "            balance_kws={},\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # encode ALL cells (test is inference-only)\n",
    "        # -------------------------\n",
    "        Z_rna = np.asarray(glue.encode_data(\"rna\", rna), dtype=np.float32)\n",
    "        Z_atac = np.asarray(glue.encode_data(\"atac\", atac), dtype=np.float32)\n",
    "\n",
    "        if fuse == \"mean\":\n",
    "            Z_fused = 0.5 * (Z_rna + Z_atac)\n",
    "        elif fuse == \"rna\":\n",
    "            Z_fused = Z_rna\n",
    "        elif fuse == \"atac\":\n",
    "            Z_fused = Z_atac\n",
    "        elif fuse == \"concat\":\n",
    "            Z_fused = np.concatenate([Z_rna, Z_atac], axis=1)\n",
    "        elif fuse is None:\n",
    "            Z_fused = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fuse={fuse!r}\")\n",
    "\n",
    "    fit_seconds = float(time.perf_counter() - t0)\n",
    "\n",
    "    if debug and not verbose:\n",
    "        # minimal one-liner summary\n",
    "        print(f\"[scglue] done. fit_seconds={fit_seconds:.2f}  Z_rna={Z_rna.shape}  Z_atac={Z_atac.shape}\")\n",
    "\n",
    "    return ensure_flags({\n",
    "        \"Z_fused\": Z_fused,\n",
    "        \"Z_rna\": Z_rna,\n",
    "        \"Z_atac\": Z_atac,\n",
    "        \"fit_seconds\": fit_seconds,\n",
    "        \"extra_json\": standard_flags(\n",
    "            transductive=False,\n",
    "            uses_labels=False,\n",
    "            note=\"HVG/HVpeaks + PCA/LSI fit on train+val only; GLUE trained on train+val only; test is inference-only.\",\n",
    "        ),\n",
    "    }, default_transductive=False, default_uses_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f81a2-d792-4e1e-ab67-297df9914f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import inspect, scglue, scglue.models\n",
    "#src = inspect.getsource(scglue.models.fit_SCGLUE)\n",
    "#print(src)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec6f34-578f-490f-9c01-1eff887e5c8e",
   "metadata": {},
   "source": [
    "### 4) MultiMAP (robust wrapper + API autodetect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf5972-7d47-41de-838e-db433337a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multimap(rna_log_hvg, atac_lsi, *, out_dir, splits, seed=0, latent_dim=30):\n",
    "    import numpy as np\n",
    "    import scipy.sparse as sp\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    if not hasattr(np, \"infty\"):\n",
    "        np.infty = np.inf  # NumPy 2 compat shim\n",
    "\n",
    "    from MultiMAP.matrix import MultiMAP as multimap_matrix\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 0) Align modalities to the SAME cells in the SAME order\n",
    "    # ------------------------------------------------------------\n",
    "    rna0 = rna_log_hvg\n",
    "    atac0 = atac_lsi\n",
    "\n",
    "    shared = rna0.obs_names.intersection(atac0.obs_names)\n",
    "    if shared.size == 0:\n",
    "        raise ValueError(\"MultiMAP: RNA/ATAC have no shared cells (obs_names intersection empty).\")\n",
    "\n",
    "    # canonical order = RNA order\n",
    "    rna = rna0[shared].copy()\n",
    "    atac = atac0[shared].copy()\n",
    "    if not rna.obs_names.equals(atac.obs_names):\n",
    "        raise ValueError(\"MultiMAP: failed to align RNA/ATAC obs_names after subsetting.\")\n",
    "\n",
    "    n_obs = int(rna.n_obs)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Remap splits (original RNA index space -> aligned shared space)\n",
    "    # ------------------------------------------------------------\n",
    "    tr0 = np.asarray(splits[\"train\"], dtype=int)\n",
    "\n",
    "    # map original RNA row index -> aligned row index (or -1 if dropped)\n",
    "    orig_pos = rna0.obs_names.get_indexer(shared)  # positions of shared cells in original RNA\n",
    "    inv = np.full(int(rna0.n_obs), -1, dtype=int)\n",
    "    inv[orig_pos] = np.arange(shared.size, dtype=int)\n",
    "\n",
    "    tr = inv[tr0]\n",
    "    tr = tr[tr >= 0]\n",
    "    if tr.size == 0:\n",
    "        raise ValueError(\n",
    "            \"MultiMAP: no TRAIN cells remain after aligning modalities. \"\n",
    "            \"Train split likely contains cells missing from one modality.\"\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Build matrices (dense), PCA-on-train for RNA, then scale\n",
    "    # ------------------------------------------------------------\n",
    "    Xr = rna.X\n",
    "    if sp.issparse(Xr):\n",
    "        Xr = Xr.toarray()\n",
    "    Xr = np.asarray(Xr, dtype=np.float32)\n",
    "\n",
    "    Xa = atac.X\n",
    "    if sp.issparse(Xa):\n",
    "        Xa = Xa.toarray()\n",
    "    X2 = np.asarray(Xa, dtype=np.float32)\n",
    "\n",
    "    # PCA fit on aligned TRAIN only; transform all aligned cells\n",
    "    n_pcs = int(X2.shape[1])\n",
    "    pca = PCA(n_components=n_pcs, random_state=int(seed))\n",
    "    pca.fit(Xr[tr])\n",
    "    X1 = pca.transform(Xr).astype(np.float32, copy=False)\n",
    "\n",
    "    # train-fit scaling per modality\n",
    "    sc1 = StandardScaler(with_mean=True, with_std=True)\n",
    "    sc2 = StandardScaler(with_mean=True, with_std=True)\n",
    "    sc1.fit(X1[tr])\n",
    "    sc2.fit(X2[tr])\n",
    "    X1s = sc1.transform(X1).astype(np.float32, copy=False)\n",
    "    X2s = sc2.transform(X2).astype(np.float32, copy=False)\n",
    "\n",
    "    if not (np.isfinite(X1s).all() and np.isfinite(X2s).all()):\n",
    "        raise ValueError(\"MultiMAP: non-finite values detected after scaling.\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3) Run MultiMAP (effectively transductive)\n",
    "    # ------------------------------------------------------------\n",
    "    t0 = now()\n",
    "    params, graph, Z = multimap_matrix(\n",
    "        [X1s, X2s],\n",
    "        n_components=int(latent_dim),\n",
    "        random_state=int(seed),\n",
    "    )\n",
    "    t1 = now()\n",
    "\n",
    "    Z = np.asarray(Z, dtype=np.float32)\n",
    "\n",
    "    if Z.ndim != 2:\n",
    "        raise ValueError(f\"MultiMAP: unexpected Z ndim={Z.ndim} (shape={Z.shape})\")\n",
    "    if Z.shape[1] != int(latent_dim):\n",
    "        raise ValueError(f\"MultiMAP: Z dim ({Z.shape[1]}) != latent_dim ({int(latent_dim)})\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4) Handle MultiMAP output shape\n",
    "    #    - Some versions return (n_obs, d) fused\n",
    "    #    - The matrix API often returns (sum_i n_i, d) concatenated\n",
    "    #      which here is (2*n_obs, d)\n",
    "    # ------------------------------------------------------------\n",
    "    if Z.shape[0] == n_obs:\n",
    "        Zr = Z\n",
    "        Za = None\n",
    "        Zf = Z\n",
    "        mode = \"fused\"\n",
    "    elif Z.shape[0] == 2 * n_obs:\n",
    "        Zr = Z[:n_obs, :]\n",
    "        Za = Z[n_obs:2 * n_obs, :]\n",
    "        Zf = 0.5 * (Zr + Za)\n",
    "        mode = \"concat_by_modality\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"MultiMAP: unexpected Z rows ({Z.shape[0]}). \"\n",
    "            f\"Expected {n_obs} (fused) or {2*n_obs} (concat for 2 modalities).\"\n",
    "        )\n",
    "\n",
    "    return ensure_flags({\n",
    "        \"Z_fused\": Zf,\n",
    "        \"Z_rna\": Zr,\n",
    "        \"Z_atac\": Za,\n",
    "        \"fit_seconds\": float(t1 - t0),\n",
    "        \"extra_json\": standard_flags(\n",
    "            transductive=True,\n",
    "            uses_labels=False,\n",
    "            n_shared=int(n_obs),\n",
    "            n_train_shared=int(tr.size),\n",
    "            output_mode=mode,\n",
    "            note=\"Aligned RNA/ATAC by shared obs_names; PCA+scaling fit on aligned TRAIN subset; MultiMAP matrix output handled.\"\n",
    "        ),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25b708-5335-448c-a3db-0d9755f203fb",
   "metadata": {},
   "source": [
    "### 5) scMoMaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192449b-9cf4-49dc-82ec-6df6b45408c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# scMoMaT runner (fork-tolerant, auto-orientation, robust parsing)\n",
    "# - Builds per-batch inputs with shared cell order across modalities\n",
    "# - Auto-detects whether your fork expects (cells x features) or (features x cells)\n",
    "# - Parses extract_cell_factors() across common fork return formats\n",
    "# - Always returns: Z_fused, and Z_rna/Z_atac/Z_adt when available\n",
    "# ============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import json\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "# --- SciPy >=1.11 compatibility: some scMoMaT forks use len(sparse) internally ---\n",
    "try:\n",
    "    def _len_shape0(self):\n",
    "        return self.shape[0]\n",
    "    sp.spmatrix.__len__ = _len_shape0  # type: ignore[attr-defined]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_scmomat_fair(\n",
    "    *,\n",
    "    rna=None,\n",
    "    atac=None,\n",
    "    adt=None,\n",
    "    out_dir=None,\n",
    "    splits=None,  # unused (transductive); kept for signature compatibility\n",
    "    batch_key=None,                  # if None => single batch\n",
    "    K=30,\n",
    "    layers_by_mod=None,              # e.g. {\"rna\":\"counts\",\"atac\":\"counts\"}\n",
    "    seed=0,\n",
    "    T=2000,\n",
    "    lr=1e-4,\n",
    "    lamb=1e-3,\n",
    "    interval=200,\n",
    "    batch_size=0.1,\n",
    "    device=\"cuda\",\n",
    "    verbose=False,\n",
    "\n",
    "    # Orientation handling:\n",
    "    # - \"auto\": detect which axis is \"cells\" by matching across modalities, then adapt for constructor\n",
    "    # - False: pass (cells x features) as-is\n",
    "    # - True:  pass (features x cells) (i.e., transpose)\n",
    "    transpose_to_features_by_cells: bool | str = \"auto\",\n",
    "\n",
    "    ensure_nonnegative: bool = False,\n",
    "    **_ignored,\n",
    "):\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "      - scMoMaT is transductive: fits on all cells it embeds.\n",
    "      - For multi-batch, this runner intersects cells within each batch across modalities.\n",
    "      - For fork quirks, we:\n",
    "          * force dense float32 contiguous arrays\n",
    "          * auto-orient inputs (\"auto\") so the model sees consistent cell axis\n",
    "          * parse extract_cell_factors() in many formats (batch-major, modality-major, single-array)\n",
    "    \"\"\"\n",
    "    # -------------------------\n",
    "    # imports + helpers\n",
    "    # -------------------------\n",
    "    def _safe_import_scmomat_model():\n",
    "        import scmomat\n",
    "        if hasattr(scmomat, \"scmomat_model\"):\n",
    "            return scmomat.scmomat_model\n",
    "        from scmomat.model import scmomat_model\n",
    "        return scmomat_model\n",
    "\n",
    "    def _call_with_signature(fn, /, **kwargs):\n",
    "        sig = inspect.signature(fn)\n",
    "        allowed = {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
    "        return fn(**allowed)\n",
    "\n",
    "    def _batch_levels(*adatas, batch_key):\n",
    "        if batch_key is None:\n",
    "            return [\"__single_batch__\"]\n",
    "        batches = set()\n",
    "        for ad in adatas:\n",
    "            if ad is None:\n",
    "                continue\n",
    "            if batch_key not in ad.obs:\n",
    "                raise KeyError(f\"batch_key={batch_key!r} not found in adata.obs\")\n",
    "            batches |= set(ad.obs[batch_key].astype(str).unique().tolist())\n",
    "        return sorted(batches)\n",
    "\n",
    "    def _subset_batch(ad, batch_key, b):\n",
    "        if ad is None:\n",
    "            return None\n",
    "        if batch_key is None:\n",
    "            return ad\n",
    "        mask = (ad.obs[batch_key].astype(str).values == str(b))\n",
    "        if not np.any(mask):\n",
    "            return None\n",
    "        return ad[mask]\n",
    "\n",
    "    def _intersect_obs_names(adatas: List):\n",
    "        present = [ad for ad in adatas if ad is not None]\n",
    "        if not present:\n",
    "            return []\n",
    "        common = set(present[0].obs_names.tolist())\n",
    "        for ad in present[1:]:\n",
    "            common &= set(ad.obs_names.tolist())\n",
    "        # IMPORTANT: preserve a deterministic order. We'll use the reference modality order later anyway.\n",
    "        return sorted(common)\n",
    "\n",
    "    def _get_mat(ad, *, mod, layers_by_mod):\n",
    "        if ad is None:\n",
    "            return None\n",
    "        layer = (layers_by_mod or {}).get(mod, None)\n",
    "        if layer is None:\n",
    "            return ad.X\n",
    "        if layer not in ad.layers:\n",
    "            raise KeyError(f\"layers_by_mod[{mod!r}]={layer!r} not found in adata.layers\")\n",
    "        return ad.layers[layer]\n",
    "\n",
    "    def _to_dense_f32(X):\n",
    "        if sp.issparse(X):\n",
    "            X = X.toarray()\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        if ensure_nonnegative:\n",
    "            X = np.maximum(X, 0.0)\n",
    "        return np.ascontiguousarray(X)\n",
    "\n",
    "    def _shape_tree(x, depth=0, max_items=4):\n",
    "        pad = \"  \" * depth\n",
    "        if isinstance(x, dict):\n",
    "            keys = list(x.keys())\n",
    "            s = f\"{pad}dict keys={keys[:max_items]}{'...' if len(keys)>max_items else ''}\\n\"\n",
    "            for k in keys[:max_items]:\n",
    "                s += f\"{pad}  [{k}] -> \" + _shape_tree(x[k], depth+2, max_items)\n",
    "            return s\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            s = f\"{pad}{type(x).__name__}[len={len(x)}]\\n\"\n",
    "            for i, xi in enumerate(x[:max_items]):\n",
    "                s += _shape_tree(xi, depth+1, max_items)\n",
    "            return s\n",
    "        try:\n",
    "            a = np.asarray(x)\n",
    "            return f\"{pad}array shape={getattr(a,'shape',None)} dtype={getattr(a,'dtype',None)}\\n\"\n",
    "        except Exception:\n",
    "            return f\"{pad}{type(x).__name__}\\n\"\n",
    "\n",
    "    def _canon_cell_factors(cf):\n",
    "        # common forks return dicts\n",
    "        if isinstance(cf, dict):\n",
    "            for key in (\"cell_factors\", \"Z\", \"H\", \"factors\"):\n",
    "                if key in cf:\n",
    "                    return cf[key]\n",
    "            return cf\n",
    "        # some return (something, factors)\n",
    "        if isinstance(cf, (tuple, list)) and len(cf) == 2:\n",
    "            a, b = cf\n",
    "            if isinstance(a, (list, tuple, dict)):\n",
    "                return a\n",
    "            if isinstance(b, (list, tuple, dict)):\n",
    "                return b\n",
    "        return cf\n",
    "\n",
    "    def _unwrap_singletons(x, max_depth=8):\n",
    "        y = x\n",
    "        for _ in range(max_depth):\n",
    "            if isinstance(y, (list, tuple)) and len(y) == 1:\n",
    "                y = y[0]\n",
    "            else:\n",
    "                break\n",
    "        return y\n",
    "\n",
    "    def _try_parse_factors(cf_raw, mod_names: List[str], n_batches: int, K: int):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          Z_by_mod: dict name -> list[batches] -> (cells,K)\n",
    "          had_modality_specific: bool\n",
    "        Accepts:\n",
    "          - modality-major: cf[mod_i][batch_i] = (cells,K)\n",
    "          - batch-major:    cf[batch_i] = (cells,K)\n",
    "          - SINGLE output:  cf is (cells,K) OR [ (cells,K) ]  (common when nbatches=1)\n",
    "          - dict formats (per-mod keys or fused)\n",
    "        \"\"\"\n",
    "        cf = _canon_cell_factors(cf_raw)\n",
    "        cf = _unwrap_singletons(cf)\n",
    "\n",
    "        # ----- Case 0: direct (cells,K) array -----\n",
    "        try:\n",
    "            arr = np.asarray(cf)\n",
    "            if arr.ndim == 2 and arr.shape[1] == K:\n",
    "                # if nbatches>1, some forks concatenate batches; we can't safely split without sizes\n",
    "                if n_batches != 1:\n",
    "                    raise RuntimeError(\n",
    "                        f\"extract_cell_factors returned a single array (n={arr.shape[0]},K={K}) \"\n",
    "                        f\"but nbatches={n_batches}. This fork likely concatenates batches; \"\n",
    "                        f\"need batch sizes to split.\"\n",
    "                    )\n",
    "                return {\"fused\": [arr.astype(np.float32, copy=False)]}, False\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # ----- Case A: modality-major list -----\n",
    "        if isinstance(cf, (list, tuple)) and len(cf) == len(mod_names):\n",
    "            ok = True\n",
    "            for mi in range(len(mod_names)):\n",
    "                vi = _unwrap_singletons(cf[mi])\n",
    "                if not (isinstance(vi, (list, tuple)) and len(vi) == n_batches):\n",
    "                    ok = False\n",
    "                    break\n",
    "                z00 = np.asarray(_unwrap_singletons(vi[0]))\n",
    "                if z00.ndim != 2 or z00.shape[1] != K:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                Z_by_mod = {}\n",
    "                for mi, mname in enumerate(mod_names):\n",
    "                    vi = _unwrap_singletons(cf[mi])\n",
    "                    Z_by_mod[mname] = [\n",
    "                        np.asarray(_unwrap_singletons(vi[bi]), dtype=np.float32)\n",
    "                        for bi in range(n_batches)\n",
    "                    ]\n",
    "                return Z_by_mod, True\n",
    "\n",
    "        # ----- Case B: batch-major fused list -----\n",
    "        if isinstance(cf, (list, tuple)) and len(cf) == n_batches:\n",
    "            z0 = np.asarray(_unwrap_singletons(cf[0]))\n",
    "            if z0.ndim == 2 and z0.shape[1] == K:\n",
    "                return {\n",
    "                    \"fused\": [np.asarray(_unwrap_singletons(cf[bi]), dtype=np.float32) for bi in range(n_batches)]\n",
    "                }, False\n",
    "\n",
    "        # ----- Case C: dict with per-mod keys or fused -----\n",
    "        if isinstance(cf, dict):\n",
    "            # per-mod keys\n",
    "            hits = [m for m in mod_names if m in cf]\n",
    "            if len(hits) == len(mod_names):\n",
    "                Z_by_mod = {}\n",
    "                ok = True\n",
    "                for m in mod_names:\n",
    "                    v = _unwrap_singletons(_canon_cell_factors(cf[m]))\n",
    "                    if not (isinstance(v, (list, tuple)) and len(v) == n_batches):\n",
    "                        ok = False\n",
    "                        break\n",
    "                    z0 = np.asarray(_unwrap_singletons(v[0]))\n",
    "                    if z0.ndim != 2 or z0.shape[1] != K:\n",
    "                        ok = False\n",
    "                        break\n",
    "                    Z_by_mod[m] = [np.asarray(_unwrap_singletons(v[bi]), dtype=np.float32) for bi in range(n_batches)]\n",
    "                if ok:\n",
    "                    return Z_by_mod, True\n",
    "\n",
    "            # fused key\n",
    "            if \"fused\" in cf:\n",
    "                v = _unwrap_singletons(_canon_cell_factors(cf[\"fused\"]))\n",
    "                if isinstance(v, (list, tuple)) and len(v) == n_batches:\n",
    "                    z0 = np.asarray(_unwrap_singletons(v[0]))\n",
    "                    if z0.ndim == 2 and z0.shape[1] == K:\n",
    "                        return {\"fused\": [np.asarray(_unwrap_singletons(v[bi]), dtype=np.float32) for bi in range(n_batches)]}, False\n",
    "\n",
    "        raise RuntimeError(\"Could not parse extract_cell_factors() into recognized structure.\")\n",
    "\n",
    "    def _auto_orient_counts(counts: Dict[str, List[np.ndarray]], mod_names: List[str], *, verbose=False):\n",
    "        \"\"\"\n",
    "        Make orientation consistent across modalities:\n",
    "          - Decide if \"cells\" is axis0 or axis1 by checking which axis matches across modalities.\n",
    "          - If transpose_to_features_by_cells is True/False, honor it.\n",
    "          - If \"auto\", choose an orientation where \"cells\" matches across modalities.\n",
    "        Returns: (counts, mode_string)\n",
    "        \"\"\"\n",
    "        # Inspect first batch (assume consistent across batches)\n",
    "        b0_shapes = {m: counts[m][0].shape for m in mod_names}\n",
    "\n",
    "        # axis0 cells hypothesis: all counts[m][0].shape[0] equal\n",
    "        axis0_ok = len({b0_shapes[m][0] for m in mod_names}) == 1\n",
    "        # axis1 cells hypothesis: all counts[m][0].shape[1] equal\n",
    "        axis1_ok = len({b0_shapes[m][1] for m in mod_names}) == 1\n",
    "\n",
    "        if verbose:\n",
    "            print(\"[scmomat][orient] shapes(batch0):\", b0_shapes)\n",
    "            print(\"[scmomat][orient] axis0_ok=\", axis0_ok, \"axis1_ok=\", axis1_ok)\n",
    "\n",
    "        # If user forced a choice\n",
    "        if transpose_to_features_by_cells is True:\n",
    "            # force features x cells (transpose everything)\n",
    "            for m in mod_names:\n",
    "                counts[m] = [np.ascontiguousarray(x.T) for x in counts[m]]\n",
    "            return counts, \"forced_features_by_cells\"\n",
    "        if transpose_to_features_by_cells is False:\n",
    "            # force cells x features (as-is)\n",
    "            return counts, \"forced_cells_by_features\"\n",
    "\n",
    "        # AUTO: prefer an arrangement with cells on axis0 (common), else axis1, else bail\n",
    "        if axis0_ok and not axis1_ok:\n",
    "            return counts, \"auto_kept_cells_by_features\"\n",
    "        if axis1_ok and not axis0_ok:\n",
    "            # transpose so cells become axis0\n",
    "            for m in mod_names:\n",
    "                counts[m] = [np.ascontiguousarray(x.T) for x in counts[m]]\n",
    "            return counts, \"auto_transposed_to_cells_by_features\"\n",
    "        if axis0_ok and axis1_ok:\n",
    "            # ambiguous (same #cells == #features for all mods is rare, but handle)\n",
    "            return counts, \"auto_ambiguous_kept_cells_by_features\"\n",
    "\n",
    "        raise ValueError(\n",
    "            \"scMoMaT input orientation mismatch across modalities: neither axis0 nor axis1 matches as 'cells'. \"\n",
    "            f\"batch0 shapes={b0_shapes}. This usually means you transposed only one modality or changed cell sets.\"\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # checks + setup\n",
    "    # -------------------------\n",
    "    if rna is None and atac is None and adt is None:\n",
    "        raise ValueError(\"Provide at least one modality AnnData: rna, atac, or adt.\")\n",
    "\n",
    "    if out_dir is not None:\n",
    "        out_dir = Path(out_dir)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ref_mod = \"rna\" if rna is not None else (\"atac\" if atac is not None else \"adt\")\n",
    "    ref_adata = {\"rna\": rna, \"atac\": atac, \"adt\": adt}[ref_mod]\n",
    "    n_ref = int(ref_adata.n_obs)\n",
    "\n",
    "    # seeds\n",
    "    np.random.seed(int(seed))\n",
    "    torch.manual_seed(int(seed))\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(int(seed))\n",
    "\n",
    "    mods_present = [(m, ad) for (m, ad) in ((\"rna\", rna), (\"atac\", atac), (\"adt\", adt)) if ad is not None]\n",
    "    mod_names = [m for m, _ in mods_present]\n",
    "    batches = _batch_levels(rna, atac, adt, batch_key=batch_key)\n",
    "\n",
    "    mats: Dict[str, List[np.ndarray]] = {m: [] for m in mod_names}\n",
    "    ref_batch_meta: List[Tuple[str, np.ndarray]] = []\n",
    "\n",
    "    # -------------------------\n",
    "    # build per-batch matrices with shared cells across modalities\n",
    "    # -------------------------\n",
    "    for b in batches:\n",
    "        sub = {m: _subset_batch(ad, batch_key, b) for m, ad in mods_present}\n",
    "        if any(sub[m] is None for m in sub):\n",
    "            continue\n",
    "\n",
    "        common = _intersect_obs_names([sub[m] for m in sub])\n",
    "        if len(common) == 0:\n",
    "            continue\n",
    "\n",
    "        # Use the *reference modality order* for stable mapping + correct filling later\n",
    "        # (sorted(common) can reorder; so reorder to ref subset order)\n",
    "        ref_sub = sub[ref_mod]\n",
    "        assert ref_sub is not None\n",
    "        common_set = set(common)\n",
    "        common_in_ref_order = [c for c in ref_sub.obs_names.tolist() if c in common_set]\n",
    "        if len(common_in_ref_order) == 0:\n",
    "            continue\n",
    "\n",
    "        sub2 = {m: sub[m][common_in_ref_order] for m in sub}\n",
    "        ref_idx_global = ref_adata.obs_names.get_indexer(common_in_ref_order).astype(int)\n",
    "        if np.any(ref_idx_global < 0):\n",
    "            continue\n",
    "\n",
    "        for m, _ad in mods_present:\n",
    "            Xb = _get_mat(sub2[m], mod=m, layers_by_mod=layers_by_mod)\n",
    "            Xb = _to_dense_f32(Xb)  # dense float32 contiguous\n",
    "            mats[m].append(Xb)\n",
    "\n",
    "        ref_batch_meta.append((str(b), ref_idx_global))\n",
    "\n",
    "    if len(ref_batch_meta) == 0:\n",
    "        raise RuntimeError(\"No usable batches after subsetting+intersection across modalities.\")\n",
    "\n",
    "    counts = {m: mats[m] for m in mod_names}\n",
    "    counts[\"nbatches\"] = int(len(ref_batch_meta))\n",
    "\n",
    "    # -------------------------\n",
    "    # auto/forced orientation handling (this prevents the \"sizes aren't the same\" fork pain)\n",
    "    # -------------------------\n",
    "    counts, orient_mode = _auto_orient_counts(counts, mod_names, verbose=bool(verbose))\n",
    "    if verbose:\n",
    "        for m in mod_names:\n",
    "            x0 = counts[m][0]\n",
    "            print(f\"[scmomat] mod={m} batch0 shape={x0.shape} dtype={x0.dtype} contiguous={x0.flags['C_CONTIGUOUS']}\")\n",
    "        print(f\"[scmomat] nbatches={counts['nbatches']} orient_mode={orient_mode}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # build model\n",
    "    # -------------------------\n",
    "    dev_obj = device\n",
    "    if isinstance(device, str):\n",
    "        try:\n",
    "            dev_obj = torch.device(device)\n",
    "        except Exception:\n",
    "            dev_obj = device\n",
    "\n",
    "    Model = _safe_import_scmomat_model()\n",
    "    try:\n",
    "        model = Model(\n",
    "            counts,\n",
    "            K=int(K),\n",
    "            batch_size=float(batch_size),\n",
    "            interval=int(interval),\n",
    "            lr=float(lr),\n",
    "            lamb=float(lamb),\n",
    "            seed=int(seed),\n",
    "            device=dev_obj,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise TypeError(\n",
    "            \"scMoMaT construction failed.\\n\"\n",
    "            f\"counts keys={sorted(list(counts.keys()))}\\n\"\n",
    "            f\"nbatches={counts.get('nbatches')}\\n\"\n",
    "            f\"orient_mode={orient_mode}\\n\"\n",
    "            f\"Error: {type(e).__name__}({e})\"\n",
    "        ) from e\n",
    "\n",
    "    # -------------------------\n",
    "    # train\n",
    "    # -------------------------\n",
    "    t0 = time.time()\n",
    "    if hasattr(model, \"train_func\") and callable(getattr(model, \"train_func\")):\n",
    "        _call_with_signature(\n",
    "            model.train_func,\n",
    "            T=int(T),\n",
    "            n_epochs=int(T),\n",
    "            lr=float(lr),\n",
    "            learning_rate=float(lr),\n",
    "            lamb=float(lamb),\n",
    "            interval=int(interval),\n",
    "            verbose=bool(verbose),\n",
    "            seed=int(seed),\n",
    "            random_seed=int(seed),\n",
    "        )\n",
    "    runtime_sec = float(time.time() - t0)\n",
    "\n",
    "    # -------------------------\n",
    "    # extract factors\n",
    "    # -------------------------\n",
    "    if not hasattr(model, \"extract_cell_factors\"):\n",
    "        raise AttributeError(\"scMoMaT model has no extract_cell_factors() in this fork.\")\n",
    "\n",
    "    cf_raw = model.extract_cell_factors()\n",
    "    if verbose:\n",
    "        print(\"[scmomat] extract_cell_factors() structure:\")\n",
    "        print(_shape_tree(cf_raw))\n",
    "\n",
    "    n_batches = len(ref_batch_meta)\n",
    "    Z_by_mod, had_modality_specific = _try_parse_factors(cf_raw, mod_names=mod_names, n_batches=n_batches, K=int(K))\n",
    "\n",
    "    # -------------------------\n",
    "    # fill full matrices in ref cell order\n",
    "    # -------------------------\n",
    "    def _fill_full(Z_full: np.ndarray, Z_batches: List[np.ndarray]):\n",
    "        for (bname, ref_idx_global), Zb in zip(ref_batch_meta, Z_batches):\n",
    "            Zb = np.asarray(Zb, dtype=np.float32)\n",
    "            if Zb.ndim != 2 or Zb.shape[1] != int(K):\n",
    "                raise RuntimeError(f\"Batch {bname}: expected (cells,K)=(*,{K}) got {Zb.shape}\")\n",
    "            if Zb.shape[0] != len(ref_idx_global):\n",
    "                raise RuntimeError(f\"Batch {bname}: Z rows {Zb.shape[0]} != n_cells {len(ref_idx_global)}\")\n",
    "            Z_full[ref_idx_global] = Zb\n",
    "\n",
    "    Z_rna_full = Z_atac_full = Z_adt_full = None\n",
    "\n",
    "    if had_modality_specific:\n",
    "        if \"rna\" in mod_names:\n",
    "            Z_rna_full = np.full((n_ref, int(K)), np.nan, dtype=np.float32)\n",
    "            _fill_full(Z_rna_full, Z_by_mod[\"rna\"])\n",
    "        if \"atac\" in mod_names:\n",
    "            Z_atac_full = np.full((n_ref, int(K)), np.nan, dtype=np.float32)\n",
    "            _fill_full(Z_atac_full, Z_by_mod[\"atac\"])\n",
    "        if \"adt\" in mod_names:\n",
    "            Z_adt_full = np.full((n_ref, int(K)), np.nan, dtype=np.float32)\n",
    "            _fill_full(Z_adt_full, Z_by_mod[\"adt\"])\n",
    "\n",
    "        # benchmark \"fused\" rule: average RNA/ATAC if both exist\n",
    "        if (Z_rna_full is not None) and (Z_atac_full is not None):\n",
    "            Z_fused = 0.5 * (Z_rna_full + Z_atac_full)\n",
    "        else:\n",
    "            if ref_mod == \"rna\" and Z_rna_full is not None:\n",
    "                Z_fused = Z_rna_full\n",
    "            elif ref_mod == \"atac\" and Z_atac_full is not None:\n",
    "                Z_fused = Z_atac_full\n",
    "            elif ref_mod == \"adt\" and Z_adt_full is not None:\n",
    "                Z_fused = Z_adt_full\n",
    "            else:\n",
    "                first = mod_names[0]\n",
    "                Z_fused = np.full((n_ref, int(K)), np.nan, dtype=np.float32)\n",
    "                _fill_full(Z_fused, Z_by_mod[first])\n",
    "    else:\n",
    "        Z_fused = np.full((n_ref, int(K)), np.nan, dtype=np.float32)\n",
    "        if \"fused\" in Z_by_mod:\n",
    "            _fill_full(Z_fused, Z_by_mod[\"fused\"])\n",
    "        else:\n",
    "            first = mod_names[0]\n",
    "            _fill_full(Z_fused, Z_by_mod[first])\n",
    "\n",
    "    # -------------------------\n",
    "    # output\n",
    "    # -------------------------\n",
    "    out = {\n",
    "        \"Z\": Z_fused,\n",
    "        \"Z_fused\": Z_fused,\n",
    "        \"Z_rna\": Z_rna_full,\n",
    "        \"Z_atac\": Z_atac_full,\n",
    "        \"Z_adt\": Z_adt_full,\n",
    "        \"runtime_sec\": float(runtime_sec),\n",
    "        \"fit_seconds\": float(runtime_sec),\n",
    "        \"transductive\": True,\n",
    "        \"uses_labels\": False,\n",
    "        \"notes\": \"Transductive: scMoMaT fits on all cells it embeds.\",\n",
    "        \"mods_used\": mod_names,\n",
    "        \"batches_used\": [b for (b, _) in ref_batch_meta],\n",
    "        \"out_dir\": str(out_dir) if out_dir is not None else None,\n",
    "        \"extra_json\": {\n",
    "            \"transductive\": True,\n",
    "            \"uses_labels\": False,\n",
    "            \"notes\": \"Transductive: scMoMaT fits on all cells it embeds.\",\n",
    "            \"had_modality_specific\": bool(had_modality_specific),\n",
    "            \"orient_mode\": orient_mode,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if out_dir is not None:\n",
    "        out_dir = Path(out_dir)\n",
    "        np.save(out_dir / \"Z_fused.npy\", Z_fused)\n",
    "        if Z_rna_full is not None:\n",
    "            np.save(out_dir / \"Z_rna.npy\", Z_rna_full)\n",
    "        if Z_atac_full is not None:\n",
    "            np.save(out_dir / \"Z_atac.npy\", Z_atac_full)\n",
    "        if Z_adt_full is not None:\n",
    "            np.save(out_dir / \"Z_adt.npy\", Z_adt_full)\n",
    "        with open(out_dir / \"run_info.json\", \"w\") as f:\n",
    "            json.dump({k: v for k, v in out.items() if not k.startswith(\"Z\")}, f, indent=2)\n",
    "\n",
    "    return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9156a68-d6e9-4266-9457-cfd6215a3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "# --- SciPy >=1.11 compatibility: scMoMaT uses len(sparse) internally ---\n",
    "try:\n",
    "    def _len_shape0(self):\n",
    "        return self.shape[0]\n",
    "    sp.spmatrix.__len__ = _len_shape0\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_scmomat_docstyle(\n",
    "    *,\n",
    "    rna=None,\n",
    "    atac=None,\n",
    "    adt=None,\n",
    "    layers_by_mod: Optional[Dict[str, str]] = None,\n",
    "    batch_key: Optional[str] = None,\n",
    "    K: int = 30,\n",
    "    T: int = 2000,\n",
    "    lr: float = 1e-4,\n",
    "    lamb: float = 1e-3,\n",
    "    interval: int = 200,\n",
    "    batch_size: float = 0.1,\n",
    "    device: str = \"cuda\",\n",
    "    seed: int = 0,\n",
    "    out_dir: Optional[str] = None,\n",
    "    verbose: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    scMoMaT runner that matches the orientation your fork is enforcing:\n",
    "      - matrices are (cells, features)\n",
    "      - batches are lists (one array per batch)\n",
    "    \"\"\"\n",
    "\n",
    "    if rna is None and atac is None and adt is None:\n",
    "        raise ValueError(\"Provide at least one modality AnnData: rna, atac, or adt.\")\n",
    "\n",
    "    # pick a reference modality for output row order\n",
    "    ref_mod = \"rna\" if rna is not None else (\"atac\" if atac is not None else \"adt\")\n",
    "    ref = {\"rna\": rna, \"atac\": atac, \"adt\": adt}[ref_mod]\n",
    "    n_ref = int(ref.n_obs)\n",
    "\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "    def _get_X(ad, mod: str):\n",
    "        if ad is None:\n",
    "            return None\n",
    "        layer = (layers_by_mod or {}).get(mod, None)\n",
    "        if layer is None:\n",
    "            X = ad.X\n",
    "        else:\n",
    "            if layer not in ad.layers:\n",
    "                raise KeyError(f\"layers_by_mod[{mod!r}]={layer!r} not in ad.layers\")\n",
    "            X = ad.layers[layer]\n",
    "        if sp.issparse(X):\n",
    "            X = X.toarray()\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        return np.ascontiguousarray(X)  # (cells, features)\n",
    "\n",
    "    def _batches(*ads):\n",
    "        if batch_key is None:\n",
    "            return [\"__single_batch__\"]\n",
    "        vals = set()\n",
    "        for ad in ads:\n",
    "            if ad is None:\n",
    "                continue\n",
    "            if batch_key not in ad.obs:\n",
    "                raise KeyError(f\"batch_key={batch_key!r} not in ad.obs\")\n",
    "            vals |= set(ad.obs[batch_key].astype(str).unique().tolist())\n",
    "        return sorted(vals)\n",
    "\n",
    "    def _subset(ad, b):\n",
    "        if ad is None or batch_key is None:\n",
    "            return ad\n",
    "        m = (ad.obs[batch_key].astype(str).values == str(b))\n",
    "        return ad[m] if np.any(m) else None\n",
    "\n",
    "    def _common_cells(ad_list):\n",
    "        present = [a for a in ad_list if a is not None]\n",
    "        common = set(present[0].obs_names.tolist())\n",
    "        for a in present[1:]:\n",
    "            common &= set(a.obs_names.tolist())\n",
    "        return sorted(common)\n",
    "\n",
    "    # build per-batch lists\n",
    "    mods = [(m, x) for m, x in [(\"rna\", rna), (\"atac\", atac), (\"adt\", adt)] if x is not None]\n",
    "    mod_names = [m for m, _ in mods]\n",
    "    batch_ids = _batches(rna, atac, adt)\n",
    "\n",
    "    mats = {m: [] for m in mod_names}\n",
    "    ref_batch_meta = []  # (batch_name, ref_global_indices)\n",
    "\n",
    "    for b in batch_ids:\n",
    "        sub = {m: _subset(ad, b) for m, ad in mods}\n",
    "        if any(sub[m] is None for m in sub):\n",
    "            continue\n",
    "\n",
    "        common = _common_cells([sub[m] for m in sub])\n",
    "        if len(common) == 0:\n",
    "            continue\n",
    "\n",
    "        sub = {m: sub[m][common] for m in sub}\n",
    "        ref_idx = ref.obs_names.get_indexer(common).astype(int)\n",
    "        if np.any(ref_idx < 0):\n",
    "            continue\n",
    "\n",
    "        for m in mod_names:\n",
    "            mats[m].append(_get_X(sub[m], m))  # (cells, features)\n",
    "        ref_batch_meta.append((str(b), ref_idx))\n",
    "\n",
    "    if len(ref_batch_meta) == 0:\n",
    "        raise RuntimeError(\"No usable batches after intersection.\")\n",
    "\n",
    "    counts = {m: mats[m] for m in mod_names}\n",
    "    counts[\"nbatches\"] = int(len(ref_batch_meta))\n",
    "\n",
    "    if verbose:\n",
    "        shapes0 = {m: mats[m][0].shape for m in mod_names}\n",
    "        print(f\"[scmomat] shapes(batch0)={shapes0} nbatches={counts['nbatches']} (cells x features)\")\n",
    "\n",
    "    # import + train\n",
    "    import scmomat\n",
    "    Model = scmomat.scmomat_model if hasattr(scmomat, \"scmomat_model\") else __import__(\"scmomat.model\", fromlist=[\"scmomat_model\"]).scmomat_model\n",
    "\n",
    "    model = Model(\n",
    "        counts,\n",
    "        K=int(K),\n",
    "        batch_size=float(batch_size),\n",
    "        interval=int(interval),\n",
    "        lr=float(lr),\n",
    "        lamb=float(lamb),\n",
    "        seed=int(seed),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.train_func(T=int(T))\n",
    "    runtime_sec = float(time.time() - t0)\n",
    "\n",
    "    cf = model.extract_cell_factors()\n",
    "\n",
    "    # Your fork returns: list[len=nbatches] of (cells, K)\n",
    "    if isinstance(cf, (list, tuple)) and len(cf) == len(ref_batch_meta):\n",
    "        Z_batches = [np.asarray(z, dtype=np.float32) for z in cf]\n",
    "    else:\n",
    "        # also handle singleton cases: [array] or array\n",
    "        if isinstance(cf, (list, tuple)) and len(cf) == 1:\n",
    "            Z_batches = [np.asarray(cf[0], dtype=np.float32)]\n",
    "        else:\n",
    "            Z_batches = [np.asarray(cf, dtype=np.float32)]\n",
    "\n",
    "    Z_full = np.full((n_ref, int(K)), np.nan, dtype=np.float32)\n",
    "    for (bname, ref_idx), Zb in zip(ref_batch_meta, Z_batches):\n",
    "        if Zb.shape[0] != len(ref_idx):\n",
    "            raise RuntimeError(f\"Batch {bname}: Z rows {Zb.shape[0]} != n_cells {len(ref_idx)}\")\n",
    "        Z_full[ref_idx] = Zb\n",
    "\n",
    "    if out_dir is not None:\n",
    "        outp = Path(out_dir)\n",
    "        outp.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(outp / \"Z_fused.npy\", Z_full)\n",
    "\n",
    "    return {\n",
    "        \"Z_fused\": Z_full,\n",
    "        \"Z\": Z_full,\n",
    "        \"fit_seconds\": runtime_sec,\n",
    "        \"transductive\": True,\n",
    "        \"uses_labels\": False,\n",
    "        \"extra_json\": {\"transductive\": True, \"mods_used\": mod_names, \"nbatches\": counts[\"nbatches\"]},\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995659f-110c-488c-b003-34cea192b69d",
   "metadata": {},
   "source": [
    "### 6) scJoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cc396-89bc-4938-9d19-4d449ba21ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def build_gene_activity_from_peaks(\n",
    "    atac,\n",
    "    rna,\n",
    "    *,\n",
    "    gene_upstream=2000,\n",
    "    gene_downstream=2000,\n",
    "    layer_out=\"gene_activity\",\n",
    "    dtype=np.float32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build ATAC gene activity by summing peak counts overlapping gene body windows.\n",
    "    Uses rna.var['chrom','chromStart','chromEnd'] and atac.var['chrom','chromStart','chromEnd'].\n",
    "    Writes atac.layers[layer_out] as CSR (cells x genes in RNA var order).\n",
    "    \"\"\"\n",
    "    required_gene_cols = {\"chrom\", \"chromStart\", \"chromEnd\"}\n",
    "    required_peak_cols = {\"chrom\", \"chromStart\", \"chromEnd\"}\n",
    "    if not required_gene_cols.issubset(set(rna.var.columns)):\n",
    "        raise KeyError(f\"RNA var missing columns: {required_gene_cols - set(rna.var.columns)}\")\n",
    "    if not required_peak_cols.issubset(set(atac.var.columns)):\n",
    "        raise KeyError(f\"ATAC var missing columns: {required_peak_cols - set(atac.var.columns)}\")\n",
    "\n",
    "    # peak coords\n",
    "    p_chr = np.asarray(atac.var[\"chrom\"]).astype(str)\n",
    "    p_start = np.asarray(atac.var[\"chromStart\"]).astype(np.int64)\n",
    "    p_end   = np.asarray(atac.var[\"chromEnd\"]).astype(np.int64)\n",
    "\n",
    "    # gene coords (RNA order)\n",
    "    g_chr = np.asarray(rna.var[\"chrom\"]).astype(str)\n",
    "    g_start = np.asarray(rna.var[\"chromStart\"]).astype(np.int64) - int(gene_upstream)\n",
    "    g_end   = np.asarray(rna.var[\"chromEnd\"]).astype(np.int64)   + int(gene_downstream)\n",
    "    g_start = np.maximum(g_start, 0)\n",
    "\n",
    "    # build COO edges: peak_idx -> gene_idx if intervals overlap\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    # per-chrom sweep for overlap edges\n",
    "    for chrom in np.unique(np.intersect1d(np.unique(p_chr), np.unique(g_chr))):\n",
    "        p_idx = np.where(p_chr == chrom)[0]\n",
    "        g_idx = np.where(g_chr == chrom)[0]\n",
    "        if p_idx.size == 0 or g_idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        # sort peaks by start\n",
    "        p_ord = p_idx[np.argsort(p_start[p_idx], kind=\"mergesort\")]\n",
    "        ps = p_start[p_ord]\n",
    "        pe = p_end[p_ord]\n",
    "\n",
    "        # sort genes by start\n",
    "        g_ord = g_idx[np.argsort(g_start[g_idx], kind=\"mergesort\")]\n",
    "        gs = g_start[g_ord]\n",
    "        ge = g_end[g_ord]\n",
    "\n",
    "        j0 = 0\n",
    "        for gi, (s, e) in zip(g_ord, zip(gs, ge)):\n",
    "            # advance j0 until peak end >= gene start (can't overlap before this)\n",
    "            while j0 < p_ord.size and pe[j0] < s:\n",
    "                j0 += 1\n",
    "            j = j0\n",
    "            # walk forward while peak start <= gene end\n",
    "            while j < p_ord.size and ps[j] <= e:\n",
    "                # overlap condition\n",
    "                if pe[j] >= s:\n",
    "                    rows.append(p_ord[j])  # peak index\n",
    "                    cols.append(gi)        # gene index (RNA var index)\n",
    "                j += 1\n",
    "\n",
    "    if len(rows) == 0:\n",
    "        raise RuntimeError(\"No peak↔gene overlaps found; cannot build gene activity.\")\n",
    "\n",
    "    rows = np.asarray(rows, dtype=np.int64)\n",
    "    cols = np.asarray(cols, dtype=np.int64)\n",
    "    data = np.ones(rows.shape[0], dtype=np.int8)\n",
    "\n",
    "    # peak->gene mapping (peaks x genes)\n",
    "    P2G = sp.coo_matrix((data, (rows, cols)), shape=(atac.n_vars, rna.n_vars)).tocsr()\n",
    "\n",
    "    Xp = atac.X\n",
    "    if not sp.issparse(Xp):\n",
    "        Xp = sp.csr_matrix(np.asarray(Xp))\n",
    "    Xp = Xp.tocsr()\n",
    "\n",
    "    # gene activity (cells x genes)\n",
    "    Xg = (Xp @ P2G).astype(dtype).tocsr()\n",
    "\n",
    "    # put it in obsm (allowed to be cells x genes)\n",
    "    atac.obsm[layer_out] = Xg\n",
    "\n",
    "    # optionally store gene names used for columns\n",
    "    atac.uns[f\"{layer_out}__gene_names\"] = rna.var_names.to_numpy()\n",
    "\n",
    "    return atac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78e92e-fac3-4f09-9c93-1df008695ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_for_scjoint = atac.copy()\n",
    "atac_for_scjoint = build_gene_activity_from_peaks(atac_for_scjoint, rna, layer_out=\"gene_activity\", gene_upstream=50000, gene_downstream=50000)\n",
    "\n",
    "print(\"gene_activity layer:\", atac_for_scjoint.obsm[\"gene_activity\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a1b35-5ccd-42b2-99c4-3d31fa8b8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def restrict_gene_activity_to_rna_features(\n",
    "    atac,\n",
    "    rna_feat_adata,\n",
    "    *,\n",
    "    ga_key=\"gene_activity\",\n",
    "    out_key=\"gene_activity_hvg\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes atac.obsm[ga_key] (cells x genes_in_full_rna_space) and slices columns\n",
    "    to match rna_feat_adata.var_names exactly (order preserved).\n",
    "\n",
    "    Requires atac.uns[f\"{ga_key}__gene_names\"] that you saved in build_gene_activity_from_peaks().\n",
    "    Writes atac.obsm[out_key] and atac.uns[f\"{out_key}__gene_names\"].\n",
    "    Returns out_key.\n",
    "    \"\"\"\n",
    "    if ga_key not in atac.obsm:\n",
    "        raise KeyError(f\"{ga_key!r} not in atac.obsm. Available: {list(atac.obsm.keys())}\")\n",
    "\n",
    "    all_genes = atac.uns.get(f\"{ga_key}__gene_names\", None)\n",
    "    if all_genes is None:\n",
    "        raise KeyError(f\"Missing atac.uns['{ga_key}__gene_names']; can't map gene-activity columns.\")\n",
    "\n",
    "    X = atac.obsm[ga_key]\n",
    "    if not sp.issparse(X):\n",
    "        X = sp.csr_matrix(np.asarray(X))\n",
    "    X = X.tocsr()\n",
    "\n",
    "    target_genes = pd.Index(rna_feat_adata.var_names.astype(str))\n",
    "    all_genes = pd.Index(np.asarray(all_genes).astype(str))\n",
    "\n",
    "    col_idx = all_genes.get_indexer(target_genes)\n",
    "    missing = np.where(col_idx < 0)[0]\n",
    "    if missing.size:\n",
    "        ex = target_genes[missing[:10]].tolist()\n",
    "        raise ValueError(\n",
    "            f\"{missing.size} / {len(target_genes)} RNA features not found in gene-activity gene list. \"\n",
    "            f\"Examples: {ex}\"\n",
    "        )\n",
    "\n",
    "    atac.obsm[out_key] = X[:, col_idx].astype(np.float32).tocsr()\n",
    "    atac.uns[f\"{out_key}__gene_names\"] = target_genes.to_numpy()\n",
    "\n",
    "    return out_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7d30b-a9da-4e18-8119-6e893dd71294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice gene activity to match the RNA HVG space used for scJoint\n",
    "ga_key_for_scjoint = restrict_gene_activity_to_rna_features(\n",
    "    atac_for_scjoint, rna_counts_hvg, ga_key=\"gene_activity\", out_key=\"gene_activity_hvg\"\n",
    ")\n",
    "\n",
    "print(\"ATAC gene activity (HVG) shape:\", atac_for_scjoint.obsm[ga_key_for_scjoint].shape)\n",
    "print(\"RNA HVG shape:\", rna_counts_hvg.n_obs, rna_counts_hvg.n_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d121c-3b0a-4d8d-ba4a-dfa3d793cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- scJoint runner (robust + notebook-friendly; patched for all known issues) ---\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, sys, time, shutil, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def _as_csr_float32(X):\n",
    "    if sp.issparse(X):\n",
    "        X = X.tocsr()\n",
    "        if X.dtype != np.float32:\n",
    "            X = X.astype(np.float32)\n",
    "        return X\n",
    "    return sp.csr_matrix(np.asarray(X, dtype=np.float32))\n",
    "\n",
    "\n",
    "def run_scjoint(\n",
    "    adata_rna,\n",
    "    adata_atac,\n",
    "    *,\n",
    "    labels_key,                       # obs col name OR array-like labels\n",
    "    atac_gene_activity_layer,          # REQUIRED: gene activity in atac.obsm or atac.layers\n",
    "    scjoint_repo,                     # path to your cloned SydneyBioX/scJoint\n",
    "    out_dir,\n",
    "    seed=0,\n",
    "    embedding_size=30,\n",
    "    batch_size=256,\n",
    "    gpu=0,\n",
    "    epochs_stage1=50,\n",
    "    epochs_stage3=50,\n",
    "    lr_stage1=1e-3,\n",
    "    lr_stage3=1e-3,\n",
    "    momentum=0.9,\n",
    "    center_weight=1,\n",
    "    p=0.8,\n",
    "    with_crossentropy=True,\n",
    "    threads=12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs scJoint via subprocess in a way that works on clusters / notebooks.\n",
    "\n",
    "    Key requirements:\n",
    "      - ATAC input must be GENE-ACTIVITY with same #features as RNA input\n",
    "      - We patch: np.float removal, non-tty stty progress bar, torch dynamo/compile\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    repo = Path(scjoint_repo)\n",
    "    if not repo.exists():\n",
    "        raise FileNotFoundError(f\"scJoint repo not found at: {repo}\")\n",
    "\n",
    "    main_src = repo / \"main.py\"\n",
    "    if not main_src.exists():\n",
    "        raise FileNotFoundError(f\"Couldn't find main.py in scJoint repo: {main_src}\")\n",
    "\n",
    "    # ---- labels ----\n",
    "    if isinstance(labels_key, str):\n",
    "        y = adata_rna.obs[labels_key].astype(str).to_numpy()\n",
    "    else:\n",
    "        y = np.asarray(labels_key).astype(str)\n",
    "\n",
    "    classes, y_int = np.unique(y, return_inverse=True)\n",
    "    n_classes = int(len(classes))\n",
    "\n",
    "    # ---- matrices ----\n",
    "    X_rna = _as_csr_float32(adata_rna.X)\n",
    "\n",
    "    if atac_gene_activity_layer is None:\n",
    "        raise ValueError(\"atac_gene_activity_layer is required for scJoint (must be gene-space matrix).\")\n",
    "\n",
    "    if atac_gene_activity_layer in adata_atac.obsm:\n",
    "        X_atac = _as_csr_float32(adata_atac.obsm[atac_gene_activity_layer])\n",
    "    elif atac_gene_activity_layer in adata_atac.layers:\n",
    "        X_atac = _as_csr_float32(adata_atac.layers[atac_gene_activity_layer])\n",
    "    else:\n",
    "        raise KeyError(\n",
    "            f\"Couldn't find atac_gene_activity_layer='{atac_gene_activity_layer}' in atac.obsm or atac.layers. \"\n",
    "            f\"Available obsm={list(adata_atac.obsm.keys())[:10]}..., layers={list(adata_atac.layers.keys())}\"\n",
    "        )\n",
    "\n",
    "    if X_atac.shape[0] != X_rna.shape[0]:\n",
    "        raise ValueError(f\"RNA/ATAC cell count mismatch: RNA {X_rna.shape}, ATAC {X_atac.shape}\")\n",
    "\n",
    "    if X_atac.shape[1] != X_rna.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"scJoint requires ATAC gene activity to have SAME number of features as RNA.\\n\"\n",
    "            f\"Got RNA features={X_rna.shape[1]}, ATAC gene-activity features={X_atac.shape[1]}.\\n\"\n",
    "            \"Fix by constructing ATAC gene activity in the same gene space/order as your RNA features (e.g. HVGs).\"\n",
    "        )\n",
    "\n",
    "    input_size = int(X_rna.shape[1])\n",
    "\n",
    "    # ---- write inputs ----\n",
    "    data_dir = out_dir / \"data\"\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    rna_npz = data_dir / \"rna.npz\"\n",
    "    atac_npz = data_dir / \"atac_gene_activity.npz\"\n",
    "    labels_txt = data_dir / \"labels.txt\"\n",
    "\n",
    "    sp.save_npz(str(rna_npz), X_rna)\n",
    "    sp.save_npz(str(atac_npz), X_atac)\n",
    "    labels_txt.write_text(\"\\n\".join(map(str, y_int.tolist())) + \"\\n\")\n",
    "\n",
    "    # ---- write config.py expected by scJoint main.py ----\n",
    "    # Must define class Config with required attributes (threads, rna_protein_paths, atac_protein_paths, etc.)\n",
    "    cfg_py = out_dir / \"config.py\"\n",
    "    cfg_py.write_text(\n",
    "        f\"\"\"\n",
    "import torch\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.use_cuda = True\n",
    "        self.threads = {int(threads)}\n",
    "\n",
    "        if not self.use_cuda:\n",
    "            self.device = torch.device('cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cuda:0')\n",
    "\n",
    "        # dataset info\n",
    "        self.number_of_class = {int(n_classes)}\n",
    "        self.input_size = {int(input_size)}\n",
    "\n",
    "        self.rna_paths = [\"{rna_npz}\"]\n",
    "        self.rna_labels = [\"{labels_txt}\"]\n",
    "\n",
    "        self.atac_paths = [\"{atac_npz}\"]\n",
    "        self.atac_labels = []  # optional\n",
    "\n",
    "        # MUST exist even if unused\n",
    "        self.rna_protein_paths = []\n",
    "        self.atac_protein_paths = []\n",
    "\n",
    "        # training\n",
    "        self.batch_size = {int(batch_size)}\n",
    "        self.lr_stage1 = {float(lr_stage1)}\n",
    "        self.lr_stage3 = {float(lr_stage3)}\n",
    "        self.lr_decay_epoch = 20\n",
    "        self.epochs_stage1 = {int(epochs_stage1)}\n",
    "        self.epochs_stage3 = {int(epochs_stage3)}\n",
    "        self.p = {float(p)}\n",
    "        self.embedding_size = {int(embedding_size)}\n",
    "        self.momentum = {float(momentum)}\n",
    "        self.center_weight = {float(center_weight)}\n",
    "        self.with_crossentorpy = {bool(with_crossentropy)}\n",
    "        self.seed = {int(seed)}\n",
    "        self.checkpoint = \"\"\n",
    "\n",
    "        # some forks read this\n",
    "        self.save_dir = \"{(out_dir / \"output\")}\"\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    # ---- copy main.py into out_dir so it imports our config.py ----\n",
    "    main_dst = out_dir / \"main.py\"\n",
    "    shutil.copy2(str(main_src), str(main_dst))\n",
    "\n",
    "    # ---- sitecustomize.py patches for numpy + stty progress bar ----\n",
    "    # - numpy 2.x removed np.float / np.int / np.bool\n",
    "    # - scJoint progress bar tries: os.popen(\"stty size\") and crashes in non-tty\n",
    "    sitecustomize = out_dir / \"sitecustomize.py\"\n",
    "    sitecustomize.write_text(\n",
    "        \"import os, io\\n\"\n",
    "        \"import numpy as np\\n\"\n",
    "        \"\\n\"\n",
    "        \"# ---- NumPy 2.x compatibility (scJoint uses np.float etc.) ----\\n\"\n",
    "        \"if not hasattr(np, 'float'): np.float = float\\n\"\n",
    "        \"if not hasattr(np, 'int'):   np.int = int\\n\"\n",
    "        \"if not hasattr(np, 'bool'):  np.bool = bool\\n\"\n",
    "        \"\\n\"\n",
    "        \"# ---- scJoint progress bar fix for non-TTY ----\\n\"\n",
    "        \"_orig_popen = os.popen\\n\"\n",
    "        \"def _patched_popen(cmd, mode='r', *args, **kwargs):\\n\"\n",
    "        \"    try:\\n\"\n",
    "        \"        s = cmd.strip() if isinstance(cmd, str) else ''\\n\"\n",
    "        \"        if s.startswith('stty size'):\\n\"\n",
    "        \"            return io.StringIO('24 120\\\\n')\\n\"\n",
    "        \"    except Exception:\\n\"\n",
    "        \"        pass\\n\"\n",
    "        \"    return _orig_popen(cmd, mode, *args, **kwargs)\\n\"\n",
    "        \"os.popen = _patched_popen\\n\"\n",
    "    )\n",
    "\n",
    "    # ---- env for subprocess ----\n",
    "    env = os.environ.copy()\n",
    "    env[\"PYTHONHASHSEED\"] = str(int(seed))\n",
    "\n",
    "    # Disable torch dynamo/compile (avoids sympy / torch._dynamo / fx symbolic shapes issues)\n",
    "    env[\"TORCH_DISABLE_DYNAMO\"] = \"1\"\n",
    "    env[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "    env[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "    # Make sure sitecustomize is found FIRST (out_dir must precede repo on PYTHONPATH)\n",
    "    env[\"PYTHONPATH\"] = str(out_dir) + os.pathsep + str(repo) + os.pathsep + env.get(\"PYTHONPATH\", \"\")\n",
    "\n",
    "    # GPU selection\n",
    "    if gpu is None:\n",
    "        env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    else:\n",
    "        env[\"CUDA_VISIBLE_DEVICES\"] = str(int(gpu))\n",
    "\n",
    "    # ---- run ----\n",
    "    proc = subprocess.run(\n",
    "        [sys.executable, str(main_dst)],\n",
    "        cwd=str(out_dir),\n",
    "        env=env,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "    log_path = out_dir / \"scjoint_run.log\"\n",
    "    log_path.write_text(proc.stdout)\n",
    "\n",
    "    if proc.returncode != 0:\n",
    "        tail = \"\\n\".join(proc.stdout.splitlines()[-80:])\n",
    "        raise RuntimeError(\n",
    "            \"scJoint failed. See scjoint_run.log for details.\\n\"\n",
    "            f\"Last 80 lines:\\n{tail}\"\n",
    "        )\n",
    "\n",
    "    # ---- find embeddings ----\n",
    "    # Different forks dump in different places; search widely.\n",
    "    cand_dirs = [out_dir / \"output\", out_dir, out_dir / \"results\", out_dir / \"Result\", out_dir / \"log\"]\n",
    "    emb_files = []\n",
    "    for d in cand_dirs:\n",
    "        if d.exists():\n",
    "            emb_files.extend(list(d.rglob(\"*embeddings*.txt\")))\n",
    "            emb_files.extend(list(d.rglob(\"*embedding*.txt\")))\n",
    "            emb_files.extend(list(d.rglob(\"*latent*.txt\")))\n",
    "    emb_files = sorted({p.resolve() for p in emb_files})\n",
    "\n",
    "    if len(emb_files) < 2:\n",
    "        # dump some directory listing hints\n",
    "        existing = []\n",
    "        for d in cand_dirs:\n",
    "            if d.exists():\n",
    "                existing.extend([str(p.relative_to(d)) for p in d.rglob(\"*\")][:50])\n",
    "        raise RuntimeError(\n",
    "            \"scJoint finished but I couldn't find 2 embedding files.\\n\"\n",
    "            f\"Searched in: {[str(d) for d in cand_dirs]}\\n\"\n",
    "            f\"Found: {[str(p) for p in emb_files]}\\n\"\n",
    "            f\"Sample files under output dirs: {existing}\\n\"\n",
    "            f\"Log: {log_path}\"\n",
    "        )\n",
    "\n",
    "    # Heuristic: take the two largest embedding-ish files (usually RNA/ATAC)\n",
    "    emb_files = sorted(emb_files, key=lambda p: p.stat().st_size, reverse=True)[:2]\n",
    "    Z0 = np.loadtxt(str(emb_files[0])).astype(np.float32)\n",
    "    Z1 = np.loadtxt(str(emb_files[1])).astype(np.float32)\n",
    "\n",
    "    if Z0.shape != Z1.shape:\n",
    "        raise RuntimeError(f\"Embedding shape mismatch: {emb_files[0]} {Z0.shape} vs {emb_files[1]} {Z1.shape}\")\n",
    "\n",
    "    Z_rna, Z_atac = Z0, Z1\n",
    "    Z_fused = 0.5 * (Z_rna + Z_atac)\n",
    "\n",
    "    fit_seconds = float(time.perf_counter() - t0)\n",
    "\n",
    "    return ensure_flags({\n",
    "        \"Z_rna\": Z_rna,\n",
    "        \"Z_atac\": Z_atac,\n",
    "        \"Z_fused\": Z_fused,\n",
    "        \"fit_seconds\": fit_seconds,\n",
    "        \"extra_json\": standard_flags(\n",
    "            transductive=True,\n",
    "            uses_labels=True,\n",
    "            note=\"scJoint trained with RNA labels; fit included all cells.\"\n",
    "        ),\n",
    "    }, default_transductive=True, default_uses_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591709b-ff8c-47d0-8406-187235208b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, sys, time, shutil, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def _as_csr_float32(X):\n",
    "    if sp.issparse(X):\n",
    "        X = X.tocsr(copy=False)\n",
    "        return X.astype(np.float32, copy=False) if X.dtype != np.float32 else X\n",
    "    return sp.csr_matrix(np.asarray(X, dtype=np.float32))\n",
    "\n",
    "\n",
    "def _scrub_finite(Z: np.ndarray, *, name: str, verbose: bool = True) -> np.ndarray:\n",
    "    \"\"\"Replace NaN/Inf with 0.0.\"\"\"\n",
    "    Z = np.asarray(Z, dtype=np.float32)\n",
    "    bad = ~np.isfinite(Z)\n",
    "    if bad.any():\n",
    "        if verbose:\n",
    "            n_nan = int(np.isnan(Z).sum())\n",
    "            n_inf = int(np.isinf(Z).sum())\n",
    "            print(f\"[scJoint] WARNING: {name} had non-finite: nan={n_nan} inf={n_inf} -> set to 0\")\n",
    "        Z = Z.copy()\n",
    "        Z[bad] = 0.0\n",
    "    return Z\n",
    "\n",
    "\n",
    "def run_scjoint_split_aware(\n",
    "    adata_rna,\n",
    "    adata_atac,\n",
    "    *,\n",
    "    splits,\n",
    "    labels_key,\n",
    "    atac_gene_activity_layer,\n",
    "    scjoint_repo,\n",
    "    out_dir,\n",
    "    seed=0,\n",
    "    latent_dim=30,\n",
    "    batch_size=256,\n",
    "    gpu=0,\n",
    "    epochs_stage1=50,\n",
    "    epochs_stage3=50,\n",
    "    lr_stage1=1e-3,\n",
    "    lr_stage3=1e-3,\n",
    "    momentum=0.9,\n",
    "    center_weight=1,\n",
    "    p=0.8,\n",
    "    with_crossentropy=True,\n",
    "    threads=12,\n",
    "    verbose=True,\n",
    "    # -------------------------\n",
    "    # NEW knobs\n",
    "    # -------------------------\n",
    "    allow_transductive_fallback: bool = False,\n",
    "    fill_missing: str = \"nan\",  # \"nan\" (recommended) or \"zeros\"\n",
    "):\n",
    "    \"\"\"\n",
    "    scJoint runner with explicit behavior:\n",
    "\n",
    "    - Inductive (default): train on FIT=train(+val) only.\n",
    "        * FIT rows get embeddings\n",
    "        * non-FIT rows are missing (NaN by default, or zeros if fill_missing=\"zeros\")\n",
    "        * Metrics that evaluate on TEST will be NaN if TEST is entirely missing (expected)\n",
    "\n",
    "    - Transductive fallback (optional): if allow_transductive_fallback=True and a test split exists,\n",
    "      we train on ALL cells to produce embeddings for TEST (sets transductive=True in flags).\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fill_missing = str(fill_missing).lower()\n",
    "    if fill_missing not in (\"nan\", \"zeros\"):\n",
    "        raise ValueError(\"fill_missing must be 'nan' or 'zeros'\")\n",
    "\n",
    "    # splits\n",
    "    n_full = int(adata_rna.n_obs)\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "    va = np.asarray(splits.get(\"val\", []), dtype=int)\n",
    "    te = np.asarray(splits.get(\"test\", []), dtype=int)\n",
    "\n",
    "    fit_idx = np.concatenate([tr, va]) if va.size else tr\n",
    "    fit_idx = np.asarray(fit_idx, dtype=int)\n",
    "\n",
    "    transductive = False\n",
    "    if allow_transductive_fallback and te.size > 0:\n",
    "        # train on ALL so scJoint produces embeddings for everyone\n",
    "        fit_idx = np.arange(n_full, dtype=int)\n",
    "        transductive = True\n",
    "\n",
    "    # subset for training\n",
    "    rna_fit = adata_rna[fit_idx].copy()\n",
    "    atac_fit = adata_atac[fit_idx].copy()\n",
    "\n",
    "    repo = Path(scjoint_repo)\n",
    "    if not repo.exists():\n",
    "        raise FileNotFoundError(f\"scJoint repo not found at: {repo}\")\n",
    "    main_src = repo / \"main.py\"\n",
    "    if not main_src.exists():\n",
    "        raise FileNotFoundError(f\"Couldn't find main.py in scJoint repo: {main_src}\")\n",
    "\n",
    "    # ---- labels on FIT ----\n",
    "    if isinstance(labels_key, str):\n",
    "        if labels_key not in rna_fit.obs:\n",
    "            raise KeyError(f\"labels_key={labels_key!r} not in rna_fit.obs\")\n",
    "        y_fit = rna_fit.obs[labels_key].astype(str).to_numpy()\n",
    "    else:\n",
    "        y_all = np.asarray(labels_key).astype(str)\n",
    "        if y_all.shape[0] != n_full:\n",
    "            raise ValueError(f\"labels array len={y_all.shape[0]} != n_full={n_full}\")\n",
    "        y_fit = y_all[fit_idx]\n",
    "\n",
    "    classes, y_int = np.unique(y_fit, return_inverse=True)\n",
    "    n_classes = int(len(classes))\n",
    "\n",
    "    # ---- matrices ----\n",
    "    X_rna = _as_csr_float32(rna_fit.X)\n",
    "\n",
    "    if atac_gene_activity_layer is None:\n",
    "        raise ValueError(\"atac_gene_activity_layer is required for scJoint (gene-space matrix).\")\n",
    "\n",
    "    if atac_gene_activity_layer in atac_fit.obsm:\n",
    "        X_atac = _as_csr_float32(atac_fit.obsm[atac_gene_activity_layer])\n",
    "    elif atac_gene_activity_layer in atac_fit.layers:\n",
    "        X_atac = _as_csr_float32(atac_fit.layers[atac_gene_activity_layer])\n",
    "    else:\n",
    "        raise KeyError(\n",
    "            f\"Couldn't find atac_gene_activity_layer='{atac_gene_activity_layer}' in atac.obsm or atac.layers. \"\n",
    "            f\"obsm={list(atac_fit.obsm.keys())[:10]}..., layers={list(atac_fit.layers.keys())}\"\n",
    "        )\n",
    "\n",
    "    if X_atac.shape[0] != X_rna.shape[0]:\n",
    "        raise ValueError(f\"RNA/ATAC cell count mismatch: RNA {X_rna.shape}, ATAC {X_atac.shape}\")\n",
    "    if X_atac.shape[1] != X_rna.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"scJoint requires ATAC gene activity to have SAME features as RNA.\\n\"\n",
    "            f\"RNA p={X_rna.shape[1]}, ATAC gene-activity p={X_atac.shape[1]}\"\n",
    "        )\n",
    "\n",
    "    input_size = int(X_rna.shape[1])\n",
    "\n",
    "    # ---- write inputs ----\n",
    "    data_dir = out_dir / \"data\"\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    rna_npz = data_dir / \"rna.npz\"\n",
    "    atac_npz = data_dir / \"atac_gene_activity.npz\"\n",
    "    labels_txt = data_dir / \"labels.txt\"\n",
    "\n",
    "    sp.save_npz(str(rna_npz), X_rna)\n",
    "    sp.save_npz(str(atac_npz), X_atac)\n",
    "    labels_txt.write_text(\"\\n\".join(map(str, y_int.tolist())) + \"\\n\")\n",
    "\n",
    "    # ---- config.py ----\n",
    "    cfg_py = out_dir / \"config.py\"\n",
    "    cfg_py.write_text(\n",
    "        f\"\"\"\n",
    "import torch\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.use_cuda = True\n",
    "        self.threads = {int(threads)}\n",
    "        self.device = torch.device('cuda:0') if self.use_cuda else torch.device('cpu')\n",
    "\n",
    "        self.number_of_class = {int(n_classes)}\n",
    "        self.input_size = {int(input_size)}\n",
    "\n",
    "        self.rna_paths = [\"{rna_npz}\"]\n",
    "        self.rna_labels = [\"{labels_txt}\"]\n",
    "        self.atac_paths = [\"{atac_npz}\"]\n",
    "        self.atac_labels = []\n",
    "\n",
    "        self.rna_protein_paths = []\n",
    "        self.atac_protein_paths = []\n",
    "\n",
    "        self.batch_size = {int(batch_size)}\n",
    "        self.lr_stage1 = {float(lr_stage1)}\n",
    "        self.lr_stage3 = {float(lr_stage3)}\n",
    "        self.lr_decay_epoch = 20\n",
    "        self.epochs_stage1 = {int(epochs_stage1)}\n",
    "        self.epochs_stage3 = {int(epochs_stage3)}\n",
    "        self.p = {float(p)}\n",
    "        self.embedding_size = {int(latent_dim)}\n",
    "        self.momentum = {float(momentum)}\n",
    "        self.center_weight = {float(center_weight)}\n",
    "        self.with_crossentorpy = {bool(with_crossentropy)}\n",
    "        self.seed = {int(seed)}\n",
    "        self.checkpoint = \"\"\n",
    "        self.save_dir = \"{(out_dir / \"output\")}\"\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    # ---- copy main.py ----\n",
    "    main_dst = out_dir / \"main.py\"\n",
    "    shutil.copy2(str(main_src), str(main_dst))\n",
    "\n",
    "    # ---- sitecustomize.py patches ----\n",
    "    sitecustomize = out_dir / \"sitecustomize.py\"\n",
    "    sitecustomize.write_text(\n",
    "        \"import os, io\\n\"\n",
    "        \"import numpy as np\\n\"\n",
    "        \"if not hasattr(np, 'float'): np.float = float\\n\"\n",
    "        \"if not hasattr(np, 'int'):   np.int = int\\n\"\n",
    "        \"if not hasattr(np, 'bool'):  np.bool = bool\\n\"\n",
    "        \"_orig_popen = os.popen\\n\"\n",
    "        \"def _patched_popen(cmd, mode='r', *args, **kwargs):\\n\"\n",
    "        \"    try:\\n\"\n",
    "        \"        s = cmd.strip() if isinstance(cmd, str) else ''\\n\"\n",
    "        \"        if s.startswith('stty size'):\\n\"\n",
    "        \"            return io.StringIO('24 120\\\\n')\\n\"\n",
    "        \"    except Exception:\\n\"\n",
    "        \"        pass\\n\"\n",
    "        \"    return _orig_popen(cmd, mode, *args, **kwargs)\\n\"\n",
    "        \"os.popen = _patched_popen\\n\"\n",
    "    )\n",
    "\n",
    "    # ---- env ----\n",
    "    env = os.environ.copy()\n",
    "    env[\"PYTHONHASHSEED\"] = str(int(seed))\n",
    "    env[\"TORCH_DISABLE_DYNAMO\"] = \"1\"\n",
    "    env[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "    env[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
    "    env[\"PYTHONPATH\"] = str(out_dir) + os.pathsep + str(repo) + os.pathsep + env.get(\"PYTHONPATH\", \"\")\n",
    "    env[\"CUDA_VISIBLE_DEVICES\"] = \"\" if gpu is None else str(int(gpu))\n",
    "\n",
    "    # ---- run ----\n",
    "    proc = subprocess.run(\n",
    "        [sys.executable, str(main_dst)],\n",
    "        cwd=str(out_dir),\n",
    "        env=env,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        check=False,\n",
    "    )\n",
    "\n",
    "    log_path = out_dir / \"scjoint_run.log\"\n",
    "    log_path.write_text(proc.stdout)\n",
    "\n",
    "    if proc.returncode != 0:\n",
    "        tail = \"\\n\".join(proc.stdout.splitlines()[-120:])\n",
    "        raise RuntimeError(\n",
    "            \"scJoint failed. See scjoint_run.log for details.\\n\"\n",
    "            f\"Last 120 lines:\\n{tail}\"\n",
    "        )\n",
    "\n",
    "    # ---- find embeddings ----\n",
    "    cand_dirs = [out_dir / \"output\", out_dir, out_dir / \"results\", out_dir / \"Result\", out_dir / \"log\"]\n",
    "    emb_files = []\n",
    "    for d in cand_dirs:\n",
    "        if d.exists():\n",
    "            emb_files.extend(list(d.rglob(\"*embeddings*.txt\")))\n",
    "            emb_files.extend(list(d.rglob(\"*embedding*.txt\")))\n",
    "            emb_files.extend(list(d.rglob(\"*latent*.txt\")))\n",
    "    emb_files = sorted({p.resolve() for p in emb_files})\n",
    "\n",
    "    if len(emb_files) < 2:\n",
    "        raise RuntimeError(f\"scJoint finished but couldn't find embeddings txts under {cand_dirs}. Log: {log_path}\")\n",
    "\n",
    "    def _score(p: Path):\n",
    "        s = p.name.lower()\n",
    "        score = 0\n",
    "        if \"rna\" in s: score += 10\n",
    "        if \"atac\" in s or \"acc\" in s: score += 10\n",
    "        if \"joint\" in s or \"fused\" in s: score += 2\n",
    "        return score\n",
    "\n",
    "    emb_sorted = sorted(emb_files, key=lambda p: (_score(p), p.stat().st_size), reverse=True)\n",
    "    p0, p1 = emb_sorted[0], emb_sorted[1]\n",
    "\n",
    "    Z0 = np.loadtxt(str(p0)).astype(np.float32)\n",
    "    Z1 = np.loadtxt(str(p1)).astype(np.float32)\n",
    "\n",
    "    fit_n = len(fit_idx)\n",
    "    if Z0.ndim != 2 or Z1.ndim != 2 or Z0.shape[0] != fit_n or Z1.shape[0] != fit_n:\n",
    "        # fallback: pick any two matching fit_n\n",
    "        cands = []\n",
    "        for p in emb_files:\n",
    "            try:\n",
    "                z = np.loadtxt(str(p))\n",
    "                if z.ndim == 2 and z.shape[0] == fit_n:\n",
    "                    cands.append((p, z.astype(np.float32)))\n",
    "            except Exception:\n",
    "                pass\n",
    "        if len(cands) < 2:\n",
    "            raise RuntimeError(\n",
    "                f\"Found embedding txts but none matched expected n_cells={fit_n}. \"\n",
    "                f\"Example {p0.name}={Z0.shape}\"\n",
    "            )\n",
    "        cands = sorted(cands, key=lambda t: t[0].stat().st_size, reverse=True)[:2]\n",
    "        p0, p1 = cands[0][0], cands[1][0]\n",
    "        Z0, Z1 = cands[0][1], cands[1][1]\n",
    "\n",
    "    if Z0.shape != Z1.shape:\n",
    "        raise RuntimeError(f\"Embedding shape mismatch: {p0} {Z0.shape} vs {p1} {Z1.shape}\")\n",
    "\n",
    "    Z0 = _scrub_finite(Z0, name=f\"Z0({p0.name})\", verbose=verbose)\n",
    "    Z1 = _scrub_finite(Z1, name=f\"Z1({p1.name})\", verbose=verbose)\n",
    "\n",
    "    emb_dim = int(Z0.shape[1])\n",
    "\n",
    "    # outputs\n",
    "    embed_mask = np.zeros(n_full, dtype=bool)\n",
    "    embed_mask[fit_idx] = True\n",
    "\n",
    "    if fill_missing == \"zeros\":\n",
    "        Z_full  = np.zeros((n_full, emb_dim), dtype=np.float32)\n",
    "        Zr_full = np.zeros_like(Z_full)\n",
    "        Za_full = np.zeros_like(Z_full)\n",
    "    else:\n",
    "        Z_full  = np.full((n_full, emb_dim), np.nan, dtype=np.float32)\n",
    "        Zr_full = np.full_like(Z_full, np.nan)\n",
    "        Za_full = np.full_like(Z_full, np.nan)\n",
    "\n",
    "    Zr_full[fit_idx] = Z0\n",
    "    Za_full[fit_idx] = Z1\n",
    "    Z_full[fit_idx]  = 0.5 * (Z0 + Z1)\n",
    "\n",
    "    # sanity: embedded rows must be finite\n",
    "    if not np.isfinite(Z_full[fit_idx]).all():\n",
    "        bad = int((~np.isfinite(Z_full[fit_idx])).sum())\n",
    "        raise RuntimeError(f\"[scJoint] BUG: embedded rows have non-finite after scrub: {bad}\")\n",
    "\n",
    "    fit_seconds = float(time.perf_counter() - t0)\n",
    "\n",
    "    if verbose:\n",
    "        n_embedded = int(embed_mask.sum())\n",
    "        n_missing = int(n_full - n_embedded)\n",
    "        miss_desc = \"zeros\" if fill_missing == \"zeros\" else \"NaN\"\n",
    "        tr_desc = \"ALL (transductive)\" if transductive else \"train(+val) only\"\n",
    "        print(\n",
    "            f\"[scJoint] emb_dim={emb_dim} | trained_on={tr_desc}: embedded {n_embedded}/{n_full} \"\n",
    "            f\"(missing={n_missing}; missing_as={miss_desc}) | files=({p0.name}, {p1.name})\"\n",
    "        )\n",
    "\n",
    "    # NOTE: keep your ensure_flags/standard_flags calls\n",
    "    return ensure_flags({\n",
    "        \"Z_rna\": Zr_full,\n",
    "        \"Z_atac\": Za_full,\n",
    "        \"Z_fused\": Z_full,\n",
    "        \"embed_mask\": embed_mask,\n",
    "        \"fit_seconds\": fit_seconds,\n",
    "        \"extra_json\": standard_flags(\n",
    "            transductive=transductive,\n",
    "            uses_labels=True,\n",
    "            note=(\n",
    "                \"Trained on ALL cells (transductive) to embed test.\"\n",
    "                if transductive else\n",
    "                f\"Trained on train(+val) only; non-FIT cells are {miss_desc}.\"\n",
    "            ) + f\" emb_dim={emb_dim}.\"\n",
    "        ),\n",
    "    }, default_transductive=transductive, default_uses_labels=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38534a-c99e-45b0-9f38-e993907f4035",
   "metadata": {},
   "source": [
    "### 7) DeepCCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6f329-4a6b-48a2-959c-140774dd7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _MLP(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = in_dim\n",
    "        for h in hidden:\n",
    "            layers += [torch.nn.Linear(d, h), torch.nn.ReLU(), torch.nn.Dropout(dropout)]\n",
    "            d = h\n",
    "        layers += [torch.nn.Linear(d, out_dim)]\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def _inv_sqrtm_psd(A, eps=1e-6):\n",
    "    # A: (d,d) symmetric PSD\n",
    "    # returns A^{-1/2} using eigendecomp\n",
    "    w, V = torch.linalg.eigh(A)\n",
    "    w = torch.clamp(w, min=eps)\n",
    "    return (V * (w.rsqrt())) @ V.T\n",
    "\n",
    "def deepcca_loss_cholesky(H1, H2, reg=1e-2, eps=1e-6, max_tries=6):\n",
    "    \"\"\"\n",
    "    Stable DeepCCA loss: uses Cholesky whitening instead of eigendecomp.\n",
    "    Returns negative sum of canonical correlations (minimize).\n",
    "    H1,H2: (b,d)\n",
    "    \"\"\"\n",
    "    # Use float64 for stability; cast back not needed for loss scalar\n",
    "    H1 = H1.double()\n",
    "    H2 = H2.double()\n",
    "\n",
    "    H1 = H1 - H1.mean(dim=0, keepdim=True)\n",
    "    H2 = H2 - H2.mean(dim=0, keepdim=True)\n",
    "\n",
    "    b = H1.shape[0]\n",
    "    I1 = torch.eye(H1.shape[1], device=H1.device, dtype=H1.dtype)\n",
    "    I2 = torch.eye(H2.shape[1], device=H2.device, dtype=H2.dtype)\n",
    "\n",
    "    C11 = (H1.T @ H1) / (b - 1)\n",
    "    C22 = (H2.T @ H2) / (b - 1)\n",
    "    C12 = (H1.T @ H2) / (b - 1)\n",
    "\n",
    "    # Robust Cholesky with increasing jitter\n",
    "    L1 = L2 = None\n",
    "    for t in range(max_tries):\n",
    "        jitter = float(reg) + (10.0**t) * float(eps)\n",
    "        try:\n",
    "            L1 = torch.linalg.cholesky(C11 + jitter * I1)\n",
    "            L2 = torch.linalg.cholesky(C22 + jitter * I2)\n",
    "            break\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "    if L1 is None or L2 is None:\n",
    "        # last-resort fallback: add big jitter and try once more\n",
    "        jitter = float(reg) + (10.0**max_tries) * float(eps)\n",
    "        L1 = torch.linalg.cholesky(C11 + jitter * I1)\n",
    "        L2 = torch.linalg.cholesky(C22 + jitter * I2)\n",
    "\n",
    "    # T = L1^{-1} C12 L2^{-T}\n",
    "    # Solve L1 * X = C12\n",
    "    X = torch.linalg.solve_triangular(L1, C12, upper=False)\n",
    "    # Solve L2 * Y = X^T  -> Y = L2^{-1} X^T, then transpose back gives X L2^{-T}\n",
    "    Y = torch.linalg.solve_triangular(L2, X.T, upper=False).T\n",
    "    T = Y\n",
    "\n",
    "    s = torch.linalg.svdvals(T)\n",
    "    corr = torch.sum(s)\n",
    "    return -corr\n",
    "\n",
    "def deepcca_loss(H1, H2, reg=1e-3, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Negative sum of canonical correlations (to minimize).\n",
    "    H1,H2: (b,d)\n",
    "    \"\"\"\n",
    "    H1 = H1 - H1.mean(dim=0, keepdim=True)\n",
    "    H2 = H2 - H2.mean(dim=0, keepdim=True)\n",
    "\n",
    "    b = H1.shape[0]\n",
    "    C11 = (H1.T @ H1) / (b - 1) + reg * torch.eye(H1.shape[1], device=H1.device)\n",
    "    C22 = (H2.T @ H2) / (b - 1) + reg * torch.eye(H2.shape[1], device=H2.device)\n",
    "    C12 = (H1.T @ H2) / (b - 1)\n",
    "\n",
    "    C11_inv_sqrt = _inv_sqrtm_psd(C11, eps=eps)\n",
    "    C22_inv_sqrt = _inv_sqrtm_psd(C22, eps=eps)\n",
    "\n",
    "    T = C11_inv_sqrt @ C12 @ C22_inv_sqrt\n",
    "    # sum singular values = total correlation\n",
    "    s = torch.linalg.svdvals(T)\n",
    "    corr = torch.sum(s)\n",
    "    return -corr\n",
    "\n",
    "\n",
    "def run_deepcca(\n",
    "    rna_log_hvg,\n",
    "    atac_lsi,\n",
    "    *,\n",
    "    out_dir,\n",
    "    splits,\n",
    "    seed=0,\n",
    "    latent_dim=30,\n",
    "    hidden=(512, 256, 128),\n",
    "    dropout=0.05,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=256,\n",
    "    max_epochs=200,\n",
    "    patience=50,\n",
    "    reg=1e-3,\n",
    "    clip_grad=5.0,\n",
    "    #align_weight=0.0,   # set to ~0.05–0.2 if you want stronger geometric mixing\n",
    "    align_weight=0.05,\n",
    "):\n",
    "    \"\"\"\n",
    "    DeepCCA baseline.\n",
    "    - Aligns RNA/ATAC by shared obs_names (critical).\n",
    "    - Remaps splits from original RNA index space -> aligned shared index space.\n",
    "    - Standardizes each modality using TRAIN stats only.\n",
    "    - Trains on TRAIN, early-stops on VAL using the same Cholesky CCA loss.\n",
    "    - Returns Z_rna, Z_atac, Z_fused=mean(Zr,Za).\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import scipy.sparse as sp\n",
    "    import torch\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    set_seed(seed)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 0) Align modalities to SAME cells in SAME order\n",
    "    # ------------------------------------------------------------\n",
    "    rna0 = rna_log_hvg\n",
    "    atac0 = atac_lsi\n",
    "\n",
    "    shared = rna0.obs_names.intersection(atac0.obs_names)\n",
    "    if shared.size == 0:\n",
    "        raise ValueError(\"DeepCCA: RNA/ATAC have no shared cells (obs_names intersection empty).\")\n",
    "\n",
    "    # canonical order = RNA order\n",
    "    rna = rna0[shared].copy()\n",
    "    atac = atac0[shared].copy()\n",
    "    if not rna.obs_names.equals(atac.obs_names):\n",
    "        raise ValueError(\"DeepCCA: failed to align RNA/ATAC obs_names after subsetting.\")\n",
    "\n",
    "    n_obs = int(rna.n_obs)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Remap splits (original RNA index space -> aligned shared space)\n",
    "    # ------------------------------------------------------------\n",
    "    tr0 = np.asarray(splits[\"train\"], dtype=int)\n",
    "    va0 = np.asarray(splits[\"val\"], dtype=int)\n",
    "\n",
    "    orig_pos = rna0.obs_names.get_indexer(shared)  # positions of shared cells in original RNA\n",
    "    if np.any(orig_pos < 0):\n",
    "        raise ValueError(\"DeepCCA: internal error computing RNA indexer for shared cells.\")\n",
    "\n",
    "    inv = np.full(int(rna0.n_obs), -1, dtype=int)\n",
    "    inv[orig_pos] = np.arange(shared.size, dtype=int)\n",
    "\n",
    "    tr = inv[tr0]\n",
    "    tr = tr[tr >= 0]\n",
    "    va = inv[va0]\n",
    "    va = va[va >= 0]\n",
    "\n",
    "    if tr.size == 0:\n",
    "        raise ValueError(\"DeepCCA: no TRAIN cells remain after aligning modalities.\")\n",
    "    if va.size == 0:\n",
    "        raise ValueError(\"DeepCCA: no VAL cells remain after aligning modalities.\")\n",
    "\n",
    "    if int(batch_size) < int(latent_dim) + 8:\n",
    "        raise ValueError(f\"DeepCCA: batch_size ({batch_size}) too small for latent_dim ({latent_dim}).\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Build dense matrices\n",
    "    # ------------------------------------------------------------\n",
    "    Xr = rna.X\n",
    "    Xa = atac.X\n",
    "    if sp.issparse(Xr):\n",
    "        Xr = Xr.toarray()\n",
    "    if sp.issparse(Xa):\n",
    "        Xa = Xa.toarray()\n",
    "    Xr = np.asarray(Xr, dtype=np.float32)\n",
    "    Xa = np.asarray(Xa, dtype=np.float32)\n",
    "\n",
    "    if Xr.shape[0] != n_obs or Xa.shape[0] != n_obs:\n",
    "        raise ValueError(f\"DeepCCA: n_obs mismatch after align: RNA {Xr.shape[0]} vs ATAC {Xa.shape[0]}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3) Train-fit scaling per modality\n",
    "    # ------------------------------------------------------------\n",
    "    sc_r = StandardScaler(with_mean=True, with_std=True)\n",
    "    sc_a = StandardScaler(with_mean=True, with_std=True)\n",
    "    sc_r.fit(Xr[tr])\n",
    "    sc_a.fit(Xa[tr])\n",
    "    Xr_s = sc_r.transform(Xr).astype(np.float32, copy=False)\n",
    "    Xa_s = sc_a.transform(Xa).astype(np.float32, copy=False)\n",
    "\n",
    "    if not (np.isfinite(Xr_s).all() and np.isfinite(Xa_s).all()):\n",
    "        raise ValueError(\"DeepCCA: non-finite values detected after scaling.\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4) Models + optimizer\n",
    "    # ------------------------------------------------------------\n",
    "    enc_r = _MLP(Xr_s.shape[1], list(hidden), int(latent_dim), dropout=dropout).to(device)\n",
    "    enc_a = _MLP(Xa_s.shape[1], list(hidden), int(latent_dim), dropout=dropout).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        list(enc_r.parameters()) + list(enc_a.parameters()),\n",
    "        lr=float(lr),\n",
    "        weight_decay=float(weight_decay),\n",
    "    )\n",
    "\n",
    "    def _iter_batches(idxs):\n",
    "        idxs = np.asarray(idxs, dtype=int).copy()\n",
    "        rng = np.random.default_rng(int(seed))\n",
    "        rng.shuffle(idxs)\n",
    "        for i in range(0, len(idxs), int(batch_size)):\n",
    "            yield idxs[i:i + int(batch_size)]\n",
    "\n",
    "    # consistent loss (Cholesky CCA)\n",
    "    def _cca_loss(hr, ha):\n",
    "        return deepcca_loss_cholesky(hr, ha, reg=float(reg), eps=1e-6)\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5) Train loop\n",
    "    # ------------------------------------------------------------\n",
    "    t0 = now()\n",
    "    for epoch in range(int(max_epochs)):\n",
    "        enc_r.train()\n",
    "        enc_a.train()\n",
    "\n",
    "        for b in _iter_batches(tr):\n",
    "            xr = torch.from_numpy(Xr_s[b]).to(device)\n",
    "            xa = torch.from_numpy(Xa_s[b]).to(device)\n",
    "\n",
    "            hr = enc_r(xr)\n",
    "            ha = enc_a(xa)\n",
    "\n",
    "            loss = _cca_loss(hr, ha)\n",
    "            if float(align_weight) > 0:\n",
    "                loss = loss + float(align_weight) * torch.mean((hr - ha) ** 2)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            if clip_grad is not None and float(clip_grad) > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(enc_r.parameters()) + list(enc_a.parameters()),\n",
    "                    max_norm=float(clip_grad),\n",
    "                )\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # Validation (same loss as train)\n",
    "        # --------------------------------------------------------\n",
    "        enc_r.eval()\n",
    "        enc_a.eval()\n",
    "        with torch.no_grad():\n",
    "            vals = []\n",
    "            for b in _iter_batches(va):\n",
    "                xr = torch.from_numpy(Xr_s[b]).to(device)\n",
    "                xa = torch.from_numpy(Xa_s[b]).to(device)\n",
    "                hr = enc_r(xr)\n",
    "                ha = enc_a(xa)\n",
    "                v = _cca_loss(hr, ha)\n",
    "                if float(align_weight) > 0:\n",
    "                    v = v + float(align_weight) * torch.mean((hr - ha) ** 2)\n",
    "                vals.append(float(v.cpu()))\n",
    "            val_loss = float(np.mean(vals)) if len(vals) else np.inf\n",
    "\n",
    "        if val_loss < best_val - 1e-6:\n",
    "            best_val = val_loss\n",
    "            best_state = (enc_r.state_dict(), enc_a.state_dict())\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= int(patience):\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        enc_r.load_state_dict(best_state[0])\n",
    "        enc_a.load_state_dict(best_state[1])\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6) Encode all aligned cells\n",
    "    # ------------------------------------------------------------\n",
    "    enc_r.eval()\n",
    "    enc_a.eval()\n",
    "    with torch.no_grad():\n",
    "        Zr = enc_r(torch.from_numpy(Xr_s).to(device)).cpu().numpy().astype(np.float32)\n",
    "        Za = enc_a(torch.from_numpy(Xa_s).to(device)).cpu().numpy().astype(np.float32)\n",
    "\n",
    "    t1 = now()\n",
    "\n",
    "    if Zr.shape != Za.shape or Zr.shape[0] != n_obs:\n",
    "        raise ValueError(f\"DeepCCA: unexpected embedding shapes: Zr {Zr.shape}, Za {Za.shape}, n_obs={n_obs}\")\n",
    "\n",
    "    Zf = 0.5 * (Zr + Za)\n",
    "\n",
    "    return ensure_flags({\n",
    "        \"Z_rna\": Zr,\n",
    "        \"Z_atac\": Za,\n",
    "        \"Z_fused\": Zf,\n",
    "        \"fit_seconds\": float(t1 - t0),\n",
    "        \"extra_json\": standard_flags(\n",
    "            transductive=False,\n",
    "            uses_labels=False,\n",
    "            model=\"DeepCCA\",\n",
    "            latent_dim=int(latent_dim),\n",
    "            hidden=list(hidden),\n",
    "            dropout=float(dropout),\n",
    "            lr=float(lr),\n",
    "            weight_decay=float(weight_decay),\n",
    "            batch_size=int(batch_size),\n",
    "            max_epochs=int(max_epochs),\n",
    "            patience=int(patience),\n",
    "            reg=float(reg),\n",
    "            clip_grad=float(clip_grad) if clip_grad is not None else None,\n",
    "            align_weight=float(align_weight),\n",
    "            n_shared=int(n_obs),\n",
    "            n_train_shared=int(tr.size),\n",
    "            n_val_shared=int(va.size),\n",
    "            best_val=float(best_val),\n",
    "            note=\"Aligned RNA/ATAC by shared obs_names; splits remapped; standardized on TRAIN; Cholesky CCA loss used for train+val.\"\n",
    "        ),\n",
    "    }, default_transductive=False, default_uses_labels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3de5aa-f873-44db-af19-c83f61451b05",
   "metadata": {},
   "source": [
    "### 8) PeakVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2007bed-8ffb-42e9-9fc2-7479ee9d34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_peakvi(\n",
    "    atac_counts_bin_hv,\n",
    "    *,\n",
    "    out_dir,\n",
    "    splits,\n",
    "    seed=0,\n",
    "    n_latent=30,\n",
    "    max_epochs=200,\n",
    "    patience=50,\n",
    "    batch_key=None,\n",
    "    layer=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    PEAKVI baseline (ATAC-only).\n",
    "\n",
    "    Trains on TRAIN+VAL only (so: transductive=False).\n",
    "    Returns:\n",
    "      Z_atac  : latent embedding for ATAC (all cells; inference for test)\n",
    "      Z_fused : same as Z_atac (unimodal)\n",
    "      Z_rna   : None\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    import scvi\n",
    "    from scvi.model import PEAKVI\n",
    "\n",
    "    set_seed(seed)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tr = np.asarray(splits[\"train\"])\n",
    "    va = np.asarray(splits[\"val\"])\n",
    "    order_tv = np.concatenate([tr, va])\n",
    "\n",
    "    # -----------------------\n",
    "    # Train on train+val only\n",
    "    # -----------------------\n",
    "    atac_tv = atac_counts_bin_hv[order_tv].copy()\n",
    "\n",
    "    PEAKVI.setup_anndata(\n",
    "        atac_tv,\n",
    "        layer=layer,\n",
    "        batch_key=batch_key,\n",
    "    )\n",
    "\n",
    "    t0 = now()\n",
    "    model = PEAKVI(atac_tv, n_latent=int(n_latent))\n",
    "\n",
    "    scvi_train_with_patience(\n",
    "        model,\n",
    "        max_epochs=int(max_epochs),\n",
    "        patience=int(patience),\n",
    "        monitor=\"elbo_validation\",\n",
    "        train_size=len(tr) / len(order_tv),\n",
    "        validation_size=len(va) / len(order_tv),\n",
    "        shuffle_set_split=False,\n",
    "    )\n",
    "    t1 = now()\n",
    "\n",
    "    # -----------------------\n",
    "    # Embed ALL cells (no fit)\n",
    "    # -----------------------\n",
    "    atac_all = atac_counts_bin_hv.copy()\n",
    "\n",
    "    # IMPORTANT: do NOT call PEAKVI.setup_anndata(atac_all, ...) again.\n",
    "    # Just pass it to the model for inference.\n",
    "    Z = np.asarray(model.get_latent_representation(adata=atac_all), dtype=np.float32)\n",
    "\n",
    "    return ensure_flags({\n",
    "        \"Z_rna\": None,\n",
    "        \"Z_atac\": Z,\n",
    "        \"Z_fused\": Z,\n",
    "        \"fit_seconds\": float(t1 - t0),\n",
    "        \"extra_json\": standard_flags(\n",
    "            transductive=False,\n",
    "            uses_labels=False,\n",
    "            scvi_version=scvi.__version__,\n",
    "            model=\"PEAKVI\",\n",
    "            n_latent=int(n_latent),\n",
    "            trained_on=\"train+val\",\n",
    "            layer=layer,\n",
    "            batch_key=batch_key,\n",
    "            note=\"ATAC-only; Z_fused == Z_atac. Test cells are inference only.\"\n",
    "        ),\n",
    "    }, default_transductive=False, default_uses_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37338f-a4fe-45f8-b334-400618119a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_peakvi_fair(\n",
    "    atac_counts_bin_hv,\n",
    "    *,\n",
    "    out_dir,\n",
    "    splits,\n",
    "    seed=0,\n",
    "    n_latent=30,\n",
    "    max_epochs=200,\n",
    "    patience=50,\n",
    "    batch_key=None,\n",
    "    layer=None,\n",
    "    encode_batch_size=256,\n",
    "):\n",
    "    import numpy as np\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    import scvi\n",
    "    from scvi.model import PEAKVI\n",
    "\n",
    "    set_seed(seed)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "    va = np.asarray(splits[\"val\"], dtype=int)\n",
    "    order_tv = np.concatenate([tr, va])\n",
    "\n",
    "    # ---- train on train+val only ----\n",
    "    atac_tv = atac_counts_bin_hv[order_tv].copy()\n",
    "    PEAKVI.setup_anndata(atac_tv, layer=layer, batch_key=batch_key)\n",
    "\n",
    "    t0 = now()\n",
    "    model = PEAKVI(atac_tv, n_latent=int(n_latent))\n",
    "    scvi_train_with_patience(\n",
    "        model,\n",
    "        max_epochs=int(max_epochs),\n",
    "        patience=int(patience),\n",
    "        monitor=\"elbo_validation\",\n",
    "        train_size=len(tr) / len(order_tv),\n",
    "        validation_size=len(va) / len(order_tv),\n",
    "        shuffle_set_split=False,\n",
    "        check_val_every_n_epoch=1,\n",
    "    )\n",
    "    t1 = now()\n",
    "\n",
    "    # ---- save + reload onto FULL data for stable inference ----\n",
    "    save_dir = out_dir / \"peakvi_model\"\n",
    "    if save_dir.exists():\n",
    "        shutil.rmtree(save_dir)\n",
    "    model.save(str(save_dir), overwrite=True)\n",
    "\n",
    "    atac_full = atac_counts_bin_hv.copy()\n",
    "    PEAKVI.setup_anndata(atac_full, layer=layer, batch_key=batch_key)\n",
    "\n",
    "    model_full = PEAKVI.load(str(save_dir), adata=atac_full)\n",
    "\n",
    "    Z = np.asarray(\n",
    "        model_full.get_latent_representation(adata=atac_full, batch_size=int(encode_batch_size)),\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    return ensure_flags({\n",
    "        \"Z_rna\": None,\n",
    "        \"Z_atac\": Z,\n",
    "        \"Z_fused\": Z,\n",
    "        \"fit_seconds\": float(t1 - t0),\n",
    "        \"extra_json\": standard_flags(\n",
    "            transductive=False,\n",
    "            uses_labels=False,\n",
    "            scvi_version=scvi.__version__,\n",
    "            model=\"PEAKVI\",\n",
    "            n_latent=int(n_latent),\n",
    "            trained_on=\"train+val\",\n",
    "            note=\"Saved+reloaded onto full AnnData for inference; training used train+val only.\"\n",
    "        ),\n",
    "    }, default_transductive=False, default_uses_labels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f65e2-e1aa-4cf6-a95f-13c9a84285e9",
   "metadata": {},
   "source": [
    "### 9) CoBOLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2551cd4-ef9c-4c03-a0bd-f781ff6ad658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, pkgutil, sys\n",
    "print(\"cobolt in pkgutil?\", any(m.name == \"cobolt\" for m in pkgutil.iter_modules()))\n",
    "import cobolt\n",
    "print(\"cobolt module file:\", cobolt.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf142ff4-5b62-4644-8bc5-df2ed443171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import inspect\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_cobolt_working(\n",
    "    rna_adata,\n",
    "    atac_adata,\n",
    "    *,\n",
    "    splits: Optional[dict] = None,\n",
    "    rna_key: Optional[str] = \"counts\",\n",
    "    atac_key: Optional[str] = None,\n",
    "    prefer_layer: bool = True,\n",
    "    n_latent: int = 30,\n",
    "    max_epochs: int = 200,\n",
    "    batch_size: int = 256,\n",
    "    device: str = \"cuda\",\n",
    "    seed: int = 0,\n",
    "    require_paired: bool = True,\n",
    "\n",
    "    # ATAC TF-IDF\n",
    "    atac_tfidf: bool = False,\n",
    "    atac_tfidf_l2_norm: bool = False,\n",
    "    tfidf_smooth_idf: bool = False,\n",
    "\n",
    "    # NEW: RNA normalization (highly recommended for CoBOLT stability)\n",
    "    rna_cp10k_log1p: bool = False,\n",
    "    rna_target_sum: float = 1e4,\n",
    "\n",
    "    out_dir: Optional[str] = None,\n",
    "    verbose: bool = True,\n",
    "    latent_cap: int = 60,\n",
    "\n",
    "    # LR control + retry\n",
    "    lr: Optional[float] = None,\n",
    "    lr_try: Optional[Sequence[float]] = (5e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5, 3e-6, 1e-6),\n",
    "    max_lr_retries: int = 15,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    CoBOLT runner (your version) with:\n",
    "      - scoped scipy.sparse.vstack patch\n",
    "      - optional ATAC TF-IDF\n",
    "      - NEW: RNA cp10k + log1p (sparse-safe) to prevent divergence\n",
    "      - LR retry ladder\n",
    "      - latent extraction via bool masks len=2:\n",
    "           [True,True]=joint, [True,False]=rna, [False,True]=atac\n",
    "    \"\"\"\n",
    "\n",
    "    def log(msg: str):\n",
    "        if verbose:\n",
    "            print(msg, flush=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # Safe manual CSR vstack\n",
    "    # -------------------------\n",
    "    def _csr_vstack(mats):\n",
    "        mats = list(mats)\n",
    "        if len(mats) == 0:\n",
    "            return sp.csr_matrix((0, 0), dtype=np.float32)\n",
    "\n",
    "        mats = [m.tocsr(copy=False) for m in mats]\n",
    "        n_cols = int(mats[0].shape[1])\n",
    "        total_rows = int(sum(m.shape[0] for m in mats))\n",
    "        if total_rows == 0:\n",
    "            return sp.csr_matrix((0, n_cols), dtype=np.float32)\n",
    "\n",
    "        data = np.concatenate([m.data for m in mats]) if mats else np.array([], dtype=np.float32)\n",
    "        indices = np.concatenate([m.indices for m in mats]) if mats else np.array([], dtype=np.int32)\n",
    "\n",
    "        indptr = np.empty(total_rows + 1, dtype=np.int64)\n",
    "        indptr[0] = 0\n",
    "        row_pos = 0\n",
    "        nnz_off = 0\n",
    "        for m in mats:\n",
    "            r = int(m.shape[0])\n",
    "            indptr[row_pos + 1: row_pos + r + 1] = nnz_off + m.indptr[1:]\n",
    "            row_pos += r\n",
    "            nnz_off += int(m.indptr[-1])\n",
    "\n",
    "        return sp.csr_matrix((data, indices, indptr), shape=(total_rows, n_cols))\n",
    "\n",
    "    # -------------------------\n",
    "    # Scoped patch: scipy.sparse.vstack\n",
    "    # -------------------------\n",
    "    class _ScopedPatchScipyVstack:\n",
    "        def __init__(self, verbose: bool = True):\n",
    "            self.verbose = verbose\n",
    "            self._saved = []\n",
    "\n",
    "        def __enter__(self):\n",
    "            import scipy.sparse as _sp\n",
    "            try:\n",
    "                import scipy.sparse._construct as _c\n",
    "            except Exception:\n",
    "                _c = None\n",
    "\n",
    "            def vstack_safe(blocks, *args, **kwargs):\n",
    "                flat = []\n",
    "                stack = [blocks]\n",
    "                while stack:\n",
    "                    x = stack.pop()\n",
    "                    if x is None:\n",
    "                        continue\n",
    "                    if sp.issparse(x):\n",
    "                        flat.append(x)\n",
    "                        continue\n",
    "                    if isinstance(x, np.ndarray):\n",
    "                        if x.dtype == object or x.ndim != 2:\n",
    "                            try:\n",
    "                                elems = x.reshape(-1).tolist()\n",
    "                            except Exception:\n",
    "                                elems = [x]\n",
    "                            stack.extend(reversed(elems))\n",
    "                        else:\n",
    "                            flat.append(sp.csr_matrix(x))\n",
    "                        continue\n",
    "                    if isinstance(x, (list, tuple)):\n",
    "                        stack.extend(reversed(x))\n",
    "                        continue\n",
    "                    arr = np.asarray(x)\n",
    "                    if arr.ndim == 2 and arr.dtype != object:\n",
    "                        flat.append(sp.csr_matrix(arr))\n",
    "                    else:\n",
    "                        raise TypeError(f\"vstack_safe: unsupported block type {type(x)} shape={getattr(x,'shape',None)}\")\n",
    "                return _csr_vstack(flat)\n",
    "\n",
    "            self._saved.append((_sp, \"vstack\", _sp.vstack))\n",
    "            _sp.vstack = vstack_safe\n",
    "\n",
    "            if _c is not None and hasattr(_c, \"vstack\"):\n",
    "                self._saved.append((_c, \"vstack\", _c.vstack))\n",
    "                _c.vstack = vstack_safe\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"[cobolt][patch] scoped scipy.sparse.vstack -> safe manual csr_vstack\", flush=True)\n",
    "            return self\n",
    "\n",
    "        def __exit__(self, exc_type, exc, tb):\n",
    "            for obj, name, old in reversed(self._saved):\n",
    "                try:\n",
    "                    setattr(obj, name, old)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            self._saved.clear()\n",
    "            if self.verbose:\n",
    "                print(\"[cobolt][patch] restored scipy.sparse.vstack\", flush=True)\n",
    "            return False\n",
    "\n",
    "    # -------------------------\n",
    "    # helpers\n",
    "    # -------------------------\n",
    "    def _resolve_matrix(adata, key: Optional[str]):\n",
    "        if key is None:\n",
    "            return adata.X, \"X\"\n",
    "        layers = getattr(adata, \"layers\", {})\n",
    "        obsm = getattr(adata, \"obsm\", {})\n",
    "        if prefer_layer and key in layers:\n",
    "            return layers[key], f\"layers[{key}]\"\n",
    "        if key in obsm:\n",
    "            return obsm[key], f\"obsm[{key}]\"\n",
    "        if key in layers:\n",
    "            return layers[key], f\"layers[{key}]\"\n",
    "        raise KeyError(f\"{key!r} not found in .layers or .obsm\")\n",
    "\n",
    "    def _as_csr_f32_2d(X, src: str):\n",
    "        if sp.issparse(X):\n",
    "            X = X.tocsr(copy=False)\n",
    "            if X.ndim != 2:\n",
    "                raise ValueError(f\"{src} sparse not 2D: shape={X.shape}\")\n",
    "            return X.astype(np.float32, copy=False)\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(f\"{src} not 2D: shape={X.shape}\")\n",
    "        if X.dtype == object:\n",
    "            raise ValueError(f\"{src} dtype=object (ragged?) shape={X.shape}\")\n",
    "        return sp.csr_matrix(X.astype(np.float32, copy=False))\n",
    "\n",
    "    def _rna_cp10k_log1p(X: sp.csr_matrix, target_sum: float = 1e4, eps: float = 1e-12) -> sp.csr_matrix:\n",
    "        \"\"\"Sparse-safe: scale rows to target_sum then log1p(data) (in-place-ish on CSR data).\"\"\"\n",
    "        X = X.tocsr(copy=True).astype(np.float32, copy=False)\n",
    "        rs = np.asarray(X.sum(axis=1)).ravel().astype(np.float32)\n",
    "        scale = (float(target_sum) / np.maximum(rs, eps)).astype(np.float32)\n",
    "\n",
    "        # apply row scaling without densifying: multiply each row's data by scale[row]\n",
    "        indptr = X.indptr\n",
    "        for i in range(X.shape[0]):\n",
    "            start, end = indptr[i], indptr[i + 1]\n",
    "            if start != end:\n",
    "                X.data[start:end] *= scale[i]\n",
    "\n",
    "        # log1p on stored nonzeros\n",
    "        np.log1p(X.data, out=X.data)\n",
    "        return X\n",
    "\n",
    "    def _tfidf_fit_idf(X_train_csr: sp.csr_matrix, *, smooth_idf: bool = True) -> np.ndarray:\n",
    "        X = X_train_csr.tocsr(copy=False)\n",
    "        N = int(X.shape[0])\n",
    "        df = np.asarray((X > 0).sum(axis=0)).ravel().astype(np.float64)\n",
    "        if smooth_idf:\n",
    "            idf = np.log1p(N / (1.0 + df)) + 1.0\n",
    "        else:\n",
    "            df = np.maximum(df, 1.0)\n",
    "            idf = np.log(N / df) + 1.0\n",
    "        return idf.astype(np.float32)\n",
    "\n",
    "    def _tfidf_apply(X_csr: sp.csr_matrix, *, idf: np.ndarray, l2_norm: bool = True, eps: float = 1e-12) -> sp.csr_matrix:\n",
    "        X = X_csr.tocsr(copy=False).astype(np.float32, copy=False)\n",
    "        rs = np.asarray(X.sum(axis=1)).ravel().astype(np.float32)\n",
    "        inv_rs = (1.0 / np.maximum(rs, eps)).astype(np.float32)\n",
    "        tf = X.multiply(inv_rs[:, None])\n",
    "        tfidf = tf.multiply(np.asarray(idf, dtype=np.float32)[None, :])\n",
    "        if not l2_norm:\n",
    "            return tfidf.tocsr(copy=False)\n",
    "        row_sq = np.asarray(tfidf.multiply(tfidf).sum(axis=1)).ravel().astype(np.float32)\n",
    "        inv_norm = (1.0 / np.sqrt(np.maximum(row_sq, eps))).astype(np.float32)\n",
    "        return tfidf.multiply(inv_norm[:, None]).tocsr(copy=False)\n",
    "\n",
    "    def _cap(Z: np.ndarray, name: str) -> np.ndarray:\n",
    "        if latent_cap is not None and Z.shape[1] > int(latent_cap):\n",
    "            log(f\"[cobolt][safe] {name} dim {Z.shape[1]} > {latent_cap}; truncating to {latent_cap}\")\n",
    "            return Z[:, : int(latent_cap)].copy()\n",
    "        return Z\n",
    "\n",
    "    def _to_full(Z_fit: np.ndarray, n_full: int, fit_idx: np.ndarray) -> np.ndarray:\n",
    "        Z_full = np.full((n_full, Z_fit.shape[1]), np.nan, dtype=np.float32)\n",
    "        Z_full[fit_idx] = Z_fit\n",
    "        return Z_full\n",
    "\n",
    "    def _looks_like_divergence(e: Exception) -> bool:\n",
    "        s = (str(e) or \"\").lower()\n",
    "        return (\"diverged\" in s) or (\"diverged.\" in s) or (\"try a smaller learning rate\" in s) or (\"nan\" in s and \"loss\" in s)\n",
    "\n",
    "    def _train_with_lr(model, epochs: int, lr_value: Optional[float]) -> None:\n",
    "        train_fn = model.train\n",
    "        sig = inspect.signature(train_fn)\n",
    "        params = sig.parameters\n",
    "\n",
    "        kwargs = {}\n",
    "        if \"num_epochs\" in params:\n",
    "            kwargs[\"num_epochs\"] = int(epochs)\n",
    "        elif \"epochs\" in params:\n",
    "            kwargs[\"epochs\"] = int(epochs)\n",
    "\n",
    "        if lr_value is not None:\n",
    "            if \"lr\" in params:\n",
    "                kwargs[\"lr\"] = float(lr_value)\n",
    "            elif \"learning_rate\" in params:\n",
    "                kwargs[\"learning_rate\"] = float(lr_value)\n",
    "\n",
    "        if kwargs:\n",
    "            train_fn(**kwargs)\n",
    "        else:\n",
    "            train_fn(int(epochs))\n",
    "\n",
    "    def _check_finite_sparse(X: sp.csr_matrix, name: str):\n",
    "        if X.nnz == 0:\n",
    "            return\n",
    "        if not np.isfinite(X.data).all():\n",
    "            bad = np.where(~np.isfinite(X.data))[0][:10]\n",
    "            raise ValueError(f\"{name} contains non-finite values in .data (first bad idx: {bad}).\")\n",
    "\n",
    "    try:\n",
    "        # seed\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        # pairing checks\n",
    "        if require_paired:\n",
    "            if rna_adata.n_obs != atac_adata.n_obs:\n",
    "                raise ValueError(f\"RNA/ATAC n_obs differ: {rna_adata.n_obs} vs {atac_adata.n_obs}\")\n",
    "            if not np.all(rna_adata.obs_names.astype(str) == atac_adata.obs_names.astype(str)):\n",
    "                raise ValueError(\"RNA and ATAC obs_names differ. Reorder/subset to match for paired runs.\")\n",
    "\n",
    "        # subset to FIT\n",
    "        n_full = int(rna_adata.n_obs)\n",
    "        if splits is None:\n",
    "            fit_idx = np.arange(n_full, dtype=int)\n",
    "            train_note = \"all\"\n",
    "        else:\n",
    "            tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "            va = np.asarray(splits.get(\"val\", []), dtype=int)\n",
    "            fit_idx = np.concatenate([tr, va]) if va.size else tr\n",
    "            fit_idx = np.asarray(fit_idx, dtype=int)\n",
    "            train_note = \"train+val\"\n",
    "\n",
    "        rna_fit = rna_adata[fit_idx].copy()\n",
    "        atac_fit = atac_adata[fit_idx].copy()\n",
    "\n",
    "        # matrices\n",
    "        Xr_raw, rna_src = _resolve_matrix(rna_fit, rna_key)\n",
    "        Xa_raw, atac_src = _resolve_matrix(atac_fit, atac_key)\n",
    "        Xr = _as_csr_f32_2d(Xr_raw, rna_src)\n",
    "        Xa = _as_csr_f32_2d(Xa_raw, atac_src)\n",
    "\n",
    "        # NEW: normalize RNA\n",
    "        if rna_cp10k_log1p:\n",
    "            Xr = _rna_cp10k_log1p(Xr, target_sum=float(rna_target_sum))\n",
    "            rna_src = rna_src + \" -> cp10k+log1p\"\n",
    "\n",
    "        # ATAC TF-IDF\n",
    "        if atac_tfidf:\n",
    "            if splits is not None:\n",
    "                tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "                tr_set = set(tr.tolist())\n",
    "                tr_in_fit = np.array([i in tr_set for i in fit_idx.tolist()], dtype=bool)\n",
    "                if tr_in_fit.sum() < 2:\n",
    "                    tr_in_fit[:] = True\n",
    "            else:\n",
    "                tr_in_fit = np.ones(Xa.shape[0], dtype=bool)\n",
    "\n",
    "            idf = _tfidf_fit_idf(Xa[tr_in_fit], smooth_idf=bool(tfidf_smooth_idf))\n",
    "            Xa = _tfidf_apply(Xa, idf=idf, l2_norm=bool(atac_tfidf_l2_norm))\n",
    "            atac_src = atac_src + \" -> TFIDF\"\n",
    "\n",
    "        _check_finite_sparse(Xr, \"RNA matrix\")\n",
    "        _check_finite_sparse(Xa, \"ATAC matrix\")\n",
    "\n",
    "        log(f\"[cobolt] training_on={train_note} n_fit={rna_fit.n_obs}\")\n",
    "        log(f\"[cobolt] RNA source={rna_src} shape={Xr.shape} nnz={Xr.nnz}\")\n",
    "        log(f\"[cobolt] ATAC source={atac_src} shape={Xa.shape} nnz={Xa.nnz}\")\n",
    "\n",
    "        # import CoBOLT components\n",
    "        from cobolt.utils.data import SingleData, MultiData\n",
    "        from cobolt.utils.dataset import MultiomicDataset\n",
    "        from cobolt.model import Cobolt\n",
    "\n",
    "        def _make_singledata(*, X, barcodes, features, dataset_name: str, feature_name: str):\n",
    "            barcodes = np.asarray(barcodes, dtype=str)\n",
    "            features = np.asarray(features, dtype=str)\n",
    "\n",
    "            sig = inspect.signature(SingleData.__init__)\n",
    "            params = set(sig.parameters.keys()) - {\"self\"}\n",
    "\n",
    "            base = {\n",
    "                \"count\": X,\n",
    "                \"feature\": features,\n",
    "                \"barcode\": barcodes,\n",
    "                \"dataset_name\": str(dataset_name),\n",
    "                \"feature_name\": str(feature_name),\n",
    "                \"dataset\": np.zeros(len(barcodes), dtype=np.int64),\n",
    "            }\n",
    "            aliases = {\"counts\": \"count\", \"barcodes\": \"barcode\", \"genes\": \"feature\"}\n",
    "\n",
    "            kw: Dict[str, Any] = {}\n",
    "            for k, v in base.items():\n",
    "                if k in params:\n",
    "                    kw[k] = v\n",
    "            for k, src in aliases.items():\n",
    "                if k in params and src in base and k not in kw:\n",
    "                    kw[k] = base[src]\n",
    "            return SingleData(**kw)\n",
    "\n",
    "        bc = np.asarray(rna_fit.obs_names, dtype=str)\n",
    "        rna_feat = np.asarray(rna_fit.var_names, dtype=str)\n",
    "        atac_feat = np.asarray(atac_fit.var_names, dtype=str)\n",
    "\n",
    "        a = _make_singledata(X=Xr, barcodes=bc, features=rna_feat, dataset_name=\"joint\", feature_name=\"rna\")\n",
    "        b = _make_singledata(X=Xa, barcodes=bc, features=atac_feat, dataset_name=\"joint\", feature_name=\"atac\")\n",
    "\n",
    "        with _ScopedPatchScipyVstack(verbose=verbose):\n",
    "            md = MultiData(a, b)\n",
    "            ds = MultiomicDataset(md)\n",
    "\n",
    "            model = Cobolt(dataset=ds, n_latent=int(n_latent), device=device, batch_size=int(batch_size))\n",
    "\n",
    "            # training with LR retries\n",
    "            lr_candidates: List[Optional[float]] = []\n",
    "            if lr is not None:\n",
    "                lr_candidates.append(float(lr))\n",
    "            if lr_try is not None:\n",
    "                for x in lr_try:\n",
    "                    xx = float(x)\n",
    "                    if not any((c is not None and abs(c - xx) < 1e-20) for c in lr_candidates):\n",
    "                        lr_candidates.append(xx)\n",
    "            if not lr_candidates:\n",
    "                lr_candidates = [None]\n",
    "\n",
    "            last_train_err = None\n",
    "            used_lr = None\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            for attempt, lr_value in enumerate(lr_candidates[: max_lr_retries]):\n",
    "                try:\n",
    "                    used_lr = lr_value\n",
    "                    if lr_value is None:\n",
    "                        log(\"[cobolt] training with cobolt default lr\")\n",
    "                    else:\n",
    "                        log(f\"[cobolt] training with lr={lr_value:g}\")\n",
    "                    _train_with_lr(model, int(max_epochs), lr_value)\n",
    "                    last_train_err = None\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    last_train_err = e\n",
    "                    if _looks_like_divergence(e):\n",
    "                        log(f\"[cobolt][warn] training diverged (attempt {attempt+1}); will retry smaller lr\")\n",
    "                        continue\n",
    "                    raise\n",
    "\n",
    "            fit_seconds = float(time.perf_counter() - t0)\n",
    "            if last_train_err is not None:\n",
    "                raise RuntimeError(f\"CoBOLT training failed after LR retries. Last error: {last_train_err!r}\") from last_train_err\n",
    "\n",
    "            # latent extraction (bool list len 2)\n",
    "            sig_lat = inspect.signature(model.get_latent)\n",
    "            log(f\"[cobolt] get_latent signature: {sig_lat}\")\n",
    "            has_data_kw = (\"data\" in sig_lat.parameters)\n",
    "            data_candidates = [\"all\", \"full\", \"train\"] if has_data_kw else [None]\n",
    "\n",
    "            def _get_latent_or_raise(comb: List[bool], kind: str):\n",
    "                last_err = None\n",
    "                for dname in data_candidates:\n",
    "                    for rb in (False, True):\n",
    "                        try:\n",
    "                            kwargs = {\"return_barcode\": rb}\n",
    "                            if has_data_kw and dname is not None:\n",
    "                                kwargs[\"data\"] = dname\n",
    "                            out = model.get_latent(comb, **kwargs)\n",
    "                            out0 = out[0] if (isinstance(out, tuple) and len(out) >= 1) else out\n",
    "                            if torch.is_tensor(out0):\n",
    "                                out0 = out0.detach().cpu().numpy()\n",
    "                            Z = np.asarray(out0, dtype=np.float32)\n",
    "                            if Z.ndim != 2:\n",
    "                                raise RuntimeError(f\"get_latent({kind}) returned non-2D: shape={Z.shape}\")\n",
    "                            if Z.shape[0] != rna_fit.n_obs:\n",
    "                                raise RuntimeError(\n",
    "                                    f\"get_latent({kind}) rows {Z.shape[0]} != n_fit {rna_fit.n_obs} \"\n",
    "                                    f\"(comb={comb}, data={dname!r}, rb={rb})\"\n",
    "                                )\n",
    "                            log(f\"[cobolt] got_latent {kind} comb={comb} data={dname!r} return_barcode={rb} -> {Z.shape}\")\n",
    "                            return Z, (dname if dname is not None else \"default\")\n",
    "                        except Exception as e:\n",
    "                            last_err = e\n",
    "                raise RuntimeError(f\"get_latent({kind}) failed for comb={comb}. Last error: {last_err!r}\") from last_err\n",
    "\n",
    "            Z_joint_fit, data_used_joint = _get_latent_or_raise([True, True],  \"joint\")\n",
    "            Z_rna_fit,   data_used_rna   = _get_latent_or_raise([True, False], \"rna\")\n",
    "            Z_atac_fit,  data_used_atac  = _get_latent_or_raise([False, True], \"atac\")\n",
    "\n",
    "        Z_joint_fit = _cap(Z_joint_fit, \"Z_joint\")\n",
    "        Z_rna_fit   = _cap(Z_rna_fit,   \"Z_rna\")\n",
    "        Z_atac_fit  = _cap(Z_atac_fit,  \"Z_atac\")\n",
    "\n",
    "        Z_joint = _to_full(Z_joint_fit, n_full, fit_idx)\n",
    "        Z_rna   = _to_full(Z_rna_fit,   n_full, fit_idx)\n",
    "        Z_atac  = _to_full(Z_atac_fit,  n_full, fit_idx)\n",
    "\n",
    "        if out_dir is not None:\n",
    "            outp = Path(out_dir)\n",
    "            outp.mkdir(parents=True, exist_ok=True)\n",
    "            np.save(outp / \"Z_fused.npy\", Z_joint)\n",
    "            np.save(outp / \"Z_rna.npy\", Z_rna)\n",
    "            np.save(outp / \"Z_atac.npy\", Z_atac)\n",
    "\n",
    "        return {\n",
    "            \"Z_fused\": Z_joint,\n",
    "            \"Z\": Z_joint,\n",
    "            \"Z_rna\": Z_rna,\n",
    "            \"Z_atac\": Z_atac,\n",
    "            \"fit_seconds\": float(fit_seconds),\n",
    "            \"transductive\": True,\n",
    "            \"uses_labels\": False,\n",
    "            \"extra_json\": {\n",
    "                \"trained_on\": train_note,\n",
    "                \"n_latent_requested\": int(n_latent),\n",
    "                \"joint_dim_returned\": int(Z_joint_fit.shape[1]),\n",
    "                \"rna_preproc\": f\"cp{rna_target_sum:g}+log1p\" if rna_cp10k_log1p else \"none\",\n",
    "                \"atac_tfidf\": bool(atac_tfidf),\n",
    "                \"patched_scipy_sparse_vstack_scoped\": True,\n",
    "                \"data_used\": {\"joint\": data_used_joint, \"rna\": data_used_rna, \"atac\": data_used_atac},\n",
    "                \"latent_cap\": int(latent_cap) if latent_cap is not None else None,\n",
    "                \"lr_used\": used_lr,\n",
    "                \"lr_try\": list(lr_try) if lr_try is not None else None,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        print(\"========== [cobolt] FULL TRACEBACK ==========\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"========== [cobolt] END TRACEBACK ==========\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05010641-a979-49b3-acb4-fdc5e6df3d47",
   "metadata": {},
   "source": [
    "### Orchestrate + evaluate everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f04492-26d4-429b-a6d9-e4792af76dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "(Path(os.environ[\"XDG_CACHE_HOME\"]) / \"torch\" / \"kernels\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The argument 'device' of Tensor\\\\.pin_memory\\\\(\\\\) is deprecated.*\",\n",
    "    category=DeprecationWarning,\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The argument 'device' of Tensor\\\\.is_pinned\\\\(\\\\) is deprecated.*\",\n",
    "    category=DeprecationWarning,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30124b-81d0-4515-b9d4-64d4d098a832",
   "metadata": {},
   "source": [
    "### 1) Run + evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f99047-4c01-4d84-aa0d-e9f8dc47f3a5",
   "metadata": {},
   "source": [
    "### Helper/evaluation/run all function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240af7c-56d6-4e59-a15c-d33e11aa8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, balanced_accuracy_score,\n",
    "    silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# NaN helpers\n",
    "# -------------------------\n",
    "def _finite_row_mask(Z):\n",
    "    Z = np.asarray(Z)\n",
    "    if Z.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D embedding, got shape={Z.shape}\")\n",
    "    return np.isfinite(Z).all(axis=1)\n",
    "\n",
    "def _subset_finite_rows(Z, idx):\n",
    "    \"\"\"Return (Z_sub, idx_sub) keeping only finite rows within idx.\"\"\"\n",
    "    Z = np.asarray(Z)\n",
    "    idx = np.asarray(idx, dtype=int)\n",
    "    m = _finite_row_mask(Z[idx])\n",
    "    idx2 = idx[m]\n",
    "    return Z[idx2], idx2\n",
    "\n",
    "def _paired_finite_on_idx(Za, Zb, idx):\n",
    "    \"\"\"Return (Za_sub, Zb_sub, idx_sub) where BOTH are finite.\"\"\"\n",
    "    Za = np.asarray(Za); Zb = np.asarray(Zb)\n",
    "    idx = np.asarray(idx, dtype=int)\n",
    "    ma = _finite_row_mask(Za[idx])\n",
    "    mb = _finite_row_mask(Zb[idx])\n",
    "    m = ma & mb\n",
    "    idx2 = idx[m]\n",
    "    return Za[idx2], Zb[idx2], idx2\n",
    "\n",
    "def _seed_int(seed, default=0):\n",
    "    \"\"\"Return a real int seed, even if seed is None / nan / weird.\"\"\"\n",
    "    if seed is None:\n",
    "        return int(default)\n",
    "    try:\n",
    "        # handle numpy scalars, floats, etc.\n",
    "        if isinstance(seed, (float, np.floating)) and not np.isfinite(seed):\n",
    "            return int(default)\n",
    "        return int(seed)\n",
    "    except Exception:\n",
    "        return int(default)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# FOSCTTM (now None/NaN-safe)\n",
    "# -------------------------\n",
    "def foscttm_values(Za, Zb, *, subsample=3000, seed=0, topk=(1, 5, 10, 50, 100)):\n",
    "    Za = np.asarray(Za); Zb = np.asarray(Zb)\n",
    "    assert Za.shape[0] == Zb.shape[0]\n",
    "\n",
    "    n = int(Za.shape[0])\n",
    "    if n < 2:\n",
    "        return {\n",
    "            \"idx\": np.arange(n, dtype=int),\n",
    "            \"fos_a2b\": np.array([], dtype=np.float32),\n",
    "            \"fos_b2a\": np.array([], dtype=np.float32),\n",
    "            \"fos_mean_bidir\": np.array([], dtype=np.float32),\n",
    "            \"mean\": np.nan, \"sem\": np.nan,\n",
    "            \"mrr_a2b\": np.nan, \"mrr_b2a\": np.nan, \"mrr_mean\": np.nan,\n",
    "            **{f\"recall@{k}_a2b\": np.nan for k in topk},\n",
    "            **{f\"recall@{k}_b2a\": np.nan for k in topk},\n",
    "            **{f\"recall@{k}_mean\": np.nan for k in topk},\n",
    "        }\n",
    "\n",
    "    # subsample handling:\n",
    "    # - None => use all\n",
    "    # - <=0  => return NaNs (nothing to compute)\n",
    "    if subsample is None:\n",
    "        m = n\n",
    "    else:\n",
    "        subsample = int(subsample)\n",
    "        if subsample <= 0:\n",
    "            return {\n",
    "                \"idx\": np.array([], dtype=int),\n",
    "                \"fos_a2b\": np.array([], dtype=np.float32),\n",
    "                \"fos_b2a\": np.array([], dtype=np.float32),\n",
    "                \"fos_mean_bidir\": np.array([], dtype=np.float32),\n",
    "                \"mean\": np.nan, \"sem\": np.nan,\n",
    "                \"mrr_a2b\": np.nan, \"mrr_b2a\": np.nan, \"mrr_mean\": np.nan,\n",
    "                **{f\"recall@{k}_a2b\": np.nan for k in topk},\n",
    "                **{f\"recall@{k}_b2a\": np.nan for k in topk},\n",
    "                **{f\"recall@{k}_mean\": np.nan for k in topk},\n",
    "            }\n",
    "        m = min(subsample, n)\n",
    "\n",
    "    rng = np.random.default_rng(int(seed) if seed is not None else 0)\n",
    "    idx = rng.choice(n, size=m, replace=False)\n",
    "\n",
    "    A = Za[idx]; B = Zb[idx]\n",
    "    m = int(A.shape[0])\n",
    "    if m < 2:\n",
    "        # can happen if n==1, but we already guarded; keep safe anyway\n",
    "        return {\n",
    "            \"idx\": idx.astype(int),\n",
    "            \"fos_a2b\": np.array([], dtype=np.float32),\n",
    "            \"fos_b2a\": np.array([], dtype=np.float32),\n",
    "            \"fos_mean_bidir\": np.array([], dtype=np.float32),\n",
    "            \"mean\": np.nan, \"sem\": np.nan,\n",
    "            \"mrr_a2b\": np.nan, \"mrr_b2a\": np.nan, \"mrr_mean\": np.nan,\n",
    "            **{f\"recall@{k}_a2b\": np.nan for k in topk},\n",
    "            **{f\"recall@{k}_b2a\": np.nan for k in topk},\n",
    "            **{f\"recall@{k}_mean\": np.nan for k in topk},\n",
    "        }\n",
    "\n",
    "    A2 = np.sum(A*A, axis=1, keepdims=True)\n",
    "    B2 = np.sum(B*B, axis=1, keepdims=True).T\n",
    "    D = np.maximum(A2 + B2 - 2.0*(A @ B.T), 0.0)\n",
    "\n",
    "    diag = np.diag(D)\n",
    "    rank_a2b = (D < diag[:, None]).sum(axis=1) + 1\n",
    "    rank_b2a = (D.T < diag[:, None]).sum(axis=1) + 1\n",
    "\n",
    "    fos_a2b = (rank_a2b - 1) / (m - 1)\n",
    "    fos_b2a = (rank_b2a - 1) / (m - 1)\n",
    "    fos_mean = 0.5 * (fos_a2b + fos_b2a)\n",
    "\n",
    "    out = {\n",
    "        \"idx\": idx.astype(int),\n",
    "        \"fos_a2b\": fos_a2b,\n",
    "        \"fos_b2a\": fos_b2a,\n",
    "        \"fos_mean_bidir\": fos_mean,\n",
    "        \"mean\": float(np.mean(fos_mean)),\n",
    "        \"sem\": float(np.std(fos_mean, ddof=1) / np.sqrt(m)),\n",
    "        \"mrr_a2b\": float(np.mean(1.0 / rank_a2b)),\n",
    "        \"mrr_b2a\": float(np.mean(1.0 / rank_b2a)),\n",
    "        \"mrr_mean\": float(0.5*(np.mean(1.0/rank_a2b) + np.mean(1.0/rank_b2a))),\n",
    "    }\n",
    "    for k in topk:\n",
    "        k = int(k)\n",
    "        out[f\"recall@{k}_a2b\"] = float(np.mean(rank_a2b <= k))\n",
    "        out[f\"recall@{k}_b2a\"] = float(np.mean(rank_b2a <= k))\n",
    "        out[f\"recall@{k}_mean\"] = float(0.5*(out[f\"recall@{k}_a2b\"] + out[f\"recall@{k}_b2a\"]))\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Label transfer (NaN-safe + shape-safe)\n",
    "# -------------------------\n",
    "def knn_label_transfer(Z_ref, y_ref, Z_query, idx_query=None, *, k=15, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_pred_used: predicted labels for USED query rows (finite)\n",
    "      idx_used: indices into the ORIGINAL query (global indices if idx_query provided)\n",
    "    \"\"\"\n",
    "    Z_ref = np.asarray(Z_ref)\n",
    "    Z_query = np.asarray(Z_query)\n",
    "    y_ref = np.asarray(y_ref).astype(str)\n",
    "\n",
    "    if Z_ref.ndim != 2 or Z_query.ndim != 2:\n",
    "        raise ValueError(f\"Z_ref/Z_query must be 2D. Got {Z_ref.shape}, {Z_query.shape}\")\n",
    "\n",
    "    if idx_query is None:\n",
    "        idx_query = np.arange(Z_query.shape[0], dtype=int)\n",
    "    idx_query = np.asarray(idx_query, dtype=int)\n",
    "\n",
    "    # filter finite rows\n",
    "    ref_mask = _finite_row_mask(Z_ref)\n",
    "    qry_mask = _finite_row_mask(Z_query[idx_query])\n",
    "\n",
    "    if ref_mask.sum() == 0 or qry_mask.sum() == 0:\n",
    "        return np.array([], dtype=str), np.array([], dtype=int)\n",
    "\n",
    "    Zr = Z_ref[ref_mask]\n",
    "    yr = y_ref[ref_mask]\n",
    "\n",
    "    idx_used = idx_query[qry_mask]\n",
    "    Zq = Z_query[idx_used]\n",
    "\n",
    "    k_eff = min(int(k), Zr.shape[0])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_eff, metric=metric).fit(Zr)\n",
    "    nn = nbrs.kneighbors(Zq, return_distance=False)\n",
    "\n",
    "    y_pred = []\n",
    "    for neigh in nn:\n",
    "        vals, counts = np.unique(yr[neigh], return_counts=True)\n",
    "        y_pred.append(vals[np.argmax(counts)])\n",
    "    return np.asarray(y_pred, dtype=str), idx_used\n",
    "\n",
    "\n",
    "def label_transfer_metrics_split(Z_rna, Z_atac, labels, *, train_idx, test_idx, k=15):\n",
    "    \"\"\"\n",
    "    Scores ONLY on the test cells that were actually predicted (finite query rows).\n",
    "    \"\"\"\n",
    "    y = np.asarray(labels).astype(str)\n",
    "    tr = np.asarray(train_idx, dtype=int)\n",
    "    te = np.asarray(test_idx, dtype=int)\n",
    "\n",
    "    # Train sets must be finite in the reference space too (we pass Z_*[tr] into knn_label_transfer,\n",
    "    # which internally filters finite rows, so OK)\n",
    "\n",
    "    # RNA -> ATAC\n",
    "    yhat_r2a, te_used_r2a = knn_label_transfer(Z_rna[tr], y[tr], Z_atac, idx_query=te, k=k)\n",
    "    if len(te_used_r2a):\n",
    "        acc_r2a = accuracy_score(y[te_used_r2a], yhat_r2a)\n",
    "        f1_r2a  = f1_score(y[te_used_r2a], yhat_r2a, average=\"macro\")\n",
    "    else:\n",
    "        acc_r2a = np.nan\n",
    "        f1_r2a = np.nan\n",
    "\n",
    "    # ATAC -> RNA\n",
    "    yhat_a2r, te_used_a2r = knn_label_transfer(Z_atac[tr], y[tr], Z_rna, idx_query=te, k=k)\n",
    "    if len(te_used_a2r):\n",
    "        acc_a2r = accuracy_score(y[te_used_a2r], yhat_a2r)\n",
    "        f1_a2r  = f1_score(y[te_used_a2r], yhat_a2r, average=\"macro\")\n",
    "    else:\n",
    "        acc_a2r = np.nan\n",
    "        f1_a2r = np.nan\n",
    "\n",
    "    acc_mean = np.nanmean([acc_r2a, acc_a2r])\n",
    "    f1_mean  = np.nanmean([f1_r2a, f1_a2r])\n",
    "\n",
    "    return {\n",
    "        \"acc_rna_to_atac\": float(acc_r2a) if np.isfinite(acc_r2a) else np.nan,\n",
    "        \"macroF1_rna_to_atac\": float(f1_r2a) if np.isfinite(f1_r2a) else np.nan,\n",
    "        \"acc_atac_to_rna\": float(acc_a2r) if np.isfinite(acc_a2r) else np.nan,\n",
    "        \"macroF1_atac_to_rna\": float(f1_a2r) if np.isfinite(f1_a2r) else np.nan,\n",
    "        \"acc_mean\": float(acc_mean) if np.isfinite(acc_mean) else np.nan,\n",
    "        \"macroF1_mean\": float(f1_mean) if np.isfinite(f1_mean) else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Mixing score (NaN-safe)\n",
    "# -------------------------\n",
    "def modality_mixing_score(Z_rna, Z_atac, k=20):\n",
    "    Z_rna = np.asarray(Z_rna)\n",
    "    Z_atac = np.asarray(Z_atac)\n",
    "    assert Z_rna.shape[0] == Z_atac.shape[0], \"paired n required\"\n",
    "\n",
    "    idx = np.arange(Z_rna.shape[0], dtype=int)\n",
    "    Zr, Za, _ = _paired_finite_on_idx(Z_rna, Z_atac, idx)\n",
    "    if Zr.shape[0] < 2:\n",
    "        return np.nan\n",
    "\n",
    "    Z = np.vstack([Zr, Za])\n",
    "    modality = np.array([\"rna\"] * Zr.shape[0] + [\"atac\"] * Za.shape[0])\n",
    "\n",
    "    k_eff = min(int(k) + 1, Z.shape[0])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_eff).fit(Z)\n",
    "    nn = nbrs.kneighbors(Z, return_distance=False)[:, 1:]\n",
    "\n",
    "    frac_other = np.mean([np.mean(modality[nn[i]] != modality[i]) for i in range(Z.shape[0])])\n",
    "    return float(frac_other)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Fused-only metrics (NaN-safe on TRAIN and TEST)\n",
    "# -------------------------\n",
    "def knn_predict_labels(Z_train, y_train, Z_query, *, k=15, metric=\"euclidean\"):\n",
    "    Z_train = np.asarray(Z_train)\n",
    "    Z_query = np.asarray(Z_query)\n",
    "    y_train = np.asarray(y_train).astype(str)\n",
    "\n",
    "    if Z_train.shape[0] == 0 or Z_query.shape[0] == 0:\n",
    "        return np.asarray([], dtype=str)\n",
    "\n",
    "    k_eff = min(int(k), Z_train.shape[0])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_eff, metric=metric).fit(Z_train)\n",
    "    nn = nbrs.kneighbors(Z_query, return_distance=False)\n",
    "\n",
    "    y_pred = []\n",
    "    for neigh in nn:\n",
    "        vals, counts = np.unique(y_train[neigh], return_counts=True)\n",
    "        y_pred.append(vals[np.argmax(counts)])\n",
    "    return np.asarray(y_pred, dtype=str)\n",
    "\n",
    "\n",
    "def fused_knn_metrics(Z_fused, labels, *, splits, k=15):\n",
    "    Zf = np.asarray(Z_fused)\n",
    "    y  = np.asarray(labels).astype(str)\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "    te = np.asarray(splits[\"test\"], dtype=int)\n",
    "\n",
    "    # finite-only TRAIN and TEST\n",
    "    Ztr, tr_used = _subset_finite_rows(Zf, tr)\n",
    "    Zte, te_used = _subset_finite_rows(Zf, te)\n",
    "\n",
    "    if len(tr_used) == 0 or len(te_used) == 0:\n",
    "        return {\n",
    "            \"fused_knn_acc_test\": np.nan,\n",
    "            \"fused_knn_macroF1_test\": np.nan,\n",
    "            \"fused_knn_balanced_acc_test\": np.nan,\n",
    "        }\n",
    "\n",
    "    yhat = knn_predict_labels(Ztr, y[tr_used], Zte, k=k)\n",
    "    if yhat.shape[0] != len(te_used):\n",
    "        # hard safety: never allow inconsistent sample lengths\n",
    "        return {\n",
    "            \"fused_knn_acc_test\": np.nan,\n",
    "            \"fused_knn_macroF1_test\": np.nan,\n",
    "            \"fused_knn_balanced_acc_test\": np.nan,\n",
    "        }\n",
    "\n",
    "    y_true = y[te_used]\n",
    "    return {\n",
    "        \"fused_knn_acc_test\": float(accuracy_score(y_true, yhat)),\n",
    "        \"fused_knn_macroF1_test\": float(f1_score(y_true, yhat, average=\"macro\")),\n",
    "        \"fused_knn_balanced_acc_test\": float(balanced_accuracy_score(y_true, yhat)),\n",
    "    }\n",
    "\n",
    "\n",
    "def fused_silhouette_by_label(Z_fused, labels, *, splits):\n",
    "    Zf = np.asarray(Z_fused)\n",
    "    y  = np.asarray(labels).astype(str)\n",
    "    te = np.asarray(splits[\"test\"], dtype=int)\n",
    "\n",
    "    Zte, te_used = _subset_finite_rows(Zf, te)\n",
    "    if len(te_used) < 3:\n",
    "        return {\"fused_silhouette_test\": np.nan}\n",
    "\n",
    "    y_te = y[te_used]\n",
    "    n_unique = len(np.unique(y_te))\n",
    "    if n_unique < 2 or n_unique >= len(y_te):\n",
    "        return {\"fused_silhouette_test\": np.nan}\n",
    "\n",
    "    try:\n",
    "        s = silhouette_score(Zte, y_te, metric=\"euclidean\")\n",
    "    except Exception:\n",
    "        s = np.nan\n",
    "    return {\"fused_silhouette_test\": float(s) if np.isfinite(s) else np.nan}\n",
    "\n",
    "\n",
    "def knn_label_purity(Z, labels, *, splits, k=30):\n",
    "    Z = np.asarray(Z)\n",
    "    y = np.asarray(labels).astype(str)\n",
    "    te = np.asarray(splits[\"test\"], dtype=int)\n",
    "\n",
    "    Zte, te_used = _subset_finite_rows(Z, te)\n",
    "    if len(te_used) < 3:\n",
    "        return {\"fused_knn_label_purity_test\": np.nan}\n",
    "\n",
    "    k_eff = min(int(k) + 1, Zte.shape[0])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_eff).fit(Zte)\n",
    "    nn = nbrs.kneighbors(Zte, return_distance=False)[:, 1:]\n",
    "    yt = y[te_used]\n",
    "    purity = np.mean([(yt[nn[i]] == yt[i]).mean() for i in range(len(yt))])\n",
    "    return {\"fused_knn_label_purity_test\": float(purity)}\n",
    "\n",
    "\n",
    "def fused_ari_nmi_kmeans(Z_fused, labels, *, splits, seed=0):\n",
    "    Z = np.asarray(Z_fused)\n",
    "    y = np.asarray(labels).astype(str)\n",
    "    te = np.asarray(splits[\"test\"], dtype=int)\n",
    "\n",
    "    Zte, te_used = _subset_finite_rows(Z, te)\n",
    "    if len(te_used) < 3:\n",
    "        return {\"fused_kmeans_ari_test\": np.nan, \"fused_kmeans_nmi_test\": np.nan}\n",
    "\n",
    "    yt = y[te_used]\n",
    "    K = len(np.unique(yt))\n",
    "    if K < 2:\n",
    "        return {\"fused_kmeans_ari_test\": np.nan, \"fused_kmeans_nmi_test\": np.nan}\n",
    "\n",
    "    #cl = KMeans(n_clusters=K, n_init=\"auto\", random_state=int(seed)).fit_predict(Zte)\n",
    "    cl = KMeans(n_clusters=K, n_init=\"auto\", random_state=_seed_int(seed)).fit_predict(Zte)\n",
    "    \n",
    "    return {\n",
    "        \"fused_kmeans_ari_test\": float(adjusted_rand_score(yt, cl)),\n",
    "        \"fused_kmeans_nmi_test\": float(normalized_mutual_info_score(yt, cl)),\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main evaluation (NaN-safe everywhere)\n",
    "# -------------------------\n",
    "def _has_pair(Zr, Za):\n",
    "    return (\n",
    "        Zr is not None and Za is not None\n",
    "        and np.asarray(Zr).ndim == 2 and np.asarray(Za).ndim == 2\n",
    "        and np.asarray(Zr).shape[0] == np.asarray(Za).shape[0]\n",
    "    )\n",
    "\n",
    "def evaluate_embeddings_split(\n",
    "    out,\n",
    "    labels,\n",
    "    *,\n",
    "    splits,\n",
    "    foscttm_sub=3000,\n",
    "    k_mix=20,\n",
    "    k_lt=15,\n",
    "    k_fused=15,\n",
    "    seed=0,\n",
    "):\n",
    "\n",
    "    seed = _seed_int(seed)\n",
    "\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "    te = np.asarray(splits[\"test\"], dtype=int)\n",
    "    y  = np.asarray(labels).astype(str)\n",
    "\n",
    "    Zr = out.get(\"Z_rna\", None)\n",
    "    Za = out.get(\"Z_atac\", None)\n",
    "    Zf = out.get(\"Z_fused\", None)\n",
    "\n",
    "    row = {}\n",
    "\n",
    "    # paired metrics on TEST, only where BOTH are finite\n",
    "    if _has_pair(Zr, Za):\n",
    "        Zr_te, Za_te, _ = _paired_finite_on_idx(Zr, Za, te)\n",
    "        if Zr_te.shape[0] >= 2:\n",
    "            fos = foscttm_values(Zr_te, Za_te, subsample=foscttm_sub, seed=seed, topk=(1, 10))\n",
    "            row[\"FOSCTTM_mean_test\"] = fos[\"mean\"]\n",
    "            row[\"FOSCTTM_sem_test\"]  = fos[\"sem\"]\n",
    "            row[\"FOSCTTM_mrr_mean_test\"] = fos[\"mrr_mean\"]\n",
    "            row[\"FOSCTTM_recall@1_mean_test\"]  = fos.get(\"recall@1_mean\", np.nan)\n",
    "            row[\"FOSCTTM_recall@10_mean_test\"] = fos.get(\"recall@10_mean\", np.nan)\n",
    "\n",
    "            lt = label_transfer_metrics_split(Zr, Za, y, train_idx=tr, test_idx=te, k=k_lt)\n",
    "            row[\"label_transfer_acc_rna_to_atac_test\"]     = lt[\"acc_rna_to_atac\"]\n",
    "            row[\"label_transfer_macroF1_rna_to_atac_test\"] = lt[\"macroF1_rna_to_atac\"]\n",
    "            row[\"label_transfer_acc_atac_to_rna_test\"]     = lt[\"acc_atac_to_rna\"]\n",
    "            row[\"label_transfer_macroF1_atac_to_rna_test\"] = lt[\"macroF1_atac_to_rna\"]\n",
    "            row[\"label_transfer_acc_mean_test\"]            = lt[\"acc_mean\"]\n",
    "            row[\"label_transfer_macroF1_mean_test\"]        = lt[\"macroF1_mean\"]\n",
    "\n",
    "            row[\"mixing_score_test\"] = modality_mixing_score(Zr_te, Za_te, k=k_mix)\n",
    "        else:\n",
    "            row.update({\n",
    "                \"FOSCTTM_mean_test\": np.nan, \"FOSCTTM_sem_test\": np.nan, \"FOSCTTM_mrr_mean_test\": np.nan,\n",
    "                \"FOSCTTM_recall@1_mean_test\": np.nan, \"FOSCTTM_recall@10_mean_test\": np.nan,\n",
    "                \"label_transfer_acc_rna_to_atac_test\": np.nan, \"label_transfer_macroF1_rna_to_atac_test\": np.nan,\n",
    "                \"label_transfer_acc_atac_to_rna_test\": np.nan, \"label_transfer_macroF1_atac_to_rna_test\": np.nan,\n",
    "                \"label_transfer_acc_mean_test\": np.nan, \"label_transfer_macroF1_mean_test\": np.nan,\n",
    "                \"mixing_score_test\": np.nan,\n",
    "            })\n",
    "    else:\n",
    "        row.update({\n",
    "            \"FOSCTTM_mean_test\": np.nan, \"FOSCTTM_sem_test\": np.nan, \"FOSCTTM_mrr_mean_test\": np.nan,\n",
    "            \"FOSCTTM_recall@1_mean_test\": np.nan, \"FOSCTTM_recall@10_mean_test\": np.nan,\n",
    "            \"label_transfer_acc_rna_to_atac_test\": np.nan, \"label_transfer_macroF1_rna_to_atac_test\": np.nan,\n",
    "            \"label_transfer_acc_atac_to_rna_test\": np.nan, \"label_transfer_macroF1_atac_to_rna_test\": np.nan,\n",
    "            \"label_transfer_acc_mean_test\": np.nan, \"label_transfer_macroF1_mean_test\": np.nan,\n",
    "            \"mixing_score_test\": np.nan,\n",
    "        })\n",
    "\n",
    "    # fused-only metrics (NaN-safe on TRAIN and TEST)\n",
    "    if Zf is not None and np.asarray(Zf).ndim == 2 and np.asarray(Zf).size > 0:\n",
    "        row.update(fused_knn_metrics(Zf, y, splits=splits, k=k_fused))\n",
    "        row.update(fused_silhouette_by_label(Zf, y, splits=splits))\n",
    "        row.update(knn_label_purity(Zf, y, splits=splits, k=30))\n",
    "        row.update(fused_ari_nmi_kmeans(Zf, y, splits=splits, seed=seed))\n",
    "    else:\n",
    "        row.update({\n",
    "            \"fused_knn_acc_test\": np.nan,\n",
    "            \"fused_knn_macroF1_test\": np.nan,\n",
    "            \"fused_knn_balanced_acc_test\": np.nan,\n",
    "            \"fused_silhouette_test\": np.nan,\n",
    "            \"fused_knn_label_purity_test\": np.nan,\n",
    "            \"fused_kmeans_ari_test\": np.nan,\n",
    "            \"fused_kmeans_nmi_test\": np.nan,\n",
    "        })\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a93c2-6c6a-44ed-bd60-b628408832d2",
   "metadata": {},
   "source": [
    "### Specify the methods/their inputs/where to save their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587902f7-d3a2-4f0e-95a8-4c7699d05e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def _minmax(X):\n",
    "    \"\"\"Return (min, max, nnz, shape, dtype, frac_nonint, frac_neg, frac_naninf).\"\"\"\n",
    "    if sp.issparse(X):\n",
    "        data = X.data\n",
    "        nnz = int(data.size)\n",
    "        # sparse min/max need care for implicit zeros\n",
    "        dmin = float(data.min()) if nnz else 0.0\n",
    "        dmax = float(data.max()) if nnz else 0.0\n",
    "        xmin = min(0.0, dmin)   # because implicit zeros exist\n",
    "        xmax = max(0.0, dmax)\n",
    "        arr = data\n",
    "    else:\n",
    "        arr = np.asarray(X).ravel()\n",
    "        nnz = int(np.count_nonzero(arr))\n",
    "        xmin = float(np.nanmin(arr)) if arr.size else np.nan\n",
    "        xmax = float(np.nanmax(arr)) if arr.size else np.nan\n",
    "\n",
    "    arr = np.asarray(arr)\n",
    "    finite = np.isfinite(arr)\n",
    "    frac_naninf = float(1.0 - (finite.mean() if arr.size else 1.0))\n",
    "    arr_f = arr[finite] if arr.size else arr\n",
    "\n",
    "    if arr_f.size:\n",
    "        frac_nonint = float(np.mean(np.abs(arr_f - np.round(arr_f)) > 1e-6))\n",
    "        frac_neg = float(np.mean(arr_f < 0))\n",
    "    else:\n",
    "        frac_nonint = 0.0\n",
    "        frac_neg = 0.0\n",
    "\n",
    "    return xmin, xmax, nnz, tuple(getattr(X, \"shape\", (len(arr),))), str(getattr(X, \"dtype\", arr.dtype)), frac_nonint, frac_neg, frac_naninf\n",
    "\n",
    "\n",
    "def _layer_or_X(adata, layer=None):\n",
    "    if layer is None or layer == \"X\":\n",
    "        return adata.X, \"X\"\n",
    "    if hasattr(adata, \"layers\") and layer in adata.layers:\n",
    "        return adata.layers[layer], f\"layers['{layer}']\"\n",
    "    if hasattr(adata, \"obsm\") and layer in adata.obsm:\n",
    "        return adata.obsm[layer], f\"obsm['{layer}']\"\n",
    "    raise KeyError(f\"{adata!r} has no layer/obsm key '{layer}'\")\n",
    "\n",
    "\n",
    "def describe_adata(adata, name, *, layers=(\"X\", \"counts\")):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(adata)\n",
    "    # basic obs/var info\n",
    "    try:\n",
    "        print(f\"  n_obs={adata.n_obs} n_vars={adata.n_vars}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for layer in layers:\n",
    "        try:\n",
    "            X, src = _layer_or_X(adata, None if layer == \"X\" else layer)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        xmin, xmax, nnz, shape, dtype, frac_nonint, frac_neg, frac_naninf = _minmax(X)\n",
    "        print(f\"  {src}: shape={shape} dtype={dtype} nnz={nnz}\")\n",
    "        print(f\"    min={xmin:.4g}  max={xmax:.4g}  frac_nonint={frac_nonint:.4f}  frac_neg={frac_neg:.4f}  frac_naninf={frac_naninf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c02560-021d-4ff5-af7e-44f0b3b5b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Run the data checks to make sure intended counts going to each method etc.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "describe_adata(rna_univi, \"rna_univi\", layers=(\"X\", \"counts\"))\n",
    "describe_adata(atac_univi, \"atac_univi\", layers=(\"X\", \"counts\"))\n",
    "\n",
    "describe_adata(rna_counts_hvg, \"rna_counts_hvg_multivi\", layers=(\"X\", \"counts\"))\n",
    "describe_adata(atac_multivi, \"atac_multivi\", layers=(\"X\", \"counts\"))\n",
    "\n",
    "describe_adata(rna_log_hvg, \"rna_log_hvg_multimap\", layers=(\"X\", \"counts\"))\n",
    "describe_adata(atac_lsi, \"atac_lsi_multimap\", layers=(\"X\", \"counts\"))\n",
    "\n",
    "describe_adata(rna, \"rna (raw input)\", layers=(\"X\", \"counts\"))\n",
    "describe_adata(atac, \"atac (raw input)\", layers=(\"X\", \"counts\"))\n",
    "\n",
    "describe_adata(rna_counts_hvg, \"rna_counts_hvg_deepcca\", layers=(\"X\", \"counts\"))\n",
    "describe_adata(atac_counts_bin_hv, \"atac_counts_bin_hv_deepcca\", layers=(\"X\", \"counts\"))\n",
    "\n",
    "describe_adata(rna_counts_hvg, \"rna_counts_hvg_scmomat\", layers=(\"X\", \"counts\"))\n",
    "describe_adata(atac_counts_bin_hv, \"rna_counts_hvg_scmomat\", layers=(\"X\", \"counts\"))\n",
    "\n",
    "describe_adata(rna_counts_hvg, \"rna_counts_hvg_scjoint\", layers=(\"X\", \"counts\"))\n",
    "describe_adata(atac_for_scjoint, \"atac_for_scjoint\", layers=(\"X\", \"counts\", \"gene_activity_hvg\"))\n",
    "\n",
    "describe_adata(atac_peakvi, \"atac_peakvi\", layers=(\"X\", \"counts\"))\n",
    "\n",
    "describe_adata(rna_counts_hvg, \"rna_counts_hvg_cobolt\", layers=(\"X\", \"counts\"))\n",
    "describe_adata(atac_counts_bin_hv, \"atac_counts_bin_hv_cobolt\", layers=(\"X\", \"counts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53cacc6-0570-443a-b160-93432cc7d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "METHODS = {}\n",
    "\n",
    "METHODS[\"univi\"]    = lambda: run_univi(rna_univi, atac_univi, out_dir=WORK/\"runs/univi\", splits=splits, seed=RNG_SEED)\n",
    "METHODS[\"multivi\"]  = lambda: run_multivi(rna_counts_hvg, atac_multivi, out_dir=WORK/\"runs/multivi\", splits=splits, seed=RNG_SEED, n_latent=30, max_epochs=200, \n",
    "                                          patience=50, rna_layer=\"counts\", atac_layer=None) #atac_layer=\"counts\")\n",
    "METHODS[\"multimap\"] = lambda: run_multimap(rna_log_hvg, atac_lsi, out_dir=WORK/\"runs/multimap\", splits=splits, seed=RNG_SEED)\n",
    "METHODS[\"scglue\"]   = lambda: run_scglue_fair(rna_raw=rna, atac_raw=atac, gtf_path=GTF, out_dir=WORK/\"runs/scglue\", splits=splits, seed=RNG_SEED, \n",
    "                                              latent_dim=30, max_epochs=200, val_split=0.1, fuse=\"mean\", verbose=False, debug=True)\n",
    "METHODS[\"deepcca\"]  = lambda: run_deepcca(rna_log_hvg, atac_lsi, out_dir=WORK/\"runs/deepcca\", splits=splits, seed=RNG_SEED, latent_dim=30, reg=1e-3)\n",
    "METHODS[\"scmomat\"]  = lambda: run_scmomat_docstyle(rna=rna_counts_hvg, atac=atac_counts_bin_hv, out_dir=WORK/\"runs/scmomat\", batch_key=None, \n",
    "                                                   layers_by_mod={\"rna\": \"counts\", \"atac\": \"counts\"}, K=30, T=4000, lr=1e-2, lamb=0.001, seed=RNG_SEED, \n",
    "                                                   interval=1000, verbose=True)\n",
    "METHODS[\"scjoint\"]  = lambda: run_scjoint_split_aware(rna_counts_hvg, atac_for_scjoint, splits=splits, labels_key=LABEL_KEY, \n",
    "                                                      atac_gene_activity_layer=\"gene_activity_hvg\", \n",
    "                                                      scjoint_repo=\"/home/groups/precepts/ashforda/external_github_packages/scJoint\", \n",
    "                                                      out_dir=WORK / \"runs\" / \"scjoint\", seed=RNG_SEED, latent_dim=30, batch_size=256, gpu=0, \n",
    "                                                      allow_transductive_fallback=True, fill_missing=\"nan\")\n",
    "METHODS[\"peakvi\"]   = lambda: run_peakvi_fair(atac_peakvi, out_dir=WORK/\"runs/peakvi\", splits=splits, seed=RNG_SEED, n_latent=30, max_epochs=200, patience=50, \n",
    "                                              layer=None)\n",
    "METHODS[\"cobolt\"]   = lambda: run_cobolt_working(rna_counts_hvg, atac_counts_bin_hv, splits=None, rna_key=\"counts\", atac_key=None, n_latent=30, max_epochs=200,\n",
    "                                                 batch_size=256, device=\"cuda\", seed=RNG_SEED, rna_cp10k_log1p=False, lr=5e-3, verbose=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3121ca-d0b4-4aed-b9e9-d53f56fc9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Method registry (callables that return ensure_flags(...) dicts)\n",
    "# ============================================================\n",
    "METHODS = {}\n",
    "\n",
    "# Core\n",
    "METHODS[\"univi\"] = lambda: run_univi(\n",
    "    rna_univi,\n",
    "    atac_univi,\n",
    "    out_dir=WORK / \"runs\" / \"univi\",\n",
    "    splits=splits,\n",
    "    seed=RNG_SEED,\n",
    ")\n",
    "\n",
    "# MultiVI (split-aware; layers explicit)\n",
    "METHODS[\"multivi\"] = lambda: run_multivi(\n",
    "    rna_counts_hvg,\n",
    "    atac_multivi,\n",
    "    out_dir=WORK / \"runs\" / \"multivi\",\n",
    "    splits=splits,\n",
    "    seed=RNG_SEED,\n",
    "    n_latent=30,\n",
    "    max_epochs=200,\n",
    "    patience=50,\n",
    "    rna_layer=\"counts\",\n",
    "    atac_layer=None,  # set to \"counts\" ONLY if atac_multivi.layers[\"counts\"] exists\n",
    ")\n",
    "\n",
    "# MultiMAP (transductive; make sure run_multimap densifies sparse + train-fit scales)\n",
    "METHODS[\"multimap\"] = lambda: run_multimap(\n",
    "    rna_log_hvg,\n",
    "    atac_lsi,\n",
    "    out_dir=WORK / \"runs\" / \"multimap\",\n",
    "    splits=splits,\n",
    "    seed=RNG_SEED,\n",
    "    latent_dim=30,\n",
    ")\n",
    "\n",
    "# scGLUE (split-aware)\n",
    "METHODS[\"scglue\"] = lambda: run_scglue_fair(\n",
    "    rna_raw=rna,\n",
    "    atac_raw=atac,\n",
    "    gtf_path=GTF,\n",
    "    out_dir=WORK / \"runs\" / \"scglue\",\n",
    "    splits=splits,\n",
    "    seed=RNG_SEED,\n",
    "    latent_dim=30,\n",
    "    max_epochs=200,\n",
    "    val_split=0.1,\n",
    "    fuse=\"mean\",\n",
    "    verbose=False,\n",
    "    debug=False,  # flip to True only when debugging; can change behavior in some implementations\n",
    ")\n",
    "\n",
    "# DeepCCA (split-aware)\n",
    "METHODS[\"deepcca\"] = lambda: run_deepcca(\n",
    "    rna_log_hvg,\n",
    "    atac_lsi,\n",
    "    out_dir=WORK / \"runs\" / \"deepcca\",\n",
    "    splits=splits,\n",
    "    seed=RNG_SEED,\n",
    "    latent_dim=30,\n",
    ")\n",
    "\n",
    "# scMoMaT (doc-style runner; uses layers_by_mod)\n",
    "METHODS[\"scmomat\"] = lambda: run_scmomat_docstyle(\n",
    "    rna=rna_counts_hvg,\n",
    "    atac=atac_counts_bin_hv,\n",
    "    out_dir=WORK / \"runs\" / \"scmomat\",\n",
    "    batch_key=None,\n",
    "    layers_by_mod={\"rna\": \"counts\", \"atac\": \"counts\"},\n",
    "    K=30,\n",
    "    T=4000,\n",
    "    lr=1e-2,\n",
    "    lamb=0.001,\n",
    "    seed=RNG_SEED,\n",
    "    interval=1000,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# scJoint (split-aware; beware fill_missing=\"nan\" if anything downstream can't handle NaNs)\n",
    "METHODS[\"scjoint\"] = lambda: run_scjoint_split_aware(\n",
    "    rna_counts_hvg,\n",
    "    atac_for_scjoint,\n",
    "    splits=splits,\n",
    "    labels_key=LABEL_KEY,\n",
    "    atac_gene_activity_layer=\"gene_activity_hvg\",\n",
    "    scjoint_repo=\"/home/groups/precepts/ashforda/external_github_packages/scJoint\",\n",
    "    out_dir=WORK / \"runs\" / \"scjoint\",\n",
    "    seed=RNG_SEED,\n",
    "    latent_dim=30,\n",
    "    batch_size=256,\n",
    "    gpu=0,\n",
    "    allow_transductive_fallback=True,\n",
    "    fill_missing=\"nan\",\n",
    ")\n",
    "\n",
    "# PeakVI (ATAC-only baseline)\n",
    "METHODS[\"peakvi\"] = lambda: run_peakvi_fair(\n",
    "    atac_peakvi,\n",
    "    out_dir=WORK / \"runs\" / \"peakvi\",\n",
    "    splits=splits,\n",
    "    seed=RNG_SEED,\n",
    "    n_latent=30,\n",
    "    max_epochs=200,\n",
    "    patience=50,\n",
    "    layer=None,\n",
    ")\n",
    "\n",
    "# CoBOLT (Can't pass splits.. ; CoBOLT runner is effectively transductive, mark it inside ensure_flags)\n",
    "METHODS[\"cobolt\"] = lambda: run_cobolt_working(\n",
    "    rna_counts_hvg,\n",
    "    atac_counts_bin_hv,\n",
    "    splits=None,\n",
    "    rna_key=\"counts\",\n",
    "    atac_key=None,\n",
    "    n_latent=30,\n",
    "    max_epochs=200,\n",
    "    batch_size=256,\n",
    "    device=\"cuda\",\n",
    "    seed=RNG_SEED,\n",
    "    rna_cp10k_log1p=False,\n",
    "    lr=5e-3,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f896c9-53d8-4e17-bd37-c59c591832f7",
   "metadata": {},
   "source": [
    "### Run all methods/save results/calculate evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293707b9-6934-4f4e-8f4e-945ebdb00385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Make it shush (warnings + logs + tqdm)\n",
    "# Put this at the very top of the notebook.\n",
    "# -----------------------------\n",
    "import os, warnings, logging\n",
    "\n",
    "# 1) Warnings (target the annoying ones)\n",
    "warnings.filterwarnings(\"ignore\", message=\"The pynvml package is deprecated.*\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"The argument 'device' of Tensor\\\\.pin_memory\\\\(\\\\) is deprecated.*\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"The argument 'device' of Tensor\\\\.is_pinned\\\\(\\\\) is deprecated.*\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning)   # subprocess still running\n",
    "# If you're truly done with warnings:\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2) Logging (scglue / ignite / lightning / scvi etc.)\n",
    "for name in [\n",
    "    \"scglue\", \"ignite\", \"lightning\", \"pytorch_lightning\", \"scvi\",\n",
    "    \"muon\", \"anndata\", \"scanpy\", \"matplotlib\"\n",
    "]:\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "\n",
    "# also quiet the root logger\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# 3) tqdm progress bars (best-effort global disable)\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # also reduces random HF noise if it appears\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2fd05-3b60-4064-9a66-f024cb8eaccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "labels_all = rna.obs[LABEL_KEY].astype(str).to_numpy()\n",
    "\n",
    "def extract_flags(out):\n",
    "    ej = (out or {}).get(\"extra_json\", {}) or {}\n",
    "    return {\n",
    "        \"transductive\": bool(ej.get(\"transductive\", True)),   # conservative default\n",
    "        \"uses_labels\": bool(ej.get(\"uses_labels\", False)),    # conservative default\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "all_embeddings = {}\n",
    "\n",
    "for method, fn in METHODS.items():\n",
    "    print(f\"\\n=== Running {method} ===\")\n",
    "    t0 = now()\n",
    "\n",
    "    try:\n",
    "        out = fn()\n",
    "        out = ensure_flags(out)  # must set out[\"extra_json\"][\"transductive\"/\"uses_labels\"]\n",
    "        fit_seconds = float(out.get(\"fit_seconds\", now() - t0))\n",
    "\n",
    "        metrics = evaluate_embeddings_split(\n",
    "            out,\n",
    "            labels_all,\n",
    "            splits=splits,\n",
    "            foscttm_sub=FOSCTTM_SUBSAMPLE_N,\n",
    "            k_mix=K_MIX,\n",
    "            k_lt=K_LT,\n",
    "            k_fused=K_LT,\n",
    "            seed=RNG_SEED,\n",
    "        )\n",
    "\n",
    "        flags = extract_flags(out)\n",
    "\n",
    "        row = {\n",
    "            \"method\": method,\n",
    "            \"failed\": False,\n",
    "            \"fit_seconds\": fit_seconds,\n",
    "            **flags,\n",
    "            **metrics,\n",
    "            # always store as JSON string for consistency\n",
    "            \"extra_json\": json.dumps(out.get(\"extra_json\", {}), default=str),\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "        all_embeddings[method] = out\n",
    "        print(pd.Series(row))\n",
    "\n",
    "    except Exception as e:\n",
    "        fit_seconds = float(now() - t0)\n",
    "        row = {\n",
    "            \"method\": method,\n",
    "            \"failed\": True,\n",
    "            \"error\": repr(e),\n",
    "            \"fit_seconds\": fit_seconds,\n",
    "            # keep schema stable\n",
    "            \"transductive\": True,\n",
    "            \"uses_labels\": False,\n",
    "            \"extra_json\": json.dumps({\"transductive\": True, \"uses_labels\": False}, default=str),\n",
    "        }\n",
    "        rows.append(row)\n",
    "        print(f\"[{method}] FAILED:\", repr(e))\n",
    "\n",
    "results = pd.DataFrame(rows)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c4f52-b3a9-4304-a06d-658a55ff66f9",
   "metadata": {},
   "source": [
    "### 2) Save results + embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa206d-ba94-4f06-a54d-e92454b7a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_np(x):\n",
    "    if x is None:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "    x = np.asarray(x)\n",
    "    return x\n",
    "\n",
    "results_path = WORK / \"results_multiome.csv\"\n",
    "results.to_csv(results_path, index=False)\n",
    "print(\"Saved:\", results_path)\n",
    "\n",
    "emb_dir = WORK / \"embeddings_multiome\"\n",
    "emb_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name, emb in all_embeddings.items():\n",
    "    path = emb_dir / f\"{name}.npz\"\n",
    "    save_npz(\n",
    "        path,\n",
    "        Z_rna=_safe_np(emb.get(\"Z_rna\")),\n",
    "        Z_atac=_safe_np(emb.get(\"Z_atac\")),\n",
    "        Z_fused=_safe_np(emb.get(\"Z_fused\")),\n",
    "    )\n",
    "    print(\"Saved:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e003b7-9163-421a-8047-3fabbafb1a0a",
   "metadata": {},
   "source": [
    "### All method comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b67f7d-2bd3-4946-bc04-b4f77be02b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGDIR = WORK / \"figures_multiome\"\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Saving figures to:\", FIGDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de5a40-3b81-4ae2-bb51-bd1a6f3854bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_umap_coords(Z, *, n_neighbors=15, min_dist=0.5, seed=0, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Compute UMAP coordinates from an embedding Z (n,d). Returns (n,2).\n",
    "    \"\"\"\n",
    "    Z = np.asarray(Z, dtype=np.float32)\n",
    "    tmp = ad.AnnData(X=np.zeros((Z.shape[0], 1), dtype=np.float32))\n",
    "    tmp.obsm[\"X_emb\"] = Z\n",
    "\n",
    "    sc.pp.neighbors(tmp, use_rep=\"X_emb\", n_neighbors=int(n_neighbors), metric=metric)\n",
    "    sc.tl.umap(tmp, min_dist=float(min_dist), random_state=int(seed))\n",
    "    return np.asarray(tmp.obsm[\"X_umap\"], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897138cb-8a9a-4c37-acab-594b7e20fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_modality_overlay_umap(\n",
    "    Z_rna, Z_atac,\n",
    "    *,\n",
    "    title=\"Modality overlay UMAP\",\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.5,\n",
    "    seed=0,\n",
    "    s=3,\n",
    "    alpha=0.5,\n",
    "    savepath=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    UMAP on stacked [Z_rna; Z_atac], then scatter plot colored by modality.\n",
    "    Uses matplotlib defaults (no explicit colors).\n",
    "    \"\"\"\n",
    "    Z_rna = np.asarray(Z_rna)\n",
    "    Z_atac = np.asarray(Z_atac)\n",
    "    assert Z_rna.shape[0] == Z_atac.shape[0], \"RNA/ATAC must have same n (paired).\"\n",
    "\n",
    "    Z = np.vstack([Z_rna, Z_atac])\n",
    "    um = compute_umap_coords(Z, n_neighbors=n_neighbors, min_dist=min_dist, seed=seed)\n",
    "\n",
    "    n = Z_rna.shape[0]\n",
    "    um_rna = um[:n]\n",
    "    um_atac = um[n:]\n",
    "\n",
    "    plt.figure(figsize=(6.5, 5.5))\n",
    "    plt.scatter(um_rna[:, 0], um_rna[:, 1], s=s, alpha=alpha, label=\"RNA\")\n",
    "    plt.scatter(um_atac[:, 0], um_atac[:, 1], s=s, alpha=alpha, label=\"ATAC\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP1\")\n",
    "    plt.ylabel(\"UMAP2\")\n",
    "    plt.legend(markerscale=3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savepath is not None:\n",
    "        savepath = Path(savepath)\n",
    "        savepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(savepath, dpi=250)\n",
    "    plt.show()\n",
    "\n",
    "    return {\"umap_stacked\": um}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a0a62-e9c2-434b-94ed-8be3664de55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foscttm_values_from_pairwise(Z_rna, Z_atac, *, subsample=3000, seed=0, metric=\"euclidean_sq\"):\n",
    "    \"\"\"\n",
    "    Returns per-cell FOSCTTM values for RNA->ATAC and ATAC->RNA on a subsample.\n",
    "    FOSCTTM(i) = fraction of \"wrong\" matches closer than the true match.\n",
    "    \"\"\"\n",
    "    Z_rna = np.asarray(Z_rna)\n",
    "    Z_atac = np.asarray(Z_atac)\n",
    "    n = Z_rna.shape[0]\n",
    "    assert n == Z_atac.shape[0], \"RNA/ATAC must have same n (paired).\"\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    m = min(int(subsample), n)\n",
    "    idx = rng.choice(n, size=m, replace=False)\n",
    "\n",
    "    A = Z_rna[idx]\n",
    "    B = Z_atac[idx]\n",
    "\n",
    "    # Pairwise distances (m x m). Use squared Euclidean by default (fast + stable).\n",
    "    # D[i,j] = dist(A[i], B[j])\n",
    "    if metric == \"euclidean_sq\":\n",
    "        # (a-b)^2 = a^2 + b^2 - 2ab\n",
    "        A2 = np.sum(A * A, axis=1, keepdims=True)          # (m,1)\n",
    "        B2 = np.sum(B * B, axis=1, keepdims=True).T        # (1,m)\n",
    "        D = A2 + B2 - 2.0 * (A @ B.T)\n",
    "        D = np.maximum(D, 0.0)\n",
    "    elif metric == \"euclidean\":\n",
    "        A2 = np.sum(A * A, axis=1, keepdims=True)\n",
    "        B2 = np.sum(B * B, axis=1, keepdims=True).T\n",
    "        D = np.sqrt(np.maximum(A2 + B2 - 2.0 * (A @ B.T), 0.0))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "    # True match is diagonal: i matches i\n",
    "    diag = np.diag(D).copy()\n",
    "\n",
    "    # RNA -> ATAC: for each i, fraction of j where D[i,j] < D[i,i]\n",
    "    # subtract 1 if you count itself (but strict < means diag not counted anyway)\n",
    "    fos_rna_to_atac = (D < diag[:, None]).sum(axis=1) / (m - 1)\n",
    "\n",
    "    # ATAC -> RNA is the same using transpose: D[j,i] comparisons\n",
    "    diag2 = diag  # same diagonal\n",
    "    fos_atac_to_rna = (D.T < diag2[:, None]).sum(axis=1) / (m - 1)\n",
    "\n",
    "    return {\n",
    "        \"idx\": idx,\n",
    "        \"fos_rna_to_atac\": np.asarray(fos_rna_to_atac, dtype=float),\n",
    "        \"fos_atac_to_rna\": np.asarray(fos_atac_to_rna, dtype=float),\n",
    "        \"fos_mean_bidir\": 0.5 * (fos_rna_to_atac + fos_atac_to_rna),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952bdcff-fc16-4c2a-bbd7-47fd53462548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_foscttm_distribution(fos_dict, *, title=\"FOSCTTM distribution\", saveprefix=None):\n",
    "    \"\"\"\n",
    "    fos_dict: output of foscttm_values_from_pairwise\n",
    "    \"\"\"\n",
    "    v1 = fos_dict[\"fos_rna_to_atac\"]\n",
    "    v2 = fos_dict[\"fos_atac_to_rna\"]\n",
    "    vb = fos_dict[\"fos_mean_bidir\"]\n",
    "\n",
    "    # Histogram\n",
    "    plt.figure(figsize=(7, 4.5))\n",
    "    plt.hist(v1, bins=40, alpha=0.6, label=\"RNA→ATAC\")\n",
    "    plt.hist(v2, bins=40, alpha=0.6, label=\"ATAC→RNA\")\n",
    "    plt.hist(vb, bins=40, alpha=0.6, label=\"Mean (bidir)\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"FOSCTTM (lower = better)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if saveprefix:\n",
    "        plt.savefig(f\"{saveprefix}_foscttm_hist.png\", dpi=250)\n",
    "    plt.show()\n",
    "\n",
    "    # Violin + box overlay (simple)\n",
    "    data = [v1, v2, vb]\n",
    "    labels = [\"RNA→ATAC\", \"ATAC→RNA\", \"Mean\"]\n",
    "    plt.figure(figsize=(7, 4.5))\n",
    "    parts = plt.violinplot(data, showmeans=False, showmedians=True, showextrema=False)\n",
    "    plt.boxplot(data, widths=0.2, showfliers=False)\n",
    "    plt.xticks([1, 2, 3], labels)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"FOSCTTM (lower = better)\")\n",
    "    plt.tight_layout()\n",
    "    if saveprefix:\n",
    "        plt.savefig(f\"{saveprefix}_foscttm_violin.png\", dpi=250)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd278989-f722-4591-803a-9367443cda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_umap_from_embedding(\n",
    "    Z,\n",
    "    obs_df,\n",
    "    color_key=None,\n",
    "    *,\n",
    "    title=\"\",\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.5,\n",
    "    seed=0,\n",
    "    s=4,\n",
    "    alpha=0.8,\n",
    "    legend=True,\n",
    "    ax=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute UMAP from an embedding matrix Z (n_cells x d) and scatter-plot it.\n",
    "    - obs_df: pandas DataFrame (e.g., adata.obs or adata.obs.iloc[idx])\n",
    "    - color_key: column in obs_df to color by (categorical or continuous)\n",
    "    Returns: dict with 'U' (umap coords), 'fig', 'ax'\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    Z = np.asarray(Z)\n",
    "    if Z.ndim != 2:\n",
    "        raise ValueError(f\"Z must be 2D (n_cells x d); got shape={Z.shape}\")\n",
    "\n",
    "    # UMAP\n",
    "    try:\n",
    "        import umap\n",
    "        reducer = umap.UMAP(\n",
    "            n_neighbors=int(n_neighbors),\n",
    "            min_dist=float(min_dist),\n",
    "            metric=\"euclidean\",\n",
    "            random_state=int(seed),\n",
    "        )\n",
    "        U = reducer.fit_transform(Z)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"UMAP failed. Do you have `umap-learn` installed? \"\n",
    "            \"Try: pip install umap-learn\"\n",
    "        ) from e\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    # Plot\n",
    "    if color_key is None:\n",
    "        ax.scatter(U[:, 0], U[:, 1], s=s, alpha=alpha)\n",
    "    else:\n",
    "        if color_key not in obs_df.columns:\n",
    "            raise KeyError(f\"color_key={color_key!r} not in obs_df.columns\")\n",
    "\n",
    "        y = obs_df[color_key].to_numpy()\n",
    "\n",
    "        # Continuous vs categorical\n",
    "        if np.issubdtype(pd.Series(y).dtype, np.number):\n",
    "            sc = ax.scatter(U[:, 0], U[:, 1], s=s, alpha=alpha, c=y)\n",
    "            fig.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)\n",
    "        else:\n",
    "            y = pd.Series(y).astype(str).fillna(\"NA\").to_numpy()\n",
    "            cats = pd.unique(y)\n",
    "            for c in cats:\n",
    "                m = (y == c)\n",
    "                ax.scatter(U[m, 0], U[m, 1], s=s, alpha=alpha, label=c)\n",
    "            if legend:\n",
    "                ax.legend(\n",
    "                    loc=\"upper left\",\n",
    "                    bbox_to_anchor=(1.02, 1),\n",
    "                    frameon=False,\n",
    "                    fontsize=7,\n",
    "                    markerscale=2.5,\n",
    "                )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"UMAP1\")\n",
    "    ax.set_ylabel(\"UMAP2\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    return {\"U\": U, \"fig\": fig, \"ax\": ax}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e886e-38d8-41b6-ad74-69a4adee1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dashboard_save_all(\n",
    "    all_embeddings,\n",
    "    results_df=None,\n",
    "    *,\n",
    "    figdir=FIGDIR,\n",
    "    foscttm_subsample=3000,\n",
    "    seed=0,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.5,\n",
    "    splits=None,\n",
    "):\n",
    "    figdir = Path(figdir)\n",
    "    figdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    te = splits[\"test\"] if splits is not None else None\n",
    "\n",
    "    def _row_subset(Z, idx):\n",
    "        if Z is None or idx is None:\n",
    "            return Z\n",
    "        # pandas DataFrame / Series\n",
    "        if hasattr(Z, \"iloc\"):\n",
    "            return Z.iloc[idx]\n",
    "        # torch Tensor\n",
    "        try:\n",
    "            import torch\n",
    "            if isinstance(Z, torch.Tensor):\n",
    "                return Z[idx]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # numpy / list-like\n",
    "        return np.asarray(Z)[idx]\n",
    "\n",
    "    fos_by_method = {}\n",
    "\n",
    "    for method, out in all_embeddings.items():\n",
    "        Zr = out.get(\"Z_rna\", None)\n",
    "        Za = out.get(\"Z_atac\", None)\n",
    "        Zf = out.get(\"Z_fused\", None)\n",
    "\n",
    "        # Prefer modality overlay only if Zr/Za exist\n",
    "        if Zr is not None and Za is not None:\n",
    "            #Zr_plot = Zr[te] if te is not None else Zr\n",
    "            #Za_plot = Za[te] if te is not None else Za\n",
    "            Zr_plot = _row_subset(Zr, te)\n",
    "            Za_plot = _row_subset(Za, te)\n",
    "            \n",
    "            overlay_path = figdir / f\"{method}__modality_overlay_umap.png\"\n",
    "            plot_modality_overlay_umap(\n",
    "                Zr_plot, Za_plot,\n",
    "                title=f\"{method}: modality overlay (RNA vs ATAC)\" + (\" [TEST]\" if te is not None else \"\"),\n",
    "                n_neighbors=n_neighbors,\n",
    "                min_dist=min_dist,\n",
    "                seed=seed,\n",
    "                savepath=overlay_path,\n",
    "            )\n",
    "\n",
    "            fos = foscttm_values_from_pairwise(\n",
    "                Zr_plot, Za_plot,\n",
    "                subsample=foscttm_subsample,\n",
    "                seed=seed,\n",
    "            )\n",
    "            fos_by_method[method] = fos[\"fos_mean_bidir\"]\n",
    "\n",
    "            fos_prefix = figdir / f\"{method}\"\n",
    "            plot_foscttm_distribution(\n",
    "                fos,\n",
    "                title=f\"{method}: FOSCTTM distribution\" + (\" [TEST]\" if te is not None else \"\"),\n",
    "                saveprefix=str(fos_prefix),\n",
    "            )\n",
    "\n",
    "        # If only fused exists, save a fused UMAP (no FOSCTTM, no overlay)\n",
    "        elif Zf is not None:\n",
    "            #Zf_plot = Zf[te] if te is not None else Zf\n",
    "            Zf_plot = _row_subset(Zf, te)\n",
    "            '''\n",
    "            tmp = plot_umap_from_embedding(\n",
    "                Zf_plot,\n",
    "                rna.obs.iloc[te] if te is not None else rna.obs,\n",
    "                LABEL_KEY,\n",
    "                title=f\"{method}: fused UMAP\" + (\" [TEST]\" if te is not None else \"\"),\n",
    "            )\n",
    "            '''\n",
    "            \n",
    "            obs_plot = rna.obs.iloc[te] if te is not None else rna.obs\n",
    "            tmp = plot_umap_from_embedding(\n",
    "                Zf_plot,\n",
    "                obs_plot,\n",
    "                LABEL_KEY,\n",
    "                title=f\"{method}: fused UMAP\" + (\" [TEST]\" if te is not None else \"\"),\n",
    "            )\n",
    "            \n",
    "            plt.savefig(figdir / f\"{method}__fused_umap.png\", dpi=250)\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            print(f\"[{method}] no embeddings found, skipping plots\")\n",
    "\n",
    "    # Combined violin across methods (only those with true paired foscttm)\n",
    "    if len(fos_by_method) > 0:\n",
    "        methods = list(fos_by_method.keys())\n",
    "        data = [fos_by_method[m] for m in methods]\n",
    "\n",
    "        plt.figure(figsize=(max(8, 0.6 * len(methods)), 5))\n",
    "        plt.violinplot(data, showmeans=False, showmedians=True, showextrema=False)\n",
    "        plt.boxplot(data, widths=0.2, showfliers=False)\n",
    "        plt.xticks(range(1, len(methods) + 1), methods, rotation=45, ha=\"right\")\n",
    "        plt.title(\"FOSCTTM (bidir mean) distribution across methods\")\n",
    "        plt.ylabel(\"FOSCTTM (lower = better)\")\n",
    "        plt.tight_layout()\n",
    "        outpath = figdir / \"ALL__foscttm_violin.png\"\n",
    "        plt.savefig(outpath, dpi=250)\n",
    "        plt.show()\n",
    "\n",
    "    if results_df is not None:\n",
    "        results_df.to_csv(figdir / \"results_metrics.csv\", index=False)\n",
    "        with open(figdir / \"results_metrics.json\", \"w\") as f:\n",
    "            json.dump(results_df.to_dict(orient=\"records\"), f, indent=2)\n",
    "\n",
    "    print(\"Saved figures to:\", figdir)\n",
    "    return {\"figdir\": str(figdir), \"fos_by_method\": fos_by_method}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c753d1b-e727-4309-b40b-05222029e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "dash = dashboard_save_all(\n",
    "    all_embeddings,\n",
    "    results_df=results,\n",
    "    figdir=FIGDIR,\n",
    "    #foscttm_subsample=FOSCTTM_SUBSAMPLE_N,\n",
    "    foscttm_subsample=3000,\n",
    "    seed=RNG_SEED,\n",
    "    splits=splits,\n",
    ")\n",
    "dash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5553234-50ce-4828-bcec-1675ba22f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# With random seed = 67\n",
    "    method\tfailed\tfit_seconds\ttransductive\tuses_labels\tFOSCTTM_mean_test\tFOSCTTM_sem_test\tFOSCTTM_mrr_mean_test\tFOSCTTM_recall@1_mean_test\tFOSCTTM_recall@10_mean_test\t...\tlabel_transfer_macroF1_mean_test\tmixing_score_test\tfused_knn_acc_test\tfused_knn_macroF1_test\tfused_knn_balanced_acc_test\tfused_silhouette_test\tfused_knn_label_purity_test\tfused_kmeans_ari_test\tfused_kmeans_nmi_test\textra_json\n",
    "0\tunivi\tFalse\t172.071677\tFalse\tFalse\t0.018010\t0.000823\t0.267465\t0.135892\t0.548755\t...\t0.874115\t0.473278\t0.954357\t0.940906\t0.934369\t0.351260\t0.837725\t0.508302\t0.760567\t{\"transductive\": false, \"uses_labels\": false}\n",
    "1\tmultivi\tFalse\t223.625776\tFalse\tFalse\t0.052636\t0.001936\t0.098820\t0.033195\t0.227178\t...\t0.689088\t0.490519\t0.866183\t0.732153\t0.746310\t0.392727\t0.775622\t0.486352\t0.734174\t{\"transductive\": false, \"uses_labels\": false, ...\n",
    "2\tmultimap\tFalse\t46.769239\tTrue\tFalse\tNaN\tNaN\tNaN\tNaN\tNaN\t...\tNaN\tNaN\t0.876556\t0.782693\t0.790407\t0.215923\t0.745574\t0.400929\t0.664981\t{\"transductive\": true, \"uses_labels\": false}\n",
    "3\tscglue\tFalse\t1038.876083\tFalse\tFalse\t0.036658\t0.001968\t0.194465\t0.093361\t0.415456\t...\t0.762499\t0.491836\t0.942946\t0.920576\t0.910559\t0.246982\t0.819398\t0.534926\t0.773158\t{\"transductive\": false, \"uses_labels\": false}\n",
    "4\tdeepcca\tFalse\t32.100153\tFalse\tFalse\t0.487022\t0.008665\t0.009648\t0.000519\t0.015041\t...\t0.045115\t0.174471\t0.931535\t0.840092\t0.841512\t0.199901\t0.792911\t0.518514\t0.673216\t{\"transductive\": false, \"uses_labels\": false, ...\n",
    "5\tscmomat\tFalse\t68.720901\tTrue\tFalse\tNaN\tNaN\tNaN\tNaN\tNaN\t...\tNaN\tNaN\t0.725104\t0.566274\t0.555586\t-0.015417\t0.543396\t0.261847\t0.532138\t{\"transductive\": true, \"uses_labels\": false, \"...\n",
    "6\tscjoint\tFalse\t107.798547\tTrue\tTrue\t0.071130\t0.002187\t0.097969\t0.036307\t0.217324\t...\t0.688012\t0.452396\t0.946058\t0.875449\t0.874119\t0.352216\t0.850657\t0.822849\t0.844262\t{\"transductive\": true, \"uses_labels\": true, \"n...\n",
    "7\tpeakvi\tFalse\t113.293390\tFalse\tFalse\tNaN\tNaN\tNaN\tNaN\tNaN\t...\tNaN\tNaN\t0.876556\t0.818412\t0.819604\t0.221018\t0.759647\t0.453958\t0.718532\t{\"transductive\": false, \"uses_labels\": false, ...\n",
    "8\tcobolt\tFalse\t808.664924\tTrue\tFalse\tNaN\tNaN\tNaN\tNaN\tNaN\t...\tNaN\tNaN\t0.918050\t0.831186\t0.818098\t0.089372\t0.710961\t0.458295\t0.704632\t{\"transductive\": true, \"uses_labels\": false}\n",
    "9 rows × 25 columns\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb3f4e-7864-4eda-8192-2a1cb0dd9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Metric metadata (edit freely)\n",
    "# ----------------------------\n",
    "\n",
    "DEFAULT_METRICS = [\n",
    "    # paired alignment\n",
    "    (\"FOSCTTM_mean_test\", \"FOSCTTM mean (↓)\", \"lower\"),\n",
    "    (\"FOSCTTM_mrr_mean_test\", \"FOSCTTM MRR (↑)\", \"higher\"),\n",
    "    (\"FOSCTTM_recall@1_mean_test\", \"FOSCTTM Recall@1 (↑)\", \"higher\"),\n",
    "    (\"FOSCTTM_recall@10_mean_test\", \"FOSCTTM Recall@10 (↑)\", \"higher\"),\n",
    "    # label transfer / mixing\n",
    "    (\"label_transfer_acc_mean_test\", \"Label transfer acc mean (↑)\", \"higher\"),\n",
    "    (\"label_transfer_macroF1_mean_test\", \"Label transfer macroF1 mean (↑)\", \"higher\"),\n",
    "    (\"mixing_score_test\", \"Mixing score (↑)\", \"higher\"),\n",
    "    # fused quality\n",
    "    (\"fused_knn_acc_test\", \"Fused kNN acc (↑)\", \"higher\"),\n",
    "    (\"fused_knn_macroF1_test\", \"Fused kNN macroF1 (↑)\", \"higher\"),\n",
    "    (\"fused_knn_balanced_acc_test\", \"Fused kNN balanced acc (↑)\", \"higher\"),\n",
    "    (\"fused_silhouette_test\", \"Fused silhouette (↑)\", \"higher\"),\n",
    "    (\"fused_knn_label_purity_test\", \"Fused label purity (↑)\", \"higher\"),\n",
    "    (\"fused_kmeans_ari_test\", \"Fused k-means ARI (↑)\", \"higher\"),\n",
    "    (\"fused_kmeans_nmi_test\", \"Fused k-means NMI (↑)\", \"higher\"),\n",
    "    # runtime\n",
    "    (\"fit_seconds\", \"Fit seconds (↓)\", \"lower\"),\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "\n",
    "def _ensure_df(results_df) -> pd.DataFrame:\n",
    "    if results_df is None:\n",
    "        raise ValueError(\"results_df is required\")\n",
    "    df = results_df.copy()\n",
    "    if \"method\" not in df.columns:\n",
    "        raise ValueError(\"results_df must contain a 'method' column.\")\n",
    "    # coerce metric columns to numeric where possible\n",
    "    for c in df.columns:\n",
    "        if c == \"method\":\n",
    "            continue\n",
    "        if df[c].dtype == object:\n",
    "            # try numeric coercion; ignore failures\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _ok_methods(df: pd.DataFrame, *, include_failed: bool = False) -> pd.DataFrame:\n",
    "    if \"failed\" not in df.columns or include_failed:\n",
    "        return df\n",
    "    return df.loc[~df[\"failed\"].astype(bool)].copy()\n",
    "\n",
    "\n",
    "def _score_for_sort(values: np.ndarray, direction: str) -> np.ndarray:\n",
    "    # used only for ordering methods\n",
    "    if direction == \"higher\":\n",
    "        return values\n",
    "    if direction == \"lower\":\n",
    "        return -values\n",
    "    return values\n",
    "\n",
    "\n",
    "def _method_order_for_metric(df: pd.DataFrame, metric: str, direction: str) -> list[str]:\n",
    "    d = df[[\"method\", metric]].dropna()\n",
    "    if d.empty:\n",
    "        return df[\"method\"].tolist()\n",
    "    v = d[metric].to_numpy()\n",
    "    s = _score_for_sort(v, direction)\n",
    "    order = d.iloc[np.argsort(-s)][\"method\"].tolist()\n",
    "    # include methods that were NaN at the end\n",
    "    rest = [m for m in df[\"method\"].tolist() if m not in order]\n",
    "    return order + rest\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Scalar metric leaderboard plots\n",
    "# ----------------------------\n",
    "\n",
    "def plot_metric_leaderboard(\n",
    "    results_df,\n",
    "    metric: str,\n",
    "    *,\n",
    "    title: str | None = None,\n",
    "    direction: str = \"higher\",   # \"higher\" or \"lower\"\n",
    "    figdir: str | Path | None = None,\n",
    "    filename: str | None = None,\n",
    "    include_failed: bool = False,\n",
    "    annotate: bool = True,\n",
    "    logy: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    One number per method -> dot plot / lollipop.\n",
    "\n",
    "    direction:\n",
    "      - \"higher\": best at top\n",
    "      - \"lower\": best at top\n",
    "    \"\"\"\n",
    "    df = _ensure_df(results_df)\n",
    "    df = _ok_methods(df, include_failed=include_failed)\n",
    "\n",
    "    if metric not in df.columns:\n",
    "        print(f\"[plot_metric_leaderboard] missing metric: {metric}\")\n",
    "        return None\n",
    "\n",
    "    d = df[[\"method\", metric]].copy()\n",
    "    d[metric] = pd.to_numeric(d[metric], errors=\"coerce\")\n",
    "\n",
    "    order = _method_order_for_metric(d, metric, direction)\n",
    "    d[\"method\"] = pd.Categorical(d[\"method\"], categories=order, ordered=True)\n",
    "    d = d.sort_values(\"method\")\n",
    "\n",
    "    y = d[\"method\"].astype(str).tolist()\n",
    "    x = d[metric].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, max(4, 0.45 * len(y))))\n",
    "    # lollipop: line from 0 to x, then dot\n",
    "    for i, val in enumerate(x):\n",
    "        if np.isfinite(val):\n",
    "            plt.plot([0, val], [i, i], linewidth=1)\n",
    "            plt.scatter([val], [i], s=40)\n",
    "\n",
    "    plt.yticks(range(len(y)), y)\n",
    "    plt.xlabel(metric)\n",
    "    plt.title(title or f\"{metric} across methods\")\n",
    "    if logy:\n",
    "        # log y doesn't make sense; log x axis does\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(metric + \" (log scale)\")\n",
    "    plt.grid(axis=\"x\", alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if annotate:\n",
    "        for i, val in enumerate(x):\n",
    "            if np.isfinite(val):\n",
    "                plt.text(val, i, f\" {val:.4g}\", va=\"center\", fontsize=9)\n",
    "\n",
    "    outpath = None\n",
    "    if figdir is not None:\n",
    "        figdir = Path(figdir)\n",
    "        figdir.mkdir(parents=True, exist_ok=True)\n",
    "        outname = filename or f\"ALL__leaderboard__{metric}.png\"\n",
    "        outpath = figdir / outname\n",
    "        plt.savefig(outpath, dpi=250)\n",
    "    plt.show()\n",
    "    return outpath\n",
    "\n",
    "\n",
    "def plot_all_scalar_leaderboards(\n",
    "    results_df,\n",
    "    *,\n",
    "    metrics=DEFAULT_METRICS,\n",
    "    figdir: str | Path,\n",
    "    include_failed: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Make one leaderboard plot per scalar metric.\n",
    "    \"\"\"\n",
    "    for metric, pretty, direction in metrics:\n",
    "        logx = (metric == \"fit_seconds\")\n",
    "        plot_metric_leaderboard(\n",
    "            results_df,\n",
    "            metric,\n",
    "            title=pretty,\n",
    "            direction=direction,\n",
    "            figdir=figdir,\n",
    "            filename=f\"ALL__leaderboard__{metric}.png\",\n",
    "            include_failed=include_failed,\n",
    "            annotate=True,\n",
    "            logy=logx,\n",
    "        )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Distribution violins (per-method arrays)\n",
    "# ----------------------------\n",
    "\n",
    "def plot_distribution_violin(\n",
    "    dist_by_method: dict[str, np.ndarray],\n",
    "    *,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    figdir: str | Path | None = None,\n",
    "    filename: str | None = None,\n",
    "    order: list[str] | None = None,\n",
    "    show_box: bool = True,\n",
    "    clip_quantiles: tuple[float, float] | None = None,  # e.g. (0.01, 0.99)\n",
    "):\n",
    "    \"\"\"\n",
    "    dist_by_method: {method: 1D array}\n",
    "    \"\"\"\n",
    "    # filter empties / nan-only\n",
    "    clean = {}\n",
    "    for m, arr in dist_by_method.items():\n",
    "        if arr is None:\n",
    "            continue\n",
    "        a = np.asarray(arr, dtype=np.float32).ravel()\n",
    "        a = a[np.isfinite(a)]\n",
    "        if a.size == 0:\n",
    "            continue\n",
    "        if clip_quantiles is not None:\n",
    "            lo, hi = np.quantile(a, clip_quantiles)\n",
    "            a = a[(a >= lo) & (a <= hi)]\n",
    "        if a.size:\n",
    "            clean[m] = a\n",
    "\n",
    "    if not clean:\n",
    "        print(\"[plot_distribution_violin] nothing to plot (all empty).\")\n",
    "        return None\n",
    "\n",
    "    methods = list(clean.keys())\n",
    "    if order is not None:\n",
    "        methods = [m for m in order if m in clean] + [m for m in methods if m not in (order or [])]\n",
    "\n",
    "    data = [clean[m] for m in methods]\n",
    "\n",
    "    plt.figure(figsize=(max(8, 0.6 * len(methods)), 5))\n",
    "    plt.violinplot(data, showmeans=False, showmedians=True, showextrema=False)\n",
    "    if show_box:\n",
    "        plt.boxplot(data, widths=0.2, showfliers=False)\n",
    "    plt.xticks(range(1, len(methods) + 1), methods, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    outpath = None\n",
    "    if figdir is not None:\n",
    "        figdir = Path(figdir)\n",
    "        figdir.mkdir(parents=True, exist_ok=True)\n",
    "        outname = filename or (title.replace(\" \", \"_\") + \".png\")\n",
    "        outpath = figdir / outname\n",
    "        plt.savefig(outpath, dpi=250)\n",
    "    plt.show()\n",
    "    return outpath\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Metric matrix view (heatmap-like, no seaborn)\n",
    "# ----------------------------\n",
    "\n",
    "def plot_metric_matrix(\n",
    "    results_df,\n",
    "    *,\n",
    "    metrics: list[tuple[str, str, str]] = DEFAULT_METRICS,\n",
    "    figdir: str | Path | None = None,\n",
    "    filename: str = \"ALL__metric_matrix.png\",\n",
    "    include_failed: bool = False,\n",
    "    normalize: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Heatmap-ish matrix:\n",
    "      rows = methods\n",
    "      cols = metrics\n",
    "    normalize=True: per-metric min-max (inverts for \"lower is better\") so higher = better (0..1)\n",
    "    \"\"\"\n",
    "    df = _ensure_df(results_df)\n",
    "    df = _ok_methods(df, include_failed=include_failed)\n",
    "\n",
    "    methods = df[\"method\"].astype(str).tolist()\n",
    "    cols = []\n",
    "    pretty = []\n",
    "    direction = []\n",
    "    for m, p, d in metrics:\n",
    "        if m in df.columns:\n",
    "            cols.append(m)\n",
    "            pretty.append(p)\n",
    "            direction.append(d)\n",
    "\n",
    "    if not cols:\n",
    "        print(\"[plot_metric_matrix] none of the requested metrics exist in results_df.\")\n",
    "        return None\n",
    "\n",
    "    M = df[cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy(dtype=np.float32)\n",
    "\n",
    "    # normalize per column\n",
    "    if normalize:\n",
    "        M2 = M.copy()\n",
    "        for j in range(M.shape[1]):\n",
    "            col = M[:, j]\n",
    "            ok = np.isfinite(col)\n",
    "            if ok.sum() < 2:\n",
    "                continue\n",
    "            v = col[ok]\n",
    "            mn, mx = float(v.min()), float(v.max())\n",
    "            if mx <= mn + 1e-12:\n",
    "                continue\n",
    "            scaled = (col - mn) / (mx - mn)\n",
    "            # invert if lower is better, so \"higher score is better\"\n",
    "            if direction[j] == \"lower\":\n",
    "                scaled = 1.0 - scaled\n",
    "            M2[:, j] = scaled\n",
    "        M = M2\n",
    "\n",
    "    # plot with imshow\n",
    "    plt.figure(figsize=(max(10, 0.75 * len(cols)), max(6, 0.45 * len(methods))))\n",
    "    im = plt.imshow(M, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    plt.colorbar(im, fraction=0.03, pad=0.02, label=(\"normalized score (higher=better)\" if normalize else \"raw value\"))\n",
    "    plt.xticks(range(len(cols)), pretty, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(methods)), methods)\n",
    "    plt.title(\"Metric matrix across methods\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    outpath = None\n",
    "    if figdir is not None:\n",
    "        figdir = Path(figdir)\n",
    "        figdir.mkdir(parents=True, exist_ok=True)\n",
    "        outpath = figdir / filename\n",
    "        plt.savefig(outpath, dpi=250)\n",
    "    plt.show()\n",
    "    return outpath\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Wrapper: make all comparison plots\n",
    "# ----------------------------\n",
    "\n",
    "def save_all_method_comparison_plots(\n",
    "    results_df,\n",
    "    *,\n",
    "    figdir: str | Path,\n",
    "    fos_by_method: dict[str, np.ndarray] | None = None,\n",
    "    include_failed: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Produces:\n",
    "      - leaderboards for each scalar metric\n",
    "      - metric matrix heatmap-ish view\n",
    "      - violin for FOSCTTM distribution (if provided)\n",
    "    \"\"\"\n",
    "    figdir = Path(figdir)\n",
    "    figdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # scalar leaderboards\n",
    "    plot_all_scalar_leaderboards(results_df, metrics=DEFAULT_METRICS, figdir=figdir, include_failed=include_failed)\n",
    "\n",
    "    # matrix view\n",
    "    plot_metric_matrix(results_df, metrics=DEFAULT_METRICS, figdir=figdir, include_failed=include_failed, normalize=True)\n",
    "\n",
    "    # distribution violin(s)\n",
    "    if fos_by_method is not None and len(fos_by_method) > 0:\n",
    "        plot_distribution_violin(\n",
    "            fos_by_method,\n",
    "            title=\"FOSCTTM distribution across methods\",\n",
    "            ylabel=\"FOSCTTM (lower = better)\",\n",
    "            figdir=figdir,\n",
    "            filename=\"ALL__foscttm_violin.png\",\n",
    "            show_box=True,\n",
    "        )\n",
    "\n",
    "    print(\"[comparison plots] saved to:\", figdir)\n",
    "    return {\"figdir\": str(figdir)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a31796-1538-49a7-84ef-ab89a3dfc22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dash = dashboard_save_all(\n",
    "    all_embeddings,\n",
    "    results_df=results,\n",
    "    figdir=FIGDIR,\n",
    "    foscttm_subsample=3000,\n",
    "    seed=RNG_SEED,\n",
    "    splits=splits,\n",
    ")\n",
    "\n",
    "save_all_method_comparison_plots(\n",
    "    results,\n",
    "    figdir=FIGDIR,\n",
    "    fos_by_method=dash[\"fos_by_method\"],  # violin uses distributions\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f71cf0-21a5-4777-9bf5-baa8d3d6fc44",
   "metadata": {},
   "source": [
    "### Run across several random seeds for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69f133-98c2-49b2-bc26-cdc3a7f0212c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Seed-sweep robustness (Multiome-ready)\n",
    "# - varies *splits* across seeds (shared across methods)\n",
    "# - REBUILDS any preprocessing that is \"fit on TRAIN\" per seed:\n",
    "#     * RNA HVGs\n",
    "#     * ATAC TFIDF+SVD/LSI\n",
    "#     * ATAC HV-peaks (for MultiVI / PeakVI style)\n",
    "# - does NOT change any existing run_* functions:\n",
    "#     it just updates globals (RNG_SEED, splits, shared inputs) that your METHODS thunks use\n",
    "# ============================================================\n",
    "\n",
    "import time, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) configure the sweep\n",
    "# -----------------------------\n",
    "SEEDS = list(range(5))\n",
    "SWEEP_TAG = \"cv_sweep_py_1-31-2026\"\n",
    "OUT_DIR = (Path(WORK) / \"runs\" / SWEEP_TAG) if \"WORK\" in globals() else (Path(\"./runs\") / SWEEP_TAG)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# split hyperparams (match your make_shared_splits signature)\n",
    "SPLIT_KWS = dict(\n",
    "    train_frac=0.8,\n",
    "    val_frac=0.1,\n",
    ")\n",
    "\n",
    "# preprocessing hyperparams (match your build_shared_inputs)\n",
    "PREP_KWS = dict(\n",
    "    rna_counts_layer=\"counts\",\n",
    "    atac_counts_layer=\"counts\",\n",
    "    n_hvg=2000,\n",
    "    target_sum=1e4,\n",
    "    n_lsi=101,\n",
    "    n_hvpeaks_multivi=4002,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) build splits for a given seed\n",
    "# -----------------------------\n",
    "def make_splits_for_seed(seed: int):\n",
    "    # uses your existing function\n",
    "    return make_shared_splits(\n",
    "        rna.n_obs,\n",
    "        labels_all,\n",
    "        seed=int(seed),\n",
    "        **SPLIT_KWS,\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 2) (optional but recommended) set RNG seeds too\n",
    "# -----------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    np.random.seed(int(seed))\n",
    "    try:\n",
    "        import random\n",
    "        random.seed(int(seed))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(int(seed))\n",
    "        torch.cuda.manual_seed_all(int(seed))\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -----------------------------\n",
    "# 3) metric extraction extended\n",
    "# -----------------------------\n",
    "import time, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "def _extract_metrics(out):\n",
    "    if out is None:\n",
    "        return {}\n",
    "\n",
    "    candidates = []\n",
    "    if isinstance(out, dict):\n",
    "        candidates.append(out)\n",
    "        for k in [\"summary\", \"metrics\", \"scores\", \"eval\", \"results\", \"extra_json\"]:\n",
    "            if k in out and isinstance(out[k], dict):\n",
    "                candidates.append(out[k])\n",
    "\n",
    "    merged = {}\n",
    "    for c in candidates:\n",
    "        merged.update(c)\n",
    "\n",
    "    flat = {}\n",
    "    for k, v in merged.items():\n",
    "        if isinstance(v, (int, float, np.integer, np.floating)) and np.isfinite(v):\n",
    "            flat[k] = float(v)\n",
    "        elif isinstance(v, (list, tuple, np.ndarray)) and len(v) == 1:\n",
    "            try:\n",
    "                flat[k] = float(v[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return flat\n",
    "\n",
    "def _maybe_add_ari_nmi(row_metrics, out, labels_test, *, k=None, seed=0):\n",
    "    \"\"\"\n",
    "    If the method didn't already report ARI/NMI, compute from Z_fused on test.\n",
    "    Expects out[\"Z_fused\"] as (cells x d) with rownames aligned to full data,\n",
    "    OR already subset to test in the runner.\n",
    "    \"\"\"\n",
    "    if (\"fused_kmeans_ari_test\" in row_metrics) and (\"fused_kmeans_nmi_test\" in row_metrics):\n",
    "        return row_metrics\n",
    "\n",
    "    if not (isinstance(out, dict) and (\"Z_fused\" in out) and (out[\"Z_fused\"] is not None)):\n",
    "        return row_metrics\n",
    "\n",
    "    Z = out[\"Z_fused\"]\n",
    "    # handle pandas / numpy\n",
    "    if hasattr(Z, \"values\"):\n",
    "        Z = Z.values\n",
    "    Z = np.asarray(Z)\n",
    "\n",
    "    if Z.ndim != 2 or Z.shape[0] != len(labels_test):\n",
    "        # If runner returned full Z for all cells, you can adapt here,\n",
    "        # but we can't guess mapping safely without names.\n",
    "        return row_metrics\n",
    "\n",
    "    if k is None:\n",
    "        k = len(np.unique(labels_test))\n",
    "    k = int(max(2, k))\n",
    "\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=int(seed))\n",
    "    pred = km.fit_predict(Z)\n",
    "\n",
    "    row_metrics[\"fused_kmeans_ari_test\"] = float(adjusted_rand_score(labels_test, pred))\n",
    "    row_metrics[\"fused_kmeans_nmi_test\"] = float(normalized_mutual_info_score(labels_test, pred))\n",
    "    return row_metrics\n",
    "\n",
    "# -----------------------------\n",
    "# fold builder: train/val/test indices\n",
    "# -----------------------------\n",
    "def make_folds(labels_all, n_splits=5, seed=0):\n",
    "    labels_all = np.asarray(labels_all)\n",
    "    skf = StratifiedKFold(n_splits=int(n_splits), shuffle=True, random_state=int(seed))\n",
    "\n",
    "    for fold, (fit_idx, test_idx) in enumerate(skf.split(np.zeros_like(labels_all), labels_all)):\n",
    "        # carve val out of fit_idx\n",
    "        y_fit = labels_all[fit_idx]\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=int(seed + 10_000 + fold))\n",
    "        tr_rel, va_rel = next(sss.split(np.zeros_like(y_fit), y_fit))\n",
    "        train_idx = fit_idx[tr_rel]\n",
    "        val_idx   = fit_idx[va_rel]\n",
    "\n",
    "        yield fold, {\n",
    "            \"train\": train_idx.tolist(),\n",
    "            \"val\":   val_idx.tolist(),\n",
    "            \"test\":  test_idx.tolist(),\n",
    "            \"unused\": []\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# your \"train-fit preprocessing\" wrapper\n",
    "# -----------------------------\n",
    "def drop_first_lsi(atac_lsi: ad.AnnData) -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Given atac_lsi with shape (cells, 101), return a new AnnData with shape (cells, 100)\n",
    "    containing cols 1..100 (i.e. LSI 2-101 in 1-based terms).\n",
    "    \"\"\"\n",
    "    X = np.asarray(atac_lsi.X)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f\"Expected atac_lsi.X to be 2D, got {X.shape}\")\n",
    "    if X.shape[1] < 2:\n",
    "        raise ValueError(f\"Need at least 2 LSI dims to drop first, got {X.shape[1]}\")\n",
    "\n",
    "    X2 = X[:, 1:].astype(np.float32, copy=False)\n",
    "\n",
    "    # keep obs, new var names\n",
    "    out = ad.AnnData(\n",
    "        X=X2,\n",
    "        obs=atac_lsi.obs.copy(),\n",
    "        var=pd.DataFrame(index=[f\"LSI_{i}\" for i in range(1, X2.shape[1] + 1)]),\n",
    "    )\n",
    "    return out\n",
    "    \n",
    "def rebuild_shared_inputs_for_split(seed: int, splits: dict):\n",
    "    shared = build_shared_inputs(\n",
    "        rna, atac, splits,\n",
    "        seed=int(seed),\n",
    "        **PREP_KWS,\n",
    "    )\n",
    "\n",
    "    globals()[\"shared\"] = shared\n",
    "\n",
    "    # ---- keep everything you already export ----\n",
    "    globals()[\"rna_counts_hvg\"]      = shared[\"rna_counts_hvg\"]\n",
    "    globals()[\"rna_log_hvg\"]         = shared[\"rna_log_hvg\"]\n",
    "    globals()[\"atac_counts_bin\"]     = shared[\"atac_counts_bin\"]\n",
    "    globals()[\"atac_counts_bin_hv\"]  = shared[\"atac_counts_bin_hv\"]\n",
    "\n",
    "    # ---- ATAC LSI: compute 101, then make drop-first variant ----\n",
    "    atac_lsi_101 = shared[\"atac_lsi\"]            # shape: (cells, 101)\n",
    "    atac_lsi_drop1 = drop_first_lsi(atac_lsi_101)  # shape: (cells, 100)\n",
    "\n",
    "    # store both for debugging / reuse\n",
    "    globals()[\"atac_lsi_101\"] = atac_lsi_101\n",
    "    globals()[\"atac_lsi_drop1\"] = atac_lsi_drop1\n",
    "\n",
    "    # IMPORTANT: choose which one each method family should see\n",
    "    # - UniVI / MultiMAP: use drop-first (LSI 2-101)\n",
    "    # - If any method truly wants the raw 101, point it to atac_lsi_101 instead\n",
    "    globals()[\"atac_lsi\"] = atac_lsi_drop1  # <-- default global LSI used by many thunks\n",
    "\n",
    "    # method-specific \"shared inputs\" wiring (same as your original intent)\n",
    "    globals()[\"rna_univi\"]     = globals()[\"rna_log_hvg\"]\n",
    "    globals()[\"atac_univi\"]    = globals()[\"atac_lsi_drop1\"]     # <-- drop first\n",
    "\n",
    "    globals()[\"rna_multimap\"]  = globals()[\"rna_log_hvg\"]\n",
    "    globals()[\"atac_multimap\"] = globals()[\"atac_lsi_drop1\"]     # <-- drop first\n",
    "\n",
    "    # methods that use peaks/binary matrices are unchanged\n",
    "    globals()[\"atac_multivi\"]  = globals()[\"atac_counts_bin_hv\"]\n",
    "    globals()[\"atac_peakvi\"]   = globals()[\"atac_counts_bin_hv\"]\n",
    "\n",
    "    return shared\n",
    "    \n",
    "\n",
    "# -----------------------------\n",
    "# main CV sweep\n",
    "# -----------------------------\n",
    "def run_cv_sweep(\n",
    "    *,\n",
    "    seeds,\n",
    "    n_folds,\n",
    "    out_dir,\n",
    "    methods,\n",
    "    labels_all,\n",
    "    add_ari_nmi_if_missing=True,\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rows = []\n",
    "    raw_out_path = out_dir / \"raw_outputs.jsonl\"\n",
    "\n",
    "    with raw_out_path.open(\"w\") as f_jsonl:\n",
    "        for seed in seeds:\n",
    "            # (optional) set seeds globally if you want determinism\n",
    "            np.random.seed(int(seed))\n",
    "            try:\n",
    "                import random\n",
    "                random.seed(int(seed))\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                import torch\n",
    "                torch.manual_seed(int(seed))\n",
    "                torch.cuda.manual_seed_all(int(seed))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            for fold, splits in make_folds(labels_all, n_splits=n_folds, seed=seed):\n",
    "                globals()[\"RNG_SEED\"] = int(seed)\n",
    "                globals()[\"splits\"] = splits\n",
    "\n",
    "                # IMPORTANT: rebuild train-fit preprocessing for THIS fold\n",
    "                rebuild_shared_inputs_for_split(seed, splits)\n",
    "\n",
    "                print(f\"\\n=== seed={seed} fold={fold} sizes ===\", {k: len(v) for k, v in splits.items()})\n",
    "\n",
    "                for method_name, thunk in methods.items():\n",
    "                    t0 = time.time()\n",
    "                    status, err, out = \"ok\", None, None\n",
    "\n",
    "                    try:\n",
    "                        out = thunk()\n",
    "                    except Exception as e:\n",
    "                        status = \"fail\"\n",
    "                        err = repr(e)\n",
    "\n",
    "                    dt = time.time() - t0\n",
    "                    metrics = _extract_metrics(out)\n",
    "\n",
    "                    # optional: compute ARI/NMI if not present AND runner returned Z_fused for test\n",
    "                    if status == \"ok\" and add_ari_nmi_if_missing:\n",
    "                        labels_test = np.asarray(labels_all)[splits[\"test\"]]\n",
    "                        metrics = _maybe_add_ari_nmi(metrics, out, labels_test, seed=seed)\n",
    "\n",
    "                    row = {\n",
    "                        \"seed\": int(seed),\n",
    "                        \"fold\": int(fold),\n",
    "                        \"method\": str(method_name),\n",
    "                        \"status\": status,\n",
    "                        \"seconds\": float(dt),\n",
    "                        \"error\": err,\n",
    "                        \"n_train\": int(len(splits[\"train\"])),\n",
    "                        \"n_val\": int(len(splits[\"val\"])),\n",
    "                        \"n_test\": int(len(splits[\"test\"])),\n",
    "                        **metrics,\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "\n",
    "                    f_jsonl.write(json.dumps({\n",
    "                        \"seed\": int(seed),\n",
    "                        \"fold\": int(fold),\n",
    "                        \"method\": str(method_name),\n",
    "                        \"status\": status,\n",
    "                        \"seconds\": float(dt),\n",
    "                        \"error\": err,\n",
    "                        \"metrics\": metrics,\n",
    "                        \"out_keys\": sorted(list(out.keys())) if isinstance(out, dict) else None,\n",
    "                    }) + \"\\n\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_dir / \"cv_sweep_long.csv\", index=False)\n",
    "\n",
    "    ok = df[df[\"status\"] == \"ok\"].copy()\n",
    "    house = {\"seed\",\"fold\",\"method\",\"status\",\"seconds\",\"error\",\"n_train\",\"n_val\",\"n_test\"}\n",
    "    metric_cols = [c for c in ok.columns if c not in house and pd.api.types.is_numeric_dtype(ok[c])]\n",
    "\n",
    "    if metric_cols:\n",
    "        # summarize over (seed,fold) replicates\n",
    "        summ = (ok.groupby(\"method\")[metric_cols]\n",
    "                  .agg([\"mean\",\"std\",\"median\",\"count\"])\n",
    "                  .sort_values((metric_cols[0], \"mean\"), ascending=False))\n",
    "        summ.to_csv(out_dir / \"cv_sweep_summary.csv\")\n",
    "    else:\n",
    "        summ = ok.groupby(\"method\").size().to_frame(\"n_ok\")\n",
    "        summ.to_csv(out_dir / \"cv_sweep_summary.csv\")\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", out_dir / \"cv_sweep_long.csv\")\n",
    "    print(\" -\", out_dir / \"cv_sweep_summary.csv\")\n",
    "    print(\" -\", raw_out_path)\n",
    "\n",
    "    return df, summ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc65c1-153c-4469-b717-2da53c019e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_Zmod(out):\n",
    "    Z_mod = out.get(\"Z_atac\", None)\n",
    "    if Z_mod is None:\n",
    "        Z_mod = out.get(\"Z_adt\", None)\n",
    "    return Z_mod\n",
    "\n",
    "def _pair_test_metrics(out, labels_all, splits, *, metric=\"euclidean\", recall_ks=(1,10,25,50,100),\n",
    "                       max_pair_n=3000, seed=0, block=512):\n",
    "    \"\"\"\n",
    "    Test-only paired retrieval metrics + permutation sanity control.\n",
    "    Returns keys prefixed with \"TEST/\" and \"PERM/\".\n",
    "    \"\"\"\n",
    "    M = {}\n",
    "    if not isinstance(out, dict):\n",
    "        return M\n",
    "\n",
    "    Z_rna = out.get(\"Z_rna\", None)\n",
    "    Z_mod = _get_Zmod(out)\n",
    "    if Z_rna is None or Z_mod is None:\n",
    "        return M\n",
    "\n",
    "    Z_rna = np.asarray(Z_rna, np.float32)\n",
    "    Z_mod = np.asarray(Z_mod, np.float32)\n",
    "    n = len(labels_all)\n",
    "\n",
    "    if Z_rna.shape[0] != n or Z_mod.shape[0] != n:\n",
    "        # can't trust pairing by index\n",
    "        M[\"TEST/pair_shape_mismatch\"] = 1.0\n",
    "        M[\"TEST/n\"] = float(min(Z_rna.shape[0], Z_mod.shape[0]))\n",
    "        return M\n",
    "\n",
    "    te = np.asarray(splits[\"test\"], dtype=int)\n",
    "    Z1 = Z_rna[te]\n",
    "    Z2 = Z_mod[te]\n",
    "    m = np.isfinite(Z1).all(1) & np.isfinite(Z2).all(1)\n",
    "    Z1 = Z1[m]; Z2 = Z2[m]\n",
    "    n_te = Z1.shape[0]\n",
    "    M[\"TEST/n\"] = float(n_te)\n",
    "\n",
    "    if n_te < 10:\n",
    "        return M\n",
    "\n",
    "    # true test pairing metrics\n",
    "    trueM = pair_ranking_metrics_exact(\n",
    "        Z1, Z2, metric=metric, ks=recall_ks, max_n=max_pair_n, seed=seed, block=block\n",
    "    )\n",
    "    for k, v in trueM.items():\n",
    "        M[f\"TEST/{k}\"] = v\n",
    "\n",
    "    # permutation control (should ~chance)\n",
    "    rng = np.random.default_rng(int(seed) + 999)\n",
    "    perm = rng.permutation(n_te)\n",
    "    shufM = pair_ranking_metrics_exact(\n",
    "        Z1, Z2[perm], metric=metric, ks=recall_ks, max_n=max_pair_n, seed=seed, block=block\n",
    "    )\n",
    "    for k, v in shufM.items():\n",
    "        M[f\"PERM/{k}\"] = v\n",
    "\n",
    "    # quick “is permutation near chance?” flag for Recall@1\n",
    "    # chance ~ 1/n_eval\n",
    "    n_eval = shufM.get(\"FOSCTTM_n_eval\", n_te)\n",
    "    if n_eval and n_eval > 0:\n",
    "        chance_r1 = 1.0 / float(n_eval)\n",
    "        perm_r1 = shufM.get(\"FOSCTTM_Recall@1\", np.nan)\n",
    "        M[\"PERM/chance_Recall@1\"] = float(chance_r1)\n",
    "        M[\"PERM/Recall@1_over_chance\"] = float(perm_r1 / chance_r1) if np.isfinite(perm_r1) else np.nan\n",
    "\n",
    "    return M\n",
    "\n",
    "def _pair_id_alignment_check(out):\n",
    "    \"\"\"\n",
    "    If a method returns per-modality ids, report match rate.\n",
    "    Looks for common key names; you can expand this list.\n",
    "    \"\"\"\n",
    "    if not isinstance(out, dict):\n",
    "        return {}\n",
    "    keysets = [\n",
    "        (\"rna_ids\", \"atac_ids\"),\n",
    "        (\"rna_ids\", \"adt_ids\"),\n",
    "        (\"obs_names_rna\", \"obs_names_atac\"),\n",
    "        (\"obs_names_rna\", \"obs_names_adt\"),\n",
    "    ]\n",
    "    for kr, km in keysets:\n",
    "        if (kr in out) and (km in out):\n",
    "            r = np.asarray(out[kr]).astype(str)\n",
    "            m = np.asarray(out[km]).astype(str)\n",
    "            if r.shape == m.shape and r.size > 0:\n",
    "                return {\"PAIR/id_match_rate\": float(np.mean(r == m)), \"PAIR/id_n\": int(r.size)}\n",
    "            return {\"PAIR/id_shape_mismatch\": 1.0}\n",
    "    return {}  # silently skip if ids absent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d31dd0-11fb-4975-a7c3-039fec24a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_preproc_on_train(\n",
    "    rna, atac, train_idx, *,\n",
    "    rna_counts_layer=\"counts\",\n",
    "    atac_counts_layer=\"counts\",\n",
    "    n_hvg=2000,\n",
    "    target_sum=1e4,\n",
    "    n_lsi=101,\n",
    "    n_peaks_multivi=4002,\n",
    "    dr_min=0.01,\n",
    "    dr_max=0.30,\n",
    "    seed=0,\n",
    "):\n",
    "    # RNA HVGs (TRAIN only)\n",
    "    hvg = fit_hvgs_on_train(rna, train_idx, counts_layer=rna_counts_layer, n_hvg=n_hvg, seed=seed)\n",
    "\n",
    "    # ATAC TFIDF+SVD(+scaler) (TRAIN only)\n",
    "    tfidf, svd, scaler = fit_atac_lsi_on_train(\n",
    "        atac, train_idx, counts_layer=atac_counts_layer, n_lsi=n_lsi, seed=seed,\n",
    "        do_l2_norm=False, do_scale=True,\n",
    "    )\n",
    "\n",
    "    # ATAC peak selection (TRAIN only)\n",
    "    peaks = fit_peaks_by_detection_window_on_train(\n",
    "        atac, train_idx,\n",
    "        counts_layer=atac_counts_layer,\n",
    "        dr_min=dr_min,\n",
    "        dr_max=dr_max,\n",
    "        n_peaks=n_peaks_multivi,\n",
    "        prefer_var=\"bernoulli\",\n",
    "    )\n",
    "\n",
    "    return dict(hvg=hvg, tfidf=tfidf, svd=svd, scaler=scaler, peaks=peaks)\n",
    "\n",
    "\n",
    "def transform_with_preproc(\n",
    "    rna, atac, artifacts, *,\n",
    "    rna_counts_layer=\"counts\",\n",
    "    atac_counts_layer=\"counts\",\n",
    "    target_sum=1e4,\n",
    "    n_lsi=101,\n",
    "    subset_chr=True,\n",
    "):\n",
    "    hvg   = artifacts[\"hvg\"]\n",
    "    tfidf = artifacts[\"tfidf\"]\n",
    "    svd   = artifacts[\"svd\"]\n",
    "    scaler= artifacts[\"scaler\"]\n",
    "    peaks = artifacts[\"peaks\"]\n",
    "\n",
    "    # RNA counts subset (HVG)\n",
    "    rna = ensure_counts_layer(rna, layer=rna_counts_layer)\n",
    "    rna_counts_hvg = rna[:, hvg].copy()\n",
    "    rna_counts_hvg.X = rna_counts_hvg.layers[rna_counts_layer].copy()\n",
    "\n",
    "    # RNA log subset (HVG)\n",
    "    rna_log_hvg = transform_rna_log_hvg(rna, hvg, counts_layer=rna_counts_layer, target_sum=target_sum)\n",
    "\n",
    "    # ATAC LSI (ALL using train-fit tfidf/svd/scaler)\n",
    "    atac_lsi = transform_atac_lsi(atac, tfidf, svd, scaler, counts_layer=atac_counts_layer, n_lsi=n_lsi)\n",
    "\n",
    "    # ATAC binarized (ALL) + selected peaks (from train)\n",
    "    atac = ensure_counts_layer(atac, layer=atac_counts_layer)\n",
    "    atac_counts_bin = atac.copy()\n",
    "    X = _to_csr_float32(atac_counts_bin.layers[atac_counts_layer])\n",
    "    atac_counts_bin.X = binarize_csr(X)\n",
    "\n",
    "    atac_counts_bin_hv = atac_counts_bin[:, peaks].copy()\n",
    "    if subset_chr:\n",
    "        atac_counts_bin_hv = subset_to_chr_features(atac_counts_bin_hv)\n",
    "\n",
    "    return dict(\n",
    "        rna_counts_hvg=rna_counts_hvg,\n",
    "        rna_log_hvg=rna_log_hvg,\n",
    "        atac_lsi=atac_lsi,\n",
    "        atac_counts_bin=atac_counts_bin,\n",
    "        atac_counts_bin_hv=atac_counts_bin_hv,\n",
    "        # also return artifacts for debugging\n",
    "        hvg=hvg, peaks=peaks, tfidf=tfidf, svd=svd, scaler=scaler,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbbae1b-cc68-49e5-96ec-32a7190b709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Seed-sweep robustness + full metric suite (Multiome-ready)\n",
    "# - varies splits across seeds (shared across methods)\n",
    "# - rebuilds any preprocessing fit-on-train per fold\n",
    "# - does NOT change run_* methods: uses globals RNG_SEED/splits/shared inputs\n",
    "# - computes: FOSCTTM mean, MRR, Recall@1/10, label transfer acc/macroF1,\n",
    "#             mixing score, fused kNN acc/macroF1/balanced acc,\n",
    "#             fused silhouette, label purity, kmeans ARI/NMI, fit seconds\n",
    "# ============================================================\n",
    "\n",
    "import time, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, balanced_accuracy_score,\n",
    "    adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 0) configure the sweep\n",
    "# -----------------------------\n",
    "SEEDS = list(range(5))\n",
    "SWEEP_TAG = \"seed_sweep_multiome_fullmetrics\"\n",
    "\n",
    "OUT_DIR = (Path(WORK) / \"runs\" / SWEEP_TAG) if \"WORK\" in globals() else (Path(\"./runs\") / SWEEP_TAG)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SPLIT_KWS = dict(\n",
    "    train_frac=0.8,\n",
    "    val_frac=0.1,\n",
    ")\n",
    "\n",
    "PREP_KWS = dict(\n",
    "    rna_counts_layer=\"counts\",\n",
    "    atac_counts_layer=\"counts\",\n",
    "    n_hvg=2000,\n",
    "    target_sum=1e4,\n",
    "    n_lsi=101,\n",
    "    n_hvpeaks_multivi=4002,\n",
    ")\n",
    "\n",
    "# evaluation hyperparams\n",
    "EVAL_KWS = dict(\n",
    "    metric=\"euclidean\",                # \"euclidean\" or \"cosine\"\n",
    "    k_label=15,                        # kNN for label transfer + fused kNN\n",
    "    k_mixing=30,                       # kNN for mixing fraction\n",
    "    recall_ks=(1, 10, 25, 50, 100),    # Recall@1/10/25/50/100\n",
    "    max_pair_n=3000,                   # subsample size for exact pair retrieval metrics\n",
    "    max_sil_n=3000,                    # subsample size for silhouette\n",
    "    seed=0,\n",
    "    block=512,                         # block size for pairwise ranking\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) optional: seed everything\n",
    "# -----------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    np.random.seed(int(seed))\n",
    "    try:\n",
    "        import random\n",
    "        random.seed(int(seed))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(int(seed))\n",
    "        torch.cuda.manual_seed_all(int(seed))\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "        \n",
    "\n",
    "# -----------------------------\n",
    "# 2) fold builder (train/val/test)\n",
    "# -----------------------------\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "def make_folds(labels_all, n_splits=5, seed=0, train_frac=0.8, val_frac=0.1, test_frac=0.1):\n",
    "    \"\"\"\n",
    "    Repeated stratified holdout splits with fractions of TOTAL data.\n",
    "    n_splits = number of repeats (what you currently call folds).\n",
    "    \"\"\"\n",
    "    labels_all = np.asarray(labels_all)\n",
    "    n = len(labels_all)\n",
    "\n",
    "    train_frac = float(train_frac)\n",
    "    val_frac   = float(val_frac)\n",
    "    test_frac  = float(test_frac)\n",
    "    if abs((train_frac + val_frac + test_frac) - 1.0) > 1e-8:\n",
    "        raise ValueError(\"train_frac + val_frac + test_frac must equal 1.\")\n",
    "\n",
    "    # 1) split off train vs (val+test)\n",
    "    sss1 = StratifiedShuffleSplit(\n",
    "        n_splits=int(n_splits),\n",
    "        test_size=(val_frac + test_frac),\n",
    "        random_state=int(seed),\n",
    "    )\n",
    "\n",
    "    for fold, (train_idx, temp_idx) in enumerate(sss1.split(np.zeros(n), labels_all)):\n",
    "        y_temp = labels_all[temp_idx]\n",
    "\n",
    "        # 2) split temp into val vs test (stratified)\n",
    "        # val is val_frac / (val_frac + test_frac) of temp\n",
    "        val_prop_of_temp = val_frac / (val_frac + test_frac)\n",
    "\n",
    "        sss2 = StratifiedShuffleSplit(\n",
    "            n_splits=1,\n",
    "            test_size=(1.0 - val_prop_of_temp),   # portion of temp that becomes test\n",
    "            random_state=int(seed + 10_000 + fold),\n",
    "        )\n",
    "        val_rel, test_rel = next(sss2.split(np.zeros(len(temp_idx)), y_temp))\n",
    "\n",
    "        val_idx  = temp_idx[val_rel]\n",
    "        test_idx = temp_idx[test_rel]\n",
    "\n",
    "        yield fold, {\n",
    "            \"train\": train_idx.tolist(),\n",
    "            \"val\":   val_idx.tolist(),\n",
    "            \"test\":  test_idx.tolist(),\n",
    "            \"unused\": []\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) rebuild preprocessing per fold (your wrapper)\n",
    "# -----------------------------\n",
    "def _call_fit_preproc_on_train_safe(fit_preproc_on_train, rna, atac, tr, *, seed, prep_kws, **extra):\n",
    "    \"\"\"\n",
    "    Calls fit_preproc_on_train but removes keys it doesn't accept.\n",
    "    Also maps common alias names.\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "\n",
    "    kws = dict(prep_kws) if prep_kws is not None else {}\n",
    "\n",
    "    # ---- alias mapping (if you store alt names in PREP_KWS) ----\n",
    "    # If PREP_KWS uses n_hvpeaks_multivi, map it to the argument your function expects.\n",
    "    # If your fit_preproc_on_train expects n_peaks_multivi, this will work.\n",
    "    if \"n_hvpeaks_multivi\" in kws and \"n_peaks_multivi\" not in kws:\n",
    "        kws[\"n_peaks_multivi\"] = kws.pop(\"n_hvpeaks_multivi\")\n",
    "\n",
    "    # ---- remove any keys not in signature ----\n",
    "    sig = inspect.signature(fit_preproc_on_train)\n",
    "    allowed = set(sig.parameters.keys())\n",
    "\n",
    "    # keep only kwargs that are accepted (or function has **kwargs)\n",
    "    has_var_kw = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values())\n",
    "    if not has_var_kw:\n",
    "        kws = {k: v for k, v in kws.items() if k in allowed}\n",
    "\n",
    "    # extra explicit overrides win\n",
    "    kws.update(extra)\n",
    "\n",
    "    return fit_preproc_on_train(rna, atac, tr, seed=int(seed), **kws)\n",
    "\n",
    "\n",
    "def rebuild_shared_inputs_for_split(seed: int, splits: dict):\n",
    "    tr = np.asarray(splits[\"train\"], dtype=int)\n",
    "\n",
    "    artifacts = _call_fit_preproc_on_train_safe(\n",
    "        fit_preproc_on_train,\n",
    "        rna, atac, tr,\n",
    "        seed=int(seed),\n",
    "        prep_kws=PREP_KWS,\n",
    "        # your extra knobs here (only if fit_preproc_on_train supports them)\n",
    "        dr_min=0.01,\n",
    "        dr_max=0.30,\n",
    "        n_peaks_multivi=PREP_KWS.get(\"n_hvpeaks_multivi\", 4002),  # still fine\n",
    "    )\n",
    "\n",
    "    shared = transform_with_preproc(\n",
    "        rna, atac, artifacts,\n",
    "        target_sum=PREP_KWS.get(\"target_sum\", 1e4),\n",
    "        subset_chr=True,\n",
    "    )\n",
    "\n",
    "    globals()[\"shared\"] = shared\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Metric helpers (FULL suite)\n",
    "# ============================================================\n",
    "def _is_labeled(y):\n",
    "    y = np.asarray(y).astype(str)\n",
    "    bad = (y == \"nan\") | (y == \"None\") | (y == \"NA\") | (y == \"\")\n",
    "    return ~bad\n",
    "\n",
    "\n",
    "def _subsample_idx(n, max_n, seed=0):\n",
    "    if (max_n is None) or (n <= max_n):\n",
    "        return np.arange(n, dtype=int)\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    return rng.choice(n, size=int(max_n), replace=False)\n",
    "\n",
    "\n",
    "def _dist_block(A, B, metric=\"euclidean\"):\n",
    "    A = np.asarray(A, np.float32)\n",
    "    B = np.asarray(B, np.float32)\n",
    "    if metric == \"euclidean\":\n",
    "        # squared euclidean; ranking invariant vs euclidean\n",
    "        A2 = (A * A).sum(1, keepdims=True)              # (a,1)\n",
    "        B2 = (B * B).sum(1, keepdims=True).T            # (1,b)\n",
    "        D2 = A2 + B2 - 2.0 * (A @ B.T)\n",
    "        return np.maximum(D2, 0.0)\n",
    "    elif metric == \"cosine\":\n",
    "        An = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-8)\n",
    "        Bn = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-8)\n",
    "        # cosine distance = 1 - cos sim\n",
    "        return 1.0 - (An @ Bn.T)\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'euclidean' or 'cosine'\")\n",
    "\n",
    "\n",
    "def pair_ranking_metrics_exact(Z1, Z2, *, metric=\"euclidean\", ks=(1,10), max_n=3000, seed=0, block=512):\n",
    "    \"\"\"\n",
    "    Exact (on subsample) FOSCTTM mean, MRR, Recall@k based on rank of true pair.\n",
    "    Z1[i] paired with Z2[i].\n",
    "    Uses blockwise distance comparisons; does NOT materialize full n×n matrix.\n",
    "    \"\"\"\n",
    "    Z1 = np.asarray(Z1, np.float32)\n",
    "    Z2 = np.asarray(Z2, np.float32)\n",
    "    assert Z1.shape == Z2.shape\n",
    "    n0 = Z1.shape[0]\n",
    "\n",
    "    idx = _subsample_idx(n0, max_n, seed=seed)\n",
    "    Z1 = Z1[idx]; Z2 = Z2[idx]\n",
    "    n = Z1.shape[0]\n",
    "\n",
    "    # true distances (diag)\n",
    "    true_d = np.empty(n, dtype=np.float32)\n",
    "    for i in range(0, n, block):\n",
    "        sl = slice(i, min(n, i+block))\n",
    "        D = _dist_block(Z1[sl], Z2[sl], metric=metric)\n",
    "        true_d[sl] = np.diag(D)\n",
    "\n",
    "    # count how many targets are closer than the true pair for each query\n",
    "    closer = np.zeros(n, dtype=np.int32)\n",
    "\n",
    "    for j in range(0, n, block):\n",
    "        Bj = Z2[j:min(n, j+block)]\n",
    "        D = _dist_block(Z1, Bj, metric=metric)  # (n, b)\n",
    "        closer += (D < true_d[:, None]).sum(axis=1).astype(np.int32)\n",
    "\n",
    "    rank = closer + 1  # 1-based rank\n",
    "    foscttm = (rank - 1) / (n - 1 if n > 1 else 1)\n",
    "    mrr = (1.0 / rank).mean()\n",
    "\n",
    "    out = {\n",
    "        \"FOSCTTM_mean\": float(foscttm.mean()),   # ↓ better\n",
    "        \"FOSCTTM_MRR\": float(mrr),               # ↑ better\n",
    "        \"FOSCTTM_n_eval\": int(n),\n",
    "    }\n",
    "    for k in ks:\n",
    "        out[f\"FOSCTTM_Recall@{int(k)}\"] = float((rank <= int(k)).mean())\n",
    "    return out\n",
    "\n",
    "\n",
    "def label_transfer_knn(Z_src, y_src, Z_tgt, y_tgt, *, k=15, metric=\"euclidean\"):\n",
    "    y_src = np.asarray(y_src).astype(str)\n",
    "    y_tgt = np.asarray(y_tgt).astype(str)\n",
    "\n",
    "    msrc = _is_labeled(y_src) & np.isfinite(Z_src).all(1)\n",
    "    mtgt = _is_labeled(y_tgt) & np.isfinite(Z_tgt).all(1)\n",
    "    if msrc.sum() < 10 or mtgt.sum() < 10:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    Zs = np.asarray(Z_src[msrc], np.float32)\n",
    "    Zt = np.asarray(Z_tgt[mtgt], np.float32)\n",
    "    ys = y_src[msrc]\n",
    "    yt = y_tgt[mtgt]\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=int(k), metric=metric)\n",
    "    nn.fit(Zs)\n",
    "    ind = nn.kneighbors(Zt, return_distance=False)\n",
    "\n",
    "    preds = []\n",
    "    for nbrs in ind:\n",
    "        labs = ys[nbrs]\n",
    "        vals, counts = np.unique(labs, return_counts=True)\n",
    "        preds.append(vals[np.argmax(counts)])\n",
    "    preds = np.asarray(preds, dtype=str)\n",
    "\n",
    "    acc = float(accuracy_score(yt, preds))\n",
    "    f1  = float(f1_score(yt, preds, average=\"macro\"))\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "def mixing_score(Z, domain, *, k=30, metric=\"euclidean\"):\n",
    "    Z = np.asarray(Z, np.float32)\n",
    "    domain = np.asarray(domain).astype(str)\n",
    "\n",
    "    if Z.shape[0] < (k + 5):\n",
    "        return np.nan, {}\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=int(k)+1, metric=metric)\n",
    "    nn.fit(Z)\n",
    "    ind = nn.kneighbors(Z, return_distance=False)[:, 1:]\n",
    "    dom_nbr = domain[ind]\n",
    "    frac_other = (dom_nbr != domain[:, None]).mean(axis=1)\n",
    "\n",
    "    per_dom = {d: float(frac_other[domain == d].mean()) for d in np.unique(domain)}\n",
    "    return float(frac_other.mean()), per_dom\n",
    "\n",
    "\n",
    "def fused_knn_metrics(Z, y, train_idx, test_idx, *, k=15, metric=\"euclidean\"):\n",
    "    Z = np.asarray(Z, np.float32)\n",
    "    y = np.asarray(y).astype(str)\n",
    "    tr = np.asarray(train_idx, dtype=int)\n",
    "    te = np.asarray(test_idx, dtype=int)\n",
    "\n",
    "    mtr = _is_labeled(y[tr]) & np.isfinite(Z[tr]).all(1)\n",
    "    mte = _is_labeled(y[te]) & np.isfinite(Z[te]).all(1)\n",
    "    if mtr.sum() < 20 or mte.sum() < 20:\n",
    "        return {\"Fused kNN acc\": np.nan, \"Fused kNN macroF1\": np.nan, \"Fused kNN balanced acc\": np.nan}\n",
    "\n",
    "    Ztr, ytr = Z[tr][mtr], y[tr][mtr]\n",
    "    Zte, yte = Z[te][mte], y[te][mte]\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=int(k), metric=metric)\n",
    "    nn.fit(Ztr)\n",
    "    ind = nn.kneighbors(Zte, return_distance=False)\n",
    "\n",
    "    preds = []\n",
    "    for nbrs in ind:\n",
    "        labs = ytr[nbrs]\n",
    "        vals, counts = np.unique(labs, return_counts=True)\n",
    "        preds.append(vals[np.argmax(counts)])\n",
    "    preds = np.asarray(preds, dtype=str)\n",
    "\n",
    "    return {\n",
    "        \"Fused kNN acc\": float(accuracy_score(yte, preds)),\n",
    "        \"Fused kNN macroF1\": float(f1_score(yte, preds, average=\"macro\")),\n",
    "        \"Fused kNN balanced acc\": float(balanced_accuracy_score(yte, preds)),\n",
    "    }\n",
    "    \n",
    "\n",
    "def clustering_metrics(Z, y, *, seed=0, max_sil_n=3000):\n",
    "    Z = np.asarray(Z, np.float32)\n",
    "    y = np.asarray(y).astype(str)\n",
    "    m = _is_labeled(y) & np.isfinite(Z).all(1)\n",
    "    Z, y = Z[m], y[m]\n",
    "    if Z.shape[0] < 50 or len(np.unique(y)) < 2:\n",
    "        return {\n",
    "            \"Fused silhouette\": np.nan,\n",
    "            \"Fused label purity\": np.nan,\n",
    "            \"Fused k-means ARI\": np.nan,\n",
    "            \"Fused k-means NMI\": np.nan,\n",
    "        }\n",
    "\n",
    "    k = max(2, len(np.unique(y)))\n",
    "    km = KMeans(n_clusters=int(k), n_init=10, random_state=int(seed))\n",
    "    cl = km.fit_predict(Z)\n",
    "\n",
    "    # purity\n",
    "    purity = 0.0\n",
    "    for c in np.unique(cl):\n",
    "        labs = y[cl == c]\n",
    "        vals, counts = np.unique(labs, return_counts=True)\n",
    "        purity += counts.max()\n",
    "    purity = float(purity / len(y))\n",
    "\n",
    "    ari = float(adjusted_rand_score(y, cl))\n",
    "    nmi = float(normalized_mutual_info_score(y, cl))\n",
    "\n",
    "    # silhouette on subsample\n",
    "    idx = _subsample_idx(len(y), max_sil_n, seed=seed)\n",
    "    sil = float(silhouette_score(Z[idx], y[idx], metric=\"euclidean\"))\n",
    "\n",
    "    return {\n",
    "        \"Fused silhouette\": sil,\n",
    "        \"Fused label purity\": purity,\n",
    "        \"Fused k-means ARI\": ari,\n",
    "        \"Fused k-means NMI\": nmi,\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_metrics(out):\n",
    "    \"\"\"Keep your old behavior: pull numeric scalars from out or out['metrics']/etc.\"\"\"\n",
    "    if out is None:\n",
    "        return {}\n",
    "    candidates = []\n",
    "    if isinstance(out, dict):\n",
    "        candidates.append(out)\n",
    "        for k in [\"summary\", \"metrics\", \"scores\", \"eval\", \"results\", \"extra_json\"]:\n",
    "            if k in out and isinstance(out[k], dict):\n",
    "                candidates.append(out[k])\n",
    "    merged = {}\n",
    "    for c in candidates:\n",
    "        merged.update(c)\n",
    "    flat = {}\n",
    "    for k, v in merged.items():\n",
    "        if isinstance(v, (int, float, np.integer, np.floating)) and np.isfinite(v):\n",
    "            flat[k] = float(v)\n",
    "        elif isinstance(v, (list, tuple, np.ndarray)) and len(v) == 1:\n",
    "            try:\n",
    "                flat[k] = float(v[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return flat\n",
    "\n",
    "\n",
    "def evaluate_out_full(\n",
    "    out, labels_all, splits, *,\n",
    "    metric=\"euclidean\", k_label=15, k_mixing=30,\n",
    "    recall_ks=(1,10), max_pair_n=3000, max_sil_n=3000,\n",
    "    seed=0, block=512,\n",
    "    pair_eval_split=\"all\",          # NEW: \"all\" or \"test\"\n",
    "    mixing_eval_split=\"all\",        # NEW: \"all\" or \"test\" (optional)\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes full metric suite using:\n",
    "      - Z_rna + Z_atac (or Z_adt) if present (paired, aligned)\n",
    "      - Z_fused if present (aligned)\n",
    "\n",
    "    pair_eval_split:\n",
    "      - \"all\": compute paired retrieval/label-transfer/mixing on all cells (old behavior)\n",
    "      - \"test\": compute those only on splits[\"test\"] (strict CV)\n",
    "\n",
    "    mixing_eval_split:\n",
    "      - defaults to \"all\" to preserve old behavior\n",
    "      - set to \"test\" if you want mixing/clustering on test only as well\n",
    "    \"\"\"\n",
    "    labels_all = np.asarray(labels_all).astype(str)\n",
    "    n = len(labels_all)\n",
    "    M = {}\n",
    "\n",
    "    if not isinstance(out, dict):\n",
    "        return M\n",
    "\n",
    "    # unify naming\n",
    "    Z_rna = out.get(\"Z_rna\", None)\n",
    "    Z_mod = out.get(\"Z_atac\", None)\n",
    "    if Z_mod is None:\n",
    "        Z_mod = out.get(\"Z_adt\", None)\n",
    "    Z_fused = out.get(\"Z_fused\", None)\n",
    "\n",
    "    # ---------- choose index subset for paired metrics ----------\n",
    "    if pair_eval_split not in (\"all\", \"test\"):\n",
    "        raise ValueError(\"pair_eval_split must be 'all' or 'test'\")\n",
    "    if mixing_eval_split not in (\"all\", \"test\"):\n",
    "        raise ValueError(\"mixing_eval_split must be 'all' or 'test'\")\n",
    "\n",
    "    idx_pair = np.arange(n, dtype=int) if pair_eval_split == \"all\" else np.asarray(splits[\"test\"], dtype=int)\n",
    "    idx_mix  = np.arange(n, dtype=int) if mixing_eval_split == \"all\" else np.asarray(splits[\"test\"], dtype=int)\n",
    "\n",
    "    # ---------- paired metrics require both modality embeddings ----------\n",
    "    if (Z_rna is not None) and (Z_mod is not None):\n",
    "        Z_rna = np.asarray(Z_rna, np.float32)\n",
    "        Z_mod = np.asarray(Z_mod, np.float32)\n",
    "\n",
    "        if Z_rna.shape[0] == n and Z_mod.shape[0] == n:\n",
    "            # subset first, then finite-mask\n",
    "            Z1 = Z_rna[idx_pair]\n",
    "            Z2 = Z_mod[idx_pair]\n",
    "            y  = labels_all[idx_pair]\n",
    "\n",
    "            m = np.isfinite(Z1).all(1) & np.isfinite(Z2).all(1)\n",
    "            Z1, Z2, y = Z1[m], Z2[m], y[m]\n",
    "\n",
    "            if Z1.shape[0] >= 10:\n",
    "                pairM = pair_ranking_metrics_exact(\n",
    "                    Z1, Z2,\n",
    "                    metric=metric, ks=recall_ks, max_n=max_pair_n,\n",
    "                    seed=seed, block=block\n",
    "                )\n",
    "                # prefix so it's obvious what split you used\n",
    "                pref = \"PAIR(all)/\" if pair_eval_split == \"all\" else \"PAIR(test)/\"\n",
    "                for k, v in pairM.items():\n",
    "                    M[pref + k] = v\n",
    "\n",
    "                # label transfer both directions (mean)\n",
    "                acc12, f112 = label_transfer_knn(Z1, y, Z2, y, k=k_label, metric=metric)\n",
    "                acc21, f121 = label_transfer_knn(Z2, y, Z1, y, k=k_label, metric=metric)\n",
    "                M[pref + \"Label transfer acc mean\"] = float(np.nanmean([acc12, acc21]))\n",
    "                M[pref + \"Label transfer macroF1 mean\"] = float(np.nanmean([f112, f121]))\n",
    "\n",
    "            # mixing/clustering on stacked embedding (optionally subset)\n",
    "            Z1m = Z_rna[idx_mix]\n",
    "            Z2m = Z_mod[idx_mix]\n",
    "            ym  = labels_all[idx_mix]\n",
    "            mm  = np.isfinite(Z1m).all(1) & np.isfinite(Z2m).all(1)\n",
    "            Z1m, Z2m, ym = Z1m[mm], Z2m[mm], ym[mm]\n",
    "\n",
    "            if Z1m.shape[0] >= (k_mixing + 5) and len(np.unique(ym)) >= 2:\n",
    "                Zstack = np.vstack([Z1m, Z2m])\n",
    "                ystack = np.concatenate([ym, ym])\n",
    "                domain = np.array([\"RNA\"] * len(ym) + [\"MOD\"] * len(ym), dtype=str)\n",
    "\n",
    "                mix, per_dom = mixing_score(Zstack, domain, k=k_mixing, metric=metric)\n",
    "                mpref = \"MIX(all)/\" if mixing_eval_split == \"all\" else \"MIX(test)/\"\n",
    "                M[mpref + \"Mixing score\"] = mix\n",
    "                M[mpref + \"Mixing score (RNA)\"] = per_dom.get(\"RNA\", np.nan)\n",
    "                M[mpref + \"Mixing score (MOD)\"] = per_dom.get(\"MOD\", np.nan)\n",
    "\n",
    "                # clustering-style metrics on stacked embedding\n",
    "                cm = clustering_metrics(Zstack, ystack, seed=seed, max_sil_n=max_sil_n)\n",
    "                for k, v in cm.items():\n",
    "                    M[mpref + k] = v\n",
    "\n",
    "        # else: shapes not aligned -> skip pair metrics\n",
    "\n",
    "    # ---------- fused metrics ----------\n",
    "    if Z_fused is not None:\n",
    "        Zf = np.asarray(Z_fused, np.float32)\n",
    "        if Zf.shape[0] == n:\n",
    "            # NOTE: this is already strict train->test by construction\n",
    "            M.update(fused_knn_metrics(Zf, labels_all, splits[\"train\"], splits[\"test\"], k=k_label, metric=metric))\n",
    "\n",
    "            # clustering metrics on fused embedding (ALL labeled by default)\n",
    "            #M.update({k.replace(\"Fused \", \"Fused(fusedZ) \"): v\n",
    "            #          for k, v in clustering_metrics(Zf, labels_all, seed=seed, max_sil_n=max_sil_n).items()})\n",
    "                       \n",
    "            # clustering metrics on fused embedding (TEST only)\n",
    "            te = np.asarray(splits[\"test\"], dtype=int)\n",
    "            Zf_te = Zf[te]\n",
    "            y_te  = labels_all[te]\n",
    "            M.update({k.replace(\"Fused \", \"Fused(fusedZ,test) \"): v\n",
    "                       for k, v in clustering_metrics(Zf_te, y_te, seed=seed, max_sil_n=max_sil_n).items()})\n",
    "\n",
    "    return M\n",
    "    \n",
    "\n",
    "# ============================================================\n",
    "# 5) main CV sweep (rewritten)\n",
    "# ============================================================\n",
    "def run_cv_sweep_fullmetrics(\n",
    "    *,\n",
    "    seeds,\n",
    "    n_folds,\n",
    "    out_dir,\n",
    "    methods,\n",
    "    labels_all,\n",
    "    eval_kws=None,\n",
    "):\n",
    "    eval_kws = dict(EVAL_KWS if eval_kws is None else eval_kws)\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rows = []\n",
    "    raw_out_path = out_dir / \"raw_outputs.jsonl\"\n",
    "\n",
    "    with raw_out_path.open(\"w\") as f_jsonl:\n",
    "        for seed in seeds:\n",
    "            set_all_seeds(int(seed))\n",
    "\n",
    "            for fold, splits in make_folds(labels_all, n_splits=n_folds, seed=seed):\n",
    "                globals()[\"RNG_SEED\"] = int(seed)\n",
    "                globals()[\"splits\"] = splits\n",
    "\n",
    "                rebuild_shared_inputs_for_split(seed, splits)\n",
    "\n",
    "                print(f\"\\n=== seed={seed} fold={fold} sizes ===\", {k: len(v) for k, v in splits.items()})\n",
    "\n",
    "                for method_name, thunk in methods.items():\n",
    "                    t0 = time.time()\n",
    "                    status, err, out = \"ok\", None, None\n",
    "\n",
    "                    try:\n",
    "                        out = thunk()\n",
    "                    except Exception as e:\n",
    "                        status = \"fail\"\n",
    "                        err = repr(e)\n",
    "\n",
    "                    dt = time.time() - t0\n",
    "\n",
    "                    # start with whatever the runner already reported\n",
    "                    metrics = _extract_metrics(out)\n",
    "\n",
    "                    # add full metric suite + sanity checks\n",
    "                    if status == \"ok\":\n",
    "                        try:\n",
    "                            eval_kws2 = dict(eval_kws)\n",
    "                            eval_kws2.pop(\"seed\", None)\n",
    "\n",
    "                            # ---- your existing suite (note: this currently uses ALL cells for pair metrics) ----\n",
    "                            #addM = evaluate_out_full(out, labels_all, splits, seed=seed, **eval_kws2)\n",
    "                            #addM = evaluate_out_full(out, labels_all, splits, seed=seed, pair_eval_split=\"test\", **eval_kws2)\n",
    "                            # can add: mixing_eval_split=\"test\", # <- optional; leave off if you want mixing on all\n",
    "                            addM = evaluate_out_full(\n",
    "                                out, labels_all, splits,\n",
    "                                seed=seed,\n",
    "                                pair_eval_split=\"test\",\n",
    "                                mixing_eval_split=\"test\",\n",
    "                                **eval_kws2\n",
    "                            )\n",
    "                            \n",
    "                            for k, v in addM.items():\n",
    "                                if k not in metrics:\n",
    "                                    metrics[k] = v\n",
    "\n",
    "                            # ---- sanity: if ids are available, check 1:1 pairing by name ----\n",
    "                            for k, v in _pair_id_alignment_check(out).items():\n",
    "                                if k not in metrics:\n",
    "                                    metrics[k] = v\n",
    "\n",
    "                            # ---- sanity: TEST-only pair metrics + permutation control ----\n",
    "                            # (this is the big one for “is FOSCTTM real?”)\n",
    "                            test_pairM = _pair_test_metrics(out, labels_all, splits, seed=seed, **eval_kws2)\n",
    "                            for k, v in test_pairM.items():\n",
    "                                if k not in metrics:\n",
    "                                    metrics[k] = v\n",
    "\n",
    "                        except Exception as e:\n",
    "                            metrics[\"__eval_error__\"] = repr(e)\n",
    "\n",
    "\n",
    "                    row = {\n",
    "                        \"seed\": int(seed),\n",
    "                        \"fold\": int(fold),\n",
    "                        \"method\": str(method_name),\n",
    "                        \"status\": status,\n",
    "                        \"fit_seconds\": float(dt),\n",
    "                        \"error\": err,\n",
    "                        \"n_train\": int(len(splits[\"train\"])),\n",
    "                        \"n_val\": int(len(splits[\"val\"])),\n",
    "                        \"n_test\": int(len(splits[\"test\"])),\n",
    "                        **metrics,\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "\n",
    "                    f_jsonl.write(json.dumps({\n",
    "                        \"seed\": int(seed),\n",
    "                        \"fold\": int(fold),\n",
    "                        \"method\": str(method_name),\n",
    "                        \"status\": status,\n",
    "                        \"fit_seconds\": float(dt),\n",
    "                        \"error\": err,\n",
    "                        \"metrics\": metrics,\n",
    "                        \"out_keys\": sorted(list(out.keys())) if isinstance(out, dict) else None,\n",
    "                    }) + \"\\n\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_dir / \"cv_sweep_long.csv\", index=False)\n",
    "\n",
    "    ok = df[df[\"status\"] == \"ok\"].copy()\n",
    "    house = {\"seed\",\"fold\",\"method\",\"status\",\"fit_seconds\",\"error\",\"n_train\",\"n_val\",\"n_test\"}\n",
    "    metric_cols = [c for c in ok.columns if c not in house and pd.api.types.is_numeric_dtype(ok[c])]\n",
    "\n",
    "    if metric_cols:\n",
    "        summ = (ok.groupby(\"method\")[metric_cols]\n",
    "                  .agg([\"mean\",\"std\",\"median\",\"count\"]))\n",
    "        # pick a stable sort if present\n",
    "        sort_key = (\"FOSCTTM_mean\", \"mean\") if (\"FOSCTTM_mean\" in ok.columns) else (metric_cols[0], \"mean\")\n",
    "        summ = summ.sort_values(sort_key, ascending=True if sort_key[0].startswith(\"FOSCTTM_mean\") else False)\n",
    "        summ.to_csv(out_dir / \"cv_sweep_summary.csv\")\n",
    "    else:\n",
    "        summ = ok.groupby(\"method\").size().to_frame(\"n_ok\")\n",
    "        summ.to_csv(out_dir / \"cv_sweep_summary.csv\")\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", out_dir / \"cv_sweep_long.csv\")\n",
    "    print(\" -\", out_dir / \"cv_sweep_summary.csv\")\n",
    "    print(\" -\", raw_out_path)\n",
    "\n",
    "    return df, summ\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run it:\n",
    "# df_long, df_summary = run_cv_sweep_fullmetrics(\n",
    "#     seeds=SEEDS,\n",
    "#     n_folds=5,\n",
    "#     out_dir=OUT_DIR,\n",
    "#     methods=methods,\n",
    "#     labels_all=labels_all,\n",
    "# )\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1661f-e984-4f27-9ac6-2e54fe90ebb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SEEDS = list(range(10))\n",
    "SEEDS = [67, 1985, 789, 3, 99]\n",
    "N_FOLDS = 3\n",
    "OUT_DIR = Path(WORK) / \"runs\" / \"cv_sweep_py_1-31-2026\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df, summ = run_cv_sweep_fullmetrics(\n",
    "    seeds=SEEDS,\n",
    "    n_folds=N_FOLDS,\n",
    "    out_dir=OUT_DIR,\n",
    "    methods=METHODS,\n",
    "    labels_all=labels_all,\n",
    ")\n",
    "\n",
    "ok = df[df[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "for m in [\"FOSCTTM_mean_test\", \"fused_kmeans_ari_test\", \"fused_kmeans_nmi_test\"]:\n",
    "    if m in ok.columns:\n",
    "        violin_metric(ok, m, savepath=OUT_DIR / f\"violin_{m}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654867f-7b0f-47e5-bce3-eb2e4a72313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# plotting\n",
    "# -----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def violin_metric(df_ok: pd.DataFrame, metric: str, *, title=None, savepath=None):\n",
    "    dd = df_ok[[\"method\", metric]].dropna().copy()\n",
    "    if dd.empty:\n",
    "        print(f\"[plot] no data for metric={metric}\")\n",
    "        return\n",
    "\n",
    "    order = dd.groupby(\"method\")[metric].mean().sort_values(ascending=False).index.tolist()\n",
    "    data = [dd.loc[dd[\"method\"] == m, metric].to_numpy() for m in order]\n",
    "\n",
    "    means = np.array([np.mean(x) for x in data], dtype=float)\n",
    "    sems  = np.array([np.std(x, ddof=1) / np.sqrt(len(x)) if len(x) > 1 else np.nan for x in data], dtype=float)\n",
    "    medians = np.array([np.median(x) for x in data], dtype=float)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(6, 0.85 * len(order)), 4.5))\n",
    "    ax.violinplot(data, positions=np.arange(1, len(order) + 1),\n",
    "                  showmeans=False, showextrema=False, showmedians=False)\n",
    "    xs = np.arange(1, len(order) + 1)\n",
    "    ax.errorbar(xs, means, yerr=sems, fmt=\"o\", capsize=4, linewidth=1.5)\n",
    "    ax.scatter(xs, medians, marker=\"_\", s=300)\n",
    "\n",
    "    ax.set_xticks(xs)\n",
    "    ax.set_xticklabels(order, rotation=30, ha=\"right\")\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(title or f\"{metric} across CV folds (mean±SEM, median tick)\")\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    if savepath is not None:\n",
    "        sp = Path(savepath)\n",
    "        sp.parent.mkdir(parents=True, exist_ok=True)  # make dirs if needed\n",
    "        fig.savefig(sp, dpi=200)                      # overwrites if exists\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e71d3-b14f-4642-bdfa-8095a05b2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "house = {\"seed\",'fold',\"method\",\"status\",\"error\",\"n_train\",\"n_val\",\"n_test\",\"n_unused\",'transductive','uses_labels','n_genes',\n",
    "        'n_peaks','latent_dim','dropout','lr','weight_decay','batch_size','max_epochs','patience','reg','best_val','T','lamb',\n",
    "        'nbatches','n_latent'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b4662-a9bd-4fcf-b625-136189707c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df.columns if \"FOSCTTM\" in c or \"Mixing\" in c or \"Label transfer\" in c or \"Fused\" in c]\n",
    "df[[\"seed\",\"fold\",\"method\",\"status\"] + cols].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f80b4-184e-4ada-8adc-630dd88f6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ok.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7cb50-25ca-41a5-80bd-7d05c289c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summ.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd9cdf-4fb8-445c-823c-296e9f24e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = [c for c in ok.columns if c not in house and pd.api.types.is_numeric_dtype(ok[c])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3104d78-8db4-4377-bd3c-6141e6667f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [\"FOSCTTM_mean_test\", \"fused_kmeans_ari_test\", \"fused_kmeans_nmi_test\"]:\n",
    "    if m in ok.columns:\n",
    "        violin_metric(ok, m, savepath=OUT_DIR / f\"violin_{m}.png\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84ad8a-ebaf-432a-a354-7545a2f70116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick metrics you care about (or use your auto-detected metric_cols)\n",
    "for metric in metric_cols:\n",
    "    violin_metric(ok, metric, savepath=OUT_DIR / f\"violin_{metric}.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33e5df-0d23-4809-be93-066ed4027b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a49443-873e-4d9f-b74e-e63b3f229479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd304e56-c3a3-4fbc-a537-a6e2f2896382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd6c90-eefb-4fc6-be49-1b26377d0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87c955-0e9d-4e32-aecf-ec9ea2f290f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eace4b-7caa-42b4-8c1d-e8b77fa25fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967d84e-6dd4-4256-8d28-fd6250e09646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c8772-b5f7-489b-b245-61361d1cf3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d04be-c3be-4052-8573-0e28f55f6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d792e20-5295-4012-a7c6-60636e82cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (univi-bench-py)",
   "language": "python",
   "name": "univi-bench-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
